import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as s,c as i,f as a,b as e,d as t,e as l,w as r,a as c,o as m}from"./app-KOUU_Wij.js";const u={},d=e("h1",{id:"espnet-nets-pytorch-backend-e2e-vc-tacotron2-tacotron2",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet-nets-pytorch-backend-e2e-vc-tacotron2-tacotron2"},[e("span",null,"espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2")])],-1),p=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2"),e("span",{class:"small-bracket"},"(idim, odim, args=None)")])],-1),h=e("code",null,"TTSInterface",-1),g=e("code",null,"Module",-1),f=c('<p>VC Tacotron2 module for VC.</p><p>This is a module of Tacotron2-based VC model, which convert the sequence of acoustic features into the sequence of acoustic features.</p><p>Initialize Tacotron2 module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>idim</strong> (<em>int</em>) – Dimension of the inputs.</li><li><strong>odim</strong> (<em>int</em>) – Dimension of the outputs.</li><li><strong>args</strong> (<em>Namespace</em> <em>,</em> <em>optional</em>) – <ul><li>spk_embed_dim (int): Dimension of the speaker embedding.</li><li>elayers (int): The number of encoder blstm layers.</li><li>eunits (int): The number of encoder blstm units.</li><li>econv_layers (int): The number of encoder conv layers.</li><li>econv_filts (int): The number of encoder conv filter size.</li><li>econv_chans (int): The number of encoder conv filter channels.</li><li>dlayers (int): The number of decoder lstm layers.</li><li>dunits (int): The number of decoder lstm units.</li><li>prenet_layers (int): The number of prenet layers.</li><li>prenet_units (int): The number of prenet units.</li><li>postnet_layers (int): The number of postnet layers.</li><li>postnet_filts (int): The number of postnet filter size.</li><li>postnet_chans (int): The number of postnet filter channels.</li><li>output_activation (int): The name of activation function for outputs.</li><li>adim (int): The number of dimension of mlp in attention.</li><li>aconv_chans (int): The number of attention conv filter channels.</li><li>aconv_filts (int): The number of attention conv filter size.</li><li>cumulate_att_w (bool): Whether to cumulate previous attention weight.</li><li>use_batch_norm (bool): Whether to use batch normalization.</li><li>use_concate (int): : Whether to concatenate encoder embedding with decoder lstm outputs.</li><li>dropout_rate (float): Dropout rate.</li><li>zoneout_rate (float): Zoneout rate.</li><li>reduction_factor (int): Reduction factor.</li><li>spk_embed_dim (int): Number of speaker embedding dimenstions.</li><li>spc_dim (int): Number of spectrogram embedding dimenstions : (only for use_cbhg=True).</li><li>use_cbhg (bool): Whether to use CBHG module.</li><li>cbhg_conv_bank_layers (int): : The number of convoluional banks in CBHG.</li><li>cbhg_conv_bank_chans (int): : The number of channels of convolutional bank in CBHG.</li><li>cbhg_proj_filts (int): : The number of filter size of projection layeri in CBHG.</li><li>cbhg_proj_chans (int): : The number of channels of projection layer in CBHG.</li><li>cbhg_highway_layers (int): : The number of layers of highway network in CBHG.</li><li>cbhg_highway_units (int): : The number of units of highway network in CBHG.</li><li>cbhg_gru_units (int): The number of units of GRU in CBHG.</li><li>use_masking (bool): Whether to mask padded part in loss calculation.</li><li>bce_pos_weight (float): Weight of positive sample of stop token : (only for use_masking=True).</li><li>use-guided-attn-loss (bool): Whether to use guided attention loss.</li><li>guided-attn-loss-sigma (float) Sigma in guided attention loss.</li><li>guided-attn-loss-lamdba (float): Lambda in guided attention loss.</li></ul></li></ul></li></ul><div class="custom-h4"><p><em>static</em> add_arguments<span class="small-bracket">(parser)</span></p></div><p>Add model-specific arguments to the parser.</p><div class="custom-h4"><p><em>property</em> base_plot_keys</p></div><p>Return base key names to plot during training.</p><p>keys should match what chainer.reporter reports. If you add the key loss, the reporter will report main/loss</p><blockquote><p>and validation/main/loss values.</p></blockquote><p>also loss.png will be created as a figure visulizing main/loss : and validation/main/loss values.</p><ul><li><strong>Returns:</strong> List of strings which are base keys to plot during training.</li><li><strong>Return type:</strong> list</li></ul><div class="custom-h4"><p>calculate_all_attentions<span class="small-bracket">(xs, ilens, ys, spembs=None, *args, **kwargs)</span></p></div><p>Calculate all of the attention weights.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs</strong> (<em>Tensor</em>) – Batch of padded acoustic features (B, Tmax, idim).</li><li><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</li><li><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</li><li><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</li><li><strong>spembs</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of speaker embedding vectors (B, spk_embed_dim).</li></ul></li><li><strong>Returns:</strong> Batch of attention weights (B, Lmax, Tmax).</li><li><strong>Return type:</strong> numpy.ndarray</li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(xs, ilens, ys, labels, olens, spembs=None, spcs=None, *args, **kwargs)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs</strong> (<em>Tensor</em>) – Batch of padded acoustic features (B, Tmax, idim).</li><li><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</li><li><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</li><li><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</li><li><strong>spembs</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of speaker embedding vectors (B, spk_embed_dim).</li><li><strong>spcs</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of groundtruth spectrograms (B, Lmax, spc_dim).</li></ul></li><li><strong>Returns:</strong> Loss value.</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>inference<span class="small-bracket">(x, inference_args, spemb=None, *args, **kwargs)</span></p></div><p>Generate the sequence of features given the sequences of characters.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> (<em>Tensor</em>) – Input sequence of acoustic features (T, idim).</li><li><strong>inference_args</strong> (<em>Namespace</em>) – <ul><li>threshold (float): Threshold in inference.</li><li>minlenratio (float): Minimum length ratio in inference.</li><li>maxlenratio (float): Maximum length ratio in inference.</li></ul></li><li><strong>spemb</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Speaker embedding vector (spk_embed_dim).</li></ul></li><li><strong>Returns:</strong> Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T).</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',22);function _(b,T){const n=s("RouteLink");return m(),i("div",null,[a(" _espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2 "),d,p,e("p",null,[t("Bases: "),l(n,{to:"/guide/espnet/nets/TTSInterface.html#espnet.nets.tts_interface.TTSInterface"},{default:r(()=>[h]),_:1}),t(", "),g]),f])}const y=o(u,[["render",_],["__file","Tacotron2.html.vue"]]),B=JSON.parse('{"path":"/guide/espnet/nets/Tacotron2.html","title":"espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.65,"words":796},"filePathRelative":"guide/espnet/nets/Tacotron2.md","excerpt":"<!-- _espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2 -->\\n<h1>espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2<span class=\\"small-bracket\\">(idim, odim, args=None)</span></p></div>\\n<p>Bases: <a href=\\"/guide/espnet/nets/TTSInterface.html#espnet.nets.tts_interface.TTSInterface\\" target=\\"_blank\\"><code>TTSInterface</code></a>, <code>Module</code></p>"}');export{y as comp,B as data};

import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as s,c as r,f as i,b as e,d as t,e as a,w as p,a as l,o as m}from"./app-KOUU_Wij.js";const c={},d=e("h1",{id:"espnet2-enh-separator-uses-separator-usesseparator",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-enh-separator-uses-separator-usesseparator"},[e("span",null,"espnet2.enh.separator.uses_separator.USESSeparator")])],-1),h=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.enh.separator.uses_separator.USESSeparator"),e("span",{class:"small-bracket"},"(input_dim: int, num_spk: int = 2, enc_channels: int = 256, bottleneck_size: int = 64, num_blocks: int = 6, num_spatial_blocks: int = 3, ref_channel: int | None = None, segment_size: int = 64, memory_size: int = 20, memory_types: int = 1, rnn_type: str = 'lstm', bidirectional: bool = True, hidden_size: int = 128, att_heads: int = 4, dropout: float = 0.0, norm_type: str = 'cLN', activation: str = 'relu', ch_mode: str | List[str] = 'att', ch_att_dim: int = 256, eps: float = 1e-05, additional: dict = {})")])],-1),u=e("code",null,"AbsSeparator",-1),g=l('<p>Unconstrained Speech Enhancement and Separation (USES) Network.</p><p>Reference: : [1] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.</p><ul><li><strong>Parameters:</strong><ul><li><p><strong>input_dim</strong> (<em>int</em>) – input feature dimension. Not used as the model is independent of the input size.</p></li><li><p><strong>num_spk</strong> (<em>int</em>) – number of speakers.</p></li><li><p><strong>enc_channels</strong> (<em>int</em>) – feature dimension after the Conv1D encoder.</p></li><li><p><strong>bottleneck_size</strong> (<em>int</em>) – dimension of the bottleneck feature. Must be a multiple of att_heads.</p></li><li><p><strong>num_blocks</strong> (<em>int</em>) – number of processing blocks.</p></li><li><p><strong>num_spatial_blocks</strong> (<em>int</em>) – number of processing blocks with channel modeling.</p></li><li><p><strong>ref_channel</strong> (<em>int</em>) – reference channel (used in channel modeling modules).</p></li><li><p><strong>segment_size</strong> (<em>int</em>) – number of frames in each non-overlapping segment. This is used to segment long utterances into smaller chunks for efficient processing.</p></li><li><p><strong>memory_size</strong> (<em>int</em>) – group size of global memory tokens. The basic use of memory tokens is to store the history information from previous segments. The memory tokens are updated by the output of the last block after processing each segment.</p></li><li><p><strong>memory_types</strong> (<em>int</em>) –</p><p>numbre of memory token groups. Each group corresponds to a different type of processing, i.e.,</p><blockquote><p>the first group is used for denoising without dereverberation, the second group is used for denoising with dereverberation,</p></blockquote></li><li><p><strong>rnn_type</strong> – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.</p></li><li><p><strong>bidirectional</strong> (<em>bool</em>) – whether the inter-chunk RNN layers are bidirectional.</p></li><li><p><strong>hidden_size</strong> (<em>int</em>) – dimension of the hidden state.</p></li><li><p><strong>att_heads</strong> (<em>int</em>) – number of attention heads.</p></li><li><p><strong>dropout</strong> (<em>float</em>) – dropout ratio. Default is 0.</p></li><li><p><strong>norm_type</strong> – type of normalization to use after each inter- or intra-chunk NN block.</p></li><li><p><strong>activation</strong> – the nonlinear activation function.</p></li><li><p><strong>ch_mode</strong> – str or list, mode of channel modeling. Select from “att” and “tac”.</p></li><li><p><strong>ch_att_dim</strong> (<em>int</em>) – dimension of the channel attention.</p></li><li><p><strong>ref_channel</strong> – Optional[int], index of the reference channel.</p></li><li><p><strong>eps</strong> (<em>float</em>) – epsilon for layer normalization.</p></li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)</span></p></div><p>Forward.</p><ul><li><p><strong>Parameters:</strong></p><ul><li><p><strong>input</strong> (<em>torch.Tensor</em> <em>or</em> <em>ComplexTensor</em>) – STFT spectrum [B, T, (C,) F (,2)] B is the batch size T is the number of time frames C is the number of microphone channels (optional) F is the number of frequency bins 2 is real and imaginary parts (optional if input is a complex tensor)</p></li><li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li><li><p><strong>additional</strong> (<em>Dict</em> <em>or</em> <em>None</em>) –</p><p>other data included in model “mode”: one of (“no_dereverb”, “dereverb”, “both”)</p><ol><li>“no_dereverb”: only use the first memory group for denoising</li></ol><blockquote><p>without dereverberation</p></blockquote><ol><li>”dereverb”: only use the second memory group for denoising : with dereverberation</li><li>”both”: use both memory groups for denoising with and without : dereverberation</li></ol></li></ul></li><li><p><strong>Returns:</strong> [(B, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[</p><blockquote><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p></blockquote><p>]</p></li><li><p><strong>Return type:</strong> masked (List[Union(torch.Tensor, ComplexTensor)])</p></li></ul><div class="custom-h4"><p><em>property</em> num_spk</p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',8);function _(f,b){const n=s("RouteLink");return m(),r("div",null,[i(" _espnet2.enh.separator.uses_separator.USESSeparator "),d,h,e("p",null,[t("Bases: "),a(n,{to:"/guide/espnet2/enh/AbsSeparator.html#espnet2.enh.separator.abs_separator.AbsSeparator"},{default:p(()=>[u]),_:1})]),g])}const y=o(c,[["render",_],["__file","USESSeparator.html.vue"]]),v=JSON.parse(`{"path":"/guide/espnet2/enh/USESSeparator.html","title":"espnet2.enh.separator.uses_separator.USESSeparator","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.89,"words":566},"filePathRelative":"guide/espnet2/enh/USESSeparator.md","excerpt":"<!-- _espnet2.enh.separator.uses_separator.USESSeparator -->\\n<h1>espnet2.enh.separator.uses_separator.USESSeparator</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.separator.uses_separator.USESSeparator<span class=\\"small-bracket\\">(input_dim: int, num_spk: int = 2, enc_channels: int = 256, bottleneck_size: int = 64, num_blocks: int = 6, num_spatial_blocks: int = 3, ref_channel: int | None = None, segment_size: int = 64, memory_size: int = 20, memory_types: int = 1, rnn_type: str = 'lstm', bidirectional: bool = True, hidden_size: int = 128, att_heads: int = 4, dropout: float = 0.0, norm_type: str = 'cLN', activation: str = 'relu', ch_mode: str | List[str] = 'att', ch_att_dim: int = 256, eps: float = 1e-05, additional: dict = {})</span></p></div>"}`);export{y as comp,v as data};

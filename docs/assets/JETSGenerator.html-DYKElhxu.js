import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,f as r,a as n,o}from"./app-KOUU_Wij.js";const s={},i=n('<h1 id="espnet2-gan-tts-jets-generator-jetsgenerator" tabindex="-1"><a class="header-anchor" href="#espnet2-gan-tts-jets-generator-jetsgenerator"><span>espnet2.gan_tts.jets.generator.JETSGenerator</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.gan_tts.jets.generator.JETSGenerator<span class="small-bracket">(idim: int, odim: int, adim: int = 256, aheads: int = 2, elayers: int = 4, eunits: int = 1024, dlayers: int = 4, dunits: int = 1024, positionwise_layer_type: str = &#39;conv1d&#39;, positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = &#39;transformer&#39;, decoder_type: str = &#39;transformer&#39;, transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = &#39;legacy&#39;, conformer_pos_enc_layer_type: str = &#39;rel_pos&#39;, conformer_self_attn_layer_type: str = &#39;rel_selfattn&#39;, conformer_activation_type: str = &#39;swish&#39;, use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = &#39;add&#39;, use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128)</span>, gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = &#39;xavier_uniform&#39;, init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, segment_size: int = 64, generator_out_channels: int = 1, generator_channels: int = 512, generator_global_channels: int = -1, generator_kernel_size: int = 7, generator_upsample_scales: List[int] = [8, 8, 2, 2], generator_upsample_kernel_sizes: List[int] = [16, 16, 4, 4], generator_resblock_kernel_sizes: List[int] = [3, 7, 11], generator_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], generator_use_additional_convs: bool = True, generator_bias: bool = True, generator_nonlinear_activation: str = &#39;LeakyReLU&#39;, generator_nonlinear_activation_params: Dict[str, Any] = {&#39;negative_slope&#39;: 0.1}, generator_use_weight_norm: bool = True)</p></div><p>Bases: <code>Module</code></p><p>Generator module in JETS.</p><p>Initialize JETS generator module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>idim</strong> (<em>int</em>) – Dimension of the inputs.</li><li><strong>odim</strong> (<em>int</em>) – Dimension of the outputs.</li><li><strong>elayers</strong> (<em>int</em>) – Number of encoder layers.</li><li><strong>eunits</strong> (<em>int</em>) – Number of encoder hidden units.</li><li><strong>dlayers</strong> (<em>int</em>) – Number of decoder layers.</li><li><strong>dunits</strong> (<em>int</em>) – Number of decoder hidden units.</li><li><strong>use_scaled_pos_enc</strong> (<em>bool</em>) – Whether to use trainable scaled pos encoding.</li><li><strong>use_batch_norm</strong> (<em>bool</em>) – Whether to use batch normalization in encoder prenet.</li><li><strong>encoder_normalize_before</strong> (<em>bool</em>) – Whether to apply layernorm layer before encoder block.</li><li><strong>decoder_normalize_before</strong> (<em>bool</em>) – Whether to apply layernorm layer before decoder block.</li><li><strong>encoder_concat_after</strong> (<em>bool</em>) – Whether to concatenate attention layer’s input and output in encoder.</li><li><strong>decoder_concat_after</strong> (<em>bool</em>) – Whether to concatenate attention layer’s input and output in decoder.</li><li><strong>reduction_factor</strong> (<em>int</em>) – Reduction factor.</li><li><strong>encoder_type</strong> (<em>str</em>) – Encoder type (“transformer” or “conformer”).</li><li><strong>decoder_type</strong> (<em>str</em>) – Decoder type (“transformer” or “conformer”).</li><li><strong>transformer_enc_dropout_rate</strong> (<em>float</em>) – Dropout rate in encoder except attention and positional encoding.</li><li><strong>transformer_enc_positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after encoder positional encoding.</li><li><strong>transformer_enc_attn_dropout_rate</strong> (<em>float</em>) – Dropout rate in encoder self-attention module.</li><li><strong>transformer_dec_dropout_rate</strong> (<em>float</em>) – Dropout rate in decoder except attention &amp; positional encoding.</li><li><strong>transformer_dec_positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after decoder positional encoding.</li><li><strong>transformer_dec_attn_dropout_rate</strong> (<em>float</em>) – Dropout rate in decoder self-attention module.</li><li><strong>conformer_rel_pos_type</strong> (<em>str</em>) – Relative pos encoding type in conformer.</li><li><strong>conformer_pos_enc_layer_type</strong> (<em>str</em>) – Pos encoding layer type in conformer.</li><li><strong>conformer_self_attn_layer_type</strong> (<em>str</em>) – Self-attention layer type in conformer</li><li><strong>conformer_activation_type</strong> (<em>str</em>) – Activation function type in conformer.</li><li><strong>use_macaron_style_in_conformer</strong> – Whether to use macaron style FFN.</li><li><strong>use_cnn_in_conformer</strong> – Whether to use CNN in conformer.</li><li><strong>zero_triu</strong> – Whether to use zero triu in relative self-attention module.</li><li><strong>conformer_enc_kernel_size</strong> – Kernel size of encoder conformer.</li><li><strong>conformer_dec_kernel_size</strong> – Kernel size of decoder conformer.</li><li><strong>duration_predictor_layers</strong> (<em>int</em>) – Number of duration predictor layers.</li><li><strong>duration_predictor_chans</strong> (<em>int</em>) – Number of duration predictor channels.</li><li><strong>duration_predictor_kernel_size</strong> (<em>int</em>) – Kernel size of duration predictor.</li><li><strong>duration_predictor_dropout_rate</strong> (<em>float</em>) – Dropout rate in duration predictor.</li><li><strong>pitch_predictor_layers</strong> (<em>int</em>) – Number of pitch predictor layers.</li><li><strong>pitch_predictor_chans</strong> (<em>int</em>) – Number of pitch predictor channels.</li><li><strong>pitch_predictor_kernel_size</strong> (<em>int</em>) – Kernel size of pitch predictor.</li><li><strong>pitch_predictor_dropout_rate</strong> (<em>float</em>) – Dropout rate in pitch predictor.</li><li><strong>pitch_embed_kernel_size</strong> (<em>float</em>) – Kernel size of pitch embedding.</li><li><strong>pitch_embed_dropout_rate</strong> (<em>float</em>) – Dropout rate for pitch embedding.</li><li><strong>stop_gradient_from_pitch_predictor</strong> – Whether to stop gradient from pitch predictor to encoder.</li><li><strong>energy_predictor_layers</strong> (<em>int</em>) – Number of energy predictor layers.</li><li><strong>energy_predictor_chans</strong> (<em>int</em>) – Number of energy predictor channels.</li><li><strong>energy_predictor_kernel_size</strong> (<em>int</em>) – Kernel size of energy predictor.</li><li><strong>energy_predictor_dropout_rate</strong> (<em>float</em>) – Dropout rate in energy predictor.</li><li><strong>energy_embed_kernel_size</strong> (<em>float</em>) – Kernel size of energy embedding.</li><li><strong>energy_embed_dropout_rate</strong> (<em>float</em>) – Dropout rate for energy embedding.</li><li><strong>stop_gradient_from_energy_predictor</strong> – Whether to stop gradient from energy predictor to encoder.</li><li><strong>spks</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Number of speakers. If set to &gt; 1, assume that the sids will be provided as the input and use sid embedding layer.</li><li><strong>langs</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Number of languages. If set to &gt; 1, assume that the lids will be provided as the input and use sid embedding layer.</li><li><strong>spk_embed_dim</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Speaker embedding dimension. If set to &gt; 0, assume that spembs will be provided as the input.</li><li><strong>spk_embed_integration_type</strong> – How to integrate speaker embedding.</li><li><strong>use_gst</strong> (<em>str</em>) – Whether to use global style token.</li><li><strong>gst_tokens</strong> (<em>int</em>) – The number of GST embeddings.</li><li><strong>gst_heads</strong> (<em>int</em>) – The number of heads in GST multihead attention.</li><li><strong>gst_conv_layers</strong> (<em>int</em>) – The number of conv layers in GST.</li><li><strong>gst_conv_chans_list</strong> – (Sequence[int]): List of the number of channels of conv layers in GST.</li><li><strong>gst_conv_kernel_size</strong> (<em>int</em>) – Kernel size of conv layers in GST.</li><li><strong>gst_conv_stride</strong> (<em>int</em>) – Stride size of conv layers in GST.</li><li><strong>gst_gru_layers</strong> (<em>int</em>) – The number of GRU layers in GST.</li><li><strong>gst_gru_units</strong> (<em>int</em>) – The number of GRU units in GST.</li><li><strong>init_type</strong> (<em>str</em>) – How to initialize transformer parameters.</li><li><strong>init_enc_alpha</strong> (<em>float</em>) – Initial value of alpha in scaled pos encoding of the encoder.</li><li><strong>init_dec_alpha</strong> (<em>float</em>) – Initial value of alpha in scaled pos encoding of the decoder.</li><li><strong>use_masking</strong> (<em>bool</em>) – Whether to apply masking for padded part in loss calculation.</li><li><strong>use_weighted_masking</strong> (<em>bool</em>) – Whether to apply weighted masking in loss calculation.</li><li><strong>segment_size</strong> (<em>int</em>) – Segment size for random windowed discriminator</li><li><strong>generator_out_channels</strong> (<em>int</em>) – Number of output channels.</li><li><strong>generator_channels</strong> (<em>int</em>) – Number of hidden representation channels.</li><li><strong>generator_global_channels</strong> (<em>int</em>) – Number of global conditioning channels.</li><li><strong>generator_kernel_size</strong> (<em>int</em>) – Kernel size of initial and final conv layer.</li><li><strong>generator_upsample_scales</strong> (<em>List</em> *[*<em>int</em> <em>]</em>) – List of upsampling scales.</li><li><strong>generator_upsample_kernel_sizes</strong> (<em>List</em> *[*<em>int</em> <em>]</em>) – List of kernel sizes for upsample layers.</li><li><strong>generator_resblock_kernel_sizes</strong> (<em>List</em> *[*<em>int</em> <em>]</em>) – List of kernel sizes for residual blocks.</li><li><strong>generator_resblock_dilations</strong> (<em>List</em> *[*<em>List</em> *[*<em>int</em> <em>]</em> <em>]</em>) – List of list of dilations for residual blocks.</li><li><strong>generator_use_additional_convs</strong> (<em>bool</em>) – Whether to use additional conv layers in residual blocks.</li><li><strong>generator_bias</strong> (<em>bool</em>) – Whether to add bias parameter in convolution layers.</li><li><strong>generator_nonlinear_activation</strong> (<em>str</em>) – Activation function module name.</li><li><strong>generator_nonlinear_activation_params</strong> (<em>Dict</em> *[*<em>str</em> <em>,</em> <em>Any</em> <em>]</em>) – Hyperparameters for activation function.</li><li><strong>generator_use_weight_norm</strong> (<em>bool</em>) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.</li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>text</strong> (<em>Tensor</em>) – Text index tensor (B, T_text).</li><li><strong>text_lengths</strong> (<em>Tensor</em>) – Text length tensor (B,).</li><li><strong>feats</strong> (<em>Tensor</em>) – Feature tensor (B, T_feats, aux_channels).</li><li><strong>feats_lengths</strong> (<em>Tensor</em>) – Feature length tensor (B,).</li><li><strong>pitch</strong> (<em>Tensor</em>) – Batch of padded token-averaged pitch (B, T_text, 1).</li><li><strong>pitch_lengths</strong> (<em>LongTensor</em>) – Batch of pitch lengths (B, T_text).</li><li><strong>energy</strong> (<em>Tensor</em>) – Batch of padded token-averaged energy (B, T_text, 1).</li><li><strong>energy_lengths</strong> (<em>LongTensor</em>) – Batch of energy lengths (B, T_text).</li><li><strong>sids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Speaker index tensor (B,) or (B, 1).</li><li><strong>spembs</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Speaker embedding tensor (B, spk_embed_dim).</li><li><strong>lids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Language index tensor (B,) or (B, 1).</li></ul></li><li><strong>Returns:</strong> Waveform tensor (B, 1, segment_size * upsample_factor). Tensor: Binarization loss (). Tensor: Log probability attention matrix (B, T_feats, T_text). Tensor: Segments start index tensor (B,). Tensor: predicted duration (B, T_text). Tensor: ground-truth duration obtained from an alignment module (B, T_text). Tensor: predicted pitch (B, T_text,1). Tensor: ground-truth averaged pitch (B, T_text, 1). Tensor: predicted energy (B, T_text, 1). Tensor: ground-truth averaged energy (B, T_text, 1).</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>inference<span class="small-bracket">(text: Tensor, text_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, use_teacher_forcing: bool = False)</span></p></div><p>Run inference.</p><ul><li><strong>Parameters:</strong><ul><li><strong>text</strong> (<em>Tensor</em>) – Input text index tensor (B, T_text,).</li><li><strong>text_lengths</strong> (<em>Tensor</em>) – Text length tensor (B,).</li><li><strong>feats</strong> (<em>Tensor</em>) – Feature tensor (B, T_feats, aux_channels).</li><li><strong>feats_lengths</strong> (<em>Tensor</em>) – Feature length tensor (B,).</li><li><strong>pitch</strong> (<em>Tensor</em>) – Pitch tensor (B, T_feats, 1)</li><li><strong>energy</strong> (<em>Tensor</em>) – Energy tensor (B, T_feats, 1)</li><li><strong>sids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Speaker index tensor (B,) or (B, 1).</li><li><strong>spembs</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Speaker embedding tensor (B, spk_embed_dim).</li><li><strong>lids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Language index tensor (B,) or (B, 1).</li><li><strong>use_teacher_forcing</strong> (<em>bool</em>) – Whether to use teacher forcing.</li></ul></li><li><strong>Returns:</strong> Generated waveform tensor (B, T_wav). Tensor: Duration tensor (B, T_text).</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',13);function a(l,_){return o(),t("div",null,[r(" _espnet2.gan_tts.jets.generator.JETSGenerator "),i])}const c=e(s,[["render",a],["__file","JETSGenerator.html.vue"]]),d=JSON.parse(`{"path":"/guide/espnet2/gan_tts/JETSGenerator.html","title":"espnet2.gan_tts.jets.generator.JETSGenerator","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":4.76,"words":1427},"filePathRelative":"guide/espnet2/gan_tts/JETSGenerator.md","excerpt":"<!-- _espnet2.gan_tts.jets.generator.JETSGenerator -->\\n<h1>espnet2.gan_tts.jets.generator.JETSGenerator</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.gan_tts.jets.generator.JETSGenerator<span class=\\"small-bracket\\">(idim: int, odim: int, adim: int = 256, aheads: int = 2, elayers: int = 4, eunits: int = 1024, dlayers: int = 4, dunits: int = 1024, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128)</span>, gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, segment_size: int = 64, generator_out_channels: int = 1, generator_channels: int = 512, generator_global_channels: int = -1, generator_kernel_size: int = 7, generator_upsample_scales: List[int] = [8, 8, 2, 2], generator_upsample_kernel_sizes: List[int] = [16, 16, 4, 4], generator_resblock_kernel_sizes: List[int] = [3, 7, 11], generator_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], generator_use_additional_convs: bool = True, generator_bias: bool = True, generator_nonlinear_activation: str = 'LeakyReLU', generator_nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, generator_use_weight_norm: bool = True)</p></div>"}`);export{c as comp,d as data};

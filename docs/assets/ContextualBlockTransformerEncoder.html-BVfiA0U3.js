import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,c as s,f as i,b as e,d as o,e as a,w as l,a as c,o as d}from"./app-KOUU_Wij.js";const p={},_=e("h1",{id:"espnet2-asr-encoder-contextual-block-transformer-encoder-contextualblocktransformerencoder",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-asr-encoder-contextual-block-transformer-encoder-contextualblocktransformerencoder"},[e("span",null,"espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder")])],-1),u=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),o(" espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder"),e("span",{class:"small-bracket"},"(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, padding_idx: int = -1, block_size: int = 40, hop_size: int = 16, look_ahead: int = 16, init_average: bool = True, ctx_pos_enc: bool = True)")])],-1),g=e("code",null,"AbsEncoder",-1),m=c('<p>Contextual Block Transformer encoder module.</p><p>Details in Tsunoo et al. “Transformer ASR with contextual block processing” (<a href="https://arxiv.org/abs/1910.07204" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1910.07204</a>)</p><ul><li><strong>Parameters:</strong><ul><li><strong>input_size</strong> – input dim</li><li><strong>output_size</strong> – dimension of attention</li><li><strong>attention_heads</strong> – the number of heads of multi head attention</li><li><strong>linear_units</strong> – the number of units of position-wise feed forward</li><li><strong>num_blocks</strong> – the number of encoder blocks</li><li><strong>dropout_rate</strong> – dropout rate</li><li><strong>attention_dropout_rate</strong> – dropout rate in attention</li><li><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</li><li><strong>input_layer</strong> – input layer type</li><li><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</li><li><strong>normalize_before</strong> – whether to use layer_norm before the first block</li><li><strong>concat_after</strong> – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -&gt; x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -&gt; x + att(x)</li><li><strong>positionwise_layer_type</strong> – linear of conv1d</li><li><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</li><li><strong>padding_idx</strong> – padding_idx for input_layer=embed</li><li><strong>block_size</strong> – block size for contextual block processing</li><li><strong>hop_Size</strong> – hop size for block processing</li><li><strong>look_ahead</strong> – look-ahead size for block_processing</li><li><strong>init_average</strong> – whether to use average as initial context (otherwise max values)</li><li><strong>ctx_pos_enc</strong> – whether to use positional encoding to the context vectors</li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final=True, infer_mode=False)</span></p></div><p>Embed positions in tensor.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs_pad</strong> – input tensor (B, L, D)</li><li><strong>ilens</strong> – input length (B)</li><li><strong>prev_states</strong> – Not to be used now.</li><li><strong>infer_mode</strong> – whether to be used for inference. This is used to distinguish between forward_train (train and validate) and forward_infer (decode).</li></ul></li><li><strong>Returns:</strong> position embedded tensor and mask</li></ul><div class="custom-h4"><p>forward_infer<span class="small-bracket">(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final: bool = True)</span></p></div><p>Embed positions in tensor.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs_pad</strong> – input tensor (B, L, D)</li><li><strong>ilens</strong> – input length (B)</li><li><strong>prev_states</strong> – Not to be used now.</li></ul></li><li><strong>Returns:</strong> position embedded tensor and mask</li></ul><div class="custom-h4"><p>forward_train<span class="small-bracket">(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)</span></p></div><p>Embed positions in tensor.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs_pad</strong> – input tensor (B, L, D)</li><li><strong>ilens</strong> – input length (B)</li><li><strong>prev_states</strong> – Not to be used now.</li></ul></li><li><strong>Returns:</strong> position embedded tensor and mask</li></ul><div class="custom-h4"><p>output_size()</p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',15);function f(b,h){const t=r("RouteLink");return d(),s("div",null,[i(" _espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder "),_,u,e("p",null,[o("Bases: "),a(t,{to:"/guide/espnet2/asr/AbsEncoder.html#espnet2.asr.encoder.abs_encoder.AbsEncoder"},{default:l(()=>[g]),_:1})]),m])}const v=n(p,[["render",f],["__file","ContextualBlockTransformerEncoder.html.vue"]]),T=JSON.parse(`{"path":"/guide/espnet2/asr/ContextualBlockTransformerEncoder.html","title":"espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.55,"words":464},"filePathRelative":"guide/espnet2/asr/ContextualBlockTransformerEncoder.md","excerpt":"<!-- _espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder -->\\n<h1>espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder<span class=\\"small-bracket\\">(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'&gt;, normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, padding_idx: int = -1, block_size: int = 40, hop_size: int = 16, look_ahead: int = 16, init_average: bool = True, ctx_pos_enc: bool = True)</span></p></div>"}`);export{v as comp,T as data};

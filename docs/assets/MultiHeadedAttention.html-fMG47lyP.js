import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,f as n,a as r,o}from"./app-KOUU_Wij.js";const s={},a=r('<h1 id="espnet-nets-pytorch-backend-transformer-attention-multiheadedattention" tabindex="-1"><a class="header-anchor" href="#espnet-nets-pytorch-backend-transformer-attention-multiheadedattention"><span>espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention</span></a></h1><div class="custom-h3"><p><em>class</em> espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention<span class="small-bracket">(n_head, n_feat, dropout_rate)</span></p></div><p>Bases: <code>Module</code></p><p>Multi-Head Attention layer.</p><ul><li><strong>Parameters:</strong><ul><li><strong>n_head</strong> (<em>int</em>) – The number of heads.</li><li><strong>n_feat</strong> (<em>int</em>) – The number of features.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li></ul></li></ul><p>Construct an MultiHeadedAttention object.</p><div class="custom-h4"><p>forward<span class="small-bracket">(query, key, value, mask, expand_kv=False)</span></p></div><p>Compute scaled dot product attention.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor (#batch, time1, size).</li><li><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor (#batch, time2, size).</li><li><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor (#batch, time2, size).</li><li><strong>mask</strong> (<em>torch.Tensor</em>) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).</li><li><strong>expand_kv</strong> (<em>bool</em>) – Used only for partially autoregressive (PAR) decoding.</li></ul></li></ul><p>When set to True, Linear layers are computed only for the first batch. This is useful to reduce the memory usage during decoding when the batch size is #beam_size x #mask_count, which can be very large. Typically, in single waveform inference of PAR, Linear layers should not be computed for all batches for source-attention.</p><ul><li><strong>Returns:</strong> Output tensor (#batch, time1, d_model).</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>forward_attention<span class="small-bracket">(value, scores, mask)</span></p></div><p>Compute attention context vector.</p><ul><li><strong>Parameters:</strong><ul><li><strong>value</strong> (<em>torch.Tensor</em>) – Transformed value (#batch, n_head, time2, d_k).</li><li><strong>scores</strong> (<em>torch.Tensor</em>) – Attention score (#batch, n_head, time1, time2).</li><li><strong>mask</strong> (<em>torch.Tensor</em>) – Mask (#batch, 1, time2) or (#batch, time1, time2).</li></ul></li><li><strong>Returns:</strong> Transformed value (#batch, time1, d_model) : weighted by the attention score (#batch, time1, time2).</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>forward_qkv<span class="small-bracket">(query, key, value, expand_kv=False)</span></p></div><p>Transform query, key and value.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor (#batch, time1, size).</li><li><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor (#batch, time2, size).</li><li><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor (#batch, time2, size).</li><li><strong>expand_kv</strong> (<em>bool</em>) – Used only for partially autoregressive (PAR) decoding.</li></ul></li><li><strong>Returns:</strong> Transformed query tensor (#batch, n_head, time1, d_k). torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k). torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',18);function i(l,c){return o(),t("div",null,[n(" _espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention "),a])}const u=e(s,[["render",i],["__file","MultiHeadedAttention.html.vue"]]),h=JSON.parse('{"path":"/guide/espnet/nets/MultiHeadedAttention.html","title":"espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.16,"words":349},"filePathRelative":"guide/espnet/nets/MultiHeadedAttention.md","excerpt":"<!-- _espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention -->\\n<h1>espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention<span class=\\"small-bracket\\">(n_head, n_feat, dropout_rate)</span></p></div>"}');export{u as comp,h as data};

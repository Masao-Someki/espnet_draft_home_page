import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,o as n,a as s}from"./app-KOUU_Wij.js";const a={},i=s('<p>&lt;!-- _espnet2.enh.layers.dptnet.DPTNet --&gt;</p><h1 id="espnet2-enh-layers-dptnet-dptnet" tabindex="-1"><a class="header-anchor" href="#espnet2-enh-layers-dptnet-dptnet"><span>espnet2.enh.layers.dptnet.DPTNet</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.enh.layers.dptnet.DPTNet<span class="small-bracket">(rnn_type, input_size, hidden_size, output_size, att_heads=4, dropout=0, activation=&#39;relu&#39;, num_layers=1, bidirectional=True, norm_type=&#39;gLN&#39;)</span></p></div><p>Bases: <code>Module</code></p><p>Dual-path transformer network.</p><ul><li><strong>Parameters:</strong><ul><li><strong>rnn_type</strong> (<em>str</em>) – select from ‘RNN’, ‘LSTM’ and ‘GRU’.</li><li><strong>input_size</strong> (<em>int</em>) – dimension of the input feature. Input size must be a multiple of att_heads.</li><li><strong>hidden_size</strong> (<em>int</em>) – dimension of the hidden state.</li><li><strong>output_size</strong> (<em>int</em>) – dimension of the output size.</li><li><strong>att_heads</strong> (<em>int</em>) – number of attention heads.</li><li><strong>dropout</strong> (<em>float</em>) – dropout ratio. Default is 0.</li><li><strong>activation</strong> (<em>str</em>) – activation function applied at the output of RNN.</li><li><strong>num_layers</strong> (<em>int</em>) – number of stacked RNN layers. Default is 1.</li><li><strong>bidirectional</strong> (<em>bool</em>) – whether the RNN layers are bidirectional. Default is True.</li><li><strong>norm_type</strong> (<em>str</em>) – type of normalization to use after each inter- or intra-chunk Transformer block.</li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(input)</span></p></div><p>Defines the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><h5 id="note" tabindex="-1"><a class="header-anchor" href="#note"><span>NOTE</span></a></h5><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p><div class="custom-h4"><p>inter_chunk_process<span class="small-bracket">(x, layer_index)</span></p></div><div class="custom-h4"><p>intra_chunk_process<span class="small-bracket">(x, layer_index)</span></p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',15),r=[i];function o(l,p){return n(),t("div",null,r)}const h=e(a,[["render",o],["__file","DPTNet.html.vue"]]),m=JSON.parse(`{"path":"/guide/espnet2/enh/DPTNet.html","title":"espnet2.enh.layers.dptnet.DPTNet","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.86,"words":258},"filePathRelative":"guide/espnet2/enh/DPTNet.md","excerpt":"<p>&lt;!-- _espnet2.enh.layers.dptnet.DPTNet --&gt;</p>\\n<h1>espnet2.enh.layers.dptnet.DPTNet</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.layers.dptnet.DPTNet<span class=\\"small-bracket\\">(rnn_type, input_size, hidden_size, output_size, att_heads=4, dropout=0, activation='relu', num_layers=1, bidirectional=True, norm_type='gLN')</span></p></div>"}`);export{h as comp,m as data};

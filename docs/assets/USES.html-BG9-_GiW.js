import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,o as n,a as s}from"./app-KOUU_Wij.js";const o={},i=s('<p>&lt;!-- _espnet2.enh.layers.uses.USES --&gt;</p><h1 id="espnet2-enh-layers-uses-uses" tabindex="-1"><a class="header-anchor" href="#espnet2-enh-layers-uses-uses"><span>espnet2.enh.layers.uses.USES</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.enh.layers.uses.USES<span class="small-bracket">(input_size, output_size, bottleneck_size=64, num_blocks=6, num_spatial_blocks=3, segment_size=64, memory_size=20, memory_types=1, rnn_type=&#39;lstm&#39;, hidden_size=128, att_heads=4, dropout=0.0, activation=&#39;relu&#39;, bidirectional=True, norm_type=&#39;cLN&#39;, ch_mode=&#39;att&#39;, ch_att_dim=256, eps=1e-05)</span></p></div><p>Bases: <code>Module</code></p><p>Unconstrained Speech Enhancement and Separation (USES) Network.</p><p>Reference: : [1] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.</p><ul><li><strong>Parameters:</strong><ul><li><p><strong>input_size</strong> (<em>int</em>) – dimension of the input feature.</p></li><li><p><strong>output_size</strong> (<em>int</em>) – dimension of the output.</p></li><li><p><strong>bottleneck_size</strong> (<em>int</em>) – dimension of the bottleneck feature. Must be a multiple of att_heads.</p></li><li><p><strong>num_blocks</strong> (<em>int</em>) – number of processing blocks.</p></li><li><p><strong>num_spatial_blocks</strong> (<em>int</em>) – number of processing blocks with channel modeling.</p></li><li><p><strong>segment_size</strong> (<em>int</em>) – number of frames in each non-overlapping segment. This is used to segment long utterances into smaller segments for efficient processing.</p></li><li><p><strong>memory_size</strong> (<em>int</em>) – group size of global memory tokens. The basic use of memory tokens is to store the history information from previous segments. The memory tokens are updated by the output of the last block after processing each segment.</p></li><li><p><strong>memory_types</strong> (<em>int</em>) –</p><p>numbre of memory token groups. Each group corresponds to a different type of processing, i.e.,</p><blockquote><p>the first group is used for denoising without dereverberation, the second group is used for denoising with dereverberation,</p></blockquote></li><li><p><strong>rnn_type</strong> (<em>str</em>) – type of the RNN cell in the improved Transformer layer.</p></li><li><p><strong>hidden_size</strong> (<em>int</em>) – hidden dimension of the RNN cell.</p></li><li><p><strong>att_heads</strong> (<em>int</em>) – number of attention heads in Transformer.</p></li><li><p><strong>dropout</strong> (<em>float</em>) – dropout ratio. Default is 0.</p></li><li><p><strong>activation</strong> (<em>str</em>) – non-linear activation function applied in each block.</p></li><li><p><strong>bidirectional</strong> (<em>bool</em>) – whether the RNN layers are bidirectional.</p></li><li><p><strong>norm_type</strong> (<em>str</em>) – normalization type in the improved Transformer layer.</p></li><li><p><strong>ch_mode</strong> (<em>str</em>) – mode of channel modeling. Select from “att” and “tac”.</p></li><li><p><strong>ch_att_dim</strong> (<em>int</em>) – dimension of the channel attention.</p></li><li><p><strong>eps</strong> (<em>float</em>) – epsilon for layer normalization.</p></li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(input, ref_channel=None, mem_idx=None)</span></p></div><p>USES forward.</p><ul><li><strong>Parameters:</strong><ul><li><strong>input</strong> (<em>torch.Tensor</em>) – input feature (batch, mics, input_size, freq, time)</li><li><strong>ref_channel</strong> (<em>None</em> <em>or</em> <em>int</em>) – index of the reference channel. if None, simply average all channels. if int, take the specified channel instead of averaging.</li><li><strong>mem_idx</strong> (<em>None</em> <em>or</em> <em>int</em>) – index of the memory token group. if None, use the only group of memory tokens in the model. if int, use the specified group from multiple existing groups.</li></ul></li><li><strong>Returns:</strong> output feature (batch, output_size, freq, time)</li><li><strong>Return type:</strong> output (torch.Tensor)</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',12),r=[i];function a(l,m){return n(),t("div",null,r)}const u=e(o,[["render",a],["__file","USES.html.vue"]]),d=JSON.parse(`{"path":"/guide/espnet2/enh/USES.html","title":"espnet2.enh.layers.uses.USES","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.45,"words":434},"filePathRelative":"guide/espnet2/enh/USES.md","excerpt":"<p>&lt;!-- _espnet2.enh.layers.uses.USES --&gt;</p>\\n<h1>espnet2.enh.layers.uses.USES</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.layers.uses.USES<span class=\\"small-bracket\\">(input_size, output_size, bottleneck_size=64, num_blocks=6, num_spatial_blocks=3, segment_size=64, memory_size=20, memory_types=1, rnn_type='lstm', hidden_size=128, att_heads=4, dropout=0.0, activation='relu', bidirectional=True, norm_type='cLN', ch_mode='att', ch_att_dim=256, eps=1e-05)</span></p></div>"}`);export{u as comp,d as data};

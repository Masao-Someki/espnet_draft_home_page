import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,c as s,f as i,b as e,d as o,e as a,w as l,a as c,o as p}from"./app-KOUU_Wij.js";const d={},m=e("h1",{id:"espnet2-spk-encoder-conformer-encoder-mfaconformerencoder",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-spk-encoder-conformer-encoder-mfaconformerencoder"},[e("span",null,"espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder")])],-1),_=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),o(" espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder"),e("span",{class:"small-bracket"},"(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d2', normalize_before: bool = True, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, stochastic_depth_rate: float | List[float] = 0.0, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000, padding_idx: int | None = None)")])],-1),u=e("code",null,"AbsEncoder",-1),f=c('<p>Conformer encoder module for MFA-Conformer.</p><p>Paper: Y. Zhang et al.,</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>``</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Mfa-conformer: Multi-scale feature aggregation conformer for automatic speaker verification,’’ in Proc. INTERSPEECH, 2022.</p><ul><li><strong>Parameters:</strong><ul><li><strong>input_size</strong> (<em>int</em>) – Input dimension.</li><li><strong>output_size</strong> (<em>int</em>) – Dimension of attention.</li><li><strong>attention_heads</strong> (<em>int</em>) – The number of heads of multi head attention.</li><li><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</li><li><strong>num_blocks</strong> (<em>int</em>) – The number of encoder blocks.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li><li><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</li><li><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</li><li><strong>input_layer</strong> (<em>Union</em> *[*<em>str</em> <em>,</em> <em>torch.nn.Module</em> <em>]</em>) – Input layer type.</li><li><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</li><li><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</li><li><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</li><li><strong>rel_pos_type</strong> (<em>str</em>) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in <a href="https://github.com/espnet/espnet/pull/2816" target="_blank" rel="noopener noreferrer">https://github.com/espnet/espnet/pull/2816</a>.</li><li><strong>encoder_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</li><li><strong>encoder_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</li><li><strong>activation_type</strong> (<em>str</em>) – Encoder activation function type.</li><li><strong>macaron_style</strong> (<em>bool</em>) – Whether to use macaron style for positionwise layer.</li><li><strong>use_cnn_module</strong> (<em>bool</em>) – Whether to use convolution module.</li><li><strong>zero_triu</strong> (<em>bool</em>) – Whether to zero the upper triangular part of attention matrix.</li><li><strong>cnn_module_kernel</strong> (<em>int</em>) – Kernerl size of convolution module.</li><li><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(x: Tensor)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</li><li><strong>Returns:</strong> Output tensor (#batch, L, output_size).</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>output_size()</p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',11);function g(h,b){const t=r("RouteLink");return p(),s("div",null,[i(" _espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder "),m,_,e("p",null,[o("Bases: "),a(t,{to:"/guide/espnet2/asr/AbsEncoder.html#espnet2.asr.encoder.abs_encoder.AbsEncoder"},{default:l(()=>[u]),_:1})]),f])}const v=n(d,[["render",g],["__file","MfaConformerEncoder.html.vue"]]),z=JSON.parse(`{"path":"/guide/espnet2/spk/MfaConformerEncoder.html","title":"espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.25,"words":376},"filePathRelative":"guide/espnet2/spk/MfaConformerEncoder.md","excerpt":"<!-- _espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder -->\\n<h1>espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder<span class=\\"small-bracket\\">(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d2', normalize_before: bool = True, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, stochastic_depth_rate: float | List[float] = 0.0, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000, padding_idx: int | None = None)</span></p></div>"}`);export{v as comp,z as data};

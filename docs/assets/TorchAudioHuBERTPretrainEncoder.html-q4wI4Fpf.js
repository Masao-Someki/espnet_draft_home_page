import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as t,c as s,f as a,b as e,d as o,e as i,w as l,a as c,o as d}from"./app-KOUU_Wij.js";const _={},m=e("h1",{id:"espnet2-asr-encoder-hubert-encoder-torchaudiohubertpretrainencoder",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-asr-encoder-hubert-encoder-torchaudiohubertpretrainencoder"},[e("span",null,"espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder")])],-1),p=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),o(" espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder"),e("span",{class:"small-bracket"},"(input_size: int | None = None, extractor_mode: str = 'group_norm', extractor_conv_layer_config: List[List[int]] | None = [[512, 10, 5], [512, 3, 2], [512, 3, 2], [512, 3, 2], [512, 3, 2], [512, 2, 2], [512, 2, 2]], extractor_conv_bias: bool = False, encoder_embed_dim: int = 768, encoder_projection_dropout: float = 0.1, encoder_pos_conv_kernel: int = 128, encoder_pos_conv_groups: int = 16, encoder_num_layers: int = 12, encoder_num_heads: int = 12, encoder_attention_dropout: float = 0.1, encoder_ff_interm_features: int = 3072, encoder_ff_interm_dropout: float = 0.0, encoder_dropout: float = 0.1, encoder_layer_norm_first: bool = False, encoder_layer_drop: float = 0.05, mask_prob: float = 0.8, mask_selection: str = 'static', mask_other: float = 0.0, mask_length: int = 10, no_mask_overlap: bool = False, mask_min_space: int = 1, mask_channel_prob: float = 0.0, mask_channel_selection: str = 'static', mask_channel_other: float = 0.0, mask_channel_length: int = 10, no_mask_channel_overlap: bool = False, mask_channel_min_space: int = 1, skip_masked: bool = False, skip_nomask: bool = False, num_classes: int = 100, final_dim: int = 256, feature_grad_mult: float | None = 0.1, finetuning: bool = False, freeze_encoder_updates: int = 0)")])],-1),u=e("code",null,"AbsEncoder",-1),h=c('<p>Torch Audio Hubert encoder module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>extractor_mode</strong> – Operation mode of feature extractor. Valid values are “group_norm” or “layer_norm”.</li><li><strong>extractor_conv_layer_config</strong> – Configuration of convolution layers in feature extractor. List of convolution configuration, i.e. [[output_channel, kernel_size, stride], …]</li><li><strong>extractor_conv_bias</strong> – Whether to include bias term to each convolution operation.</li><li><strong>encoder_embed_dim</strong> – The dimension of embedding in encoder.</li><li><strong>encoder_projection_dropout</strong> – The dropout probability applied after the input feature is projected to “encoder_embed_dim”.</li><li><strong>encoder_pos_conv_kernel</strong> – Kernel size of convolutional positional embeddings.</li><li><strong>encoder_pos_conv_groups</strong> – Number of groups of convolutional positional embeddings.</li><li><strong>encoder_num_layers</strong> – Number of self attention layers in transformer block.</li><li><strong>encoder_num_heads</strong> – Number of heads in self attention layers.</li><li><strong>encoder_attention_dropout</strong> – Dropout probability applied after softmax in self-attention layer.</li><li><strong>encoder_ff_interm_features</strong> – Dimension of hidden features in feed forward layer.</li><li><strong>encoder_ff_interm_dropout</strong> – Dropout probability applied in feedforward layer.</li><li><strong>encoder_dropout</strong> – Dropout probability applied at the end of feed forward layer.</li><li><strong>encoder_layer_norm_first</strong> – Control the order of layer norm in transformer layer and each encoder layer. If True, in transformer layer, layer norm is applied before features are fed to encoder layers.</li><li><strong>encoder_layer_drop</strong> – Probability to drop each encoder layer during training.</li><li><strong>mask_prob</strong> – Probability for each token to be chosen as start of the span to be masked.</li><li><strong>mask_selection</strong> – How to choose the mask length. Options: [static, uniform, normal, poisson].</li><li><strong>mask_other</strong> – Secondary mask argument (used for more complex distributions).</li><li><strong>mask_length</strong> – The lengths of the mask.</li><li><strong>no_mask_overlap</strong> – Whether to allow masks to overlap.</li><li><strong>mask_min_space</strong> – Minimum space between spans (if no overlap is enabled).</li><li><strong>mask_channel_prob</strong> – (float): The probability of replacing a feature with 0.</li><li><strong>mask_channel_selection</strong> – How to choose the mask length for channel masking. Options: [static, uniform, normal, poisson].</li><li><strong>mask_channel_other</strong> – Secondary mask argument for channel masking(used for more complex distributions).</li><li><strong>mask_channel_length</strong> – Minimum space between spans (if no overlap is enabled) for channel masking.</li><li><strong>no_mask_channel_overlap</strong> – Whether to allow channel masks to overlap.</li><li><strong>mask_channel_min_space</strong> – Minimum space between spans for channel masking(if no overlap is enabled).</li><li><strong>skip_masked</strong> – If True, skip computing losses over masked frames.</li><li><strong>skip_nomask</strong> – If True, skip computing losses over unmasked frames.</li><li><strong>num_classes</strong> – The number of classes in the labels.</li><li><strong>final_dim</strong> – Project final representations and targets to final_dim.</li><li><strong>feature_grad_mult</strong> – The factor to scale the convolutional feature extraction layer gradients by. The scale factor will not affect the forward pass.</li><li><strong>finetuning</strong> – Whether to finetuning the model with ASR or other tasks.</li><li><strong>freeze_encoder_updates</strong> – The number of steps to freeze the encoder parameters in ASR finetuning.</li></ul></li></ul><p>Hubert specific Args: : Please refer to: <a href="https://pytorch.org/audio/stable/generated/torchaudio.models.hubert_pretrain_model.html#torchaudio.models.hubert_pretrain_model" target="_blank" rel="noopener noreferrer">https://pytorch.org/audio/stable/generated/torchaudio.models.hubert_pretrain_model.html#torchaudio.models.hubert_pretrain_model</a></p><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(xs_pad: Tensor, ilens: Tensor, ys_pad: Tensor | None = None, ys_pad_length: Tensor | None = None, prev_states: Tensor | None = None)</span></p></div><p>Forward Hubert Pretrain Encoder.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs_pad</strong> – input tensor (B, L, D)</li><li><strong>ilens</strong> – input length (B)</li><li><strong>prev_states</strong> – Not to be used now.</li></ul></li><li><strong>Returns:</strong> position embedded tensor and mask</li></ul><div class="custom-h4"><p>output_size()</p></div><div class="custom-h4"><p>reload_pretrained_parameters()</p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',10);function f(g,b){const n=t("RouteLink");return d(),s("div",null,[a(" _espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder "),m,p,e("p",null,[o("Bases: "),i(n,{to:"/guide/espnet2/asr/AbsEncoder.html#espnet2.asr.encoder.abs_encoder.AbsEncoder"},{default:l(()=>[u]),_:1})]),h])}const y=r(_,[["render",f],["__file","TorchAudioHuBERTPretrainEncoder.html.vue"]]),T=JSON.parse(`{"path":"/guide/espnet2/asr/TorchAudioHuBERTPretrainEncoder.html","title":"espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.16,"words":647},"filePathRelative":"guide/espnet2/asr/TorchAudioHuBERTPretrainEncoder.md","excerpt":"<!-- _espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder -->\\n<h1>espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder<span class=\\"small-bracket\\">(input_size: int | None = None, extractor_mode: str = 'group_norm', extractor_conv_layer_config: List[List[int]] | None = [[512, 10, 5], [512, 3, 2], [512, 3, 2], [512, 3, 2], [512, 3, 2], [512, 2, 2], [512, 2, 2]], extractor_conv_bias: bool = False, encoder_embed_dim: int = 768, encoder_projection_dropout: float = 0.1, encoder_pos_conv_kernel: int = 128, encoder_pos_conv_groups: int = 16, encoder_num_layers: int = 12, encoder_num_heads: int = 12, encoder_attention_dropout: float = 0.1, encoder_ff_interm_features: int = 3072, encoder_ff_interm_dropout: float = 0.0, encoder_dropout: float = 0.1, encoder_layer_norm_first: bool = False, encoder_layer_drop: float = 0.05, mask_prob: float = 0.8, mask_selection: str = 'static', mask_other: float = 0.0, mask_length: int = 10, no_mask_overlap: bool = False, mask_min_space: int = 1, mask_channel_prob: float = 0.0, mask_channel_selection: str = 'static', mask_channel_other: float = 0.0, mask_channel_length: int = 10, no_mask_channel_overlap: bool = False, mask_channel_min_space: int = 1, skip_masked: bool = False, skip_nomask: bool = False, num_classes: int = 100, final_dim: int = 256, feature_grad_mult: float | None = 0.1, finetuning: bool = False, freeze_encoder_updates: int = 0)</span></p></div>"}`);export{y as comp,T as data};

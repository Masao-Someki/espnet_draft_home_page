import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as a,c as o,f as s,b as e,d as t,e as i,w as l,a as m,o as p}from"./app-KOUU_Wij.js";const c={},d=e("h1",{id:"espnet2-enh-extractor-td-speakerbeam-extractor-tdspeakerbeamextractor",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-enh-extractor-td-speakerbeam-extractor-tdspeakerbeamextractor"},[e("span",null,"espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor")])],-1),_=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor"),e("span",{class:"small-bracket"},"(input_dim: int, layer: int = 8, stack: int = 3, bottleneck_dim: int = 128, hidden_dim: int = 512, skip_dim: int = 128, kernel: int = 3, causal: bool = False, norm_type: str = 'gLN', pre_nonlinear: str = 'prelu', nonlinear: str = 'relu', i_adapt_layer: int = 7, adapt_layer_type: str = 'mul', adapt_enroll_dim: int = 128, use_spk_emb: bool = False, spk_emb_dim: int = 256)")])],-1),u=e("code",null,"AbsExtractor",-1),g=m('<p>Time-Domain SpeakerBeam Extractor.</p><ul><li><strong>Parameters:</strong><ul><li><strong>input_dim</strong> – input feature dimension</li><li><strong>layer</strong> – int, number of layers in each stack</li><li><strong>stack</strong> – int, number of stacks</li><li><strong>bottleneck_dim</strong> – bottleneck dimension</li><li><strong>hidden_dim</strong> – number of convolution channel</li><li><strong>skip_dim</strong> – int, number of skip connection channels</li><li><strong>kernel</strong> – int, kernel size.</li><li><strong>causal</strong> – bool, defalut False.</li><li><strong>norm_type</strong> – str, choose from ‘BN’, ‘gLN’, ‘cLN’</li><li><strong>pre_nonlinear</strong> – the nonlinear function right before mask estimation select from ‘prelu’, ‘relu’, ‘tanh’, ‘sigmoid’, ‘linear’</li><li><strong>nonlinear</strong> – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’, ‘linear’</li><li><strong>i_adapt_layer</strong> – int, index of adaptation layer</li><li><strong>adapt_layer_type</strong> – str, type of adaptation layer see espnet2.enh.layers.adapt_layers for options</li><li><strong>adapt_enroll_dim</strong> – int, dimensionality of the speaker embedding</li><li><strong>use_spk_emb</strong> – bool, whether to use speaker embeddings as enrollment</li><li><strong>spk_emb_dim</strong> – int, dimension of input speaker embeddings only used when use_spk_emb is True</li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(input: Tensor | ComplexTensor, ilens: Tensor, input_aux: Tensor, ilens_aux: Tensor, suffix_tag: str = &#39;&#39;, additional: Dict | None = None)</span></p></div><p>TD-SpeakerBeam Forward.</p><ul><li><p><strong>Parameters:</strong></p><ul><li><strong>input</strong> (<em>torch.Tensor</em> <em>or</em> <em>ComplexTensor</em>) – Encoded feature [B, T, N]</li><li><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</li><li><strong>input_aux</strong> (<em>torch.Tensor</em> <em>or</em> <em>ComplexTensor</em>) – Encoded auxiliary feature for the target speaker [B, T, N] or [B, N]</li><li><strong>ilens_aux</strong> (<em>torch.Tensor</em>) – input lengths of auxiliary input for the target speaker [Batch]</li><li><strong>suffix_tag</strong> (<em>str</em>) – suffix to append to the keys in others</li><li><strong>additional</strong> (<em>None</em> <em>or</em> <em>dict</em>) – additional parameters not used in this model</li></ul></li><li><p><strong>Returns:</strong> [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[</p><blockquote><p>f’mask{suffix_tag}’: torch.Tensor(Batch, Frames, Freq), f’enroll_emb{suffix_tag}’: torch.Tensor(Batch, adapt_enroll_dim/adapt_enroll_dim*2),</p></blockquote><p>]</p></li><li><p><strong>Return type:</strong> masked (List[Union(torch.Tensor, ComplexTensor)])</p></li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',6);function h(k,x){const r=a("RouteLink");return p(),o("div",null,[s(" _espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor "),d,_,e("p",null,[t("Bases: "),i(r,{to:"/guide/espnet2/enh/AbsExtractor.html#espnet2.enh.extractor.abs_extractor.AbsExtractor"},{default:l(()=>[u]),_:1})]),g])}const T=n(c,[["render",h],["__file","TDSpeakerBeamExtractor.html.vue"]]),y=JSON.parse(`{"path":"/guide/espnet2/enh/TDSpeakerBeamExtractor.html","title":"espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.12,"words":335},"filePathRelative":"guide/espnet2/enh/TDSpeakerBeamExtractor.md","excerpt":"<!-- _espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor -->\\n<h1>espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor<span class=\\"small-bracket\\">(input_dim: int, layer: int = 8, stack: int = 3, bottleneck_dim: int = 128, hidden_dim: int = 512, skip_dim: int = 128, kernel: int = 3, causal: bool = False, norm_type: str = 'gLN', pre_nonlinear: str = 'prelu', nonlinear: str = 'relu', i_adapt_layer: int = 7, adapt_layer_type: str = 'mul', adapt_enroll_dim: int = 128, use_spk_emb: bool = False, spk_emb_dim: int = 256)</span></p></div>"}`);export{T as comp,y as data};

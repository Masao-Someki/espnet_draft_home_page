import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,c as s,f as i,b as e,d as t,e as a,w as l,a as d,o as m}from"./app-KOUU_Wij.js";const p={},c=e("h1",{id:"espnet-nets-pytorch-backend-e2e-tts-transformer-transformer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet-nets-pytorch-backend-e2e-tts-transformer-transformer"},[e("span",null,"espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer")])],-1),u=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer"),e("span",{class:"small-bracket"},"(idim, odim, args=None)")])],-1),f=e("code",null,"TTSInterface",-1),_=e("code",null,"Module",-1),g=d('<p>Text-to-Speech Transformer module.</p><p>This is a module of text-to-speech Transformer described in <a href="https://arxiv.org/pdf/1809.08895.pdf" target="_blank" rel="noopener noreferrer">Neural Speech Synthesis with Transformer Network</a>, which convert the sequence of characters or phonemes into the sequence of Mel-filterbanks.</p><p>Initialize TTS-Transformer module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>idim</strong> (<em>int</em>) – Dimension of the inputs.</li><li><strong>odim</strong> (<em>int</em>) – Dimension of the outputs.</li><li><strong>args</strong> (<em>Namespace</em> <em>,</em> <em>optional</em>) – <ul><li>embed_dim (int): Dimension of character embedding.</li><li>eprenet_conv_layers (int): : Number of encoder prenet convolution layers.</li><li>eprenet_conv_chans (int): : Number of encoder prenet convolution channels.</li><li>eprenet_conv_filts (int): Filter size of encoder prenet convolution.</li><li>dprenet_layers (int): Number of decoder prenet layers.</li><li>dprenet_units (int): Number of decoder prenet hidden units.</li><li>elayers (int): Number of encoder layers.</li><li>eunits (int): Number of encoder hidden units.</li><li>adim (int): Number of attention transformation dimensions.</li><li>aheads (int): Number of heads for multi head attention.</li><li>dlayers (int): Number of decoder layers.</li><li>dunits (int): Number of decoder hidden units.</li><li>postnet_layers (int): Number of postnet layers.</li><li>postnet_chans (int): Number of postnet channels.</li><li>postnet_filts (int): Filter size of postnet.</li><li>use_scaled_pos_enc (bool): : Whether to use trainable scaled positional encoding.</li><li>use_batch_norm (bool): : Whether to use batch normalization in encoder prenet.</li><li>encoder_normalize_before (bool): : Whether to perform layer normalization before encoder block.</li><li>decoder_normalize_before (bool): : Whether to perform layer normalization before decoder block.</li><li>encoder_concat_after (bool): Whether to concatenate attention : layer’s input and output in encoder.</li><li>decoder_concat_after (bool): Whether to concatenate attention : layer’s input and output in decoder.</li><li>reduction_factor (int): Reduction factor.</li><li>spk_embed_dim (int): Number of speaker embedding dimenstions.</li><li>spk_embed_integration_type: How to integrate speaker embedding.</li><li>transformer_init (float): How to initialize transformer parameters.</li><li>transformer_lr (float): Initial value of learning rate.</li><li>transformer_warmup_steps (int): Optimizer warmup steps.</li><li>transformer_enc_dropout_rate (float): : Dropout rate in encoder except attention &amp; positional encoding.</li><li>transformer_enc_positional_dropout_rate (float): : Dropout rate after encoder positional encoding.</li><li>transformer_enc_attn_dropout_rate (float): : Dropout rate in encoder self-attention module.</li><li>transformer_dec_dropout_rate (float): : Dropout rate in decoder except attention &amp; positional encoding.</li><li>transformer_dec_positional_dropout_rate (float): : Dropout rate after decoder positional encoding.</li><li>transformer_dec_attn_dropout_rate (float): : Dropout rate in deocoder self-attention module.</li><li>transformer_enc_dec_attn_dropout_rate (float): : Dropout rate in encoder-deocoder attention module.</li><li>eprenet_dropout_rate (float): Dropout rate in encoder prenet.</li><li>dprenet_dropout_rate (float): Dropout rate in decoder prenet.</li><li>postnet_dropout_rate (float): Dropout rate in postnet.</li><li>use_masking (bool): : Whether to apply masking for padded part in loss calculation.</li><li>use_weighted_masking (bool): : Whether to apply weighted masking in loss calculation.</li><li>bce_pos_weight (float): Positive sample weight in bce calculation : (only for use_masking=true).</li><li>loss_type (str): How to calculate loss.</li><li>use_guided_attn_loss (bool): Whether to use guided attention loss.</li><li>num_heads_applied_guided_attn (int): : Number of heads in each layer to apply guided attention loss.</li><li>num_layers_applied_guided_attn (int): : Number of layers to apply guided attention loss.</li><li>modules_applied_guided_attn (list): : List of module names to apply guided attention loss.</li><li>guided-attn-loss-sigma (float) Sigma in guided attention loss.</li><li>guided-attn-loss-lambda (float): Lambda in guided attention loss.</li></ul></li></ul></li></ul><div class="custom-h4"><p><em>static</em> add_arguments<span class="small-bracket">(parser)</span></p></div><p>Add model-specific arguments to the parser.</p><div class="custom-h4"><p><em>property</em> attention_plot_class</p></div><p>Return plot class for attention weight plot.</p><div class="custom-h4"><p><em>property</em> base_plot_keys</p></div><p>Return base key names to plot during training.</p><p>keys should match what chainer.reporter reports. If you add the key loss, the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values.</p><ul><li><strong>Returns:</strong> List of strings which are base keys to plot during training.</li><li><strong>Return type:</strong> list</li></ul><div class="custom-h4"><p>calculate_all_attentions<span class="small-bracket">(xs, ilens, ys, olens, spembs=None, skip_output=False, keep_tensor=False, *args, **kwargs)</span></p></div><p>Calculate all of the attention weights.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</li><li><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</li><li><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</li><li><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</li><li><strong>spembs</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of speaker embedding vectors (B, spk_embed_dim).</li><li><strong>skip_output</strong> (<em>bool</em> <em>,</em> <em>optional</em>) – Whether to skip calculate the final output.</li><li><strong>keep_tensor</strong> (<em>bool</em> <em>,</em> <em>optional</em>) – Whether to keep original tensor.</li></ul></li><li><strong>Returns:</strong> Dict of attention weights and outputs.</li><li><strong>Return type:</strong> dict</li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(xs, ilens, ys, labels, olens, spembs=None, *args, **kwargs)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</li><li><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</li><li><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</li><li><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</li><li><strong>spembs</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of speaker embedding vectors (B, spk_embed_dim).</li></ul></li><li><strong>Returns:</strong> Loss value.</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>inference<span class="small-bracket">(x, inference_args, spemb=None, *args, **kwargs)</span></p></div><p>Generate the sequence of features given the sequences of characters.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> (<em>Tensor</em>) – Input sequence of characters (T,).</li><li><strong>inference_args</strong> (<em>Namespace</em>) – <ul><li>threshold (float): Threshold in inference.</li><li>minlenratio (float): Minimum length ratio in inference.</li><li>maxlenratio (float): Maximum length ratio in inference.</li></ul></li><li><strong>spemb</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Speaker embedding vector (spk_embed_dim).</li></ul></li><li><strong>Returns:</strong> Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Encoder-decoder (source) attention weights (#layers, #heads, L, T).</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',22);function h(b,k){const o=r("RouteLink");return m(),s("div",null,[i(" _espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer "),c,u,e("p",null,[t("Bases: "),a(o,{to:"/guide/espnet/nets/TTSInterface.html#espnet.nets.tts_interface.TTSInterface"},{default:l(()=>[f]),_:1}),t(", "),_]),g])}const v=n(p,[["render",h],["__file","Transformer.html.vue"]]),N=JSON.parse('{"path":"/guide/espnet/nets/Transformer.html","title":"espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.98,"words":894},"filePathRelative":"guide/espnet/nets/Transformer.md","excerpt":"<!-- _espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer -->\\n<h1>espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer<span class=\\"small-bracket\\">(idim, odim, args=None)</span></p></div>"}');export{v as comp,N as data};

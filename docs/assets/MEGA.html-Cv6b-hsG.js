import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,f as s,a as o,o as n}from"./app-KOUU_Wij.js";const r={},a=o('<h1 id="espnet2-asr-transducer-decoder-blocks-mega-mega" tabindex="-1"><a class="header-anchor" href="#espnet2-asr-transducer-decoder-blocks-mega-mega"><span>espnet2.asr_transducer.decoder.blocks.mega.MEGA</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.asr_transducer.decoder.blocks.mega.MEGA<span class="small-bracket">(size: int = 512, num_heads: int = 4, qk_size: int = 128, v_size: int = 1024, activation: ~torch.nn.modules.module.Module = ReLU()</span>, normalization: ~torch.nn.modules.module.Module = &lt;class &#39;torch.nn.modules.normalization.LayerNorm&#39;&gt;, rel_pos_bias_type: str = &#39;simple&#39;, max_positions: int = 2048, truncation_length: int | None = None, chunk_size: int = -1, dropout_rate: float = 0.0, att_dropout_rate: float = 0.0, ema_dropout_rate: float = 0.0)</p></div><p>Bases: <code>Module</code></p><p>MEGA module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>size</strong> – Input/Output size.</li><li><strong>num_heads</strong> – Number of EMA heads.</li><li><strong>qk_size</strong> – Shared query and key size for attention module.</li><li><strong>v_size</strong> – Value size for attention module.</li><li><strong>qk_v_size</strong> – (QK, V) sizes for attention module.</li><li><strong>activation</strong> – Activation function type.</li><li><strong>normalization</strong> – Normalization module.</li><li><strong>rel_pos_bias_type</strong> – Type of relative position bias in attention module.</li><li><strong>max_positions</strong> – Maximum number of position for RelativePositionBias.</li><li><strong>truncation_length</strong> – Maximum length for truncation in EMA module.</li><li><strong>chunk_size</strong> – Chunk size for attention computation (-1 = full context).</li><li><strong>dropout_rate</strong> – Dropout rate for inner modules.</li><li><strong>att_dropout_rate</strong> – Dropout rate for the attention module.</li><li><strong>ema_dropout_rate</strong> – Dropout rate for the EMA module.</li></ul></li></ul><p>Construct a MEGA object.</p><div class="custom-h4"><p>forward<span class="small-bracket">(x: Tensor, mask: Tensor | None = None, attn_mask: Tensor | None = None, state: Dict[str, Tensor | None] | None = None)</span></p></div><p>Compute moving average equiped gated attention.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> – MEGA input sequences. (L, B, size)</li><li><strong>mask</strong> – MEGA input sequence masks. (B, 1, L)</li><li><strong>attn_mask</strong> – MEGA attention mask. (1, L, L)</li><li><strong>state</strong> – Decoder hidden states.</li></ul></li><li><strong>Returns:</strong> MEGA output sequences. (B, L, size) state: Decoder hidden states.</li><li><strong>Return type:</strong> x</li></ul><div class="custom-h4"><p>reset_parameters<span class="small-bracket">(val: int = 0.0, std: int = 0.02)</span></p></div><p>Reset module parameters.</p><ul><li><strong>Parameters:</strong><ul><li><strong>val</strong> – Initialization value.</li><li><strong>std</strong> – Standard deviation.</li></ul></li></ul><div class="custom-h4"><p>softmax_attention<span class="small-bracket">(query: Tensor, key: Tensor, mask: Tensor | None = None, attn_mask: Tensor | None = None)</span></p></div><p>Compute attention weights with softmax.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> – Query tensor. (B, 1, L, D)</li><li><strong>key</strong> – Key tensor. (B, 1, L, D)</li><li><strong>mask</strong> – Sequence mask. (B, 1, L)</li><li><strong>attn_mask</strong> – Attention mask. (1, L, L)</li></ul></li><li><strong>Returns:</strong> Attention weights. (B, 1, L, L)</li><li><strong>Return type:</strong> attn_weights</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',16);function i(l,u){return n(),e("div",null,[s(" _espnet2.asr_transducer.decoder.blocks.mega.MEGA "),a])}const c=t(r,[["render",i],["__file","MEGA.html.vue"]]),p=JSON.parse(`{"path":"/guide/espnet2/asr_transducer/MEGA.html","title":"espnet2.asr_transducer.decoder.blocks.mega.MEGA","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.15,"words":345},"filePathRelative":"guide/espnet2/asr_transducer/MEGA.md","excerpt":"<!-- _espnet2.asr_transducer.decoder.blocks.mega.MEGA -->\\n<h1>espnet2.asr_transducer.decoder.blocks.mega.MEGA</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr_transducer.decoder.blocks.mega.MEGA<span class=\\"small-bracket\\">(size: int = 512, num_heads: int = 4, qk_size: int = 128, v_size: int = 1024, activation: ~torch.nn.modules.module.Module = ReLU()</span>, normalization: ~torch.nn.modules.module.Module = &lt;class 'torch.nn.modules.normalization.LayerNorm'&gt;, rel_pos_bias_type: str = 'simple', max_positions: int = 2048, truncation_length: int | None = None, chunk_size: int = -1, dropout_rate: float = 0.0, att_dropout_rate: float = 0.0, ema_dropout_rate: float = 0.0)</p></div>"}`);export{c as comp,p as data};

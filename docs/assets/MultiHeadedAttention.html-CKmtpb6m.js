import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as a,c as o,f as r,b as e,d as t,e as i,w as l,a as c,o as d}from"./app-KOUU_Wij.js";const m={},u=e("h1",{id:"espnet2-asr-state-spaces-attention-multiheadedattention",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-asr-state-spaces-attention-multiheadedattention"},[e("span",null,"espnet2.asr.state_spaces.attention.MultiHeadedAttention")])],-1),p=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.asr.state_spaces.attention.MultiHeadedAttention"),e("span",{class:"small-bracket"},"(n_feat, n_head, dropout=0.0, transposed=False, **kwargs)")])],-1),h=e("code",null,"SequenceModule",-1),g=c('<p>Multi-Head Attention layer inheriting SequenceModule.</p><p>Comparing default MHA module in ESPnet, this module returns additional dummy state and has step function for autoregressive inference.</p><ul><li><strong>Parameters:</strong><ul><li><strong>n_head</strong> (<em>int</em>) – The number of heads.</li><li><strong>n_feat</strong> (<em>int</em>) – The number of features.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li></ul></li></ul><p>Construct an MultiHeadedAttention object.</p><div class="custom-h4"><p>forward<span class="small-bracket">(query, memory=None, mask=None, *args, **kwargs)</span></p></div><p>Compute scaled dot product attention.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor (#batch, time1, size).</li><li><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor (#batch, time2, size).</li><li><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor (#batch, time2, size).</li><li><strong>mask</strong> (<em>torch.Tensor</em>) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).</li></ul></li><li><strong>Returns:</strong> Output tensor (#batch, time1, d_model).</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>forward_attention<span class="small-bracket">(value, scores, mask)</span></p></div><p>Compute attention context vector.</p><ul><li><strong>Parameters:</strong><ul><li><strong>value</strong> (<em>torch.Tensor</em>) – Transformed value (#batch, n_head, time2, d_k).</li><li><strong>scores</strong> (<em>torch.Tensor</em>) – Attention score (#batch, n_head, time1, time2).</li><li><strong>mask</strong> (<em>torch.Tensor</em>) – Mask (#batch, 1, time2) or (#batch, time1, time2).</li></ul></li><li><strong>Returns:</strong> Transformed value (#batch, time1, d_model) : weighted by the attention score (#batch, time1, time2).</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>forward_qkv<span class="small-bracket">(query, key, value)</span></p></div><p>Transform query, key and value.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor (#batch, time1, size).</li><li><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor (#batch, time2, size).</li><li><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor (#batch, time2, size).</li></ul></li><li><strong>Returns:</strong> Transformed query tensor (#batch, n_head, time1, d_k). torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k). torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>step<span class="small-bracket">(query, state, memory=None, mask=None, **kwargs)</span></p></div><p>Step the model recurrently for one step of the input sequence.</p><p>For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -&gt; (B, L, H2), this method should generally have signature (B, H1) -&gt; (B, H2) with an optional recurrent state.</p><div class="custom-h4"><p>training <em>: bool</em></p></div>',17);function _(f,b){const s=a("RouteLink");return d(),o("div",null,[r(" _espnet2.asr.state_spaces.attention.MultiHeadedAttention "),u,p,e("p",null,[t("Bases: "),i(s,{to:"/guide/espnet2/asr/SequenceModule.html#espnet2.asr.state_spaces.base.SequenceModule"},{default:l(()=>[h]),_:1})]),g])}const T=n(m,[["render",_],["__file","MultiHeadedAttention.html.vue"]]),y=JSON.parse('{"path":"/guide/espnet2/asr/MultiHeadedAttention.html","title":"espnet2.asr.state_spaces.attention.MultiHeadedAttention","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.24,"words":372},"filePathRelative":"guide/espnet2/asr/MultiHeadedAttention.md","excerpt":"<!-- _espnet2.asr.state_spaces.attention.MultiHeadedAttention -->\\n<h1>espnet2.asr.state_spaces.attention.MultiHeadedAttention</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr.state_spaces.attention.MultiHeadedAttention<span class=\\"small-bracket\\">(n_feat, n_head, dropout=0.0, transposed=False, **kwargs)</span></p></div>"}');export{T as comp,y as data};

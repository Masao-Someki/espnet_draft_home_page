import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as o,c as l,f as c,b as e,d as t,e as n,w as a,a as i,o as p}from"./app-KOUU_Wij.js";const u={},d=e("h1",{id:"espnet-nets-pytorch-backend-lm-default-defaultrnnlm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet-nets-pytorch-backend-lm-default-defaultrnnlm"},[e("span",null,"espnet.nets.pytorch_backend.lm.default.DefaultRNNLM")])],-1),m=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet.nets.pytorch_backend.lm.default.DefaultRNNLM"),e("span",{class:"small-bracket"},"(n_vocab, args)")])],-1),h=e("code",null,"BatchScorerInterface",-1),f=e("code",null,"LMInterface",-1),g=e("code",null,"Module",-1),_=i('<p>Default RNNLM for LMInterface Implementation.</p><h5 id="note" tabindex="-1"><a class="header-anchor" href="#note"><span>NOTE</span></a></h5><p>PyTorch seems to have memory leak when one GPU compute this after data parallel. If parallel GPUs compute this, it seems to be fine. See also <a href="https://github.com/espnet/espnet/issues/1075" target="_blank" rel="noopener noreferrer">https://github.com/espnet/espnet/issues/1075</a></p><p>Initialize class.</p><ul><li><strong>Parameters:</strong><ul><li><strong>n_vocab</strong> (<em>int</em>) – The size of the vocabulary</li><li><strong>args</strong> (<em>argparse.Namespace</em>) – configurations. see py:method:add_arguments</li></ul></li></ul><div class="custom-h4"><p><em>static</em> add_arguments<span class="small-bracket">(parser)</span></p></div><p>Add arguments to command line argument parser.</p><div class="custom-h4"><p>batch_score<span class="small-bracket">(ys: Tensor, states: List[Any], xs: Tensor)</span></p></div><p>Score new token batch.</p><ul><li><strong>Parameters:</strong><ul><li><strong>ys</strong> (<em>torch.Tensor</em>) – torch.int64 prefix tokens (n_batch, ylen).</li><li><strong>states</strong> (<em>List</em> *[*<em>Any</em> <em>]</em>) – Scorer states for prefix tokens.</li><li><strong>xs</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys (n_batch, xlen, n_feat).</li></ul></li><li><strong>Returns:</strong> Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.</li><li><strong>Return type:</strong> tuple[torch.Tensor, List[Any]]</li></ul><div class="custom-h4"><p>final_score<span class="small-bracket">(state)</span></p></div><p>Score eos.</p><ul><li><strong>Parameters:</strong><strong>state</strong> – Scorer state for prefix tokens</li><li><strong>Returns:</strong> final score</li><li><strong>Return type:</strong> float</li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(x, t)</span></p></div><p>Compute LM loss value from buffer sequences.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> (<em>torch.Tensor</em>) – Input ids. (batch, len)</li><li><strong>t</strong> (<em>torch.Tensor</em>) – Target ids. (batch, len)</li></ul></li><li><strong>Returns:</strong> Tuple of : loss to backward (scalar), negative log-likelihood of t: -log p(t) (scalar) and the number of elements in x (scalar)</li><li><strong>Return type:</strong> tuple[torch.Tensor, torch.Tensor, torch.Tensor]</li></ul><div class="custom-h3"><p>Notes</p></div><p>The last two return values are used in perplexity: p(t)^{-n} = exp(-log p(t) / n)</p><div class="custom-h4"><p>load_state_dict<span class="small-bracket">(d)</span></p></div><p>Load state dict.</p><div class="custom-h4"><p>score<span class="small-bracket">(y, state, x)</span></p></div><p>Score new token.</p><ul><li><strong>Parameters:</strong><ul><li><strong>y</strong> (<em>torch.Tensor</em>) – 1D torch.int64 prefix tokens.</li><li><strong>state</strong> – Scorer state for prefix tokens</li><li><strong>x</strong> (<em>torch.Tensor</em>) – 2D encoder feature that generates ys.</li></ul></li><li><strong>Returns:</strong> Tuple of : torch.float32 scores for next token (n_vocab) and next state for ys</li><li><strong>Return type:</strong> tuple[torch.Tensor, Any]</li></ul><div class="custom-h4"><p>state_dict()</p></div><p>Dump state dict.</p><div class="custom-h4"><p>training <em>: bool</em></p></div>',26);function b(k,v){const s=o("RouteLink");return p(),l("div",null,[c(" _espnet.nets.pytorch_backend.lm.default.DefaultRNNLM "),d,m,e("p",null,[t("Bases: "),n(s,{to:"/guide/espnet/nets/BatchScorerInterface.html#espnet.nets.scorer_interface.BatchScorerInterface"},{default:a(()=>[h]),_:1}),t(", "),n(s,{to:"/guide/espnet/nets/LMInterface.html#espnet.nets.lm_interface.LMInterface"},{default:a(()=>[f]),_:1}),t(", "),g]),_])}const x=r(u,[["render",b],["__file","DefaultRNNLM.html.vue"]]),L=JSON.parse('{"path":"/guide/espnet/nets/DefaultRNNLM.html","title":"espnet.nets.pytorch_backend.lm.default.DefaultRNNLM","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.34,"words":401},"filePathRelative":"guide/espnet/nets/DefaultRNNLM.md","excerpt":"<!-- _espnet.nets.pytorch_backend.lm.default.DefaultRNNLM -->\\n<h1>espnet.nets.pytorch_backend.lm.default.DefaultRNNLM</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet.nets.pytorch_backend.lm.default.DefaultRNNLM<span class=\\"small-bracket\\">(n_vocab, args)</span></p></div>\\n<p>Bases: <a href=\\"/guide/espnet/nets/BatchScorerInterface.html#espnet.nets.scorer_interface.BatchScorerInterface\\" target=\\"_blank\\"><code>BatchScorerInterface</code></a>, <a href=\\"/guide/espnet/nets/LMInterface.html#espnet.nets.lm_interface.LMInterface\\" target=\\"_blank\\"><code>LMInterface</code></a>, <code>Module</code></p>"}');export{x as comp,L as data};

import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,f as r,a as n,o as a}from"./app-KOUU_Wij.js";const o={},s=n('<h1 id="espnet2-enh-layers-dptnet-improvedtransformerlayer" tabindex="-1"><a class="header-anchor" href="#espnet2-enh-layers-dptnet-improvedtransformerlayer"><span>espnet2.enh.layers.dptnet.ImprovedTransformerLayer</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.enh.layers.dptnet.ImprovedTransformerLayer<span class="small-bracket">(rnn_type, input_size, att_heads, hidden_size, dropout=0.0, activation=&#39;relu&#39;, bidirectional=True, norm=&#39;gLN&#39;)</span></p></div><p>Bases: <code>Module</code></p><p>Container module of the (improved) Transformer proposed in [1].</p><p>Reference: : Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation; Chen et al, Interspeech 2020.</p><ul><li><strong>Parameters:</strong><ul><li><strong>rnn_type</strong> (<em>str</em>) – select from ‘RNN’, ‘LSTM’ and ‘GRU’.</li><li><strong>input_size</strong> (<em>int</em>) – Dimension of the input feature.</li><li><strong>att_heads</strong> (<em>int</em>) – Number of attention heads.</li><li><strong>hidden_size</strong> (<em>int</em>) – Dimension of the hidden state.</li><li><strong>dropout</strong> (<em>float</em>) – Dropout ratio. Default is 0.</li><li><strong>activation</strong> (<em>str</em>) – activation function applied at the output of RNN.</li><li><strong>bidirectional</strong> (<em>bool</em> <em>,</em> <em>optional</em>) – True for bidirectional Inter-Chunk RNN (Intra-Chunk is always bidirectional).</li><li><strong>norm</strong> (<em>str</em> <em>,</em> <em>optional</em>) – Type of normalization to use.</li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(x, attn_mask=None)</span></p></div><p>Defines the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><h5 id="note" tabindex="-1"><a class="header-anchor" href="#note"><span>NOTE</span></a></h5><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p><div class="custom-h4"><p>training <em>: bool</em></p></div>',13);function i(l,d){return a(),t("div",null,[r(" _espnet2.enh.layers.dptnet.ImprovedTransformerLayer "),s])}const c=e(o,[["render",i],["__file","ImprovedTransformerLayer.html.vue"]]),h=JSON.parse(`{"path":"/guide/espnet2/enh/ImprovedTransformerLayer.html","title":"espnet2.enh.layers.dptnet.ImprovedTransformerLayer","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.74,"words":223},"filePathRelative":"guide/espnet2/enh/ImprovedTransformerLayer.md","excerpt":"<!-- _espnet2.enh.layers.dptnet.ImprovedTransformerLayer -->\\n<h1>espnet2.enh.layers.dptnet.ImprovedTransformerLayer</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.layers.dptnet.ImprovedTransformerLayer<span class=\\"small-bracket\\">(rnn_type, input_size, att_heads, hidden_size, dropout=0.0, activation='relu', bidirectional=True, norm='gLN')</span></p></div>"}`);export{c as comp,h as data};

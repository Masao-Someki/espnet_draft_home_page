import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as n,c as s,f as a,b as e,d as o,e as i,w as l,a as m,o as p}from"./app-KOUU_Wij.js";const _={},c=e("h1",{id:"espnet2-enh-separator-conformer-separator-conformerseparator",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-enh-separator-conformer-separator-conformerseparator"},[e("span",null,"espnet2.enh.separator.conformer_separator.ConformerSeparator")])],-1),d=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),o(" espnet2.enh.separator.conformer_separator.ConformerSeparator"),e("span",{class:"small-bracket"},"(input_dim: int, num_spk: int = 2, predict_noise: bool = False, adim: int = 384, aheads: int = 4, layers: int = 6, linear_units: int = 1536, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, normalize_before: bool = False, concat_after: bool = False, dropout_rate: float = 0.1, input_layer: str = 'linear', positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, nonlinear: str = 'relu', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, conformer_enc_kernel_size: int = 7, padding_idx: int = -1)")])],-1),u=e("code",null,"AbsSeparator",-1),f=m('<p>Conformer separator.</p><ul><li><strong>Parameters:</strong><ul><li><strong>input_dim</strong> – input feature dimension</li><li><strong>num_spk</strong> – number of speakers</li><li><strong>predict_noise</strong> – whether to output the estimated noise signal</li><li><strong>adim</strong> (<em>int</em>) – Dimension of attention.</li><li><strong>aheads</strong> (<em>int</em>) – The number of heads of multi head attention.</li><li><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</li><li><strong>layers</strong> (<em>int</em>) – The number of transformer blocks.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li><li><strong>input_layer</strong> (<em>Union</em> *[*<em>str</em> <em>,</em> <em>torch.nn.Module</em> <em>]</em>) – Input layer type.</li><li><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</li><li><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</li><li><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</li><li><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -&gt; x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -&gt; x + att(x)</li><li><strong>conformer_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</li><li><strong>conformer_self_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</li><li><strong>conformer_activation_type</strong> (<em>str</em>) – Encoder activation function type.</li><li><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</li><li><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</li><li><strong>use_macaron_style_in_conformer</strong> (<em>bool</em>) – Whether to use macaron style for positionwise layer.</li><li><strong>use_cnn_in_conformer</strong> (<em>bool</em>) – Whether to use convolution module.</li><li><strong>conformer_enc_kernel_size</strong> (<em>int</em>) – Kernerl size of convolution module.</li><li><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</li><li><strong>nonlinear</strong> – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’</li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)</span></p></div><p>Forward.</p><ul><li><p><strong>Parameters:</strong></p><ul><li><strong>input</strong> (<em>torch.Tensor</em> <em>or</em> <em>ComplexTensor</em>) – Encoded feature [B, T, N]</li><li><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</li><li><strong>additional</strong> (<em>Dict</em> <em>or</em> <em>None</em>) – other data included in model NOTE: not used in this model</li></ul></li><li><p><strong>Returns:</strong> [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[</p><blockquote><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p></blockquote><p>]</p></li><li><p><strong>Return type:</strong> masked (List[Union(torch.Tensor, ComplexTensor)])</p></li></ul><div class="custom-h4"><p><em>property</em> num_spk</p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',7);function g(h,y){const r=n("RouteLink");return p(),s("div",null,[a(" _espnet2.enh.separator.conformer_separator.ConformerSeparator "),c,d,e("p",null,[o("Bases: "),i(r,{to:"/guide/espnet2/enh/AbsSeparator.html#espnet2.enh.separator.abs_separator.AbsSeparator"},{default:l(()=>[u]),_:1})]),f])}const T=t(_,[["render",g],["__file","ConformerSeparator.html.vue"]]),v=JSON.parse(`{"path":"/guide/espnet2/enh/ConformerSeparator.html","title":"espnet2.enh.separator.conformer_separator.ConformerSeparator","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.35,"words":404},"filePathRelative":"guide/espnet2/enh/ConformerSeparator.md","excerpt":"<!-- _espnet2.enh.separator.conformer_separator.ConformerSeparator -->\\n<h1>espnet2.enh.separator.conformer_separator.ConformerSeparator</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.separator.conformer_separator.ConformerSeparator<span class=\\"small-bracket\\">(input_dim: int, num_spk: int = 2, predict_noise: bool = False, adim: int = 384, aheads: int = 4, layers: int = 6, linear_units: int = 1536, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, normalize_before: bool = False, concat_after: bool = False, dropout_rate: float = 0.1, input_layer: str = 'linear', positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, nonlinear: str = 'relu', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, conformer_enc_kernel_size: int = 7, padding_idx: int = -1)</span></p></div>"}`);export{T as comp,v as data};

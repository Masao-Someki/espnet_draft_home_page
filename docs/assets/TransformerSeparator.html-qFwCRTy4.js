import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as n,c as a,f as s,b as e,d as r,e as i,w as l,a as p,o as m}from"./app-KOUU_Wij.js";const d={},c=e("h1",{id:"espnet2-enh-separator-transformer-separator-transformerseparator",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-enh-separator-transformer-separator-transformerseparator"},[e("span",null,"espnet2.enh.separator.transformer_separator.TransformerSeparator")])],-1),_=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),r(" espnet2.enh.separator.transformer_separator.TransformerSeparator"),e("span",{class:"small-bracket"},"(input_dim: int, num_spk: int = 2, predict_noise: bool = False, adim: int = 384, aheads: int = 4, layers: int = 6, linear_units: int = 1536, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, normalize_before: bool = False, concat_after: bool = False, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, use_scaled_pos_enc: bool = True, nonlinear: str = 'relu')")])],-1),u=e("code",null,"AbsSeparator",-1),f=p('<p>Transformer separator.</p><ul><li><strong>Parameters:</strong><ul><li><strong>input_dim</strong> – input feature dimension</li><li><strong>num_spk</strong> – number of speakers</li><li><strong>predict_noise</strong> – whether to output the estimated noise signal</li><li><strong>adim</strong> (<em>int</em>) – Dimension of attention.</li><li><strong>aheads</strong> (<em>int</em>) – The number of heads of multi head attention.</li><li><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</li><li><strong>layers</strong> (<em>int</em>) – The number of transformer blocks.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li><li><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</li><li><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</li><li><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</li><li><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -&gt; x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -&gt; x + att(x)</li><li><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</li><li><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</li><li><strong>use_scaled_pos_enc</strong> (<em>bool</em>) – use scaled positional encoding or not</li><li><strong>nonlinear</strong> – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’</li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)</span></p></div><p>Forward.</p><ul><li><p><strong>Parameters:</strong></p><ul><li><strong>input</strong> (<em>torch.Tensor</em> <em>or</em> <em>ComplexTensor</em>) – Encoded feature [B, T, N]</li><li><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</li><li><strong>additional</strong> (<em>Dict</em> <em>or</em> <em>None</em>) – other data included in model NOTE: not used in this model</li></ul></li><li><p><strong>Returns:</strong> [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[</p><blockquote><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p></blockquote><p>]</p></li><li><p><strong>Return type:</strong> masked (List[Union(torch.Tensor, ComplexTensor)])</p></li></ul><div class="custom-h4"><p><em>property</em> num_spk</p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',7);function h(g,T){const t=n("RouteLink");return m(),a("div",null,[s(" _espnet2.enh.separator.transformer_separator.TransformerSeparator "),c,_,e("p",null,[r("Bases: "),i(t,{to:"/guide/espnet2/enh/AbsSeparator.html#espnet2.enh.separator.abs_separator.AbsSeparator"},{default:l(()=>[u]),_:1})]),f])}const v=o(d,[["render",h],["__file","TransformerSeparator.html.vue"]]),w=JSON.parse(`{"path":"/guide/espnet2/enh/TransformerSeparator.html","title":"espnet2.enh.separator.transformer_separator.TransformerSeparator","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.11,"words":334},"filePathRelative":"guide/espnet2/enh/TransformerSeparator.md","excerpt":"<!-- _espnet2.enh.separator.transformer_separator.TransformerSeparator -->\\n<h1>espnet2.enh.separator.transformer_separator.TransformerSeparator</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.separator.transformer_separator.TransformerSeparator<span class=\\"small-bracket\\">(input_dim: int, num_spk: int = 2, predict_noise: bool = False, adim: int = 384, aheads: int = 4, layers: int = 6, linear_units: int = 1536, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, normalize_before: bool = False, concat_after: bool = False, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, use_scaled_pos_enc: bool = True, nonlinear: str = 'relu')</span></p></div>"}`);export{v as comp,w as data};

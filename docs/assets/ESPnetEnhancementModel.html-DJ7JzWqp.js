import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as o,c as a,f as r,b as e,d as i,e as l,w as p,a as s,o as c}from"./app-KOUU_Wij.js";const d={},h=s('<h1 id="espnet2-enh-espnet-model-espnetenhancementmodel" tabindex="-1"><a class="header-anchor" href="#espnet2-enh-espnet-model-espnetenhancementmodel"><span>espnet2.enh.espnet_model.ESPnetEnhancementModel</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.enh.espnet_model.ESPnetEnhancementModel<span class="small-bracket">(encoder: <a href="AbsEncoder.md#espnet2.enh.encoder.abs_encoder.AbsEncoder">AbsEncoder</a></span>, separator: <a href="AbsSeparator.md#espnet2.enh.separator.abs_separator.AbsSeparator">AbsSeparator</a> | None, decoder: <a href="AbsDecoder.md#espnet2.enh.decoder.abs_decoder.AbsDecoder">AbsDecoder</a>, mask_module: <a href="../diar/AbsMask.md#espnet2.diar.layers.abs_mask.AbsMask">AbsMask</a> | None, loss_wrappers: List[<a href="AbsLossWrapper.md#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper">AbsLossWrapper</a>] | None, stft_consistency: bool = False, loss_type: str = &#39;mask_mse&#39;, mask_type: str | None = None, flexible_numspk: bool = False, extract_feats_in_collect_stats: bool = False, normalize_variance: bool = False, normalize_variance_per_ch: bool = False, categories: list = [], category_weights: list = [], always_forward_in_48k: bool = False)</p></div>',2),m=e("code",null,"AbsESPnetModel",-1),_=s('<p>Speech enhancement or separation Frontend model</p><p>Main entry of speech enhancement/separation model training.</p><ul><li><strong>Parameters:</strong><ul><li><p><strong>encoder</strong> – waveform encoder that converts waveforms to feature representations</p></li><li><p><strong>separator</strong> – separator that enhance or separate the feature representations</p></li><li><p><strong>decoder</strong> – waveform decoder that converts the feature back to waveforms</p></li><li><p><strong>mask_module</strong> – mask module that converts the feature to masks NOTE: Only used for compatibility with joint speaker diarization. See test/espnet2/enh/test_espnet_enh_s2t_model.py for details.</p></li><li><p><strong>loss_wrappers</strong> – list of loss wrappers Each loss wrapper contains a criterion for loss calculation and the corresonding loss weight. The losses will be calculated in the order of the list and summed up.</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>stft_consistency</strong> – (deprecated, kept for compatibility) whether to compute the TF-domain loss while enforcing STFT consistency NOTE: STFT consistency is now always used for frequency-domain spectrum losses.</p></li><li><p><strong>loss_type</strong> – (deprecated, kept for compatibility) loss type</p></li><li><p><strong>mask_type</strong> – (deprecated, kept for compatibility) mask type in TF-domain model</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>flexible_numspk</strong> – whether to allow the model to predict a variable number of speakers in its output. NOTE: This should be used when training a speech separation model for unknown number of speakers.</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>extract_feats_in_collect_stats</strong> – used in espnet2/tasks/abs_task.py for determining whether or not to skip model building in collect_stats stage (stage 5 in egs2/</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>*</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>/enh1/enh.sh).</p></li><li><p><strong>normalize_variance</strong> – whether to normalize the signal variance before model forward, and revert it back after.</p></li><li><p><strong>normalize_variance_per_ch</strong> – whether to normalize the signal variance for each channel instead of the whole signal. NOTE: normalize_variance and normalize_variance_per_ch cannot be True at the same time.</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>categories</strong> – list of all possible categories of minibatches (order matters!) (e.g. [“1ch_8k_reverb”, “1ch_8k_both”] for multi-condition training) NOTE: this will be used to convert category index to the corresponding name for logging in forward_loss. Different categories will have different loss name suffixes.</p></li><li><p><strong>category_weights</strong> – list of weights for each category. Used to set loss weights for batches of different categories.</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>always_forward_in_48k</strong> – whether to always upsample the input speech to 48kHz for forward, and then downsample to the original sample rate for loss calculation. NOTE: this can be useful to train a model capable of handling various sampling rates while unifying bandwidth extension + speech enhancement.</p></li></ul></li></ul><div class="custom-h4"><p>collect_feats<span class="small-bracket">(speech_mix: Tensor, speech_mix_lengths: Tensor, **kwargs)</span></p></div><div class="custom-h4"><p>forward<span class="small-bracket">(speech_mix: Tensor, speech_mix_lengths: Tensor | None = None, **kwargs)</span></p></div><p>Frontend + Encoder + Decoder + Calc loss</p><ul><li><strong>Parameters:</strong><ul><li><strong>speech_mix</strong> – (Batch, samples) or (Batch, samples, channels)</li><li><strong>speech_ref</strong> – (Batch, num_speaker, samples) or (Batch, num_speaker, samples, channels)</li><li><strong>speech_mix_lengths</strong> – (Batch,), default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py</li><li><strong>kwargs</strong> – “utt_id” is among the input.</li></ul></li></ul><div class="custom-h4"><p>forward_enhance<span class="small-bracket">(speech_mix: Tensor, speech_lengths: Tensor, additional: Dict | None = None, fs: int | None = None)</span></p></div><div class="custom-h4"><p>forward_loss<span class="small-bracket">(speech_pre: Tensor, speech_lengths: Tensor, feature_mix: Tensor, feature_pre: List[Tensor], others: OrderedDict, speech_ref: List[Tensor], noise_ref: List[Tensor] | None = None, dereverb_speech_ref: List[Tensor] | None = None, category: Tensor | None = None, num_spk: int | None = None, fs: int | None = None)</span></p></div><div class="custom-h4"><p><em>static</em> sort_by_perm<span class="small-bracket">(nn_output, perm)</span></p></div><p>Sort the input list of tensors by the specified permutation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>nn_output</strong> – List[torch.Tensor(Batch, …)], len(nn_output) == num_spk</li><li><strong>perm</strong> – (Batch, num_spk) or List[torch.Tensor(num_spk)]</li></ul></li><li><strong>Returns:</strong> List[torch.Tensor(Batch, …)]</li><li><strong>Return type:</strong> nn_output_new</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',13);function g(u,b){const t=o("RouteLink");return c(),a("div",null,[r(" _espnet2.enh.espnet_model.ESPnetEnhancementModel "),h,e("p",null,[i("Bases: "),l(t,{to:"/guide/espnet2/train/AbsESPnetModel.html#espnet2.train.abs_espnet_model.AbsESPnetModel"},{default:p(()=>[m]),_:1})]),_])}const w=n(d,[["render",g],["__file","ESPnetEnhancementModel.html.vue"]]),v=JSON.parse(`{"path":"/guide/espnet2/enh/ESPnetEnhancementModel.html","title":"espnet2.enh.espnet_model.ESPnetEnhancementModel","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.17,"words":652},"filePathRelative":"guide/espnet2/enh/ESPnetEnhancementModel.md","excerpt":"<!-- _espnet2.enh.espnet_model.ESPnetEnhancementModel -->\\n<h1>espnet2.enh.espnet_model.ESPnetEnhancementModel</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.espnet_model.ESPnetEnhancementModel<span class=\\"small-bracket\\">(encoder: <a href=\\"AbsEncoder.md#espnet2.enh.encoder.abs_encoder.AbsEncoder\\">AbsEncoder</a></span>, separator: <a href=\\"AbsSeparator.md#espnet2.enh.separator.abs_separator.AbsSeparator\\">AbsSeparator</a> | None, decoder: <a href=\\"AbsDecoder.md#espnet2.enh.decoder.abs_decoder.AbsDecoder\\">AbsDecoder</a>, mask_module: <a href=\\"../diar/AbsMask.md#espnet2.diar.layers.abs_mask.AbsMask\\">AbsMask</a> | None, loss_wrappers: List[<a href=\\"AbsLossWrapper.md#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper\\">AbsLossWrapper</a>] | None, stft_consistency: bool = False, loss_type: str = 'mask_mse', mask_type: str | None = None, flexible_numspk: bool = False, extract_feats_in_collect_stats: bool = False, normalize_variance: bool = False, normalize_variance_per_ch: bool = False, categories: list = [], category_weights: list = [], always_forward_in_48k: bool = False)</p></div>"}`);export{w as comp,v as data};

import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,f as o,a as n,o as s}from"./app-KOUU_Wij.js";const t={},a=n('<h1 id="espnet2-asr-transducer-encoder-blocks-ebranchformer-ebranchformer" tabindex="-1"><a class="header-anchor" href="#espnet2-asr-transducer-encoder-blocks-ebranchformer-ebranchformer"><span>espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer<span class="small-bracket">(block_size: int, linear_size: int, self_att: ~torch.nn.modules.module.Module, feed_forward: ~torch.nn.modules.module.Module, feed_forward_macaron: ~torch.nn.modules.module.Module, conv_mod: ~torch.nn.modules.module.Module, depthwise_conv_mod: ~torch.nn.modules.module.Module, norm_class: ~torch.nn.modules.module.Module = &lt;class &#39;torch.nn.modules.normalization.LayerNorm&#39;&gt;, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0)</span></p></div><p>Bases: <code>Module</code></p><p>E-Branchformer module definition.</p><p>Reference: <a href="https://arxiv.org/pdf/2210.00077.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2210.00077.pdf</a></p><ul><li><strong>Parameters:</strong><ul><li><strong>block_size</strong> – Input/output size.</li><li><strong>linear_size</strong> – Linear layers’ hidden size.</li><li><strong>self_att</strong> – Self-attention module instance.</li><li><strong>feed_forward</strong> – Feed-forward module instance.</li><li><strong>feed_forward_macaron</strong> – Feed-forward module instance for macaron network.</li><li><strong>conv_mod</strong> – ConvolutionalSpatialGatingUnit module instance.</li><li><strong>depthwise_conv_mod</strong> – DepthwiseConvolution module instance.</li><li><strong>norm_class</strong> – Normalization class.</li><li><strong>norm_args</strong> – Normalization module arguments.</li><li><strong>dropout_rate</strong> – Dropout rate.</li></ul></li></ul><p>Construct a E-Branchformer object.</p><div class="custom-h4"><p>chunk_forward<span class="small-bracket">(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0)</span></p></div><p>Encode chunk of input sequence.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> – E-Branchformer input sequences. (B, T, D_block)</li><li><strong>pos_enc</strong> – Positional embedding sequences. (B, 2 * (T - 1), D_block)</li><li><strong>mask</strong> – Source mask. (B, T_2)</li><li><strong>left_context</strong> – Number of previous frames the attention module can see in current chunk.</li></ul></li><li><strong>Returns:</strong> E-Branchformer output sequences. (B, T, D_block) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)</li><li><strong>Return type:</strong> x</li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None)</span></p></div><p>Encode input sequences.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> – E-Branchformer input sequences. (B, T, D_block)</li><li><strong>pos_enc</strong> – Positional embedding sequences. (B, 2 * (T - 1), D_block)</li><li><strong>mask</strong> – Source mask. (B, T)</li><li><strong>chunk_mask</strong> – Chunk mask. (T_2, T_2)</li></ul></li><li><strong>Returns:</strong> E-Branchformer output sequences. (B, T, D_block) mask: Source mask. (B, T) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)</li><li><strong>Return type:</strong> x</li></ul><div class="custom-h4"><p>reset_streaming_cache<span class="small-bracket">(left_context: int, device: device)</span></p></div><p>Initialize/Reset self-attention and convolution modules cache for streaming.</p><ul><li><strong>Parameters:</strong><ul><li><strong>left_context</strong> – Number of previous frames the attention module can see in current chunk.</li><li><strong>device</strong> – Device to use for cache tensor.</li></ul></li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',17);function l(c,i){return s(),r("div",null,[o(" _espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer "),a])}const u=e(t,[["render",l],["__file","EBranchformer.html.vue"]]),p=JSON.parse(`{"path":"/guide/espnet2/asr_transducer/EBranchformer.html","title":"espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.08,"words":324},"filePathRelative":"guide/espnet2/asr_transducer/EBranchformer.md","excerpt":"<!-- _espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer -->\\n<h1>espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer<span class=\\"small-bracket\\">(block_size: int, linear_size: int, self_att: ~torch.nn.modules.module.Module, feed_forward: ~torch.nn.modules.module.Module, feed_forward_macaron: ~torch.nn.modules.module.Module, conv_mod: ~torch.nn.modules.module.Module, depthwise_conv_mod: ~torch.nn.modules.module.Module, norm_class: ~torch.nn.modules.module.Module = &lt;class 'torch.nn.modules.normalization.LayerNorm'&gt;, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0)</span></p></div>"}`);export{u as comp,p as data};

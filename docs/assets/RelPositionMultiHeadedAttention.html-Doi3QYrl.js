import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,f as n,a as s,o}from"./app-KOUU_Wij.js";const r={},i=s('<h1 id="espnet2-asr-transducer-encoder-modules-attention-relpositionmultiheadedattention" tabindex="-1"><a class="header-anchor" href="#espnet2-asr-transducer-encoder-modules-attention-relpositionmultiheadedattention"><span>espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention<span class="small-bracket">(num_heads: int, embed_size: int, dropout_rate: float = 0.0, simplified_attention_score: bool = False)</span></p></div><p>Bases: <code>Module</code></p><p>RelPositionMultiHeadedAttention definition.</p><ul><li><strong>Parameters:</strong><ul><li><strong>num_heads</strong> – Number of attention heads.</li><li><strong>embed_size</strong> – Embedding size.</li><li><strong>dropout_rate</strong> – Dropout rate.</li></ul></li></ul><p>Construct an MultiHeadedAttention object.</p><div class="custom-h4"><p>compute_attention_score<span class="small-bracket">(query: Tensor, key: Tensor, pos_enc: Tensor, left_context: int = 0)</span></p></div><p>Attention score computation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> – Transformed query tensor. (B, H, T_1, d_k)</li><li><strong>key</strong> – Transformed key tensor. (B, H, T_2, d_k)</li><li><strong>pos_enc</strong> – Positional embedding tensor. (B, 2 * T_1 - 1, size)</li><li><strong>left_context</strong> – Number of previous frames to use for current chunk attention computation.</li></ul></li><li><strong>Returns:</strong> Attention score. (B, H, T_1, T_2)</li></ul><div class="custom-h4"><p>compute_simplified_attention_score<span class="small-bracket">(query: Tensor, key: Tensor, pos_enc: Tensor, left_context: int = 0)</span></p></div><p>Simplified attention score computation.</p><p>Reference: <a href="https://github.com/k2-fsa/icefall/pull/458" target="_blank" rel="noopener noreferrer">https://github.com/k2-fsa/icefall/pull/458</a></p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> – Transformed query tensor. (B, H, T_1, d_k)</li><li><strong>key</strong> – Transformed key tensor. (B, H, T_2, d_k)</li><li><strong>pos_enc</strong> – Positional embedding tensor. (B, 2 * T_1 - 1, size)</li><li><strong>left_context</strong> – Number of previous frames to use for current chunk attention computation.</li></ul></li><li><strong>Returns:</strong> Attention score. (B, H, T_1, T_2)</li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(query: Tensor, key: Tensor, value: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None, left_context: int = 0)</span></p></div><p>Compute scaled dot product attention with rel. positional encoding.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> – Query tensor. (B, T_1, size)</li><li><strong>key</strong> – Key tensor. (B, T_2, size)</li><li><strong>value</strong> – Value tensor. (B, T_2, size)</li><li><strong>pos_enc</strong> – Positional embedding tensor. (B, 2 * T_1 - 1, size)</li><li><strong>mask</strong> – Source mask. (B, T_2)</li><li><strong>chunk_mask</strong> – Chunk mask. (T_1, T_1)</li><li><strong>left_context</strong> – Number of previous frames to use for current chunk attention computation.</li></ul></li><li><strong>Returns:</strong> Output tensor. (B, T_1, H * d_k)</li></ul><div class="custom-h4"><p>forward_attention<span class="small-bracket">(value: Tensor, scores: Tensor, mask: Tensor, chunk_mask: Tensor | None = None)</span></p></div><p>Compute attention context vector.</p><ul><li><strong>Parameters:</strong><ul><li><strong>value</strong> – Transformed value. (B, H, T_2, d_k)</li><li><strong>scores</strong> – Attention score. (B, H, T_1, T_2)</li><li><strong>mask</strong> – Source mask. (B, T_2)</li><li><strong>chunk_mask</strong> – Chunk mask. (T_1, T_1)</li></ul></li><li><strong>Returns:</strong> Transformed value weighted by attention score. (B, T_1, H * d_k)</li><li><strong>Return type:</strong> attn_output</li></ul><div class="custom-h4"><p>forward_qkv<span class="small-bracket">(query: Tensor, key: Tensor, value: Tensor)</span></p></div><p>Transform query, key and value.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> – Query tensor. (B, T_1, size)</li><li><strong>key</strong> – Key tensor. (B, T_2, size)</li><li><strong>v</strong> – Value tensor. (B, T_2, size)</li></ul></li><li><strong>Returns:</strong> Transformed query tensor. (B, H, T_1, d_k) k: Transformed key tensor. (B, H, T_2, d_k) v: Transformed value tensor. (B, H, T_2, d_k)</li><li><strong>Return type:</strong> q</li></ul><div class="custom-h4"><p>rel_shift<span class="small-bracket">(x: Tensor, left_context: int = 0)</span></p></div><p>Compute relative positional encoding.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> – Input sequence. (B, H, T_1, 2 * T_1 - 1)</li><li><strong>left_context</strong> – Number of previous frames to use for current chunk attention computation.</li></ul></li><li><strong>Returns:</strong> Output sequence. (B, H, T_1, T_2)</li><li><strong>Return type:</strong> x</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',26);function l(a,u){return o(),t("div",null,[n(" _espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention "),i])}const p=e(r,[["render",l],["__file","RelPositionMultiHeadedAttention.html.vue"]]),m=JSON.parse('{"path":"/guide/espnet2/asr_transducer/RelPositionMultiHeadedAttention.html","title":"espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.64,"words":493},"filePathRelative":"guide/espnet2/asr_transducer/RelPositionMultiHeadedAttention.md","excerpt":"<!-- _espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention -->\\n<h1>espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention<span class=\\"small-bracket\\">(num_heads: int, embed_size: int, dropout_rate: float = 0.0, simplified_attention_score: bool = False)</span></p></div>"}');export{p as comp,m as data};

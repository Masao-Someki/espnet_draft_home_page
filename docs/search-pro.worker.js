const V=Object.entries,et=Object.fromEntries,st="ENTRIES",L="KEYS",T="VALUES",_="";class D{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case T:return this.value();case L:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],nt=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return R(e,t,s,n,i,1,o,""),n},R=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const v=g!==t[F],z=o[p+F]+ +v,A=o[p+F+1]+1,w=o[m+F]+1,j=o[m+F+1]=Math.min(z,A,w);j<l&&(l=j)}if(l>s)continue t}R(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=O(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,st)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return nt(this._tree,t,s)}get(t){const s=k(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=k(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new D(this,L)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,I(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},k=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return k(e.get(s),t.slice(s.length))},I=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)W(n);else if(s.size===1){const[o,u]=s.entries().next().value;q(n,o,u)}}},W=e=>{if(e.length===0)return;const[t,s]=O(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&q(e.slice(0,-1),n,o)}},q=(e,t,s)=>{if(e.length===0)return;const[n,o]=O(e);n.set(o+t,s),n.delete(o)},O=e=>e[e.length-1],ut=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,M="or",$="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},N=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},P=({score:e},{score:t})=>t-e,lt=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[M]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),N(n.terms,u)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);N(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},dt=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},at=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},H=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},ft=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?H(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},gt={k:1.2,b:.7,d:.5},mt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:M,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:gt},pt={combineWith:$,prefix:(e,t,s)=>t===s.length-1},Ft={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},_t={...Ft,...U},K=Symbol("*"),yt=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},X=(e,t=M)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=ht[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},S=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){ft(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],v=dt(y,m,e._documentCount,F,p,r),z=n*a*f*v,A=d.get(l);if(A){A.score+=z,ct(A.terms,t);const w=G(A.match,s);w?w.push(c):A.match[s]=[c]}else d.set(l,{score:z,terms:[t],match:{[s]:[c]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:G(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...J.weights,...i},h=e._index.get(t.term),g=S(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);S(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);S(e,t.term,l,F,f,o,u,d,g)}return g},Y=(e,t,s={})=>{if(t===K)return yt(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Y(e,g,a));return X(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(at(i)).map(a=>At(e,a,i));return X(c,i.combineWith)},Q=(e,t,s={})=>{const n=Y(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===K&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(P),o},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Q(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(P),o};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?_t:t.autoVacuum;this._options={...mt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...pt,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},B=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},wt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),xt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),Z=(e,t,s={})=>{const n={};return Q(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>B(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>B(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>B(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),V(n).sort(([,o],[,u])=>"max"==="total"?wt(o,u):xt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=ut(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},tt=(e,t,s={})=>{const n=Ct(t,e,{fuzzy:.2,maxFuzzy:3,...s}).map(({suggestion:o})=>o);return e.includes(" ")?n:n.filter(o=>!o.includes(" "))},bt=et(V(JSON.parse("{\"/\":{\"documentCount\":2733,\"nextId\":2733,\"documentIds\":{\"0\":\"0\",\"1\":\"0#execute-in-docker\",\"2\":\"0#espnet-2-recipes\",\"3\":\"0#using-gpu-based-containers\",\"4\":\"0#using-cpu-based-container\",\"5\":\"0#local-builds\",\"6\":\"0#deprecated\",\"7\":\"0#tags\",\"8\":\"0#ubuntu-18-04\",\"9\":\"1\",\"10\":\"1#install\",\"11\":\"1#style-check-using-flake8-docstrings\",\"12\":\"1#generate-html\",\"13\":\"1#deploy\",\"14\":\"2\",\"15\":\"2#directory-structure\",\"16\":\"2#execution-of-example-scripts\",\"17\":\"2#logging\",\"18\":\"2#change-options-in-run-sh\",\"19\":\"2#use-of-gpu\",\"20\":\"2#espnet1-transducer\",\"21\":\"2#architecture\",\"22\":\"2#multi-task-learning\",\"23\":\"2#inference\",\"24\":\"2#additional-notes\",\"25\":\"2#changing-the-training-configuration\",\"26\":\"2#how-to-set-minibatch\",\"27\":\"2#how-to-use-finetuning\",\"28\":\"2#transfer-learning\",\"29\":\"2#freezing\",\"30\":\"2#important-notes\",\"31\":\"2#chainer-and-pytorch-backends\",\"32\":\"3\",\"33\":\"3#examples\",\"34\":\"3#single-node-with-4gpus-with-distributed-mode\",\"35\":\"3#to-enable-sharded-training\",\"36\":\"3#_2host-and-2gpus-for-each-host-with-multiprocessing-distributed-mode\",\"37\":\"3#rank-and-world-size\",\"38\":\"3#about-init-method\",\"39\":\"3#_2hosts-which-have-2gpus-and-1gpu-respectively\",\"40\":\"3#_2hosts-and-2gpus-for-each-node-using-slurm-with-multiprocessing-distributed\",\"41\":\"3#_5gpus-with-3nodes-using-slurm\",\"42\":\"3#_2hosts-and-2gpus-for-each-node-using-mpi-with-multiprocessing-distributed\",\"43\":\"3#espnet2-bin-launch\",\"44\":\"3#troubleshooting-for-nccl-with-ethernet-case\",\"45\":\"3#the-rules-of-nccl-socket-ifname\",\"46\":\"4\",\"47\":\"4#quick-usage\",\"48\":\"4#why-is-audio-file-formatting-necessary\",\"49\":\"4#the-audio-file-formats-supported-in-espnet2\",\"50\":\"4#use-case\",\"51\":\"4#case1-extract-segmentations-with-long-recoding\",\"52\":\"4#case2-extract-audio-data-from-video-codec-use-non-supported-format-by-soundfile\",\"53\":\"4#case3-convert-nist-sphere-files-to-wav\",\"54\":\"4#case4-using-a-mechanism-for-multi-channels-inputs\",\"55\":\"5\",\"56\":\"5#task-class\",\"57\":\"5#data-input-system\",\"58\":\"5#scp-file\",\"59\":\"5#required-data-names-and-optional-data-names\",\"60\":\"5#customize-collcate-fn-for-pytorch-data-loader\",\"61\":\"6\",\"62\":\"6#show-usage\",\"63\":\"6#configuration-file\",\"64\":\"6#change-the-configuration-for-dict-type-value\",\"65\":\"6#resume-training-process\",\"66\":\"6#transfer-learning-fine-tuning-using-pretrained-model\",\"67\":\"6#freeze-parameters\",\"68\":\"6#change-logging-interval\",\"69\":\"6#change-the-number-of-iterations-in-each-epoch\",\"70\":\"6#weights-biases-integration\",\"71\":\"6#multi-gpus\",\"72\":\"6#the-relation-between-mini-batch-size-and-number-of-gpus\",\"73\":\"6#change-mini-batch-type\",\"74\":\"6#batch-type-unsorted\",\"75\":\"6#batch-type-sorted\",\"76\":\"6#batch-type-folded\",\"77\":\"6#batch-type-length\",\"78\":\"6#batch-type-numel\",\"79\":\"6#batch-type-catbel\",\"80\":\"6#gradient-accumulating\",\"81\":\"6#automatic-mixed-precision-training\",\"82\":\"6#reproducibility-and-determinization\",\"83\":\"7\",\"84\":\"7#main-changing-from-espnet1\",\"85\":\"7#recipes-using-espnet2\",\"86\":\"7#see-training-status\",\"87\":\"7#show-the-log-file\",\"88\":\"7#show-the-training-status-in-a-image-file\",\"89\":\"7#use-tensorboard\",\"90\":\"7#how-to-parse-command-line-arguments-in-shell-scripts\",\"91\":\"7#start-from-a-specified-stage-and-stop-at-a-specified-stage\",\"92\":\"7#change-the-configuration-for-training\",\"93\":\"7#change-the-number-of-parallel-jobs\",\"94\":\"7#multi-gpus-training-and-distributed-training\",\"95\":\"7#relationship-between-mini-batch-size-and-number-of-gpus\",\"96\":\"7#use-specified-experiment-directory-for-evaluation\",\"97\":\"7#evaluation-without-training-using-pretrained-model\",\"98\":\"7#evaluation-using-openai-whisper\",\"99\":\"7#packing-and-sharing-your-trained-model\",\"100\":\"7#usage-of-self-supervised-learning-representations-as-feature\",\"101\":\"7#prerequisite\",\"102\":\"7#usage\",\"103\":\"7#streaming-asr\",\"104\":\"7#training\",\"105\":\"7#decoding\",\"106\":\"7#faq\",\"107\":\"7#real-time-factor-and-latency\",\"108\":\"7#usage-1\",\"109\":\"7#notes\",\"110\":\"7#example\",\"111\":\"7#limitations\",\"112\":\"7#transducer-asr\",\"113\":\"7#general-usage\",\"114\":\"7#architecture\",\"115\":\"7#encoder\",\"116\":\"7#decoder\",\"117\":\"7#joint-network\",\"118\":\"7#multi-task-learning\",\"119\":\"7#inference\",\"120\":\"7#streaming\",\"121\":\"7#training-1\",\"122\":\"7#decoding-1\",\"123\":\"7#faq-1\",\"124\":\"7#how-to-add-a-new-block-type-to-the-custom-encoder\",\"125\":\"8\",\"126\":\"8#how-to-build-espnet-on-a-cloud-machine-such-as-gcp-aws-etc\",\"127\":\"8#modulenotfounderror-no-module-named-espnet-or-etc\",\"128\":\"8#to-detect-the-installation-problem-with-a-normal-installation\",\"129\":\"9\",\"130\":\"9#citations\",\"131\":\"10\",\"132\":\"10#requirements\",\"133\":\"10#supported-linux-distributions-and-other-requirements\",\"134\":\"10#step-1-optional-install-kaldi\",\"135\":\"10#step-2-installation-espnet\",\"136\":\"10#step-3-optional-custom-tool-installation\",\"137\":\"10#check-installation\",\"138\":\"11\",\"139\":\"11#espnet1-n\",\"140\":\"11#espnet2-n\",\"141\":\"12\",\"142\":\"12#select-job-scheduler\",\"143\":\"12#usage-of-run-pl\",\"144\":\"12#configuration\",\"145\":\"13\",\"146\":\"13#espnet1\",\"147\":\"13#espnet2\",\"148\":\"13#multiple-gpu-tips\",\"149\":\"13#start-from-the-middle-stage-or-stop-at-the-specified-stage\",\"150\":\"13#ctc-attention-and-hybrid-ctc-attention\",\"151\":\"14\",\"152\":\"14#what-s-new\",\"153\":\"14#espnet2\",\"154\":\"14#demo\",\"155\":\"14#asr-speech-recognition\",\"156\":\"14#se-speech-enhancement-separation\",\"157\":\"14#slu-spoken-language-understanding\",\"158\":\"14#tts-text-to-speech\",\"159\":\"14#others-other-utilities\",\"160\":\"14#course\",\"161\":\"14#cmu-speechprocessing-spring2023\",\"162\":\"14#cmu-speechrecognition-fall2022\",\"163\":\"14#cmu-speecrecognition-fall2021\",\"164\":\"14#espnet1\",\"165\":\"15\",\"166\":\"16\",\"167\":\"16#installation\",\"168\":\"16#espnet-data-preparation\",\"169\":\"16#kaldi-style-directories\",\"170\":\"16#espnet-as-a-library\",\"171\":\"16#load-train-dev-dataset-1-4\",\"172\":\"16#create-minibatches-2-4\",\"173\":\"16#build-neural-networks-3-4\",\"174\":\"16#update-neural-networks-by-iterating-datasets-4-4\",\"175\":\"16#recognize-speech\",\"176\":\"17\",\"177\":\"17#abstract\",\"178\":\"17#installation\",\"179\":\"17#espnet-command-line-usage-espnet-egs-xxx\",\"180\":\"17#stage-0-2-data-preparation\",\"181\":\"17#kaldi-style-directory-structure\",\"182\":\"17#tips-essential-files-in-data-preparation\",\"183\":\"17#raw-speech-file-list\",\"184\":\"17#raw-text-list\",\"185\":\"17#tips-explore-datasets-with-data-json\",\"186\":\"17#stage-3-4-nn-training\",\"187\":\"17#tips-change-yaml-py\",\"188\":\"17#tips-tensorboard\",\"189\":\"17#decoding-and-evaluation\",\"190\":\"17#command-line-usage\",\"191\":\"17#asr-result-as-data-json\",\"192\":\"17#recognize-speech-from-python\",\"193\":\"17#recap-load-speech-from-data-json\",\"194\":\"17#load-model\",\"195\":\"18\",\"196\":\"18#setup-envrionment\",\"197\":\"18#recognize-speech-using-pretrained-models\",\"198\":\"18#synthesize-speech-using-pretrained-models\",\"199\":\"19\",\"200\":\"19#install\",\"201\":\"19#spanish-speech-english-text-translation\",\"202\":\"19#english-translated-text-to-speech-synthesis\",\"203\":\"19#check-decoding-log\",\"204\":\"19#training-st-models-from-scratch\",\"205\":\"19#details-of-espnet-tools\",\"206\":\"20\",\"207\":\"20#install\",\"208\":\"20#english-demo\",\"209\":\"20#download-pretrained-feature-generation-model\",\"210\":\"20#a-tacotron2\",\"211\":\"20#b-transformer\",\"212\":\"20#c-fastspeech\",\"213\":\"20#download-pretrained-vocoder-model\",\"214\":\"20#a-parallel-wavegan\",\"215\":\"20#b-melgan\",\"216\":\"20#c-multi-band-melgan\",\"217\":\"20#setup\",\"218\":\"20#synthesis\",\"219\":\"20#japanese-demo\",\"220\":\"20#install-japanese-dependencies\",\"221\":\"20#download-pretrained-models\",\"222\":\"20#a-tacotron-2\",\"223\":\"20#b-transformer-1\",\"224\":\"20#setup-1\",\"225\":\"20#synthesis-1\",\"226\":\"20#mandarin-demo\",\"227\":\"20#install-mandarin-dependencies\",\"228\":\"20#download-pretrained-models-1\",\"229\":\"20#a-transformer\",\"230\":\"20#b-fastspeech\",\"231\":\"20#setup-2\",\"232\":\"20#synthesis-2\",\"233\":\"21\",\"234\":\"21#setup-envrionment\",\"235\":\"21#run-the-recipe\",\"236\":\"21#stage-1-data-download\",\"237\":\"21#stage-0-data-preparation\",\"238\":\"21#stage-1-feature-extration\",\"239\":\"21#stage-2-dictionary-and-json-preparation\",\"240\":\"21#stage-3-network-training\",\"241\":\"21#stage-4-network-decoding\",\"242\":\"21#stage-5-waveform-synthesis\",\"243\":\"21#next-step\",\"244\":\"22\",\"245\":\"23\",\"246\":\"23#named-arguments\",\"247\":\"24\",\"248\":\"24#named-arguments\",\"249\":\"25\",\"250\":\"25#named-arguments\",\"251\":\"26\",\"252\":\"26#named-arguments\",\"253\":\"27\",\"254\":\"27#named-arguments\",\"255\":\"28\",\"256\":\"28#named-arguments\",\"257\":\"29\",\"258\":\"29#named-arguments\",\"259\":\"30\",\"260\":\"30#named-arguments\",\"261\":\"31\",\"262\":\"31#named-arguments\",\"263\":\"32\",\"264\":\"32#named-arguments\",\"265\":\"33\",\"266\":\"33#named-arguments\",\"267\":\"34\",\"268\":\"34#named-arguments\",\"269\":\"35\",\"270\":\"35#named-arguments\",\"271\":\"36\",\"272\":\"37\",\"273\":\"39\",\"274\":\"40\",\"275\":\"41\",\"276\":\"42\",\"277\":\"44\",\"278\":\"45\",\"279\":\"46\",\"280\":\"47\",\"281\":\"48\",\"282\":\"50\",\"283\":\"51\",\"284\":\"52\",\"285\":\"53\",\"286\":\"54\",\"287\":\"55\",\"288\":\"56\",\"289\":\"57\",\"290\":\"58\",\"291\":\"59\",\"292\":\"60\",\"293\":\"61\",\"294\":\"62\",\"295\":\"63\",\"296\":\"64\",\"297\":\"65\",\"298\":\"66\",\"299\":\"67\",\"300\":\"67#named-arguments\",\"301\":\"68\",\"302\":\"68#named-arguments\",\"303\":\"68#model-configuration-related\",\"304\":\"68#text-converter-related\",\"305\":\"68#ctc-segmentation-related\",\"306\":\"68#input-output-arguments\",\"307\":\"69\",\"308\":\"69#named-arguments\",\"309\":\"69#input-data-related\",\"310\":\"69#the-model-configuration-related\",\"311\":\"69#quantization-related\",\"312\":\"69#beam-search-related\",\"313\":\"69#text-converter-related\",\"314\":\"69#partially-ar-related\",\"315\":\"70\",\"316\":\"70#named-arguments\",\"317\":\"70#input-data-related\",\"318\":\"70#the-model-configuration-related\",\"319\":\"70#beam-search-related\",\"320\":\"70#text-converter-related\",\"321\":\"71\",\"322\":\"71#named-arguments\",\"323\":\"71#input-data-related\",\"324\":\"71#the-model-configuration-related\",\"325\":\"71#decoding-related\",\"326\":\"71#text-converter-related\",\"327\":\"72\",\"328\":\"72#named-arguments\",\"329\":\"72#input-data-related\",\"330\":\"72#the-model-configuration-related\",\"331\":\"72#beam-search-related\",\"332\":\"72#text-converter-related\",\"333\":\"73\",\"334\":\"73#named-arguments\",\"335\":\"73#input-data-related\",\"336\":\"73#the-model-configuration-related\",\"337\":\"73#beam-search-related\",\"338\":\"73#text-converter-related\",\"339\":\"74\",\"340\":\"74#named-arguments\",\"341\":\"74#input-data-related\",\"342\":\"74#the-model-configuration-related\",\"343\":\"75\",\"344\":\"75#named-arguments\",\"345\":\"75#input-data-related\",\"346\":\"75#the-model-configuration-related\",\"347\":\"75#data-loading-related\",\"348\":\"75#diarize-speech-related\",\"349\":\"75#enh-diar-related\",\"350\":\"76\",\"351\":\"76#named-arguments\",\"352\":\"76#input-data-related\",\"353\":\"76#output-data-related\",\"354\":\"76#the-model-configuration-related\",\"355\":\"76#data-loading-related\",\"356\":\"76#separatespeech-related\",\"357\":\"77\",\"358\":\"77#named-arguments\",\"359\":\"77#input-data-related\",\"360\":\"77#the-model-configuration-related\",\"361\":\"77#data-loading-related\",\"362\":\"77#separatespeech-related\",\"363\":\"78\",\"364\":\"78#named-arguments\",\"365\":\"78#input-data-related\",\"366\":\"78#dnsmos-related\",\"367\":\"78#pesq-related\",\"368\":\"79\",\"369\":\"79#named-arguments\",\"370\":\"79#input-data-related\",\"371\":\"79#output-data-related\",\"372\":\"79#the-model-configuration-related\",\"373\":\"79#data-loading-related\",\"374\":\"79#separatespeech-related\",\"375\":\"80\",\"376\":\"80#named-arguments\",\"377\":\"81\",\"378\":\"81#positional-arguments\",\"379\":\"81#named-arguments\",\"380\":\"82\",\"381\":\"82#named-arguments\",\"382\":\"82#input-data-related\",\"383\":\"82#the-model-configuration-related\",\"384\":\"83\",\"385\":\"83#named-arguments\",\"386\":\"83#input-data-related\",\"387\":\"83#the-model-configuration-related\",\"388\":\"83#quantization-related\",\"389\":\"83#beam-search-related\",\"390\":\"83#text-converter-related\",\"391\":\"84\",\"392\":\"84#named-arguments\",\"393\":\"84#input-data-related\",\"394\":\"84#the-model-configuration-related\",\"395\":\"84#beam-search-related\",\"396\":\"84#text-converter-related\",\"397\":\"85\",\"398\":\"85#sub-commands\",\"399\":\"86\",\"400\":\"86#named-arguments\",\"401\":\"86#input-data-related\",\"402\":\"86#the-model-configuration-related\",\"403\":\"86#decoding-related\",\"404\":\"86#spectrogram-based-generation-related\",\"405\":\"86#beam-search-discrete-unit-multi-pass-related\",\"406\":\"86#vocoder-related\",\"407\":\"86#text-converter-related\",\"408\":\"87\",\"409\":\"87#named-arguments\",\"410\":\"87#input-data-related\",\"411\":\"87#model-configuration-related\",\"412\":\"87#quantization-related\",\"413\":\"87#beam-search-related\",\"414\":\"87#text-converter-related\",\"415\":\"87#partially-ar-related\",\"416\":\"88\",\"417\":\"88#named-arguments\",\"418\":\"88#input-data-related\",\"419\":\"88#model-configuration-related\",\"420\":\"88#quantization-related\",\"421\":\"88#beam-search-related\",\"422\":\"89\",\"423\":\"89#named-arguments\",\"424\":\"89#input-data-related\",\"425\":\"89#the-model-configuration-related\",\"426\":\"89#quantization-related\",\"427\":\"89#beam-search-related\",\"428\":\"89#text-converter-related\",\"429\":\"90\",\"430\":\"90#named-arguments\",\"431\":\"90#input-data-related\",\"432\":\"90#the-model-configuration-related\",\"433\":\"90#distributed-training-related\",\"434\":\"90#trainer-initialization-related\",\"435\":\"90#cudnn-mode-related\",\"436\":\"90#the-inference-hyperparameter-related\",\"437\":\"91\",\"438\":\"91#named-arguments\",\"439\":\"91#input-data-related\",\"440\":\"91#the-model-configuration-related\",\"441\":\"92\",\"442\":\"92#named-arguments\",\"443\":\"93\",\"444\":\"93#named-arguments\",\"445\":\"93#input-data-related\",\"446\":\"93#the-model-configuration-related\",\"447\":\"93#beam-search-related\",\"448\":\"93#text-converter-related\",\"449\":\"94\",\"450\":\"94#named-arguments\",\"451\":\"94#input-data-related\",\"452\":\"94#the-model-configuration-related\",\"453\":\"94#beam-search-related\",\"454\":\"94#text-converter-related\",\"455\":\"95\",\"456\":\"95#named-arguments\",\"457\":\"95#input-data-related\",\"458\":\"95#the-model-configuration-related\",\"459\":\"95#decoding-related\",\"460\":\"95#vocoder-related\",\"461\":\"96\",\"462\":\"96#named-arguments\",\"463\":\"96#write-vocabulary-mode-related\",\"464\":\"97\",\"465\":\"97#named-arguments\",\"466\":\"97#input-data-related\",\"467\":\"97#the-model-configuration-related\",\"468\":\"97#decoding-related\",\"469\":\"97#vocoder-related\",\"470\":\"98\",\"471\":\"98#named-arguments\",\"472\":\"98#input-data-related\",\"473\":\"98#the-model-configuration-related\",\"474\":\"98#decoding-related\",\"475\":\"98#vocoder-related\",\"476\":\"99\",\"477\":\"99#named-arguments\",\"478\":\"100\",\"479\":\"100#named-arguments\",\"480\":\"100#input-data-related\",\"481\":\"100#the-model-configuration-related\",\"482\":\"100#quantization-related\",\"483\":\"100#beam-search-related\",\"484\":\"100#text-converter-related\",\"485\":\"101\",\"486\":\"101#named-arguments\",\"487\":\"101#input-data-related\",\"488\":\"101#the-model-configuration-related\",\"489\":\"101#beam-search-related\",\"490\":\"101#text-converter-related\",\"491\":\"102\",\"492\":\"102#named-arguments\",\"493\":\"103\",\"494\":\"103#positional-arguments\",\"495\":\"103#named-arguments\",\"496\":\"104\",\"497\":\"104#positional-arguments\",\"498\":\"104#named-arguments\",\"499\":\"105\",\"500\":\"105#named-arguments\",\"501\":\"106\",\"502\":\"106#named-arguments\",\"503\":\"107\",\"504\":\"107#positional-arguments\",\"505\":\"107#named-arguments\",\"506\":\"108\",\"507\":\"108#positional-arguments\",\"508\":\"108#named-arguments\",\"509\":\"109\",\"510\":\"109#positional-arguments\",\"511\":\"109#named-arguments\",\"512\":\"110\",\"513\":\"110#positional-arguments\",\"514\":\"110#named-arguments\",\"515\":\"111\",\"516\":\"111#positional-arguments\",\"517\":\"112\",\"518\":\"112#positional-arguments\",\"519\":\"113\",\"520\":\"113#positional-arguments\",\"521\":\"113#named-arguments\",\"522\":\"114\",\"523\":\"114#positional-arguments\",\"524\":\"114#named-arguments\",\"525\":\"115\",\"526\":\"115#positional-arguments\",\"527\":\"115#named-arguments\",\"528\":\"116\",\"529\":\"116#named-arguments\",\"530\":\"117\",\"531\":\"117#positional-arguments\",\"532\":\"117#named-arguments\",\"533\":\"118\",\"534\":\"118#positional-arguments\",\"535\":\"118#named-arguments\",\"536\":\"119\",\"537\":\"119#positional-arguments\",\"538\":\"120\",\"539\":\"120#positional-arguments\",\"540\":\"120#named-arguments\",\"541\":\"121\",\"542\":\"121#positional-arguments\",\"543\":\"121#named-arguments\",\"544\":\"122\",\"545\":\"122#positional-arguments\",\"546\":\"123\",\"547\":\"123#positional-arguments\",\"548\":\"123#named-arguments\",\"549\":\"124\",\"550\":\"124#positional-arguments\",\"551\":\"125\",\"552\":\"125#positional-arguments\",\"553\":\"125#named-arguments\",\"554\":\"126\",\"555\":\"126#positional-arguments\",\"556\":\"126#named-arguments\",\"557\":\"127\",\"558\":\"127#positional-arguments\",\"559\":\"127#named-arguments\",\"560\":\"128\",\"561\":\"128#named-arguments\",\"562\":\"129\",\"563\":\"129#named-arguments\",\"564\":\"130\",\"565\":\"130#named-arguments\",\"566\":\"131\",\"567\":\"131#named-arguments\",\"568\":\"132\",\"569\":\"132#positional-arguments\",\"570\":\"133\",\"571\":\"133#named-arguments\",\"572\":\"134\",\"573\":\"134#named-arguments\",\"574\":\"135\",\"575\":\"135#named-arguments\",\"576\":\"136\",\"577\":\"136#positional-arguments\",\"578\":\"136#named-arguments\",\"579\":\"137\",\"580\":\"137#positional-arguments\",\"581\":\"137#named-arguments\",\"582\":\"138\",\"583\":\"138#positional-arguments\",\"584\":\"138#named-arguments\",\"585\":\"139\",\"586\":\"139#positional-arguments\",\"587\":\"139#named-arguments\",\"588\":\"140\",\"589\":\"140#positional-arguments\",\"590\":\"141\",\"591\":\"141#positional-arguments\",\"592\":\"141#named-arguments\",\"593\":\"142\",\"594\":\"143\",\"595\":\"144\",\"596\":\"145\",\"597\":\"145#note\",\"598\":\"146\",\"599\":\"147\",\"600\":\"148\",\"601\":\"149\",\"602\":\"150\",\"603\":\"151\",\"604\":\"152\",\"605\":\"153\",\"606\":\"154\",\"607\":\"155\",\"608\":\"155#note\",\"609\":\"156\",\"610\":\"157\",\"611\":\"158\",\"612\":\"159\",\"613\":\"159#note\",\"614\":\"160\",\"615\":\"161\",\"616\":\"162\",\"617\":\"163\",\"618\":\"164\",\"619\":\"165\",\"620\":\"166\",\"621\":\"167\",\"622\":\"168\",\"623\":\"169\",\"624\":\"170\",\"625\":\"171\",\"626\":\"172\",\"627\":\"173\",\"628\":\"174\",\"629\":\"175\",\"630\":\"176\",\"631\":\"177\",\"632\":\"178\",\"633\":\"179\",\"634\":\"180\",\"635\":\"181\",\"636\":\"182\",\"637\":\"183\",\"638\":\"184\",\"639\":\"185\",\"640\":\"186\",\"641\":\"187\",\"642\":\"188\",\"643\":\"189\",\"644\":\"190\",\"645\":\"191\",\"646\":\"192\",\"647\":\"193\",\"648\":\"194\",\"649\":\"195\",\"650\":\"196\",\"651\":\"197\",\"652\":\"198\",\"653\":\"199\",\"654\":\"200\",\"655\":\"201\",\"656\":\"202\",\"657\":\"203\",\"658\":\"204\",\"659\":\"205\",\"660\":\"206\",\"661\":\"207\",\"662\":\"208\",\"663\":\"209\",\"664\":\"210\",\"665\":\"211\",\"666\":\"212\",\"667\":\"213\",\"668\":\"214\",\"669\":\"215\",\"670\":\"216\",\"671\":\"217\",\"672\":\"218\",\"673\":\"219\",\"674\":\"220\",\"675\":\"221\",\"676\":\"222\",\"677\":\"223\",\"678\":\"224\",\"679\":\"225\",\"680\":\"226\",\"681\":\"227\",\"682\":\"228\",\"683\":\"229\",\"684\":\"230\",\"685\":\"231\",\"686\":\"232\",\"687\":\"233\",\"688\":\"234\",\"689\":\"235\",\"690\":\"236\",\"691\":\"237\",\"692\":\"238\",\"693\":\"239\",\"694\":\"240\",\"695\":\"241\",\"696\":\"242\",\"697\":\"243\",\"698\":\"244\",\"699\":\"245\",\"700\":\"246\",\"701\":\"247\",\"702\":\"248\",\"703\":\"249\",\"704\":\"250\",\"705\":\"251\",\"706\":\"252\",\"707\":\"253\",\"708\":\"254\",\"709\":\"255\",\"710\":\"256\",\"711\":\"257\",\"712\":\"258\",\"713\":\"259\",\"714\":\"260\",\"715\":\"261\",\"716\":\"262\",\"717\":\"263\",\"718\":\"264\",\"719\":\"265\",\"720\":\"266\",\"721\":\"267\",\"722\":\"268\",\"723\":\"269\",\"724\":\"270\",\"725\":\"271\",\"726\":\"272\",\"727\":\"273\",\"728\":\"274\",\"729\":\"275\",\"730\":\"276\",\"731\":\"277\",\"732\":\"278\",\"733\":\"279\",\"734\":\"279#note\",\"735\":\"280\",\"736\":\"281\",\"737\":\"281#note\",\"738\":\"282\",\"739\":\"282#note\",\"740\":\"283\",\"741\":\"284\",\"742\":\"285\",\"743\":\"286\",\"744\":\"286#examples\",\"745\":\"287\",\"746\":\"288\",\"747\":\"289\",\"748\":\"290\",\"749\":\"291\",\"750\":\"292\",\"751\":\"293\",\"752\":\"294\",\"753\":\"294#note\",\"754\":\"295\",\"755\":\"296\",\"756\":\"297\",\"757\":\"297#note\",\"758\":\"298\",\"759\":\"299\",\"760\":\"300\",\"761\":\"300#note\",\"762\":\"301\",\"763\":\"302\",\"764\":\"303\",\"765\":\"304\",\"766\":\"305\",\"767\":\"306\",\"768\":\"307\",\"769\":\"308\",\"770\":\"309\",\"771\":\"310\",\"772\":\"311\",\"773\":\"312\",\"774\":\"313\",\"775\":\"314\",\"776\":\"315\",\"777\":\"316\",\"778\":\"317\",\"779\":\"317#note\",\"780\":\"318\",\"781\":\"319\",\"782\":\"320\",\"783\":\"321\",\"784\":\"322\",\"785\":\"323\",\"786\":\"324\",\"787\":\"325\",\"788\":\"326\",\"789\":\"327\",\"790\":\"328\",\"791\":\"329\",\"792\":\"330\",\"793\":\"331\",\"794\":\"332\",\"795\":\"333\",\"796\":\"333#examples\",\"797\":\"334\",\"798\":\"335\",\"799\":\"336\",\"800\":\"337\",\"801\":\"338\",\"802\":\"339\",\"803\":\"340\",\"804\":\"340#note\",\"805\":\"341\",\"806\":\"342\",\"807\":\"343\",\"808\":\"344\",\"809\":\"345\",\"810\":\"346\",\"811\":\"347\",\"812\":\"348\",\"813\":\"349\",\"814\":\"350\",\"815\":\"350#examples\",\"816\":\"351\",\"817\":\"352\",\"818\":\"353\",\"819\":\"354\",\"820\":\"355\",\"821\":\"356\",\"822\":\"357\",\"823\":\"358\",\"824\":\"359\",\"825\":\"360\",\"826\":\"361\",\"827\":\"362\",\"828\":\"363\",\"829\":\"364\",\"830\":\"365\",\"831\":\"366\",\"832\":\"366#note\",\"833\":\"367\",\"834\":\"368\",\"835\":\"369\",\"836\":\"370\",\"837\":\"371\",\"838\":\"371#examples\",\"839\":\"372\",\"840\":\"373\",\"841\":\"374\",\"842\":\"375\",\"843\":\"376\",\"844\":\"377\",\"845\":\"378\",\"846\":\"379\",\"847\":\"380\",\"848\":\"381\",\"849\":\"382\",\"850\":\"383\",\"851\":\"384\",\"852\":\"385\",\"853\":\"386\",\"854\":\"387\",\"855\":\"388\",\"856\":\"389\",\"857\":\"390\",\"858\":\"391\",\"859\":\"392\",\"860\":\"393\",\"861\":\"394\",\"862\":\"395\",\"863\":\"396\",\"864\":\"397\",\"865\":\"398\",\"866\":\"399\",\"867\":\"400\",\"868\":\"401\",\"869\":\"402\",\"870\":\"403\",\"871\":\"404\",\"872\":\"405\",\"873\":\"406\",\"874\":\"407\",\"875\":\"408\",\"876\":\"408#examples\",\"877\":\"409\",\"878\":\"410\",\"879\":\"411\",\"880\":\"412\",\"881\":\"413\",\"882\":\"414\",\"883\":\"415\",\"884\":\"416\",\"885\":\"417\",\"886\":\"418\",\"887\":\"419\",\"888\":\"420\",\"889\":\"421\",\"890\":\"422\",\"891\":\"423\",\"892\":\"424\",\"893\":\"425\",\"894\":\"426\",\"895\":\"427\",\"896\":\"428\",\"897\":\"429\",\"898\":\"430\",\"899\":\"431\",\"900\":\"431#examples\",\"901\":\"432\",\"902\":\"432#examples\",\"903\":\"433\",\"904\":\"433#examples\",\"905\":\"434\",\"906\":\"435\",\"907\":\"435#examples\",\"908\":\"436\",\"909\":\"437\",\"910\":\"438\",\"911\":\"439\",\"912\":\"440\",\"913\":\"441\",\"914\":\"442\",\"915\":\"443\",\"916\":\"444\",\"917\":\"445\",\"918\":\"446\",\"919\":\"447\",\"920\":\"448\",\"921\":\"449\",\"922\":\"450\",\"923\":\"451\",\"924\":\"452\",\"925\":\"453\",\"926\":\"454\",\"927\":\"455\",\"928\":\"455#examples\",\"929\":\"456\",\"930\":\"457\",\"931\":\"458\",\"932\":\"459\",\"933\":\"460\",\"934\":\"461\",\"935\":\"462\",\"936\":\"463\",\"937\":\"464\",\"938\":\"465\",\"939\":\"466\",\"940\":\"467\",\"941\":\"468\",\"942\":\"469\",\"943\":\"470\",\"944\":\"471\",\"945\":\"472\",\"946\":\"473\",\"947\":\"474\",\"948\":\"475\",\"949\":\"476\",\"950\":\"477\",\"951\":\"478\",\"952\":\"479\",\"953\":\"480\",\"954\":\"481\",\"955\":\"482\",\"956\":\"483\",\"957\":\"484\",\"958\":\"485\",\"959\":\"485#examples\",\"960\":\"486\",\"961\":\"487\",\"962\":\"488\",\"963\":\"489\",\"964\":\"490\",\"965\":\"491\",\"966\":\"492\",\"967\":\"493\",\"968\":\"494\",\"969\":\"495\",\"970\":\"496\",\"971\":\"497\",\"972\":\"498\",\"973\":\"499\",\"974\":\"500\",\"975\":\"501\",\"976\":\"502\",\"977\":\"503\",\"978\":\"504\",\"979\":\"505\",\"980\":\"506\",\"981\":\"507\",\"982\":\"508\",\"983\":\"509\",\"984\":\"509#examples\",\"985\":\"510\",\"986\":\"511\",\"987\":\"512\",\"988\":\"513\",\"989\":\"514\",\"990\":\"515\",\"991\":\"516\",\"992\":\"516#examples\",\"993\":\"517\",\"994\":\"518\",\"995\":\"518#examples\",\"996\":\"519\",\"997\":\"520\",\"998\":\"521\",\"999\":\"522\",\"1000\":\"523\",\"1001\":\"524\",\"1002\":\"525\",\"1003\":\"526\",\"1004\":\"527\",\"1005\":\"528\",\"1006\":\"529\",\"1007\":\"530\",\"1008\":\"531\",\"1009\":\"532\",\"1010\":\"533\",\"1011\":\"534\",\"1012\":\"535\",\"1013\":\"536\",\"1014\":\"536#examples\",\"1015\":\"537\",\"1016\":\"538\",\"1017\":\"538#examples\",\"1018\":\"539\",\"1019\":\"540\",\"1020\":\"541\",\"1021\":\"542\",\"1022\":\"543\",\"1023\":\"544\",\"1024\":\"544#examples\",\"1025\":\"545\",\"1026\":\"546\",\"1027\":\"547\",\"1028\":\"548\",\"1029\":\"549\",\"1030\":\"549#examples\",\"1031\":\"550\",\"1032\":\"551\",\"1033\":\"552\",\"1034\":\"553\",\"1035\":\"554\",\"1036\":\"555\",\"1037\":\"556\",\"1038\":\"557\",\"1039\":\"558\",\"1040\":\"559\",\"1041\":\"560\",\"1042\":\"561\",\"1043\":\"562\",\"1044\":\"563\",\"1045\":\"564\",\"1046\":\"565\",\"1047\":\"566\",\"1048\":\"567\",\"1049\":\"568\",\"1050\":\"569\",\"1051\":\"570\",\"1052\":\"571\",\"1053\":\"572\",\"1054\":\"573\",\"1055\":\"574\",\"1056\":\"575\",\"1057\":\"576\",\"1058\":\"577\",\"1059\":\"578\",\"1060\":\"579\",\"1061\":\"580\",\"1062\":\"581\",\"1063\":\"582\",\"1064\":\"583\",\"1065\":\"584\",\"1066\":\"585\",\"1067\":\"586\",\"1068\":\"587\",\"1069\":\"588\",\"1070\":\"589\",\"1071\":\"590\",\"1072\":\"591\",\"1073\":\"592\",\"1074\":\"593\",\"1075\":\"594\",\"1076\":\"595\",\"1077\":\"596\",\"1078\":\"597\",\"1079\":\"598\",\"1080\":\"599\",\"1081\":\"600\",\"1082\":\"601\",\"1083\":\"602\",\"1084\":\"603\",\"1085\":\"604\",\"1086\":\"605\",\"1087\":\"606\",\"1088\":\"607\",\"1089\":\"608\",\"1090\":\"609\",\"1091\":\"610\",\"1092\":\"611\",\"1093\":\"612\",\"1094\":\"613\",\"1095\":\"614\",\"1096\":\"615\",\"1097\":\"616\",\"1098\":\"617\",\"1099\":\"618\",\"1100\":\"619\",\"1101\":\"620\",\"1102\":\"621\",\"1103\":\"622\",\"1104\":\"623\",\"1105\":\"624\",\"1106\":\"625\",\"1107\":\"626\",\"1108\":\"627\",\"1109\":\"628\",\"1110\":\"628#note\",\"1111\":\"629\",\"1112\":\"629#note\",\"1113\":\"630\",\"1114\":\"631\",\"1115\":\"632\",\"1116\":\"633\",\"1117\":\"634\",\"1118\":\"634#note\",\"1119\":\"635\",\"1120\":\"635#note\",\"1121\":\"636\",\"1122\":\"636#note\",\"1123\":\"637\",\"1124\":\"637#note\",\"1125\":\"638\",\"1126\":\"638#note\",\"1127\":\"639\",\"1128\":\"639#note\",\"1129\":\"640\",\"1130\":\"641\",\"1131\":\"641#note\",\"1132\":\"642\",\"1133\":\"643\",\"1134\":\"644\",\"1135\":\"644#note\",\"1136\":\"645\",\"1137\":\"645#note\",\"1138\":\"646\",\"1139\":\"647\",\"1140\":\"648\",\"1141\":\"649\",\"1142\":\"650\",\"1143\":\"651\",\"1144\":\"651#note\",\"1145\":\"652\",\"1146\":\"653\",\"1147\":\"654\",\"1148\":\"655\",\"1149\":\"656\",\"1150\":\"657\",\"1151\":\"658\",\"1152\":\"658#note\",\"1153\":\"659\",\"1154\":\"660\",\"1155\":\"661\",\"1156\":\"662\",\"1157\":\"662#note\",\"1158\":\"663\",\"1159\":\"663#note\",\"1160\":\"664\",\"1161\":\"665\",\"1162\":\"666\",\"1163\":\"667\",\"1164\":\"668\",\"1165\":\"669\",\"1166\":\"670\",\"1167\":\"671\",\"1168\":\"672\",\"1169\":\"673\",\"1170\":\"674\",\"1171\":\"675\",\"1172\":\"676\",\"1173\":\"677\",\"1174\":\"678\",\"1175\":\"678#note\",\"1176\":\"679\",\"1177\":\"680\",\"1178\":\"681\",\"1179\":\"682\",\"1180\":\"683\",\"1181\":\"684\",\"1182\":\"685\",\"1183\":\"686\",\"1184\":\"687\",\"1185\":\"687#note\",\"1186\":\"688\",\"1187\":\"689\",\"1188\":\"690\",\"1189\":\"690#note\",\"1190\":\"691\",\"1191\":\"692\",\"1192\":\"693\",\"1193\":\"694\",\"1194\":\"695\",\"1195\":\"696\",\"1196\":\"697\",\"1197\":\"698\",\"1198\":\"699\",\"1199\":\"700\",\"1200\":\"701\",\"1201\":\"702\",\"1202\":\"703\",\"1203\":\"704\",\"1204\":\"705\",\"1205\":\"706\",\"1206\":\"707\",\"1207\":\"708\",\"1208\":\"708#note\",\"1209\":\"709\",\"1210\":\"710\",\"1211\":\"711\",\"1212\":\"712\",\"1213\":\"712#note\",\"1214\":\"713\",\"1215\":\"714\",\"1216\":\"714#note\",\"1217\":\"715\",\"1218\":\"716\",\"1219\":\"717\",\"1220\":\"718\",\"1221\":\"718#note\",\"1222\":\"719\",\"1223\":\"719#note\",\"1224\":\"720\",\"1225\":\"721\",\"1226\":\"722\",\"1227\":\"723\",\"1228\":\"723#note\",\"1229\":\"724\",\"1230\":\"724#note\",\"1231\":\"725\",\"1232\":\"725#note\",\"1233\":\"726\",\"1234\":\"726#note\",\"1235\":\"727\",\"1236\":\"727#note\",\"1237\":\"728\",\"1238\":\"728#note\",\"1239\":\"729\",\"1240\":\"729#note\",\"1241\":\"730\",\"1242\":\"730#position-wise-feedforward-components\",\"1243\":\"730#other-arguments\",\"1244\":\"731\",\"1245\":\"732\",\"1246\":\"732#note\",\"1247\":\"733\",\"1248\":\"734\",\"1249\":\"735\",\"1250\":\"735#note\",\"1251\":\"736\",\"1252\":\"737\",\"1253\":\"738\",\"1254\":\"739\",\"1255\":\"740\",\"1256\":\"741\",\"1257\":\"742\",\"1258\":\"742#note\",\"1259\":\"743\",\"1260\":\"743#note\",\"1261\":\"744\",\"1262\":\"744#note\",\"1263\":\"745\",\"1264\":\"745#note\",\"1265\":\"746\",\"1266\":\"746#note\",\"1267\":\"747\",\"1268\":\"747#note\",\"1269\":\"748\",\"1270\":\"749\",\"1271\":\"750\",\"1272\":\"751\",\"1273\":\"752\",\"1274\":\"753\",\"1275\":\"753#note\",\"1276\":\"754\",\"1277\":\"754#note\",\"1278\":\"755\",\"1279\":\"756\",\"1280\":\"757\",\"1281\":\"757#note\",\"1282\":\"758\",\"1283\":\"758#note\",\"1284\":\"759\",\"1285\":\"759#note\",\"1286\":\"760\",\"1287\":\"761\",\"1288\":\"762\",\"1289\":\"763\",\"1290\":\"764\",\"1291\":\"765\",\"1292\":\"766\",\"1293\":\"767\",\"1294\":\"768\",\"1295\":\"769\",\"1296\":\"770\",\"1297\":\"771\",\"1298\":\"772\",\"1299\":\"773\",\"1300\":\"774\",\"1301\":\"775\",\"1302\":\"776\",\"1303\":\"777\",\"1304\":\"778\",\"1305\":\"779\",\"1306\":\"780\",\"1307\":\"781\",\"1308\":\"782\",\"1309\":\"783\",\"1310\":\"784\",\"1311\":\"785\",\"1312\":\"786\",\"1313\":\"787\",\"1314\":\"788\",\"1315\":\"789\",\"1316\":\"790\",\"1317\":\"791\",\"1318\":\"792\",\"1319\":\"793\",\"1320\":\"794\",\"1321\":\"795\",\"1322\":\"796\",\"1323\":\"797\",\"1324\":\"798\",\"1325\":\"799\",\"1326\":\"800\",\"1327\":\"801\",\"1328\":\"802\",\"1329\":\"803\",\"1330\":\"804\",\"1331\":\"805\",\"1332\":\"806\",\"1333\":\"807\",\"1334\":\"808\",\"1335\":\"809\",\"1336\":\"810\",\"1337\":\"811\",\"1338\":\"812\",\"1339\":\"813\",\"1340\":\"814\",\"1341\":\"815\",\"1342\":\"816\",\"1343\":\"817\",\"1344\":\"818\",\"1345\":\"818#note\",\"1346\":\"819\",\"1347\":\"819#note\",\"1348\":\"820\",\"1349\":\"821\",\"1350\":\"822\",\"1351\":\"823\",\"1352\":\"824\",\"1353\":\"825\",\"1354\":\"826\",\"1355\":\"827\",\"1356\":\"828\",\"1357\":\"829\",\"1358\":\"830\",\"1359\":\"831\",\"1360\":\"832\",\"1361\":\"832#note\",\"1362\":\"833\",\"1363\":\"833#note\",\"1364\":\"834\",\"1365\":\"834#note\",\"1366\":\"835\",\"1367\":\"835#note\",\"1368\":\"836\",\"1369\":\"837\",\"1370\":\"838\",\"1371\":\"839\",\"1372\":\"840\",\"1373\":\"841\",\"1374\":\"842\",\"1375\":\"843\",\"1376\":\"844\",\"1377\":\"845\",\"1378\":\"846\",\"1379\":\"847\",\"1380\":\"848\",\"1381\":\"849\",\"1382\":\"850\",\"1383\":\"850#examples\",\"1384\":\"851\",\"1385\":\"851#examples\",\"1386\":\"852\",\"1387\":\"852#examples\",\"1388\":\"853\",\"1389\":\"853#examples\",\"1390\":\"854\",\"1391\":\"854#examples\",\"1392\":\"854#note\",\"1393\":\"855\",\"1394\":\"856\",\"1395\":\"856#examples\",\"1396\":\"857\",\"1397\":\"857#examples\",\"1398\":\"858\",\"1399\":\"859\",\"1400\":\"859#examples\",\"1401\":\"860\",\"1402\":\"860#examples\",\"1403\":\"861\",\"1404\":\"861#examples\",\"1405\":\"862\",\"1406\":\"862#examples\",\"1407\":\"863\",\"1408\":\"863#examples\",\"1409\":\"864\",\"1410\":\"864#examples\",\"1411\":\"865\",\"1412\":\"865#examples\",\"1413\":\"866\",\"1414\":\"866#examples\",\"1415\":\"867\",\"1416\":\"867#examples\",\"1417\":\"868\",\"1418\":\"868#examples\",\"1419\":\"869\",\"1420\":\"870\",\"1421\":\"870#examples\",\"1422\":\"871\",\"1423\":\"871#examples\",\"1424\":\"872\",\"1425\":\"872#examples\",\"1426\":\"873\",\"1427\":\"874\",\"1428\":\"875\",\"1429\":\"876\",\"1430\":\"877\",\"1431\":\"878\",\"1432\":\"878#note\",\"1433\":\"879\",\"1434\":\"879#note\",\"1435\":\"880\",\"1436\":\"880#note\",\"1437\":\"881\",\"1438\":\"881#note\",\"1439\":\"882\",\"1440\":\"882#note\",\"1441\":\"883\",\"1442\":\"883#note\",\"1443\":\"884\",\"1444\":\"884#note\",\"1445\":\"885\",\"1446\":\"885#note\",\"1447\":\"886\",\"1448\":\"886#note\",\"1449\":\"887\",\"1450\":\"887#note\",\"1451\":\"888\",\"1452\":\"889\",\"1453\":\"889#note\",\"1454\":\"890\",\"1455\":\"891\",\"1456\":\"892\",\"1457\":\"892#note\",\"1458\":\"893\",\"1459\":\"893#note\",\"1460\":\"894\",\"1461\":\"894#note\",\"1462\":\"895\",\"1463\":\"896\",\"1464\":\"897\",\"1465\":\"898\",\"1466\":\"899\",\"1467\":\"899#note\",\"1468\":\"900\",\"1469\":\"900#note\",\"1470\":\"901\",\"1471\":\"902\",\"1472\":\"903\",\"1473\":\"904\",\"1474\":\"905\",\"1475\":\"905#note\",\"1476\":\"906\",\"1477\":\"906#note\",\"1478\":\"907\",\"1479\":\"907#note\",\"1480\":\"908\",\"1481\":\"908#note\",\"1482\":\"909\",\"1483\":\"909#note\",\"1484\":\"910\",\"1485\":\"911\",\"1486\":\"911#note\",\"1487\":\"912\",\"1488\":\"912#note\",\"1489\":\"913\",\"1490\":\"913#note\",\"1491\":\"914\",\"1492\":\"914#note\",\"1493\":\"915\",\"1494\":\"915#note\",\"1495\":\"916\",\"1496\":\"916#note\",\"1497\":\"917\",\"1498\":\"917#note\",\"1499\":\"918\",\"1500\":\"918#note\",\"1501\":\"919\",\"1502\":\"919#note\",\"1503\":\"920\",\"1504\":\"920#note\",\"1505\":\"921\",\"1506\":\"922\",\"1507\":\"922#note\",\"1508\":\"923\",\"1509\":\"923#note\",\"1510\":\"924\",\"1511\":\"925\",\"1512\":\"926\",\"1513\":\"926#note\",\"1514\":\"927\",\"1515\":\"928\",\"1516\":\"929\",\"1517\":\"930\",\"1518\":\"931\",\"1519\":\"931#note\",\"1520\":\"932\",\"1521\":\"932#note\",\"1522\":\"933\",\"1523\":\"934\",\"1524\":\"935\",\"1525\":\"936\",\"1526\":\"937\",\"1527\":\"938\",\"1528\":\"939\",\"1529\":\"940\",\"1530\":\"941\",\"1531\":\"942\",\"1532\":\"943\",\"1533\":\"943#note\",\"1534\":\"944\",\"1535\":\"945\",\"1536\":\"945#note\",\"1537\":\"946\",\"1538\":\"946#note\",\"1539\":\"947\",\"1540\":\"948\",\"1541\":\"948#note\",\"1542\":\"949\",\"1543\":\"950\",\"1544\":\"950#note\",\"1545\":\"951\",\"1546\":\"952\",\"1547\":\"953\",\"1548\":\"953#note\",\"1549\":\"954\",\"1550\":\"954#note\",\"1551\":\"955\",\"1552\":\"956\",\"1553\":\"957\",\"1554\":\"958\",\"1555\":\"959\",\"1556\":\"959#note\",\"1557\":\"960\",\"1558\":\"961\",\"1559\":\"962\",\"1560\":\"963\",\"1561\":\"964\",\"1562\":\"964#note\",\"1563\":\"965\",\"1564\":\"966\",\"1565\":\"966#note\",\"1566\":\"967\",\"1567\":\"968\",\"1568\":\"969\",\"1569\":\"970\",\"1570\":\"971\",\"1571\":\"972\",\"1572\":\"973\",\"1573\":\"974\",\"1574\":\"974#note\",\"1575\":\"975\",\"1576\":\"976\",\"1577\":\"977\",\"1578\":\"978\",\"1579\":\"979\",\"1580\":\"980\",\"1581\":\"981\",\"1582\":\"981#note\",\"1583\":\"982\",\"1584\":\"982#note\",\"1585\":\"983\",\"1586\":\"984\",\"1587\":\"984#note\",\"1588\":\"985\",\"1589\":\"985#note\",\"1590\":\"986\",\"1591\":\"986#note\",\"1592\":\"987\",\"1593\":\"987#note\",\"1594\":\"988\",\"1595\":\"989\",\"1596\":\"990\",\"1597\":\"990#note\",\"1598\":\"991\",\"1599\":\"991#note\",\"1600\":\"992\",\"1601\":\"993\",\"1602\":\"994\",\"1603\":\"995\",\"1604\":\"996\",\"1605\":\"997\",\"1606\":\"997#note\",\"1607\":\"998\",\"1608\":\"998#note\",\"1609\":\"999\",\"1610\":\"999#note\",\"1611\":\"1000\",\"1612\":\"1001\",\"1613\":\"1002\",\"1614\":\"1002#note\",\"1615\":\"1003\",\"1616\":\"1004\",\"1617\":\"1005\",\"1618\":\"1006\",\"1619\":\"1007\",\"1620\":\"1008\",\"1621\":\"1008#note\",\"1622\":\"1009\",\"1623\":\"1010\",\"1624\":\"1011\",\"1625\":\"1011#note\",\"1626\":\"1012\",\"1627\":\"1013\",\"1628\":\"1013#note\",\"1629\":\"1014\",\"1630\":\"1014#note\",\"1631\":\"1015\",\"1632\":\"1015#note\",\"1633\":\"1016\",\"1634\":\"1016#note\",\"1635\":\"1017\",\"1636\":\"1017#note\",\"1637\":\"1018\",\"1638\":\"1019\",\"1639\":\"1020\",\"1640\":\"1021\",\"1641\":\"1022\",\"1642\":\"1022#note\",\"1643\":\"1023\",\"1644\":\"1024\",\"1645\":\"1025\",\"1646\":\"1026\",\"1647\":\"1026#note\",\"1648\":\"1027\",\"1649\":\"1027#note\",\"1650\":\"1028\",\"1651\":\"1028#note\",\"1652\":\"1029\",\"1653\":\"1029#note\",\"1654\":\"1030\",\"1655\":\"1031\",\"1656\":\"1032\",\"1657\":\"1032#note\",\"1658\":\"1033\",\"1659\":\"1034\",\"1660\":\"1035\",\"1661\":\"1036\",\"1662\":\"1037\",\"1663\":\"1038\",\"1664\":\"1039\",\"1665\":\"1040\",\"1666\":\"1041\",\"1667\":\"1042\",\"1668\":\"1043\",\"1669\":\"1044\",\"1670\":\"1045\",\"1671\":\"1046\",\"1672\":\"1047\",\"1673\":\"1047#note\",\"1674\":\"1048\",\"1675\":\"1048#note\",\"1676\":\"1049\",\"1677\":\"1049#note\",\"1678\":\"1050\",\"1679\":\"1051\",\"1680\":\"1052\",\"1681\":\"1053\",\"1682\":\"1054\",\"1683\":\"1055\",\"1684\":\"1056\",\"1685\":\"1057\",\"1686\":\"1058\",\"1687\":\"1059\",\"1688\":\"1060\",\"1689\":\"1061\",\"1690\":\"1062\",\"1691\":\"1063\",\"1692\":\"1064\",\"1693\":\"1065\",\"1694\":\"1066\",\"1695\":\"1067\",\"1696\":\"1068\",\"1697\":\"1069\",\"1698\":\"1070\",\"1699\":\"1071\",\"1700\":\"1072\",\"1701\":\"1073\",\"1702\":\"1074\",\"1703\":\"1075\",\"1704\":\"1076\",\"1705\":\"1077\",\"1706\":\"1078\",\"1707\":\"1079\",\"1708\":\"1080\",\"1709\":\"1081\",\"1710\":\"1082\",\"1711\":\"1083\",\"1712\":\"1084\",\"1713\":\"1085\",\"1714\":\"1086\",\"1715\":\"1087\",\"1716\":\"1088\",\"1717\":\"1089\",\"1718\":\"1090\",\"1719\":\"1091\",\"1720\":\"1092\",\"1721\":\"1093\",\"1722\":\"1094\",\"1723\":\"1095\",\"1724\":\"1096\",\"1725\":\"1097\",\"1726\":\"1098\",\"1727\":\"1099\",\"1728\":\"1100\",\"1729\":\"1101\",\"1730\":\"1102\",\"1731\":\"1103\",\"1732\":\"1104\",\"1733\":\"1105\",\"1734\":\"1106\",\"1735\":\"1107\",\"1736\":\"1108\",\"1737\":\"1109\",\"1738\":\"1110\",\"1739\":\"1111\",\"1740\":\"1112\",\"1741\":\"1113\",\"1742\":\"1114\",\"1743\":\"1115\",\"1744\":\"1116\",\"1745\":\"1117\",\"1746\":\"1118\",\"1747\":\"1119\",\"1748\":\"1120\",\"1749\":\"1121\",\"1750\":\"1122\",\"1751\":\"1123\",\"1752\":\"1124\",\"1753\":\"1125\",\"1754\":\"1126\",\"1755\":\"1127\",\"1756\":\"1128\",\"1757\":\"1129\",\"1758\":\"1130\",\"1759\":\"1131\",\"1760\":\"1132\",\"1761\":\"1133\",\"1762\":\"1133#note\",\"1763\":\"1134\",\"1764\":\"1134#note\",\"1765\":\"1135\",\"1766\":\"1136\",\"1767\":\"1137\",\"1768\":\"1138\",\"1769\":\"1139\",\"1770\":\"1139#note\",\"1771\":\"1140\",\"1772\":\"1141\",\"1773\":\"1142\",\"1774\":\"1143\",\"1775\":\"1143#note\",\"1776\":\"1144\",\"1777\":\"1145\",\"1778\":\"1146\",\"1779\":\"1147\",\"1780\":\"1147#note\",\"1781\":\"1148\",\"1782\":\"1149\",\"1783\":\"1149#note\",\"1784\":\"1150\",\"1785\":\"1151\",\"1786\":\"1152\",\"1787\":\"1153\",\"1788\":\"1154\",\"1789\":\"1155\",\"1790\":\"1155#note\",\"1791\":\"1156\",\"1792\":\"1156#note\",\"1793\":\"1157\",\"1794\":\"1157#note\",\"1795\":\"1158\",\"1796\":\"1158#note\",\"1797\":\"1159\",\"1798\":\"1160\",\"1799\":\"1161\",\"1800\":\"1162\",\"1801\":\"1163\",\"1802\":\"1163#note\",\"1803\":\"1164\",\"1804\":\"1165\",\"1805\":\"1166\",\"1806\":\"1167\",\"1807\":\"1167#note\",\"1808\":\"1168\",\"1809\":\"1169\",\"1810\":\"1170\",\"1811\":\"1171\",\"1812\":\"1172\",\"1813\":\"1173\",\"1814\":\"1174\",\"1815\":\"1175\",\"1816\":\"1176\",\"1817\":\"1177\",\"1818\":\"1178\",\"1819\":\"1179\",\"1820\":\"1180\",\"1821\":\"1181\",\"1822\":\"1182\",\"1823\":\"1183\",\"1824\":\"1184\",\"1825\":\"1185\",\"1826\":\"1186\",\"1827\":\"1187\",\"1828\":\"1188\",\"1829\":\"1189\",\"1830\":\"1190\",\"1831\":\"1191\",\"1832\":\"1192\",\"1833\":\"1193\",\"1834\":\"1194\",\"1835\":\"1195\",\"1836\":\"1196\",\"1837\":\"1197\",\"1838\":\"1198\",\"1839\":\"1199\",\"1840\":\"1200\",\"1841\":\"1201\",\"1842\":\"1202\",\"1843\":\"1203\",\"1844\":\"1204\",\"1845\":\"1205\",\"1846\":\"1206\",\"1847\":\"1207\",\"1848\":\"1208\",\"1849\":\"1209\",\"1850\":\"1210\",\"1851\":\"1211\",\"1852\":\"1212\",\"1853\":\"1213\",\"1854\":\"1214\",\"1855\":\"1215\",\"1856\":\"1216\",\"1857\":\"1217\",\"1858\":\"1218\",\"1859\":\"1219\",\"1860\":\"1220\",\"1861\":\"1221\",\"1862\":\"1222\",\"1863\":\"1223\",\"1864\":\"1224\",\"1865\":\"1225\",\"1866\":\"1226\",\"1867\":\"1227\",\"1868\":\"1228\",\"1869\":\"1229\",\"1870\":\"1230\",\"1871\":\"1231\",\"1872\":\"1232\",\"1873\":\"1233\",\"1874\":\"1234\",\"1875\":\"1235\",\"1876\":\"1236\",\"1877\":\"1237\",\"1878\":\"1238\",\"1879\":\"1239\",\"1880\":\"1240\",\"1881\":\"1241\",\"1882\":\"1242\",\"1883\":\"1243\",\"1884\":\"1244\",\"1885\":\"1245\",\"1886\":\"1246\",\"1887\":\"1247\",\"1888\":\"1248\",\"1889\":\"1249\",\"1890\":\"1250\",\"1891\":\"1250#note\",\"1892\":\"1251\",\"1893\":\"1252\",\"1894\":\"1253\",\"1895\":\"1254\",\"1896\":\"1255\",\"1897\":\"1255#examples\",\"1898\":\"1256\",\"1899\":\"1257\",\"1900\":\"1258\",\"1901\":\"1259\",\"1902\":\"1260\",\"1903\":\"1260#note\",\"1904\":\"1261\",\"1905\":\"1262\",\"1906\":\"1263\",\"1907\":\"1264\",\"1908\":\"1264#note\",\"1909\":\"1265\",\"1910\":\"1266\",\"1911\":\"1267\",\"1912\":\"1268\",\"1913\":\"1268#note\",\"1914\":\"1269\",\"1915\":\"1270\",\"1916\":\"1271\",\"1917\":\"1272\",\"1918\":\"1273\",\"1919\":\"1274\",\"1920\":\"1275\",\"1921\":\"1276\",\"1922\":\"1277\",\"1923\":\"1278\",\"1924\":\"1279\",\"1925\":\"1280\",\"1926\":\"1281\",\"1927\":\"1281#note\",\"1928\":\"1282\",\"1929\":\"1283\",\"1930\":\"1284\",\"1931\":\"1285\",\"1932\":\"1286\",\"1933\":\"1287\",\"1934\":\"1288\",\"1935\":\"1289\",\"1936\":\"1290\",\"1937\":\"1291\",\"1938\":\"1292\",\"1939\":\"1293\",\"1940\":\"1294\",\"1941\":\"1295\",\"1942\":\"1296\",\"1943\":\"1297\",\"1944\":\"1298\",\"1945\":\"1299\",\"1946\":\"1300\",\"1947\":\"1301\",\"1948\":\"1302\",\"1949\":\"1303\",\"1950\":\"1304\",\"1951\":\"1305\",\"1952\":\"1305#note\",\"1953\":\"1306\",\"1954\":\"1306#note\",\"1955\":\"1307\",\"1956\":\"1307#note\",\"1957\":\"1308\",\"1958\":\"1309\",\"1959\":\"1309#note\",\"1960\":\"1310\",\"1961\":\"1311\",\"1962\":\"1312\",\"1963\":\"1313\",\"1964\":\"1314\",\"1965\":\"1315\",\"1966\":\"1316\",\"1967\":\"1317\",\"1968\":\"1318\",\"1969\":\"1318#examples\",\"1970\":\"1319\",\"1971\":\"1320\",\"1972\":\"1321\",\"1973\":\"1322\",\"1974\":\"1323\",\"1975\":\"1324\",\"1976\":\"1325\",\"1977\":\"1325#note\",\"1978\":\"1326\",\"1979\":\"1326#note\",\"1980\":\"1327\",\"1981\":\"1328\",\"1982\":\"1328#note\",\"1983\":\"1329\",\"1984\":\"1330\",\"1985\":\"1330#note\",\"1986\":\"1331\",\"1987\":\"1332\",\"1988\":\"1332#note\",\"1989\":\"1333\",\"1990\":\"1333#note\",\"1991\":\"1334\",\"1992\":\"1334#note\",\"1993\":\"1335\",\"1994\":\"1336\",\"1995\":\"1336#note\",\"1996\":\"1337\",\"1997\":\"1338\",\"1998\":\"1338#note\",\"1999\":\"1339\",\"2000\":\"1340\",\"2001\":\"1341\",\"2002\":\"1342\",\"2003\":\"1343\",\"2004\":\"1344\",\"2005\":\"1345\",\"2006\":\"1346\",\"2007\":\"1347\",\"2008\":\"1348\",\"2009\":\"1349\",\"2010\":\"1350\",\"2011\":\"1351\",\"2012\":\"1352\",\"2013\":\"1353\",\"2014\":\"1354\",\"2015\":\"1355\",\"2016\":\"1356\",\"2017\":\"1357\",\"2018\":\"1358\",\"2019\":\"1359\",\"2020\":\"1360\",\"2021\":\"1361\",\"2022\":\"1362\",\"2023\":\"1363\",\"2024\":\"1364\",\"2025\":\"1364#note\",\"2026\":\"1365\",\"2027\":\"1366\",\"2028\":\"1367\",\"2029\":\"1368\",\"2030\":\"1369\",\"2031\":\"1369#note\",\"2032\":\"1370\",\"2033\":\"1370#note\",\"2034\":\"1371\",\"2035\":\"1371#note\",\"2036\":\"1372\",\"2037\":\"1372#note\",\"2038\":\"1373\",\"2039\":\"1373#note\",\"2040\":\"1374\",\"2041\":\"1374#note\",\"2042\":\"1375\",\"2043\":\"1375#note\",\"2044\":\"1376\",\"2045\":\"1376#note\",\"2046\":\"1377\",\"2047\":\"1378\",\"2048\":\"1378#note\",\"2049\":\"1379\",\"2050\":\"1380\",\"2051\":\"1380#note\",\"2052\":\"1381\",\"2053\":\"1381#note\",\"2054\":\"1382\",\"2055\":\"1383\",\"2056\":\"1383#note\",\"2057\":\"1384\",\"2058\":\"1384#note\",\"2059\":\"1385\",\"2060\":\"1385#note\",\"2061\":\"1386\",\"2062\":\"1386#note\",\"2063\":\"1387\",\"2064\":\"1388\",\"2065\":\"1388#note\",\"2066\":\"1389\",\"2067\":\"1389#note\",\"2068\":\"1390\",\"2069\":\"1390#note\",\"2070\":\"1391\",\"2071\":\"1391#note\",\"2072\":\"1392\",\"2073\":\"1392#note\",\"2074\":\"1393\",\"2075\":\"1394\",\"2076\":\"1395\",\"2077\":\"1396\",\"2078\":\"1397\",\"2079\":\"1397#note\",\"2080\":\"1397#note-1\",\"2081\":\"1398\",\"2082\":\"1399\",\"2083\":\"1400\",\"2084\":\"1401\",\"2085\":\"1402\",\"2086\":\"1403\",\"2087\":\"1404\",\"2088\":\"1405\",\"2089\":\"1406\",\"2090\":\"1407\",\"2091\":\"1408\",\"2092\":\"1409\",\"2093\":\"1410\",\"2094\":\"1411\",\"2095\":\"1412\",\"2096\":\"1413\",\"2097\":\"1414\",\"2098\":\"1415\",\"2099\":\"1416\",\"2100\":\"1417\",\"2101\":\"1418\",\"2102\":\"1419\",\"2103\":\"1420\",\"2104\":\"1421\",\"2105\":\"1422\",\"2106\":\"1423\",\"2107\":\"1424\",\"2108\":\"1425\",\"2109\":\"1426\",\"2110\":\"1427\",\"2111\":\"1428\",\"2112\":\"1429\",\"2113\":\"1430\",\"2114\":\"1431\",\"2115\":\"1432\",\"2116\":\"1433\",\"2117\":\"1434\",\"2118\":\"1435\",\"2119\":\"1436\",\"2120\":\"1437\",\"2121\":\"1438\",\"2122\":\"1439\",\"2123\":\"1440\",\"2124\":\"1441\",\"2125\":\"1442\",\"2126\":\"1443\",\"2127\":\"1444\",\"2128\":\"1445\",\"2129\":\"1446\",\"2130\":\"1447\",\"2131\":\"1448\",\"2132\":\"1449\",\"2133\":\"1450\",\"2134\":\"1450#examples\",\"2135\":\"1451\",\"2136\":\"1452\",\"2137\":\"1453\",\"2138\":\"1454\",\"2139\":\"1455\",\"2140\":\"1456\",\"2141\":\"1457\",\"2142\":\"1458\",\"2143\":\"1458#examples\",\"2144\":\"1459\",\"2145\":\"1460\",\"2146\":\"1461\",\"2147\":\"1462\",\"2148\":\"1463\",\"2149\":\"1463#examples\",\"2150\":\"1463#note\",\"2151\":\"1464\",\"2152\":\"1465\",\"2153\":\"1466\",\"2154\":\"1467\",\"2155\":\"1468\",\"2156\":\"1469\",\"2157\":\"1470\",\"2158\":\"1470#examples\",\"2159\":\"1471\",\"2160\":\"1472\",\"2161\":\"1473\",\"2162\":\"1474\",\"2163\":\"1475\",\"2164\":\"1476\",\"2165\":\"1477\",\"2166\":\"1478\",\"2167\":\"1479\",\"2168\":\"1480\",\"2169\":\"1480#note\",\"2170\":\"1481\",\"2171\":\"1482\",\"2172\":\"1483\",\"2173\":\"1484\",\"2174\":\"1485\",\"2175\":\"1486\",\"2176\":\"1487\",\"2177\":\"1488\",\"2178\":\"1489\",\"2179\":\"1490\",\"2180\":\"1491\",\"2181\":\"1492\",\"2182\":\"1493\",\"2183\":\"1493#examples\",\"2184\":\"1494\",\"2185\":\"1495\",\"2186\":\"1496\",\"2187\":\"1497\",\"2188\":\"1498\",\"2189\":\"1499\",\"2190\":\"1499#examples\",\"2191\":\"1500\",\"2192\":\"1501\",\"2193\":\"1502\",\"2194\":\"1503\",\"2195\":\"1504\",\"2196\":\"1505\",\"2197\":\"1506\",\"2198\":\"1507\",\"2199\":\"1508\",\"2200\":\"1509\",\"2201\":\"1510\",\"2202\":\"1511\",\"2203\":\"1512\",\"2204\":\"1513\",\"2205\":\"1514\",\"2206\":\"1515\",\"2207\":\"1516\",\"2208\":\"1517\",\"2209\":\"1517#examples\",\"2210\":\"1518\",\"2211\":\"1519\",\"2212\":\"1520\",\"2213\":\"1521\",\"2214\":\"1522\",\"2215\":\"1523\",\"2216\":\"1524\",\"2217\":\"1525\",\"2218\":\"1526\",\"2219\":\"1527\",\"2220\":\"1528\",\"2221\":\"1529\",\"2222\":\"1530\",\"2223\":\"1531\",\"2224\":\"1532\",\"2225\":\"1533\",\"2226\":\"1534\",\"2227\":\"1535\",\"2228\":\"1536\",\"2229\":\"1537\",\"2230\":\"1538\",\"2231\":\"1539\",\"2232\":\"1540\",\"2233\":\"1541\",\"2234\":\"1541#note\",\"2235\":\"1542\",\"2236\":\"1543\",\"2237\":\"1543#note\",\"2238\":\"1543#note-1\",\"2239\":\"1544\",\"2240\":\"1545\",\"2241\":\"1546\",\"2242\":\"1546#note\",\"2243\":\"1547\",\"2244\":\"1548\",\"2245\":\"1549\",\"2246\":\"1550\",\"2247\":\"1550#note\",\"2248\":\"1551\",\"2249\":\"1551#note\",\"2250\":\"1552\",\"2251\":\"1552#note\",\"2252\":\"1553\",\"2253\":\"1554\",\"2254\":\"1555\",\"2255\":\"1556\",\"2256\":\"1557\",\"2257\":\"1558\",\"2258\":\"1559\",\"2259\":\"1560\",\"2260\":\"1561\",\"2261\":\"1562\",\"2262\":\"1563\",\"2263\":\"1564\",\"2264\":\"1565\",\"2265\":\"1566\",\"2266\":\"1567\",\"2267\":\"1567#note\",\"2268\":\"1568\",\"2269\":\"1569\",\"2270\":\"1570\",\"2271\":\"1571\",\"2272\":\"1572\",\"2273\":\"1573\",\"2274\":\"1574\",\"2275\":\"1575\",\"2276\":\"1575#note\",\"2277\":\"1576\",\"2278\":\"1577\",\"2279\":\"1578\",\"2280\":\"1579\",\"2281\":\"1580\",\"2282\":\"1580#note\",\"2283\":\"1581\",\"2284\":\"1581#note\",\"2285\":\"1582\",\"2286\":\"1582#note\",\"2287\":\"1583\",\"2288\":\"1584\",\"2289\":\"1584#note\",\"2290\":\"1585\",\"2291\":\"1585#note\",\"2292\":\"1586\",\"2293\":\"1586#note\",\"2294\":\"1587\",\"2295\":\"1588\",\"2296\":\"1589\",\"2297\":\"1590\",\"2298\":\"1590#note\",\"2299\":\"1591\",\"2300\":\"1591#note\",\"2301\":\"1592\",\"2302\":\"1593\",\"2303\":\"1594\",\"2304\":\"1595\",\"2305\":\"1596\",\"2306\":\"1597\",\"2307\":\"1598\",\"2308\":\"1599\",\"2309\":\"1600\",\"2310\":\"1601\",\"2311\":\"1602\",\"2312\":\"1603\",\"2313\":\"1604\",\"2314\":\"1604#examples\",\"2315\":\"1605\",\"2316\":\"1606\",\"2317\":\"1607\",\"2318\":\"1608\",\"2319\":\"1609\",\"2320\":\"1609#examples\",\"2321\":\"1610\",\"2322\":\"1611\",\"2323\":\"1611#examples\",\"2324\":\"1612\",\"2325\":\"1613\",\"2326\":\"1614\",\"2327\":\"1615\",\"2328\":\"1615#examples\",\"2329\":\"1616\",\"2330\":\"1617\",\"2331\":\"1618\",\"2332\":\"1619\",\"2333\":\"1620\",\"2334\":\"1621\",\"2335\":\"1621#examples\",\"2336\":\"1622\",\"2337\":\"1622#examples\",\"2338\":\"1623\",\"2339\":\"1624\",\"2340\":\"1624#examples\",\"2341\":\"1625\",\"2342\":\"1626\",\"2343\":\"1627\",\"2344\":\"1628\",\"2345\":\"1629\",\"2346\":\"1630\",\"2347\":\"1631\",\"2348\":\"1632\",\"2349\":\"1633\",\"2350\":\"1634\",\"2351\":\"1635\",\"2352\":\"1636\",\"2353\":\"1637\",\"2354\":\"1638\",\"2355\":\"1638#useful-links\",\"2356\":\"1638#speech-recognition-demo\",\"2357\":\"1638#model-selection\",\"2358\":\"1638#model-setup\",\"2359\":\"1638#recognize-our-examples-of-pre-recorded-samples\",\"2360\":\"1638#recognize-your-own-live-recordings\",\"2361\":\"1638#speech-synthesis-demo\",\"2362\":\"1638#installation\",\"2363\":\"1638#model-selection-1\",\"2364\":\"1638#model-setup-1\",\"2365\":\"1638#synthesis\",\"2366\":\"1638#speech-enhancement-demo\",\"2367\":\"1638#single-channel-enhancement-the-chime-example\",\"2368\":\"1638#download-and-load-the-pretrained-conv-tasnet\",\"2369\":\"1638#enhance-the-single-channel-real-noisy-speech-in-chime4\",\"2370\":\"1638#speech-separation\",\"2371\":\"1638#model-selection-2\",\"2372\":\"1638#separate-the-example-in-wsj0-2mix-testing-set\",\"2373\":\"1638#data-preparation\",\"2374\":\"1638#language-modeling-skip-in-this-tutorial\",\"2375\":\"1638#end-to-end-asr\",\"2376\":\"1638#how-to-change-the-training-configs\",\"2377\":\"1638#config-file-based\",\"2378\":\"1638#command-line-argument-based\",\"2379\":\"1638#how-to-make-a-new-recipe\",\"2380\":\"1639\",\"2381\":\"1639#objectives\",\"2382\":\"1639#useful-links\",\"2383\":\"1639#download-espnet\",\"2384\":\"1639#setup-python-environment-based-on-anaconda\",\"2385\":\"1639#data-preparation-for-an4\",\"2386\":\"1639#how-to-read-file-in-pipe\",\"2387\":\"1639#data-preparation-for-totonac\",\"2388\":\"1640\",\"2389\":\"1640#❗important-notes❗\",\"2390\":\"1640#useful-links\",\"2391\":\"1640#objectives\",\"2392\":\"1640#function-to-print-date-and-time\",\"2393\":\"1640#download-espnet\",\"2394\":\"1640#setup-python-environment-based-on-anaconda-install-espnet\",\"2395\":\"1640#asvspoof-data-preparation\",\"2396\":\"1640#download-dataset\",\"2397\":\"1640#prepare-data-stage1-stage2\",\"2398\":\"1640#asvspoof-collect-stats-✅-checkpoint-1-0-5-point\",\"2399\":\"1640#asvspoof-model\",\"2400\":\"1640#encoder-✅-checkpoint-2-0-5-point\",\"2401\":\"1640#decoder-✅-checkpoint-3-0-5-point\",\"2402\":\"1640#model-inference\",\"2403\":\"1640#✅-checkpoint-4-0-5-point\",\"2404\":\"1640#scoring\",\"2405\":\"1640#✅-checkpoint-5-0-5-point\",\"2406\":\"1641\",\"2407\":\"1641#objectives\",\"2408\":\"1641#❗important-notes❗\",\"2409\":\"1641#espnet-installation\",\"2410\":\"1641#speaker-recognition\",\"2411\":\"1641#dataset\",\"2412\":\"1641#data-preparation\",\"2413\":\"1641#data-preprocessing\",\"2414\":\"1641#question1-✅-checkpoint-1-1-point\",\"2415\":\"1641#use-pre-trained-speaker-representation\",\"2416\":\"1641#extract-speaker-embedding-from-speechbrain\",\"2417\":\"1641#training-for-speaker-recognition\",\"2418\":\"1641#question2-✅-checkpoint-2-0-5-point\",\"2419\":\"1641#question3-✅-checkpoint-3-0-5-point\",\"2420\":\"1641#question4-✅-checkpoint-4-0-5-point\",\"2421\":\"1642\",\"2422\":\"1642#❗important-notes❗\",\"2423\":\"1642#objectives\",\"2424\":\"1642#useful-links\",\"2425\":\"1642#function-to-print-date-and-time\",\"2426\":\"1642#check-gpu-type\",\"2427\":\"1642#download-espnet\",\"2428\":\"1642#setup-python-environment-based-on-anaconda\",\"2429\":\"1642#install-espnet-same-procedure-as-your-first-tutorial\",\"2430\":\"1642#data-preparation\",\"2431\":\"1642#⭕-ssl-stage-3-5-extract-ssl-features\",\"2432\":\"1642#📗-check-the-shape-of-dumped-feature-1-0-pt\",\"2433\":\"1642#⭕-ssl-stage-3-format-feats-scp-data-dump-extracted\",\"2434\":\"1642#language-modeling-skipped-in-this-tutorial\",\"2435\":\"1642#how-to-change-the-configs\",\"2436\":\"1642#config-file-based\",\"2437\":\"1642#command-line-argument-based\",\"2438\":\"1642#📗-exercise-1\",\"2439\":\"1642#⭕-ssl-config-modifications\",\"2440\":\"1642#⭕-ssl-normalization\",\"2441\":\"1642#📗-questions\",\"2442\":\"1642#environments\",\"2443\":\"1642#exp-asr-train-asr-demo-branchformer-extracted-bpe30\",\"2444\":\"1642#wer\",\"2445\":\"1642#cer\",\"2446\":\"1642#ter\",\"2447\":\"1643\",\"2448\":\"1643#objectives\",\"2449\":\"1643#❗important-notes❗\",\"2450\":\"1643#espnet-installation-inference-vesion\",\"2451\":\"1643#speech-translation\",\"2452\":\"1643#overview-of-the-espnet-st-v2\",\"2453\":\"1643#_1-offline-speech-to-text-translation-st\",\"2454\":\"1643#_1-1-model-download\",\"2455\":\"1643#_1-2-model-setup\",\"2456\":\"1643#_1-3-translate-our-example-recordings\",\"2457\":\"1643#task1-✅-checkpoint-1-2-point\",\"2458\":\"1643#_1-4-translate-your-own-live-recordings\",\"2459\":\"1643#task2-✅-checkpoint-2-1-point\",\"2460\":\"1643#_2-simultaneous-speech-to-text-translation-sst\",\"2461\":\"1643#question3-✅-checkpoint-3-1-point\",\"2462\":\"1643#question4-✅-checkpoint-4-1-point\",\"2463\":\"1644\",\"2464\":\"1644#objectives\",\"2465\":\"1644#❗important-notes❗\",\"2466\":\"1644#espnet-installation\",\"2467\":\"1644#spoken-language-understanding\",\"2468\":\"1644#overview-of-the-espnet-slu\",\"2469\":\"1644#_1-e2e-slu\",\"2470\":\"1644#_1-1-download-sample-audio-file\",\"2471\":\"1644#question1-✅-checkpoint-1-1-points\",\"2472\":\"1644#_1-2-download-and-load-pretrained-e2e-slu-model\",\"2473\":\"1644#_2-two-pass-e2e-slu\",\"2474\":\"1644#question2-✅-checkpoint-2-1-points\",\"2475\":\"1644#_3-e2e-slu-for-slot-filling\",\"2476\":\"1644#question3-✅-checkpoint-3-1-point\",\"2477\":\"1644#_4-e2e-slu-for-sentiment-analysis\",\"2478\":\"1644#question4-✅-checkpoint-4-1-point\",\"2479\":\"1644#question5-✅-checkpoint-5-1-point\",\"2480\":\"1645\",\"2481\":\"1645#❗important-notes❗\",\"2482\":\"1645#install\",\"2483\":\"1645#speech-enhancement-with-pretrained-models\",\"2484\":\"1645#single-channel-enhancement-the-chime-example\",\"2485\":\"1645#task1-✅-checkpoint-1-1-point\",\"2486\":\"1645#download-and-load-the-pretrained-conv-tasnet\",\"2487\":\"1645#enhance-the-single-channel-real-noisy-speech-in-chime4\",\"2488\":\"1645#multi-channel-enhancement\",\"2489\":\"1645#download-and-load-the-pretrained-mvdr-neural-beamformer\",\"2490\":\"1645#task2-✅-checkpoint-2-1-point\",\"2491\":\"1645#enhance-the-multi-channel-real-noisy-speech-in-chime4\",\"2492\":\"1645#portable-speech-enhancement-scripts-for-other-tasks\",\"2493\":\"1645#speech-separation\",\"2494\":\"1645#model-selection\",\"2495\":\"1645#separate-speech-mixture\",\"2496\":\"1645#separate-the-example-in-wsj0-2mix-testing-set\",\"2497\":\"1645#task3-✅-checkpoint-3-1-point\",\"2498\":\"1645#show-spectrums-of-separated-speech\",\"2499\":\"1645#evaluate-separated-speech-with-pretrained-asr-model\",\"2500\":\"1645#task4-✅-checkpoint-4-1-point\",\"2501\":\"1645#task5-✅-checkpoint-5-1-point\",\"2502\":\"1646\",\"2503\":\"1646#❗important-notes❗\",\"2504\":\"1646#installation\",\"2505\":\"1646#single-speaker-tts-model-demo\",\"2506\":\"1646#tts-model\",\"2507\":\"1646#model-setup\",\"2508\":\"1646#synthesis-✅-checkpoint-1-2-point\",\"2509\":\"1646#tts-model-selection\",\"2510\":\"1646#question2-✅-checkpoint-2-1-point\",\"2511\":\"1646#multi-speaker-model-demo\",\"2512\":\"1646#model-selection\",\"2513\":\"1646#model-setup-1\",\"2514\":\"1646#speaker-selection\",\"2515\":\"1646#synthesis-✅-checkpoint3-2-point\",\"2516\":\"1647\",\"2517\":\"1647#setup-envrionments\",\"2518\":\"1647#offline-speech-to-speech-translation-s2st\",\"2519\":\"1647#model-download\",\"2520\":\"1647#model-setup\",\"2521\":\"1647#translate-our-example-recordings\",\"2522\":\"1647#translate-your-own-pre-recordings\",\"2523\":\"1647#translate-your-own-live-recordings\",\"2524\":\"1648\",\"2525\":\"1648#❗important-notes❗\",\"2526\":\"1648#useful-links\",\"2527\":\"1648#objectives\",\"2528\":\"1648#function-to-print-date-and-time\",\"2529\":\"1648#download-espnet\",\"2530\":\"1648#setup-python-environment-based-on-anaconda-install-espnet\",\"2531\":\"1648#asvspoof-data-preparation\",\"2532\":\"1648#download-dataset\",\"2533\":\"1648#prepare-data-stage1-stage2\",\"2534\":\"1648#asvspoof-collect-stats-✅-checkpint-1-1-point\",\"2535\":\"1648#asvspoof-model\",\"2536\":\"1648#encoder-✅-checkpint-2-1-point\",\"2537\":\"1648#decoder-✅-checkpint-3-1-point\",\"2538\":\"1648#model-inference\",\"2539\":\"1648#✅-checkpint-4-1-point\",\"2540\":\"1648#scoring\",\"2541\":\"1648#✅-checkpint-5-1-point\",\"2542\":\"1648#📗-exercise-1-1-point-bonus\",\"2543\":\"1648#📗-exercise-2-1-point-bonus\",\"2544\":\"1649\",\"2545\":\"1649#❗important-notes❗\",\"2546\":\"1649#objectives\",\"2547\":\"1649#useful-links\",\"2548\":\"1649#function-to-print-date-and-time\",\"2549\":\"1649#check-gpu-type\",\"2550\":\"1649#download-espnet\",\"2551\":\"1649#setup-python-environment-based-on-anaconda\",\"2552\":\"1649#install-espnet-1\",\"2553\":\"1649#check-installation\",\"2554\":\"1649#✅-checkpoint-1-1-point\",\"2555\":\"1649#data-preparation\",\"2556\":\"1649#✅-checkpint-2-1-point\",\"2557\":\"1649#language-modeling-skipped-in-this-tutorial\",\"2558\":\"1649#end-to-end-asr\",\"2559\":\"1649#✅-checkpoint-3-1-point\",\"2560\":\"1649#✅-checkpoint-4-1-point\",\"2561\":\"1649#how-to-change-the-configs\",\"2562\":\"1649#config-file-based\",\"2563\":\"1649#command-line-argument-based\",\"2564\":\"1649#📗-exercise-1-1-point-bonus\",\"2565\":\"1649#📗-exercise-2-1-point\",\"2566\":\"1649#_1-create-a-new-directory\",\"2567\":\"1649#_2-download-data\",\"2568\":\"1649#_3-finish-the-script-for-data-preparation\",\"2569\":\"1649#_4-create-a-script-as-the-entry-point\",\"2570\":\"1649#_5-create-the-training-config\",\"2571\":\"1649#_6-execute-the-script\",\"2572\":\"1649#_7-print-the-results\",\"2573\":\"1649#fine-tune-a-pre-trained-model\",\"2574\":\"1649#use-self-supervised-pre-trained-models-as-the-front-end\",\"2575\":\"1649#contribute-to-espnet\",\"2576\":\"1650\",\"2577\":\"1650#asr-model-demo\",\"2578\":\"1650#model-selection\",\"2579\":\"1650#model-setup\",\"2580\":\"1650#recognize-our-example-recordings\",\"2581\":\"1650#recognize-your-own-pre-recordings\",\"2582\":\"1650#recognize-your-own-live-recordings\",\"2583\":\"1651\",\"2584\":\"1651#why-using-such-pre-trained-models\",\"2585\":\"1651#use-a-trained-model-from-espnet-repository-on-huggingface\",\"2586\":\"1652\",\"2587\":\"1652#train-a-streaming-transformer-model\",\"2588\":\"1652#download-pre-trained-model-and-audio-file-for-demo\",\"2589\":\"1652#for-mandarin-task-pretrained-using-aishell-1\",\"2590\":\"1652#for-english-task-pretrained-using-tedlium2\",\"2591\":\"1652#import-packages\",\"2592\":\"1652#prepare-for-inference\",\"2593\":\"1652#recognize-the-audio-file\",\"2594\":\"1652#recognize-the-speech-from-speaker\",\"2595\":\"1652#install-pyaudio\",\"2596\":\"1652#streamingly-recognize-with-pyaudio\",\"2597\":\"1653\",\"2598\":\"1653#table-of-contents\",\"2599\":\"1653#export-model-from-espnet-model-zoo\",\"2600\":\"1653#export-from-custom-model\",\"2601\":\"1654\",\"2602\":\"1654#install\",\"2603\":\"1654#speech-enhancement\",\"2604\":\"1654#single-channel-enhancement-the-chime-example\",\"2605\":\"1654#download-and-load-the-pretrained-conv-tasnet\",\"2606\":\"1654#enhance-the-single-channel-real-noisy-speech-in-chime4\",\"2607\":\"1654#enhance-your-own-pre-recordings\",\"2608\":\"1654#multi-channel-enhancement\",\"2609\":\"1654#download-and-load-the-pretrained-mvdr-neural-beamformer\",\"2610\":\"1654#enhance-the-multi-channel-real-noisy-speech-in-chime4\",\"2611\":\"1654#speech-separation\",\"2612\":\"1654#model-selection\",\"2613\":\"1654#separate-speech-mixture\",\"2614\":\"1654#separate-the-example-in-wsj0-2mix-testing-set\",\"2615\":\"1654#separate-your-own-recordings\",\"2616\":\"1654#show-spectrums-of-separated-speech\",\"2617\":\"1654#evluate-separated-speech-with-pretrained-asr-model\",\"2618\":\"1655\",\"2619\":\"1655#install\",\"2620\":\"1655#speech-enhancement-with-pretrained-models\",\"2621\":\"1655#single-channel-enhancement-the-chime-example\",\"2622\":\"1655#download-and-load-the-pretrained-conv-tasnet\",\"2623\":\"1655#enhance-the-single-channel-real-noisy-speech-in-chime4\",\"2624\":\"1655#enhance-your-own-pre-recordings\",\"2625\":\"1655#multi-channel-enhancement\",\"2626\":\"1655#download-and-load-the-pretrained-mvdr-neural-beamformer\",\"2627\":\"1655#enhance-the-multi-channel-real-noisy-speech-in-chime4\",\"2628\":\"1655#portable-speech-enhancement-scripts-for-other-tasks\",\"2629\":\"1655#speech-separation\",\"2630\":\"1655#model-selection\",\"2631\":\"1655#separate-speech-mixture\",\"2632\":\"1655#separate-the-example-in-wsj0-2mix-testing-set\",\"2633\":\"1655#separate-your-own-recordings\",\"2634\":\"1655#show-spectrums-of-separated-speech\",\"2635\":\"1655#evluate-separated-speech-with-pretrained-asr-model\",\"2636\":\"1655#creating-a-new-recipe\",\"2637\":\"1655#step-1-create-recipe-directory\",\"2638\":\"1655#step-2-write-scripts-for-data-preparation\",\"2639\":\"1655#step-3-prepare-training-configuration\",\"2640\":\"1655#step-4-prepare-run-sh\",\"2641\":\"1655#implementing-a-new-speech-enhancement-separation-model\",\"2642\":\"1655#step-1-create-model-scripts\",\"2643\":\"1655#step-2-add-the-new-model-to-related-scripts\",\"2644\":\"1655#step-3-optional-create-new-loss-functions\",\"2645\":\"1655#step-4-create-unit-tests-for-the-new-model\",\"2646\":\"1656\",\"2647\":\"1656#download-audio-file\",\"2648\":\"1656#download-and-load-pretrained-first-pass-model\",\"2649\":\"1656#download-and-load-pretrained-second-pass-model\",\"2650\":\"1657\",\"2651\":\"1657#installation\",\"2652\":\"1657#single-speaker-model-demo\",\"2653\":\"1657#model-selection\",\"2654\":\"1657#model-setup\",\"2655\":\"1657#synthesis\",\"2656\":\"1657#multi-speaker-model-demo\",\"2657\":\"1657#model-selection-1\",\"2658\":\"1657#model-setup-1\",\"2659\":\"1657#speaker-selection\",\"2660\":\"1657#synthesis-1\",\"2661\":\"1658\",\"2662\":\"1659\",\"2663\":\"1660\",\"2664\":\"1661\",\"2665\":\"1662\",\"2666\":\"1663\",\"2667\":\"1664\",\"2668\":\"1665\",\"2669\":\"1666\",\"2670\":\"1667\",\"2671\":\"1668\",\"2672\":\"1669\",\"2673\":\"1670\",\"2674\":\"1671\",\"2675\":\"1672\",\"2676\":\"1673\",\"2677\":\"1674\",\"2678\":\"1675\",\"2679\":\"1676\",\"2680\":\"1677\",\"2681\":\"1678\",\"2682\":\"1679\",\"2683\":\"1680\",\"2684\":\"1681\",\"2685\":\"1682\",\"2686\":\"1683\",\"2687\":\"1684\",\"2688\":\"1685\",\"2689\":\"1686\",\"2690\":\"1687\",\"2691\":\"1688\",\"2692\":\"1689\",\"2693\":\"1690\",\"2694\":\"1691\",\"2695\":\"1692\",\"2696\":\"1693\",\"2697\":\"1694\",\"2698\":\"1695\",\"2699\":\"1696\",\"2700\":\"1697\",\"2701\":\"1698\",\"2702\":\"1699\",\"2703\":\"1700\",\"2704\":\"1701\",\"2705\":\"1702\",\"2706\":\"1703\",\"2707\":\"1704\",\"2708\":\"1705\",\"2709\":\"1706\",\"2710\":\"1707\",\"2711\":\"1708\",\"2712\":\"1709\",\"2713\":\"1710\",\"2714\":\"1711\",\"2715\":\"1712\",\"2716\":\"1713\",\"2717\":\"1714\",\"2718\":\"1715\",\"2719\":\"1716\",\"2720\":\"1717\",\"2721\":\"1718\",\"2722\":\"1719\",\"2723\":\"1720\",\"2724\":\"1721\",\"2725\":\"1722\",\"2726\":\"1723\",\"2727\":\"1724\",\"2728\":\"1725\",\"2729\":\"1726\",\"2730\":\"1727\",\"2731\":\"1728\",\"2732\":\"1729\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1],\"1\":[3,120],\"2\":[3,37],\"3\":[4,110],\"4\":[4,45],\"5\":[2,128],\"6\":[1,31],\"7\":[1,22],\"8\":[3,18],\"9\":[3],\"10\":[1,20],\"11\":[5,73],\"12\":[2,22],\"13\":[1,20],\"14\":[1,41],\"15\":[2,60],\"16\":[4,73],\"17\":[1,185],\"18\":[5,42],\"19\":[3,89],\"20\":[2,46],\"21\":[1,198],\"22\":[3,131],\"23\":[1,133],\"24\":[2,90],\"25\":[4,95],\"26\":[4,98],\"27\":[4,41],\"28\":[2,100],\"29\":[1,53],\"30\":[2,50],\"31\":[4,30],\"32\":[2,40],\"33\":[1,40],\"34\":[6,43],\"35\":[4,30],\"36\":[10,39],\"37\":[4,33],\"38\":[3,79],\"39\":[7,28],\"40\":[10,54],\"41\":[4,32],\"42\":[10,29],\"43\":[1,3],\"44\":[6,75],\"45\":[3,126],\"46\":[9,62],\"47\":[2,108],\"48\":[7,111],\"49\":[7,146],\"50\":[2],\"51\":[6,40],\"52\":[13,51],\"53\":[7,18],\"54\":[8,43],\"55\":[8],\"56\":[2,124],\"57\":[3,141],\"58\":[1,78],\"59\":[1,104],\"60\":[5,146],\"61\":[5],\"62\":[2,116],\"63\":[2,45],\"64\":[7,48],\"65\":[3,44],\"66\":[7,36],\"67\":[2,11],\"68\":[3,23],\"69\":[8,90],\"70\":[3,44],\"71\":[2,26],\"72\":[10,60],\"73\":[4,49],\"74\":[1,107],\"75\":[1,90],\"76\":[1,94],\"77\":[1,64],\"78\":[1,69],\"79\":[1,85],\"80\":[2,117],\"81\":[4,9],\"82\":[3,78],\"83\":[2,45],\"84\":[4,132],\"85\":[3,180],\"86\":[3],\"87\":[4,75],\"88\":[8,24],\"89\":[2,5],\"90\":[10,46],\"91\":[8,59],\"92\":[5,77],\"93\":[6,31],\"94\":[5,37],\"95\":[9,39],\"96\":[6,54],\"97\":[6,40],\"98\":[4,114],\"99\":[6,92],\"100\":[8,31],\"101\":[1,18],\"102\":[1,128],\"103\":[2,20],\"104\":[1,51],\"105\":[1,17],\"106\":[1,44],\"107\":[5,63],\"108\":[1,56],\"109\":[1,71],\"110\":[1,107],\"111\":[1,36],\"112\":[2,117],\"113\":[2,160],\"114\":[1,37],\"115\":[1,283],\"116\":[1,149],\"117\":[2,57],\"118\":[3,90],\"119\":[1,147],\"120\":[1,47],\"121\":[1,84],\"122\":[1,59],\"123\":[1],\"124\":[11,108],\"125\":[1],\"126\":[14,46],\"127\":[8,52],\"128\":[8,25],\"129\":[1],\"130\":[1,306],\"131\":[1],\"132\":[1,56],\"133\":[6,53],\"134\":[5,122],\"135\":[4,164],\"136\":[6,80],\"137\":[2,38],\"138\":[1,8],\"139\":[2,9],\"140\":[2,36],\"141\":[4,49],\"142\":[3,48],\"143\":[4,76],\"144\":[1,124],\"145\":[2],\"146\":[1,5],\"147\":[1,5],\"148\":[3,119],\"149\":[9,41],\"150\":[4,149],\"151\":[3],\"152\":[3,12],\"153\":[1],\"154\":[1],\"155\":[4,24],\"156\":[5,21],\"157\":[5,14],\"158\":[5,12],\"159\":[4,13],\"160\":[1],\"161\":[3,74],\"162\":[3,24],\"163\":[3,11],\"164\":[1,35],\"165\":[2,10],\"166\":[4,29],\"167\":[1,95],\"168\":[3,62],\"169\":[3,63],\"170\":[4,43],\"171\":[7,71],\"172\":[5,38],\"173\":[6,68],\"174\":[8,103],\"175\":[2,98],\"176\":[4,9],\"177\":[1,23],\"178\":[1,88],\"179\":[8,49],\"180\":[5,24],\"181\":[4,57],\"182\":[6,23],\"183\":[4,10],\"184\":[3,9],\"185\":[6,59],\"186\":[5,28],\"187\":[4,33],\"188\":[2,10],\"189\":[3,15],\"190\":[3,10],\"191\":[3,20],\"192\":[4,23],\"193\":[6,47],\"194\":[2,107],\"195\":[2,36],\"196\":[2,92],\"197\":[5,57],\"198\":[5,84],\"199\":[4,26],\"200\":[1,72],\"201\":[6,71],\"202\":[6,79],\"203\":[3,89],\"204\":[5,43],\"205\":[4,7],\"206\":[6,29],\"207\":[1,36],\"208\":[2],\"209\":[5,15],\"210\":[3,50],\"211\":[3,53],\"212\":[3,52],\"213\":[4,15],\"214\":[4,37],\"215\":[3,40],\"216\":[5,47],\"217\":[1,137],\"218\":[1,41],\"219\":[2],\"220\":[3,4],\"221\":[3,13],\"222\":[4,57],\"223\":[3,57],\"224\":[1,101],\"225\":[1,36],\"226\":[2,16],\"227\":[3,4],\"228\":[3,7],\"229\":[3,57],\"230\":[3,58],\"231\":[1,125],\"232\":[1,36],\"233\":[5,41],\"234\":[2,95],\"235\":[3,180],\"236\":[4,26],\"237\":[4,106],\"238\":[4,141],\"239\":[6,95],\"240\":[4,136],\"241\":[4,56],\"242\":[4,67],\"243\":[2,24],\"244\":[2,37],\"245\":[3,90],\"246\":[2],\"247\":[3,76],\"248\":[2],\"249\":[3,166],\"250\":[2],\"251\":[3,284],\"252\":[2],\"253\":[3,101],\"254\":[2],\"255\":[3,194],\"256\":[2],\"257\":[3,75],\"258\":[2],\"259\":[3,225],\"260\":[2],\"261\":[3,75],\"262\":[2],\"263\":[3,75],\"264\":[2],\"265\":[3,163],\"266\":[2],\"267\":[3,69],\"268\":[2],\"269\":[3,167],\"270\":[2],\"271\":[2,20],\"272\":[2,38],\"273\":[4,4],\"274\":[3,39],\"275\":[3,66],\"276\":[2,64],\"277\":[5,33],\"278\":[2,10],\"279\":[3,46],\"280\":[4,41],\"281\":[4,49],\"282\":[3,57],\"283\":[3,41],\"284\":[3,40],\"285\":[3,30],\"286\":[3,93],\"287\":[4,8],\"288\":[3,6],\"289\":[3,4],\"290\":[3,10],\"291\":[4,4],\"292\":[5,11],\"293\":[3,4],\"294\":[3,57],\"295\":[3,176],\"296\":[3,80],\"297\":[3,53],\"298\":[3,24],\"299\":[4,30],\"300\":[2],\"301\":[3,91],\"302\":[2],\"303\":[3],\"304\":[3],\"305\":[3],\"306\":[3],\"307\":[3,150],\"308\":[2],\"309\":[3],\"310\":[4],\"311\":[2],\"312\":[3],\"313\":[3],\"314\":[3],\"315\":[4,106],\"316\":[2],\"317\":[3],\"318\":[4],\"319\":[3],\"320\":[3],\"321\":[4,82],\"322\":[2],\"323\":[3],\"324\":[4],\"325\":[2],\"326\":[3],\"327\":[4,113],\"328\":[2],\"329\":[3],\"330\":[4],\"331\":[3],\"332\":[3],\"333\":[4,103],\"334\":[2],\"335\":[3],\"336\":[4],\"337\":[3],\"338\":[3],\"339\":[3,65],\"340\":[2],\"341\":[3],\"342\":[4],\"343\":[3,95],\"344\":[2],\"345\":[3],\"346\":[4],\"347\":[3],\"348\":[3],\"349\":[4],\"350\":[3,91],\"351\":[2],\"352\":[3],\"353\":[3],\"354\":[4],\"355\":[3],\"356\":[2],\"357\":[4,78],\"358\":[2],\"359\":[3],\"360\":[4],\"361\":[3],\"362\":[2],\"363\":[3,73],\"364\":[2],\"365\":[3],\"366\":[2],\"367\":[2],\"368\":[4,87],\"369\":[2],\"370\":[3],\"371\":[3],\"372\":[4],\"373\":[3],\"374\":[2],\"375\":[5,35],\"376\":[2],\"377\":[2,48],\"378\":[2],\"379\":[2],\"380\":[4,67],\"381\":[2],\"382\":[3],\"383\":[4],\"384\":[3,95],\"385\":[2],\"386\":[3],\"387\":[4],\"388\":[2],\"389\":[3],\"390\":[3],\"391\":[3,96],\"392\":[2],\"393\":[3],\"394\":[4],\"395\":[3],\"396\":[3],\"397\":[2,23],\"398\":[2,35],\"399\":[3,113],\"400\":[2],\"401\":[3],\"402\":[4],\"403\":[2],\"404\":[4],\"405\":[7],\"406\":[2],\"407\":[3],\"408\":[3,129],\"409\":[2],\"410\":[3],\"411\":[3],\"412\":[2],\"413\":[3],\"414\":[3],\"415\":[3],\"416\":[4,82],\"417\":[2],\"418\":[3],\"419\":[3],\"420\":[2],\"421\":[3],\"422\":[3,109],\"423\":[2],\"424\":[3],\"425\":[4],\"426\":[2],\"427\":[3],\"428\":[3],\"429\":[4,223],\"430\":[2],\"431\":[3],\"432\":[4],\"433\":[3],\"434\":[3],\"435\":[3],\"436\":[4],\"437\":[3,61],\"438\":[2],\"439\":[3],\"440\":[4],\"441\":[3,29],\"442\":[2],\"443\":[3,120],\"444\":[2],\"445\":[3],\"446\":[4],\"447\":[3],\"448\":[3],\"449\":[4,135],\"450\":[2],\"451\":[3],\"452\":[4],\"453\":[3],\"454\":[3],\"455\":[3,82],\"456\":[2],\"457\":[3],\"458\":[4],\"459\":[2],\"460\":[2],\"461\":[3,100],\"462\":[2],\"463\":[4],\"464\":[3,105],\"465\":[2],\"466\":[3],\"467\":[4],\"468\":[2],\"469\":[2],\"470\":[3,105],\"471\":[2],\"472\":[3],\"473\":[4],\"474\":[2],\"475\":[2],\"476\":[4,64],\"477\":[2],\"478\":[3,89],\"479\":[2],\"480\":[3],\"481\":[4],\"482\":[2],\"483\":[3],\"484\":[3],\"485\":[4,109],\"486\":[2],\"487\":[3],\"488\":[4],\"489\":[3],\"490\":[3],\"491\":[4,46],\"492\":[2],\"493\":[2,23],\"494\":[2],\"495\":[2],\"496\":[3,50],\"497\":[2],\"498\":[2],\"499\":[3,32],\"500\":[2],\"501\":[3,38],\"502\":[2],\"503\":[3,23],\"504\":[2],\"505\":[2],\"506\":[4,44],\"507\":[2],\"508\":[2],\"509\":[4,59],\"510\":[2],\"511\":[2],\"512\":[4,53],\"513\":[2],\"514\":[2],\"515\":[4,16],\"516\":[2],\"517\":[2,11],\"518\":[2],\"519\":[5,43],\"520\":[2],\"521\":[2],\"522\":[3,36],\"523\":[2],\"524\":[2],\"525\":[3,51],\"526\":[2],\"527\":[2],\"528\":[4,54],\"529\":[2],\"530\":[5,17],\"531\":[2],\"532\":[2],\"533\":[4,24],\"534\":[2],\"535\":[2],\"536\":[2,17],\"537\":[2],\"538\":[2,14],\"539\":[2],\"540\":[2],\"541\":[5,28],\"542\":[2],\"543\":[2],\"544\":[3,15],\"545\":[2],\"546\":[2,27],\"547\":[2],\"548\":[2],\"549\":[2,15],\"550\":[2],\"551\":[2,24],\"552\":[2],\"553\":[2],\"554\":[3,23],\"555\":[2],\"556\":[2],\"557\":[4,25],\"558\":[2],\"559\":[2],\"560\":[4,31],\"561\":[2],\"562\":[3,31],\"563\":[2],\"564\":[3,67],\"565\":[2],\"566\":[2,19],\"567\":[2],\"568\":[5,19],\"569\":[2],\"570\":[2,17],\"571\":[2],\"572\":[4,18],\"573\":[2],\"574\":[2,13],\"575\":[2],\"576\":[2,15],\"577\":[2],\"578\":[2],\"579\":[2,30],\"580\":[2],\"581\":[2],\"582\":[2,20],\"583\":[2],\"584\":[2],\"585\":[3,41],\"586\":[2],\"587\":[2],\"588\":[2,11],\"589\":[2],\"590\":[2,14],\"591\":[2],\"592\":[2],\"593\":[6,24],\"594\":[6,30],\"595\":[7,34],\"596\":[5,21],\"597\":[1,8],\"598\":[8,11],\"599\":[5,16],\"600\":[7,26],\"601\":[5,20],\"602\":[5,18],\"603\":[5,18],\"604\":[5,32],\"605\":[5,53],\"606\":[5,45],\"607\":[5,84],\"608\":[1,44],\"609\":[6,27],\"610\":[4,51],\"611\":[6,29],\"612\":[4,64],\"613\":[1,25],\"614\":[5,32],\"615\":[5,24],\"616\":[5,19],\"617\":[6,31],\"618\":[5,32],\"619\":[5,60],\"620\":[6,18],\"621\":[5,36],\"622\":[5,17],\"623\":[4,43],\"624\":[5,30],\"625\":[5,37],\"626\":[5,61],\"627\":[5,88],\"628\":[5,52],\"629\":[5,120],\"630\":[6,21],\"631\":[6,22],\"632\":[6,58],\"633\":[7,39],\"634\":[5,23],\"635\":[10,47],\"636\":[5,19],\"637\":[7,32],\"638\":[6,26],\"639\":[7,31],\"640\":[9,28],\"641\":[6,33],\"642\":[9,32],\"643\":[10,33],\"644\":[7,12],\"645\":[8,37],\"646\":[8,44],\"647\":[5,23],\"648\":[5,77],\"649\":[5,18],\"650\":[6,38],\"651\":[5,17],\"652\":[5,51],\"653\":[5,24],\"654\":[5,25],\"655\":[5,21],\"656\":[5,26],\"657\":[5,18],\"658\":[7,39],\"659\":[4,31],\"660\":[4,31],\"661\":[4,38],\"662\":[4,31],\"663\":[4,9],\"664\":[4,9],\"665\":[5,23],\"666\":[4,9],\"667\":[4,24],\"668\":[3,51],\"669\":[3,27],\"670\":[3,29],\"671\":[4,24],\"672\":[3,60],\"673\":[3,15],\"674\":[4,20],\"675\":[3,6],\"676\":[5,152],\"677\":[7,86],\"678\":[7,101],\"679\":[7,99],\"680\":[7,35],\"681\":[7,109],\"682\":[7,111],\"683\":[7,44],\"684\":[7,112],\"685\":[7,113],\"686\":[7,102],\"687\":[7,99],\"688\":[7,116],\"689\":[7,128],\"690\":[8,48],\"691\":[6,192],\"692\":[7,191],\"693\":[8,190],\"694\":[6,40],\"695\":[5,56],\"696\":[5,62],\"697\":[5,234],\"698\":[6,100],\"699\":[7,106],\"700\":[6,184],\"701\":[7,97],\"702\":[7,59],\"703\":[7,49],\"704\":[6,52],\"705\":[6,107],\"706\":[5,112],\"707\":[7,23],\"708\":[6,47],\"709\":[7,37],\"710\":[7,107],\"711\":[10,133],\"712\":[7,100],\"713\":[9,61],\"714\":[7,53],\"715\":[7,55],\"716\":[7,56],\"717\":[7,44],\"718\":[7,63],\"719\":[7,55],\"720\":[7,55],\"721\":[7,55],\"722\":[9,53],\"723\":[7,45],\"724\":[7,19],\"725\":[8,146],\"726\":[8,100],\"727\":[7,117],\"728\":[7,117],\"729\":[9,86],\"730\":[9,79],\"731\":[7,96],\"732\":[8,64],\"733\":[7,20],\"734\":[1,138],\"735\":[8,67],\"736\":[8,50],\"737\":[1,74],\"738\":[8,64],\"739\":[1,10],\"740\":[8,89],\"741\":[8,90],\"742\":[7,150],\"743\":[8,69],\"744\":[1,24],\"745\":[8,171],\"746\":[8,159],\"747\":[7,101],\"748\":[8,51],\"749\":[8,132],\"750\":[6,68],\"751\":[6,37],\"752\":[8,66],\"753\":[1,35],\"754\":[8,251],\"755\":[8,70],\"756\":[7,80],\"757\":[1,35],\"758\":[7,137],\"759\":[7,66],\"760\":[8,112],\"761\":[1,35],\"762\":[8,88],\"763\":[8,73],\"764\":[7,45],\"765\":[7,41],\"766\":[8,62],\"767\":[5,90],\"768\":[9,56],\"769\":[8,19],\"770\":[7,95],\"771\":[7,103],\"772\":[7,68],\"773\":[6,66],\"774\":[8,88],\"775\":[7,87],\"776\":[7,89],\"777\":[7,40],\"778\":[8,142],\"779\":[1,35],\"780\":[8],\"781\":[5,129],\"782\":[8,53],\"783\":[5,8],\"784\":[7,68],\"785\":[7,118],\"786\":[9,72],\"787\":[7,35],\"788\":[5,7],\"789\":[5,7],\"790\":[5,7],\"791\":[7,29],\"792\":[7,42],\"793\":[6,50],\"794\":[8,87],\"795\":[5,35],\"796\":[1,51],\"797\":[7,220],\"798\":[7,48],\"799\":[7,71],\"800\":[7,33],\"801\":[9,51],\"802\":[7,95],\"803\":[7,55],\"804\":[1,52],\"805\":[7,37],\"806\":[8,133],\"807\":[7,53],\"808\":[7,54],\"809\":[7,106],\"810\":[7,74],\"811\":[7,29],\"812\":[5,87],\"813\":[7,59],\"814\":[5,22],\"815\":[1,99],\"816\":[7,36],\"817\":[8,125],\"818\":[7,60],\"819\":[7,30],\"820\":[5,104],\"821\":[8,240],\"822\":[8,81],\"823\":[7,35],\"824\":[6,82],\"825\":[7,150],\"826\":[8,263],\"827\":[9,63],\"828\":[7,118],\"829\":[5,34],\"830\":[6,46],\"831\":[8,76],\"832\":[1,35],\"833\":[7,23],\"834\":[7,75],\"835\":[6,122],\"836\":[7,73],\"837\":[7,37],\"838\":[1,51],\"839\":[9,13],\"840\":[10,15],\"841\":[10,15],\"842\":[10,15],\"843\":[9,13],\"844\":[9,14],\"845\":[9,15],\"846\":[9,15],\"847\":[9,17],\"848\":[9,15],\"849\":[9,14],\"850\":[9,15],\"851\":[9,15],\"852\":[8,34],\"853\":[7,15],\"854\":[9,13],\"855\":[8,25],\"856\":[9,29],\"857\":[4,108],\"858\":[7,85],\"859\":[9,38],\"860\":[9,29],\"861\":[9,41],\"862\":[9,35],\"863\":[9,41],\"864\":[9,19],\"865\":[8,44],\"866\":[10,34],\"867\":[6,27],\"868\":[9,28],\"869\":[8,31],\"870\":[8,48],\"871\":[7,11],\"872\":[6,28],\"873\":[6,30],\"874\":[6,30],\"875\":[7,87],\"876\":[1,21],\"877\":[8,30],\"878\":[8,30],\"879\":[7,11],\"880\":[7,40],\"881\":[8,12],\"882\":[7,10],\"883\":[7,11],\"884\":[9,28],\"885\":[9,63],\"886\":[12,33],\"887\":[11,35],\"888\":[9,18],\"889\":[7,41],\"890\":[8,28],\"891\":[9,21],\"892\":[8,51],\"893\":[6,19],\"894\":[6,16],\"895\":[8,25],\"896\":[8,22],\"897\":[9,17],\"898\":[8,25],\"899\":[9,59],\"900\":[1,29],\"901\":[8,59],\"902\":[1,28],\"903\":[8,30],\"904\":[1,19],\"905\":[9,51],\"906\":[7,31],\"907\":[1,13],\"908\":[8,25],\"909\":[9,69],\"910\":[9,30],\"911\":[9,38],\"912\":[8,24],\"913\":[8,20],\"914\":[6,37],\"915\":[10,28],\"916\":[7,9],\"917\":[9,58],\"918\":[9,41],\"919\":[10,18],\"920\":[7,37],\"921\":[7,33],\"922\":[7,32],\"923\":[8,25],\"924\":[7,34],\"925\":[7,33],\"926\":[7,29],\"927\":[8,31],\"928\":[1,22],\"929\":[9,38],\"930\":[7,9],\"931\":[8,14],\"932\":[9,37],\"933\":[11,53],\"934\":[9,38],\"935\":[10,26],\"936\":[5,36],\"937\":[5,18],\"938\":[5,18],\"939\":[5,12],\"940\":[4,79],\"941\":[4,21],\"942\":[5,19],\"943\":[5,39],\"944\":[4,59],\"945\":[4,16],\"946\":[4,8],\"947\":[4,20],\"948\":[4,22],\"949\":[4,12],\"950\":[5,84],\"951\":[4,16],\"952\":[4,72],\"953\":[4,20],\"954\":[4,15],\"955\":[5,38],\"956\":[5,54],\"957\":[4,14],\"958\":[4,15],\"959\":[1,38],\"960\":[4,15],\"961\":[4,14],\"962\":[4,17],\"963\":[4,10],\"964\":[5,10],\"965\":[6,38],\"966\":[4,15],\"967\":[4,22],\"968\":[4,86],\"969\":[3,14],\"970\":[4,19],\"971\":[4,15],\"972\":[6,37],\"973\":[6,52],\"974\":[5,13],\"975\":[5,40],\"976\":[5,50],\"977\":[5,14],\"978\":[5,13],\"979\":[5,20],\"980\":[5,12],\"981\":[4,54],\"982\":[5,13],\"983\":[5,15],\"984\":[1,12],\"985\":[5,14],\"986\":[5,17],\"987\":[4,101],\"988\":[5,22],\"989\":[4,63],\"990\":[5,13],\"991\":[5,17],\"992\":[1,13],\"993\":[5,13],\"994\":[5,17],\"995\":[1,14],\"996\":[6,35],\"997\":[5,68],\"998\":[5,45],\"999\":[4,44],\"1000\":[4,24],\"1001\":[6,57],\"1002\":[7,9],\"1003\":[6,75],\"1004\":[6,78],\"1005\":[6,78],\"1006\":[5,14],\"1007\":[7,28],\"1008\":[4,33],\"1009\":[7,14],\"1010\":[8,40],\"1011\":[7,112],\"1012\":[4,28],\"1013\":[7,63],\"1014\":[1,18],\"1015\":[7,104],\"1016\":[5,29],\"1017\":[1,27],\"1018\":[7,11],\"1019\":[6,44],\"1020\":[6,7],\"1021\":[8,13],\"1022\":[7,12],\"1023\":[8,11],\"1024\":[1,9],\"1025\":[6,68],\"1026\":[6,15],\"1027\":[7,11],\"1028\":[6,141],\"1029\":[6,12],\"1030\":[1,7],\"1031\":[5,37],\"1032\":[5,17],\"1033\":[5,17],\"1034\":[7,30],\"1035\":[6,13],\"1036\":[7,23],\"1037\":[5,75],\"1038\":[4,8],\"1039\":[6,43],\"1040\":[6,23],\"1041\":[5,13],\"1042\":[5,40],\"1043\":[5,52],\"1044\":[5,14],\"1045\":[5,13],\"1046\":[6,90],\"1047\":[5,56],\"1048\":[6,199],\"1049\":[7,111],\"1050\":[7,104],\"1051\":[7,69],\"1052\":[7,123],\"1053\":[8,60],\"1054\":[7,84],\"1055\":[7,64],\"1056\":[7,119],\"1057\":[6,182],\"1058\":[5,80],\"1059\":[6,84],\"1060\":[6,51],\"1061\":[5,59],\"1062\":[9,62],\"1063\":[6,51],\"1064\":[6,67],\"1065\":[7,134],\"1066\":[6,174],\"1067\":[5,68],\"1068\":[8,105],\"1069\":[11,109],\"1070\":[9,66],\"1071\":[8,93],\"1072\":[5,64],\"1073\":[6,112],\"1074\":[7,88],\"1075\":[6,137],\"1076\":[7,111],\"1077\":[8,67],\"1078\":[9,51],\"1079\":[9,67],\"1080\":[5,54],\"1081\":[8,89],\"1082\":[5,61],\"1083\":[6,102],\"1084\":[5,65],\"1085\":[5,35],\"1086\":[8,49],\"1087\":[8,30],\"1088\":[8,23],\"1089\":[8,23],\"1090\":[8,21],\"1091\":[8,24],\"1092\":[8,21],\"1093\":[8,105],\"1094\":[8,23],\"1095\":[7,34],\"1096\":[5,68],\"1097\":[8,45],\"1098\":[5,37],\"1099\":[7,51],\"1100\":[10,19],\"1101\":[7,56],\"1102\":[7,32],\"1103\":[7,30],\"1104\":[8,27],\"1105\":[8,24],\"1106\":[6,55],\"1107\":[5,44],\"1108\":[6,58],\"1109\":[5,40],\"1110\":[1,41],\"1111\":[5,36],\"1112\":[1,35],\"1113\":[5,75],\"1114\":[5,43],\"1115\":[5,126],\"1116\":[5,75],\"1117\":[5,43],\"1118\":[1,35],\"1119\":[5,44],\"1120\":[1,38],\"1121\":[5,36],\"1122\":[1,38],\"1123\":[5,36],\"1124\":[1,38],\"1125\":[5,36],\"1126\":[1,38],\"1127\":[5,52],\"1128\":[1,35],\"1129\":[6,11],\"1130\":[6,62],\"1131\":[1,35],\"1132\":[5,130],\"1133\":[5,192],\"1134\":[5,41],\"1135\":[1,35],\"1136\":[6,59],\"1137\":[1,52],\"1138\":[6,214],\"1139\":[7,180],\"1140\":[5,104],\"1141\":[5,94],\"1142\":[9,161],\"1143\":[10,71],\"1144\":[1,62],\"1145\":[4,117],\"1146\":[6],\"1147\":[6],\"1148\":[5,183],\"1149\":[7,173],\"1150\":[7,173],\"1151\":[5,52],\"1152\":[1,35],\"1153\":[5,57],\"1154\":[10,66],\"1155\":[10,67],\"1156\":[6,48],\"1157\":[1,35],\"1158\":[5,110],\"1159\":[1,37],\"1160\":[6,118],\"1161\":[6,117],\"1162\":[6,98],\"1163\":[6,81],\"1164\":[6,118],\"1165\":[6,84],\"1166\":[6,35],\"1167\":[5,72],\"1168\":[5,72],\"1169\":[6,118],\"1170\":[6,87],\"1171\":[5,165],\"1172\":[6,103],\"1173\":[6,70],\"1174\":[5,36],\"1175\":[1,37],\"1176\":[6,35],\"1177\":[6,108],\"1178\":[5,94],\"1179\":[5,118],\"1180\":[5,134],\"1181\":[5,117],\"1182\":[5,66],\"1183\":[6,21],\"1184\":[5,45],\"1185\":[1,37],\"1186\":[10,182],\"1187\":[5,138],\"1188\":[6,41],\"1189\":[1,35],\"1190\":[7,122],\"1191\":[7,41],\"1192\":[7,46],\"1193\":[6,32],\"1194\":[11,30],\"1195\":[6,41],\"1196\":[5,72],\"1197\":[5,72],\"1198\":[5,218],\"1199\":[6,28],\"1200\":[5,85],\"1201\":[5,29],\"1202\":[9,142],\"1203\":[5,197],\"1204\":[5,104],\"1205\":[5,34],\"1206\":[5,148],\"1207\":[6,72],\"1208\":[1,42],\"1209\":[6,122],\"1210\":[10,198],\"1211\":[7,139],\"1212\":[6,38],\"1213\":[1,38],\"1214\":[5,157],\"1215\":[5,76],\"1216\":[1,71],\"1217\":[6,44],\"1218\":[6,62],\"1219\":[6,58],\"1220\":[5,90],\"1221\":[1,90],\"1222\":[5,85],\"1223\":[1,37],\"1224\":[7,111],\"1225\":[10,24],\"1226\":[11,29],\"1227\":[10,47],\"1228\":[1,88],\"1229\":[5,35],\"1230\":[1,39],\"1231\":[5,43],\"1232\":[1,35],\"1233\":[6,56],\"1234\":[1,35],\"1235\":[6,34],\"1236\":[1,35],\"1237\":[6,34],\"1238\":[1,35],\"1239\":[5,90],\"1240\":[1,40],\"1241\":[6,90],\"1242\":[5,29],\"1243\":[3,107],\"1244\":[5,176],\"1245\":[6,215],\"1246\":[1,55],\"1247\":[6,69],\"1248\":[6,150],\"1249\":[5,35],\"1250\":[1,35],\"1251\":[6,43],\"1252\":[6,204],\"1253\":[6,164],\"1254\":[6,194],\"1255\":[5,118],\"1256\":[5,45],\"1257\":[4,95],\"1258\":[1,35],\"1259\":[6,32],\"1260\":[1,35],\"1261\":[6,41],\"1262\":[1,35],\"1263\":[5,36],\"1264\":[1,35],\"1265\":[6,34],\"1266\":[1,35],\"1267\":[6,34],\"1268\":[1,35],\"1269\":[5,240],\"1270\":[5,135],\"1271\":[5,65],\"1272\":[5,141],\"1273\":[5,161],\"1274\":[6,65],\"1275\":[1,35],\"1276\":[6,56],\"1277\":[1,35],\"1278\":[6,19],\"1279\":[6,116],\"1280\":[6,41],\"1281\":[1,39],\"1282\":[6,82],\"1283\":[1,37],\"1284\":[5,65],\"1285\":[1,50],\"1286\":[7,156],\"1287\":[7,153],\"1288\":[9,12],\"1289\":[7,39],\"1290\":[6],\"1291\":[7],\"1292\":[7],\"1293\":[8,14],\"1294\":[8,11],\"1295\":[8,11],\"1296\":[8,12],\"1297\":[6,15],\"1298\":[12,114],\"1299\":[12,114],\"1300\":[11,18],\"1301\":[12,151],\"1302\":[13,136],\"1303\":[13,135],\"1304\":[13,175],\"1305\":[5,10],\"1306\":[11,17],\"1307\":[10,14],\"1308\":[5,10],\"1309\":[5,10],\"1310\":[6,11],\"1311\":[6,13],\"1312\":[7,11],\"1313\":[8,12],\"1314\":[6,25],\"1315\":[10,11],\"1316\":[9,11],\"1317\":[9,10],\"1318\":[10,13],\"1319\":[7,12],\"1320\":[10,11],\"1321\":[9,10],\"1322\":[7,10],\"1323\":[7,17],\"1324\":[11,18],\"1325\":[9,11],\"1326\":[6,10],\"1327\":[6,54],\"1328\":[7,11],\"1329\":[7,11],\"1330\":[7,8],\"1331\":[10,14],\"1332\":[8,14],\"1333\":[11,15],\"1334\":[11,68],\"1335\":[9,12],\"1336\":[8,113],\"1337\":[9,139],\"1338\":[9,11],\"1339\":[6,44],\"1340\":[8,21],\"1341\":[6,22],\"1342\":[7,23],\"1343\":[8,34],\"1344\":[10,30],\"1345\":[1,77],\"1346\":[10,30],\"1347\":[1,77],\"1348\":[7,77],\"1349\":[8,92],\"1350\":[8,93],\"1351\":[6,28],\"1352\":[7,66],\"1353\":[12,13],\"1354\":[6,21],\"1355\":[7,27],\"1356\":[7,32],\"1357\":[6,17],\"1358\":[6,13],\"1359\":[11,12],\"1360\":[5,38],\"1361\":[1,35],\"1362\":[5,36],\"1363\":[1,39],\"1364\":[4,37],\"1365\":[1,40],\"1366\":[6,42],\"1367\":[1,41],\"1368\":[6,42],\"1369\":[6,42],\"1370\":[6,44],\"1371\":[5,130],\"1372\":[6,41],\"1373\":[5,48],\"1374\":[5,46],\"1375\":[6,120],\"1376\":[5,62],\"1377\":[6,102],\"1378\":[6,40],\"1379\":[6,63],\"1380\":[7,10],\"1381\":[7,28],\"1382\":[5,20],\"1383\":[1,23],\"1384\":[6,27],\"1385\":[1,14],\"1386\":[6,29],\"1387\":[1,16],\"1388\":[5,20],\"1389\":[1,23],\"1390\":[6,37],\"1391\":[1,42],\"1392\":[1,46],\"1393\":[5,14],\"1394\":[5,21],\"1395\":[1,21],\"1396\":[5,22],\"1397\":[1,26],\"1398\":[5,78],\"1399\":[4,15],\"1400\":[1,67],\"1401\":[5,16],\"1402\":[1,17],\"1403\":[5,16],\"1404\":[1,21],\"1405\":[5,25],\"1406\":[1,84],\"1407\":[5,50],\"1408\":[1,25],\"1409\":[5,43],\"1410\":[1,20],\"1411\":[5,18],\"1412\":[1,25],\"1413\":[5,18],\"1414\":[1,23],\"1415\":[5,19],\"1416\":[1,25],\"1417\":[7,23],\"1418\":[1,21],\"1419\":[5,21],\"1420\":[5,19],\"1421\":[1,13],\"1422\":[5,18],\"1423\":[1,24],\"1424\":[6,26],\"1425\":[1,19],\"1426\":[6,29],\"1427\":[8,103],\"1428\":[7,32],\"1429\":[8,74],\"1430\":[5,120],\"1431\":[5,43],\"1432\":[1,77],\"1433\":[5,37],\"1434\":[1,35],\"1435\":[5,43],\"1436\":[1,78],\"1437\":[6,40],\"1438\":[1,40],\"1439\":[4,37],\"1440\":[1,40],\"1441\":[5,45],\"1442\":[1,35],\"1443\":[7,44],\"1444\":[1,39],\"1445\":[5,41],\"1446\":[1,44],\"1447\":[5,36],\"1448\":[1,35],\"1449\":[5,36],\"1450\":[1,35],\"1451\":[6,59],\"1452\":[5,89],\"1453\":[1,35],\"1454\":[7,115],\"1455\":[6,52],\"1456\":[6,39],\"1457\":[1,35],\"1458\":[7,48],\"1459\":[1,35],\"1460\":[6,50],\"1461\":[1,35],\"1462\":[5,132],\"1463\":[5,153],\"1464\":[5,74],\"1465\":[5,38],\"1466\":[7,88],\"1467\":[1,35],\"1468\":[6,38],\"1469\":[1,35],\"1470\":[5,71],\"1471\":[5,54],\"1472\":[5,43],\"1473\":[5,41],\"1474\":[7,43],\"1475\":[1,35],\"1476\":[5,71],\"1477\":[1,38],\"1478\":[5,58],\"1479\":[1,35],\"1480\":[5,39],\"1481\":[1,35],\"1482\":[5,49],\"1483\":[1,35],\"1484\":[5,56],\"1485\":[6,41],\"1486\":[1,35],\"1487\":[6,41],\"1488\":[1,35],\"1489\":[6,42],\"1490\":[1,35],\"1491\":[6,45],\"1492\":[1,35],\"1493\":[7,39],\"1494\":[1,35],\"1495\":[7,39],\"1496\":[1,35],\"1497\":[7,39],\"1498\":[1,35],\"1499\":[7,39],\"1500\":[1,35],\"1501\":[6,50],\"1502\":[1,35],\"1503\":[7,39],\"1504\":[1,35],\"1505\":[5,171],\"1506\":[5,56],\"1507\":[1,35],\"1508\":[10,56],\"1509\":[1,35],\"1510\":[5,90],\"1511\":[5,95],\"1512\":[6,41],\"1513\":[1,35],\"1514\":[6,55],\"1515\":[5,139],\"1516\":[5,130],\"1517\":[5,98],\"1518\":[5,59],\"1519\":[1,35],\"1520\":[5,55],\"1521\":[1,35],\"1522\":[7,129],\"1523\":[7,175],\"1524\":[7,167],\"1525\":[7,105],\"1526\":[6,40],\"1527\":[6,13],\"1528\":[6,156],\"1529\":[5,155],\"1530\":[7,73],\"1531\":[5,92],\"1532\":[5,78],\"1533\":[1,35],\"1534\":[5,123],\"1535\":[6,86],\"1536\":[1,35],\"1537\":[5,99],\"1538\":[1,42],\"1539\":[5,148],\"1540\":[5,35],\"1541\":[1,35],\"1542\":[6,25],\"1543\":[5,106],\"1544\":[1,35],\"1545\":[6,72],\"1546\":[5,45],\"1547\":[5,46],\"1548\":[1,35],\"1549\":[6,36],\"1550\":[1,35],\"1551\":[4,234],\"1552\":[6,167],\"1553\":[5,249],\"1554\":[6,106],\"1555\":[5,38],\"1556\":[1,35],\"1557\":[6,52],\"1558\":[5,120],\"1559\":[6,46],\"1560\":[6,91],\"1561\":[5,49],\"1562\":[1,35],\"1563\":[7,56],\"1564\":[5,68],\"1565\":[1,35],\"1566\":[7,73],\"1567\":[7,56],\"1568\":[7,96],\"1569\":[7,53],\"1570\":[7,50],\"1571\":[7,53],\"1572\":[6,73],\"1573\":[7,44],\"1574\":[1,35],\"1575\":[5,44],\"1576\":[6,58],\"1577\":[6,66],\"1578\":[5,44],\"1579\":[5,44],\"1580\":[5,44],\"1581\":[5,110],\"1582\":[1,35],\"1583\":[7,37],\"1584\":[1,35],\"1585\":[6,52],\"1586\":[5,41],\"1587\":[1,35],\"1588\":[5,36],\"1589\":[1,35],\"1590\":[5,36],\"1591\":[1,35],\"1592\":[6,37],\"1593\":[1,35],\"1594\":[5,48],\"1595\":[6,54],\"1596\":[6,39],\"1597\":[1,35],\"1598\":[5,106],\"1599\":[1,38],\"1600\":[7,71],\"1601\":[5,59],\"1602\":[5,64],\"1603\":[8,121],\"1604\":[7,124],\"1605\":[5,98],\"1606\":[1,38],\"1607\":[6,42],\"1608\":[1,35],\"1609\":[5,42],\"1610\":[1,35],\"1611\":[6,132],\"1612\":[6,53],\"1613\":[7,37],\"1614\":[1,35],\"1615\":[6,54],\"1616\":[5,52],\"1617\":[5,47],\"1618\":[5,114],\"1619\":[5,119],\"1620\":[5,38],\"1621\":[1,35],\"1622\":[7,107],\"1623\":[6,56],\"1624\":[6,38],\"1625\":[1,35],\"1626\":[5,115],\"1627\":[6,42],\"1628\":[1,35],\"1629\":[6,49],\"1630\":[1,35],\"1631\":[7,55],\"1632\":[1,35],\"1633\":[6,50],\"1634\":[1,35],\"1635\":[7,55],\"1636\":[1,35],\"1637\":[6,52],\"1638\":[5,106],\"1639\":[7,138],\"1640\":[7,84],\"1641\":[7,48],\"1642\":[1,35],\"1643\":[5,127],\"1644\":[5,132],\"1645\":[5,139],\"1646\":[6,92],\"1647\":[1,50],\"1648\":[5,73],\"1649\":[1,35],\"1650\":[5,76],\"1651\":[1,35],\"1652\":[5,112],\"1653\":[1,39],\"1654\":[5,152],\"1655\":[5,152],\"1656\":[5,80],\"1657\":[1,35],\"1658\":[5,129],\"1659\":[6,142],\"1660\":[5,242],\"1661\":[5,254],\"1662\":[5,250],\"1663\":[5,42],\"1664\":[5,97],\"1665\":[5,114],\"1666\":[7,43],\"1667\":[7,38],\"1668\":[7,43],\"1669\":[5,150],\"1670\":[5,197],\"1671\":[5,238],\"1672\":[6,36],\"1673\":[1,35],\"1674\":[6,39],\"1675\":[1,35],\"1676\":[7,37],\"1677\":[1,35],\"1678\":[7,13],\"1679\":[6,22],\"1680\":[7,34],\"1681\":[6,16],\"1682\":[6,11],\"1683\":[6,29],\"1684\":[6,11],\"1685\":[6,14],\"1686\":[7,12],\"1687\":[8,16],\"1688\":[12,105],\"1689\":[8,17],\"1690\":[7,22],\"1691\":[7,23],\"1692\":[7,16],\"1693\":[11,90],\"1694\":[6,11],\"1695\":[7,80],\"1696\":[7,117],\"1697\":[8,68],\"1698\":[9,119],\"1699\":[7,15],\"1700\":[6,10],\"1701\":[6,46],\"1702\":[6,47],\"1703\":[8,37],\"1704\":[7,86],\"1705\":[9,93],\"1706\":[7,79],\"1707\":[9,94],\"1708\":[7,64],\"1709\":[7,15],\"1710\":[6,27],\"1711\":[9,40],\"1712\":[8,141],\"1713\":[6,76],\"1714\":[7,40],\"1715\":[8,163],\"1716\":[11,26],\"1717\":[7,38],\"1718\":[5,46],\"1719\":[5,294],\"1720\":[7,12],\"1721\":[6,13],\"1722\":[6,13],\"1723\":[6,8],\"1724\":[8,10],\"1725\":[6,11],\"1726\":[9,10],\"1727\":[6,14],\"1728\":[6,11],\"1729\":[12,15],\"1730\":[12,15],\"1731\":[7,24],\"1732\":[7,27],\"1733\":[7,15],\"1734\":[6,10],\"1735\":[7,83],\"1736\":[7,37],\"1737\":[7,27],\"1738\":[5,11],\"1739\":[6,89],\"1740\":[6,14],\"1741\":[6,59],\"1742\":[6,20],\"1743\":[6,12],\"1744\":[6,16],\"1745\":[6,9],\"1746\":[6,35],\"1747\":[6,8],\"1748\":[7,11],\"1749\":[7,11],\"1750\":[8,11],\"1751\":[6,13],\"1752\":[7,52],\"1753\":[6,13],\"1754\":[7,19],\"1755\":[11,90],\"1756\":[12,105],\"1757\":[7,22],\"1758\":[4,46],\"1759\":[6,55],\"1760\":[5,36],\"1761\":[5,99],\"1762\":[1,35],\"1763\":[5,120],\"1764\":[1,35],\"1765\":[5,139],\"1766\":[6,80],\"1767\":[5,76],\"1768\":[5,73],\"1769\":[6,43],\"1770\":[1,35],\"1771\":[7,108],\"1772\":[7,57],\"1773\":[6,138],\"1774\":[7,46],\"1775\":[1,37],\"1776\":[7,77],\"1777\":[7,67],\"1778\":[6,337],\"1779\":[6,36],\"1780\":[1,35],\"1781\":[7,71],\"1782\":[5,54],\"1783\":[1,35],\"1784\":[5,11],\"1785\":[6,103],\"1786\":[6,85],\"1787\":[7,98],\"1788\":[7,100],\"1789\":[6,39],\"1790\":[1,35],\"1791\":[7,104],\"1792\":[1,40],\"1793\":[5,49],\"1794\":[1,35],\"1795\":[5,49],\"1796\":[1,35],\"1797\":[7,96],\"1798\":[7,135],\"1799\":[6,41],\"1800\":[5,183],\"1801\":[6,119],\"1802\":[1,35],\"1803\":[6,149],\"1804\":[6,298],\"1805\":[5,345],\"1806\":[6,50],\"1807\":[1,48],\"1808\":[7,93],\"1809\":[9,12],\"1810\":[8,81],\"1811\":[6,37],\"1812\":[7,15],\"1813\":[7,13],\"1814\":[7,10],\"1815\":[6,10],\"1816\":[6,13],\"1817\":[7,12],\"1818\":[7,14],\"1819\":[8,10],\"1820\":[6,14],\"1821\":[7,11],\"1822\":[8,13],\"1823\":[6,12],\"1824\":[7,11],\"1825\":[7,9],\"1826\":[7,13],\"1827\":[6,11],\"1828\":[5,36],\"1829\":[6,69],\"1830\":[7,40],\"1831\":[7,38],\"1832\":[7,40],\"1833\":[6,71],\"1834\":[7,83],\"1835\":[6,63],\"1836\":[6,52],\"1837\":[6,108],\"1838\":[6,49],\"1839\":[6,56],\"1840\":[6,53],\"1841\":[6,60],\"1842\":[7,68],\"1843\":[6,47],\"1844\":[5,140],\"1845\":[5,83],\"1846\":[5,107],\"1847\":[5,121],\"1848\":[5,107],\"1849\":[5,120],\"1850\":[5,314],\"1851\":[6,284],\"1852\":[6,292],\"1853\":[6,54],\"1854\":[6,43],\"1855\":[6,60],\"1856\":[5,104],\"1857\":[5,134],\"1858\":[5,143],\"1859\":[6,92],\"1860\":[6,82],\"1861\":[6,100],\"1862\":[6,133],\"1863\":[7,101],\"1864\":[7,102],\"1865\":[7,89],\"1866\":[7,74],\"1867\":[7,78],\"1868\":[7,95],\"1869\":[7,47],\"1870\":[6,107],\"1871\":[6,123],\"1872\":[9,65],\"1873\":[9,69],\"1874\":[7,127],\"1875\":[6,25],\"1876\":[7,68],\"1877\":[5,294],\"1878\":[6,252],\"1879\":[6,71],\"1880\":[5,125],\"1881\":[8,39],\"1882\":[9],\"1883\":[8,54],\"1884\":[7,28],\"1885\":[7,28],\"1886\":[8,25],\"1887\":[8,26],\"1888\":[9,27],\"1889\":[7,38],\"1890\":[4,57],\"1891\":[1,35],\"1892\":[5,100],\"1893\":[5,80],\"1894\":[6,19],\"1895\":[6,78],\"1896\":[6,58],\"1897\":[1,65],\"1898\":[6,24],\"1899\":[6,13],\"1900\":[6,77],\"1901\":[8,19],\"1902\":[5,41],\"1903\":[1,35],\"1904\":[5,61],\"1905\":[4,125],\"1906\":[5,87],\"1907\":[7,38],\"1908\":[1,35],\"1909\":[5,17],\"1910\":[5,73],\"1911\":[5,30],\"1912\":[5,118],\"1913\":[1,35],\"1914\":[6,76],\"1915\":[6,88],\"1916\":[5,49],\"1917\":[5,150],\"1918\":[4,95],\"1919\":[5,74],\"1920\":[5,71],\"1921\":[5,57],\"1922\":[5,48],\"1923\":[5,40],\"1924\":[9,19],\"1925\":[4,48],\"1926\":[4,37],\"1927\":[1,96],\"1928\":[4,56],\"1929\":[5,73],\"1930\":[4,30],\"1931\":[6,15],\"1932\":[6,87],\"1933\":[8,22],\"1934\":[8,22],\"1935\":[4,49],\"1936\":[5,52],\"1937\":[7,16],\"1938\":[5,44],\"1939\":[5,44],\"1940\":[5,40],\"1941\":[5,84],\"1942\":[5,9],\"1943\":[4,48],\"1944\":[7,19],\"1945\":[4,10],\"1946\":[5,56],\"1947\":[5,91],\"1948\":[4,25],\"1949\":[4,35],\"1950\":[7,20],\"1951\":[5,74],\"1952\":[1,35],\"1953\":[5,85],\"1954\":[1,57],\"1955\":[6,108],\"1956\":[1,70],\"1957\":[6,86],\"1958\":[5,113],\"1959\":[1,72],\"1960\":[4,104],\"1961\":[5,38],\"1962\":[6,55],\"1963\":[6,37],\"1964\":[5,50],\"1965\":[10,15],\"1966\":[8,13],\"1967\":[4,17],\"1968\":[5,31],\"1969\":[1,11],\"1970\":[5,97],\"1971\":[5,77],\"1972\":[4,53],\"1973\":[7,44],\"1974\":[6,12],\"1975\":[5,114],\"1976\":[6,46],\"1977\":[1,38],\"1978\":[6,40],\"1979\":[1,38],\"1980\":[5,52],\"1981\":[7,39],\"1982\":[1,41],\"1983\":[5,42],\"1984\":[5,124],\"1985\":[1,74],\"1986\":[5,70],\"1987\":[8,63],\"1988\":[1,44],\"1989\":[9,83],\"1990\":[1,44],\"1991\":[8,67],\"1992\":[1,44],\"1993\":[6,62],\"1994\":[5,42],\"1995\":[1,35],\"1996\":[6,49],\"1997\":[6,44],\"1998\":[1,35],\"1999\":[7,46],\"2000\":[6,80],\"2001\":[5,228],\"2002\":[5,225],\"2003\":[5,113],\"2004\":[5,172],\"2005\":[5,19],\"2006\":[6,26],\"2007\":[6,34],\"2008\":[6,31],\"2009\":[7,32],\"2010\":[6,32],\"2011\":[6,50],\"2012\":[5,76],\"2013\":[7,10],\"2014\":[5,19],\"2015\":[5,19],\"2016\":[5,19],\"2017\":[5,20],\"2018\":[7,54],\"2019\":[5,83],\"2020\":[7,40],\"2021\":[5,51],\"2022\":[5,75],\"2023\":[6,52],\"2024\":[5,54],\"2025\":[1,38],\"2026\":[5,82],\"2027\":[5,118],\"2028\":[7,47],\"2029\":[5,128],\"2030\":[5,80],\"2031\":[1,35],\"2032\":[6,70],\"2033\":[1,35],\"2034\":[5,40],\"2035\":[1,35],\"2036\":[5,37],\"2037\":[1,38],\"2038\":[5,36],\"2039\":[1,38],\"2040\":[7,124],\"2041\":[1,39],\"2042\":[6,43],\"2043\":[1,35],\"2044\":[7,82],\"2045\":[1,37],\"2046\":[5,145],\"2047\":[6,41],\"2048\":[1,35],\"2049\":[6,95],\"2050\":[5,62],\"2051\":[1,37],\"2052\":[5,59],\"2053\":[1,37],\"2054\":[5,168],\"2055\":[5,86],\"2056\":[1,37],\"2057\":[5,37],\"2058\":[1,37],\"2059\":[6,47],\"2060\":[1,35],\"2061\":[6,35],\"2062\":[1,35],\"2063\":[6,41],\"2064\":[6,97],\"2065\":[1,37],\"2066\":[6,37],\"2067\":[1,37],\"2068\":[5,77],\"2069\":[1,37],\"2070\":[5,85],\"2071\":[1,37],\"2072\":[5,36],\"2073\":[1,37],\"2074\":[6,46],\"2075\":[6,46],\"2076\":[5,123],\"2077\":[4,52],\"2078\":[6,149],\"2079\":[1,96],\"2080\":[1,10],\"2081\":[7,82],\"2082\":[5,122],\"2083\":[6,127],\"2084\":[6,91],\"2085\":[6,8],\"2086\":[5,228],\"2087\":[6,237],\"2088\":[5,61],\"2089\":[6,87],\"2090\":[4,311],\"2091\":[5,87],\"2092\":[6,10],\"2093\":[6,10],\"2094\":[8,14],\"2095\":[4,301],\"2096\":[4,104],\"2097\":[5,70],\"2098\":[4,104],\"2099\":[5,258],\"2100\":[4,104],\"2101\":[5,105],\"2102\":[4,177],\"2103\":[5,111],\"2104\":[5,110],\"2105\":[4,104],\"2106\":[5,50],\"2107\":[4,104],\"2108\":[4,104],\"2109\":[4,114],\"2110\":[4,104],\"2111\":[4,81],\"2112\":[4,104],\"2113\":[4,114],\"2114\":[4,104],\"2115\":[4,114],\"2116\":[4,114],\"2117\":[5,103],\"2118\":[4,105],\"2119\":[5,18],\"2120\":[5,31],\"2121\":[6,44],\"2122\":[5,49],\"2123\":[8,29],\"2124\":[6,19],\"2125\":[5,53],\"2126\":[5,28],\"2127\":[5,13],\"2128\":[7,41],\"2129\":[5,33],\"2130\":[5,32],\"2131\":[5,54],\"2132\":[5,22],\"2133\":[4,17],\"2134\":[1,15],\"2135\":[6,29],\"2136\":[5,28],\"2137\":[4,39],\"2138\":[6,7],\"2139\":[7,8],\"2140\":[9,10],\"2141\":[7,8],\"2142\":[7,55],\"2143\":[1,22],\"2144\":[6,7],\"2145\":[7,8],\"2146\":[9,10],\"2147\":[7,8],\"2148\":[6,37],\"2149\":[1,45],\"2150\":[1,35],\"2151\":[6,58],\"2152\":[9,28],\"2153\":[7,45],\"2154\":[9,79],\"2155\":[7,55],\"2156\":[4,43],\"2157\":[6,33],\"2158\":[1,9],\"2159\":[5,7],\"2160\":[6,7],\"2161\":[6,14],\"2162\":[6,10],\"2163\":[6,14],\"2164\":[7,9],\"2165\":[7,11],\"2166\":[6,19],\"2167\":[4,17],\"2168\":[6,112],\"2169\":[1,35],\"2170\":[7,141],\"2171\":[4,10],\"2172\":[4,10],\"2173\":[4,10],\"2174\":[4,14],\"2175\":[4,18],\"2176\":[5,66],\"2177\":[5,26],\"2178\":[4,68],\"2179\":[5,69],\"2180\":[5,40],\"2181\":[4,32],\"2182\":[4,43],\"2183\":[1,21],\"2184\":[4,69],\"2185\":[5,77],\"2186\":[5,69],\"2187\":[4,11],\"2188\":[5,43],\"2189\":[5,34],\"2190\":[1,23],\"2191\":[4,64],\"2192\":[4,9],\"2193\":[4,95],\"2194\":[4,73],\"2195\":[4,60],\"2196\":[4,65],\"2197\":[4,85],\"2198\":[5,56],\"2199\":[4,69],\"2200\":[4,74],\"2201\":[4,119],\"2202\":[4,65],\"2203\":[5,79],\"2204\":[5,71],\"2205\":[4,20],\"2206\":[4,10],\"2207\":[5,9],\"2208\":[5,33],\"2209\":[1,36],\"2210\":[6,40],\"2211\":[4,19],\"2212\":[6,35],\"2213\":[7,14],\"2214\":[7,14],\"2215\":[7,9],\"2216\":[7,41],\"2217\":[7,41],\"2218\":[6,15],\"2219\":[7,14],\"2220\":[8,9],\"2221\":[8,9],\"2222\":[5,20],\"2223\":[5,9],\"2224\":[6,10],\"2225\":[7,13],\"2226\":[6,11],\"2227\":[6,8],\"2228\":[5,9],\"2229\":[5,15],\"2230\":[6,19],\"2231\":[7,14],\"2232\":[6,11],\"2233\":[6,37],\"2234\":[1,40],\"2235\":[4,52],\"2236\":[6,67],\"2237\":[1,44],\"2238\":[1,39],\"2239\":[6,51],\"2240\":[5,95],\"2241\":[6,67],\"2242\":[1,39],\"2243\":[4,282],\"2244\":[4,284],\"2245\":[5,80],\"2246\":[7,62],\"2247\":[1,43],\"2248\":[8,82],\"2249\":[1,43],\"2250\":[7,66],\"2251\":[1,43],\"2252\":[5,45],\"2253\":[6,29],\"2254\":[8,45],\"2255\":[4,290],\"2256\":[5,81],\"2257\":[6,82],\"2258\":[5,45],\"2259\":[5,85],\"2260\":[5,109],\"2261\":[6,97],\"2262\":[6,73],\"2263\":[4,270],\"2264\":[4,283],\"2265\":[6,80],\"2266\":[6,73],\"2267\":[1,108],\"2268\":[6,33],\"2269\":[6,12],\"2270\":[6,40],\"2271\":[6,32],\"2272\":[7,40],\"2273\":[5,21],\"2274\":[6,36],\"2275\":[6,59],\"2276\":[1,35],\"2277\":[4,56],\"2278\":[5,101],\"2279\":[5,260],\"2280\":[5,85],\"2281\":[6,41],\"2282\":[1,35],\"2283\":[5,38],\"2284\":[1,35],\"2285\":[5,37],\"2286\":[1,38],\"2287\":[5,28],\"2288\":[5,39],\"2289\":[1,38],\"2290\":[5,71],\"2291\":[1,35],\"2292\":[5,72],\"2293\":[1,37],\"2294\":[5,121],\"2295\":[5,46],\"2296\":[5,42],\"2297\":[5,35],\"2298\":[1,35],\"2299\":[5,34],\"2300\":[1,35],\"2301\":[5,51],\"2302\":[6,54],\"2303\":[6,50],\"2304\":[6,54],\"2305\":[6,51],\"2306\":[6,10],\"2307\":[4,9],\"2308\":[4,10],\"2309\":[5,52],\"2310\":[4,9],\"2311\":[4,15],\"2312\":[6,15],\"2313\":[6,23],\"2314\":[1,28],\"2315\":[8,35],\"2316\":[5,17],\"2317\":[5,56],\"2318\":[4,11],\"2319\":[6,11],\"2320\":[1,22],\"2321\":[4,12],\"2322\":[5,13],\"2323\":[1,12],\"2324\":[6,21],\"2325\":[4,52],\"2326\":[8,10],\"2327\":[6,11],\"2328\":[1,21],\"2329\":[3,8],\"2330\":[5,50],\"2331\":[5,10],\"2332\":[5,10],\"2333\":[4,9],\"2334\":[5,9],\"2335\":[1,19],\"2336\":[5,9],\"2337\":[1,7],\"2338\":[6,10],\"2339\":[6,10],\"2340\":[1,21],\"2341\":[4,12],\"2342\":[7,14],\"2343\":[5,31],\"2344\":[5,31],\"2345\":[3,19],\"2346\":[3,14],\"2347\":[5,35],\"2348\":[4,20],\"2349\":[4,49],\"2350\":[3,31],\"2351\":[4,13],\"2352\":[4,15],\"2353\":[6,11],\"2354\":[6,63],\"2355\":[2,69],\"2356\":[3,5],\"2357\":[2,96],\"2358\":[2,53],\"2359\":[7,66],\"2360\":[5,115],\"2361\":[3,30],\"2362\":[1,27],\"2363\":[2,127],\"2364\":[2,51],\"2365\":[1,46],\"2366\":[3,19],\"2367\":[6,39],\"2368\":[7,81],\"2369\":[9,26],\"2370\":[2],\"2371\":[2,107],\"2372\":[8,283],\"2373\":[2,184],\"2374\":[7,17],\"2375\":[4,122],\"2376\":[7],\"2377\":[3,86],\"2378\":[4,42],\"2379\":[7,9],\"2380\":[7,46],\"2381\":[1,14],\"2382\":[2,25],\"2383\":[2,27],\"2384\":[6,161],\"2385\":[4,228],\"2386\":[6,82],\"2387\":[4,229],\"2388\":[10,80],\"2389\":[2,89],\"2390\":[2,16],\"2391\":[1,23],\"2392\":[6,44],\"2393\":[2,70],\"2394\":[9,176],\"2395\":[3,67],\"2396\":[2,36],\"2397\":[5,44],\"2398\":[10,100],\"2399\":[2,33],\"2400\":[8,126],\"2401\":[8,138],\"2402\":[2],\"2403\":[7,83],\"2404\":[1],\"2405\":[6,45],\"2406\":[7,30],\"2407\":[1,20],\"2408\":[2,80],\"2409\":[2,51],\"2410\":[2,58],\"2411\":[1,66],\"2412\":[2,66],\"2413\":[2,18],\"2414\":[6,75],\"2415\":[5,77],\"2416\":[5,34],\"2417\":[4,19],\"2418\":[8,52],\"2419\":[8,60],\"2420\":[8,45],\"2421\":[7,75],\"2422\":[2,95],\"2423\":[1,37],\"2424\":[2,38],\"2425\":[6,44],\"2426\":[3,12],\"2427\":[2,44],\"2428\":[6,26],\"2429\":[9,233],\"2430\":[2,198],\"2431\":[7,163],\"2432\":[11,122],\"2433\":[11,104],\"2434\":[7,17],\"2435\":[6,16],\"2436\":[3,78],\"2437\":[4,66],\"2438\":[3,22],\"2439\":[5,20],\"2440\":[3,292],\"2441\":[2,192],\"2442\":[1,35],\"2443\":[7],\"2444\":[1,41],\"2445\":[1,36],\"2446\":[1,67],\"2447\":[7,32],\"2448\":[1,15],\"2449\":[2,80],\"2450\":[5,95],\"2451\":[2,40],\"2452\":[6,86],\"2453\":[8],\"2454\":[3,31],\"2455\":[4,52],\"2456\":[6,71],\"2457\":[7,86],\"2458\":[7,111],\"2459\":[7,21],\"2460\":[8,122],\"2461\":[7,108],\"2462\":[7,44],\"2463\":[8,28],\"2464\":[1,16],\"2465\":[2,80],\"2466\":[2,31],\"2467\":[3,157],\"2468\":[5,109],\"2469\":[3],\"2470\":[5,23],\"2471\":[6,11],\"2472\":[9,80],\"2473\":[5,75],\"2474\":[7,106],\"2475\":[6],\"2476\":[7,107],\"2477\":[6],\"2478\":[7,74],\"2479\":[7,23],\"2480\":[7,34],\"2481\":[2,129],\"2482\":[1,64],\"2483\":[5],\"2484\":[6],\"2485\":[6,46],\"2486\":[7,81],\"2487\":[9,42],\"2488\":[3],\"2489\":[9],\"2490\":[7,93],\"2491\":[9,40],\"2492\":[7,72],\"2493\":[2],\"2494\":[2,96],\"2495\":[3],\"2496\":[8],\"2497\":[7,70],\"2498\":[5,90],\"2499\":[7,64],\"2500\":[7,211],\"2501\":[7,92],\"2502\":[8,34],\"2503\":[2,80],\"2504\":[1,45],\"2505\":[5],\"2506\":[2,133],\"2507\":[2,70],\"2508\":[7,78],\"2509\":[3],\"2510\":[7,162],\"2511\":[4],\"2512\":[2,83],\"2513\":[2,75],\"2514\":[2,122],\"2515\":[6,71],\"2516\":[4,21],\"2517\":[2,38],\"2518\":[7,46],\"2519\":[2,25],\"2520\":[2,97],\"2521\":[4,108],\"2522\":[5,77],\"2523\":[5,150],\"2524\":[10,80],\"2525\":[2,94],\"2526\":[2,16],\"2527\":[1,23],\"2528\":[6,44],\"2529\":[2,79],\"2530\":[9,177],\"2531\":[3,67],\"2532\":[2,30],\"2533\":[5,44],\"2534\":[8,95],\"2535\":[2,33],\"2536\":[7,126],\"2537\":[7,139],\"2538\":[2],\"2539\":[6,83],\"2540\":[1],\"2541\":[6,45],\"2542\":[6,61],\"2543\":[7,103],\"2544\":[7,63],\"2545\":[2,95],\"2546\":[1,37],\"2547\":[2,38],\"2548\":[6,44],\"2549\":[3,12],\"2550\":[2,51],\"2551\":[6,26],\"2552\":[2,87],\"2553\":[2,21],\"2554\":[5,166],\"2555\":[2,247],\"2556\":[6,19],\"2557\":[7,17],\"2558\":[4,263],\"2559\":[6,116],\"2560\":[6,38],\"2561\":[6,16],\"2562\":[3,75],\"2563\":[4,66],\"2564\":[6,271],\"2565\":[6,43],\"2566\":[5,39],\"2567\":[3,51],\"2568\":[7,225],\"2569\":[8,116],\"2570\":[5,39],\"2571\":[4,48],\"2572\":[4,101],\"2573\":[6,92],\"2574\":[10,77],\"2575\":[3,28],\"2576\":[4,47],\"2577\":[3],\"2578\":[2,81],\"2579\":[2,53],\"2580\":[4,66],\"2581\":[5,58],\"2582\":[5,115],\"2583\":[7,60],\"2584\":[7,334],\"2585\":[10,221],\"2586\":[5,42],\"2587\":[5,26],\"2588\":[9,14],\"2589\":[8,16],\"2590\":[7,16],\"2591\":[2,28],\"2592\":[3,119],\"2593\":[4,50],\"2594\":[5],\"2595\":[2,2],\"2596\":[4,76],\"2597\":[3,30],\"2598\":[3,43],\"2599\":[5,50],\"2600\":[4,183],\"2601\":[4,34],\"2602\":[1,10],\"2603\":[2],\"2604\":[6,39],\"2605\":[7,81],\"2606\":[9,26],\"2607\":[5,40],\"2608\":[3],\"2609\":[9,88],\"2610\":[9,22],\"2611\":[2],\"2612\":[2,107],\"2613\":[3],\"2614\":[8,44],\"2615\":[4,48],\"2616\":[5,86],\"2617\":[7,192],\"2618\":[4,113],\"2619\":[1,10],\"2620\":[5],\"2621\":[6,39],\"2622\":[7,81],\"2623\":[9,26],\"2624\":[5,40],\"2625\":[3],\"2626\":[9,88],\"2627\":[9,22],\"2628\":[7,72],\"2629\":[2],\"2630\":[2,112],\"2631\":[3],\"2632\":[8,44],\"2633\":[4,48],\"2634\":[5,86],\"2635\":[7,231],\"2636\":[4],\"2637\":[5,32],\"2638\":[7,117],\"2639\":[5,34],\"2640\":[5,41],\"2641\":[7,39],\"2642\":[5,48],\"2643\":[9,28],\"2644\":[7,43],\"2645\":[9,23],\"2646\":[5,50],\"2647\":[3,24],\"2648\":[7,80],\"2649\":[7,100],\"2650\":[4,33],\"2651\":[1,30],\"2652\":[4],\"2653\":[2,130],\"2654\":[2,50],\"2655\":[1,46],\"2656\":[4],\"2657\":[2,73],\"2658\":[2,50],\"2659\":[2,120],\"2660\":[1,49],\"2661\":[1,3],\"2662\":[2],\"2663\":[1],\"2664\":[1],\"2665\":[1],\"2666\":[2],\"2667\":[2],\"2668\":[1],\"2669\":[1],\"2670\":[1],\"2671\":[1],\"2672\":[1],\"2673\":[1],\"2674\":[1],\"2675\":[1],\"2676\":[1],\"2677\":[1],\"2678\":[1],\"2679\":[1],\"2680\":[1],\"2681\":[1],\"2682\":[2],\"2683\":[1],\"2684\":[1],\"2685\":[1],\"2686\":[1],\"2687\":[1],\"2688\":[1],\"2689\":[1],\"2690\":[2],\"2691\":[2],\"2692\":[1],\"2693\":[1],\"2694\":[1],\"2695\":[1],\"2696\":[2],\"2697\":[1],\"2698\":[1],\"2699\":[1],\"2700\":[1],\"2701\":[1],\"2702\":[1],\"2703\":[1],\"2704\":[1],\"2705\":[1],\"2706\":[1],\"2707\":[1],\"2708\":[1],\"2709\":[2],\"2710\":[1],\"2711\":[1],\"2712\":[1],\"2713\":[1],\"2714\":[1],\"2715\":[1],\"2716\":[1],\"2717\":[1],\"2718\":[1],\"2719\":[1],\"2720\":[1],\"2721\":[1],\"2722\":[1],\"2723\":[4],\"2724\":[1],\"2725\":[4],\"2726\":[4],\"2727\":[1],\"2728\":[1],\"2729\":[1],\"2730\":[1],\"2731\":[1],\"2732\":[1]},\"averageFieldLength\":[4.710574460300033,60.42429044167099],\"storedFields\":{\"0\":{\"h\":\"Docker\"},\"1\":{\"h\":\"Execute in docker\",\"t\":[\"To work inside a docker container, execute run.sh located inside the docker directory. It will download the requested image and build a container to execute the main program specified by the following GPU, ASR example, and outside directory information, as follows:\",\"$ cd docker $ ./run.sh --docker-gpu 0 --docker-egs chime4/asr1 --docker-folders /export/corpora4/CHiME4/CHiME3 --dlayers 1 --ngpu 1\",\"Optionally, you can set the CUDA version with the arguments --docker-cuda respectively (default version set at CUDA=9.1). The docker container can be built based on the CUDA installed in your computer if you empty this arguments. By default, all GPU-based images are built with NCCL v2 and CUDNN v7. The arguments required for the docker configuration have a prefix \\\"--docker\\\" (e.g., --docker-gpu, --docker-egs, --docker-folders). run.sh accept all normal ESPnet arguments, which must be followed by these docker arguments. All docker containers are executed using the same user as your login account. If you want to run the docker in root access, add the flag --is-root to command line. In addition, you can pass any environment variable using --docker-env (e.g., --docker-env \\\"foo=path\\\")\"]},\"2\":{\"h\":\"ESPnet 2 Recipes\",\"t\":[\"To work with recipes of ESPnet 2, you will need to add the flag --is-egs2 to the command line:\",\"$ cd docker $ ./run.sh --docker-gpu 0 --is-egs2 --docker-egs an4/asr1 --ngpu1\",\"Remember to add the flag before the arguments you want to pass to the recipe run.sh file.\"]},\"3\":{\"h\":\"Using GPU-based containers\",\"t\":[\"You can run any bash script implemented in the egs folder using --docker-cmd:\",\"$ cd docker $ ./run.sh --docker-gpu 0 --docker-egs chime4/asr1 --docker-cmd foo.sh --arg_1 &lt;arg_1&gt; --arg_2 &lt;arg_2&gt;\",\"The arguments for the desired script should follow the docker arguments. run.sh is the default script to be executed.\",\"Multiple GPUs should be specified with the following options:\",\"$ cd docker $ ./run.sh --docker-gpu 0,1,2 --docker-egs chime5/asr1 --docker-folders /export/corpora4/CHiME5 --ngpu 3\",\"Note that all experimental files and results are created under the normal example directories (egs/&lt;example&gt;/).\",\"Multiple folders and environment variables should be specified with commas and without spaces:\",\"$ cd docker $ ./run.sh --docker-gpu 0 --docker-egs chime4/asr1 --docker-folders /export/corpus/CHiME4,/export/corpus/LDC/LDC93S6B,/export/corpus/LDC/LDC94S13B --docker-env \\\"CHIME4_CORPUS=/export/corpus/CHiME4/CHiME3,WSJ0_CORPUS=/export/corpus/LDC/LDC93S6B,WSJ1_CORPUS=/export/corpus/LDC/LDC94S13B\\\" --ngpu 1\",\"Remember that for some recipes, you first need to download the Corpus before running the experiments, such as CHiME, WSJ, and LDC corporas. You will need to set the directories where these were downloaded and replace them in the recipe (e.g.: CHIME4_CORPUS=/&lt;dir_where_chime4_was_downloaded&gt;/CHiME4/CHiME3)\"]},\"4\":{\"h\":\"Using CPU-based container\",\"t\":[\"You can train a model in CPU using the following command:\",\"$ cd docker $ ./run.sh --docker-gpu -1 --docker-egs an4/asr1 --ngpu 0\",\"The script will build a docker if your are using a user different from root user. To use containers with root access add the flag --is-root to the command line.\"]},\"5\":{\"h\":\"Local builds\",\"t\":[\"When building the docker container on a local machine, the espnet source is downloaded from the github espnet master branch. However, in some cases, \\\"local\\\" builds are preferable, that are built based on the source code from the local repository:\",\"After writing own modifications on the espnet code, the build environment, etc., and to test it in the docker container. Prebuilt docker containers do not import these.\",\"Reproducability: It is possible to go back to an espnet version at a certain commit and test the neural network with an older version of a library.\",\"The script build.sh supports making local builds for this purpose. During the docker build process, the local espnet source code is imported through a git archive based on git HEAD (the previous commit), and copied over within a file.\",\"For example, a local build that the base image from Docker Hub (espnet/espnet:runtime, based on Ubuntu 16), that already contains a kaldi installation, using Cuda 10.0:\",\"./build.sh local 10.0\",\"Also, docker images can also be built based on the Ubuntu version specified in prebuilt/runtime/Dockerfile (currently set to Ubuntu 18.04), in this example case using the cpu:\",\"./build.sh fully_local cpu\",\"Local container builds then are started by adding the flag --is-local when using run.sh, e.g., for the Cuda 10.0 image:\",\"$ ./run.sh --is-local --docker_cuda 10.0 --docker_gpu 0 ...\"]},\"6\":{\"h\":\"Deprecated\",\"t\":[\"Containers build on ubuntu-16.04 will be deprecated and no longer receive support. However, these container will remain in Docker Hub. To use containers with ubuntu 16.04, empty the flag --docker_os.\"]},\"7\":{\"h\":\"Tags\",\"t\":[\"Runtime: Base image for ESPnet. It includes libraries and Kaldi installation.\",\"CPU: Image to execute only in CPU.\",\"GPU: Image to execute examples with GPU support.\"]},\"8\":{\"h\":\"Ubuntu 18.04\",\"t\":[\"Pytorch 1.3.1, No warp-ctc:\",\"cuda10.1-cudnn7 (docker/prebuilt/gpu/10.1/cudnn7/Dockerfile)\",\"Pytorch 1.0.1, warp-ctc:\",\"cuda10.0-cudnn7 (docker/prebuilt/gpu/10.0/cudnn7/Dockerfile)\",\"cpu-u18 (docker/prebuilt/devel/Dockerfile)\"]},\"9\":{\"h\":\"ESPnet document generation\"},\"10\":{\"h\":\"Install\",\"t\":[\"We use sphinx to generate HTML documentation.\",\"$ cd &lt;espnet_root&gt; $ pip install -e \\\".[doc]\\\" $ pip install -U flake8-docstrings\"]},\"11\":{\"h\":\"Style check using flake8-docstrings\",\"t\":[\"You can check that your docstring style is correct by ci/test_flake8.sh using flake8-docstrings. Note that many existing files have been added to the black list in the script to avoid this check by default. You can freely remove files that you wanna improve.\",\"# in ci/test_flake8.sh # you can improve poorly written docstrings from here flake8_black_list=\\\"\\\\ espnet/__init__.py espnet/asr/asr_mix_utils.py espnet/asr/asr_utils.py espnet/asr/chainer_backend/asr.py ... \\\" # --extend-ignore for wip files for flake8-docstrings flake8 --extend-ignore=D test utils doc ${flake8_black_list} # white list of files that should support flake8-docstrings flake8 espnet --exclude=${flake8_black_list//$'\\\\n'/,}\",\"DO NOT ADD NEW FILES TO THIS BLACK LIST!\"]},\"12\":{\"h\":\"Generate HTML\",\"t\":[\"You can generate local HTML manually using sphinx Makefile\",\"$ cd &lt;espnet_root&gt; $ ./ci/doc.sh\",\"open doc/build/index.html\"]},\"13\":{\"h\":\"Deploy\",\"t\":[\"When your PR is merged into master branch, our CI will automatically deploy your sphinx html into https://espnet.github.io/espnet/.\"]},\"14\":{\"h\":\"Usage\",\"t\":[\"If you're a new user, we suggest checking out the ESPnet2 tutorial as ESPnet1 is an older implementation. The majority of the development has now shifted to ESPnet2. Please be aware that certain information in this document may be outdated due to this shift.\"]},\"15\":{\"h\":\"Directory structure\",\"t\":[\"espnet/ # Python modules utils/ # Utility scripts of ESPnet test/ # Unit test test_utils/ # Unit test for executable scripts egs/ # The complete recipe for each corpora an4/ # AN4 is tiny corpus and can be obtained freely, so it might be suitable for tutorial asr1/ # ASR recipe - run.sh # Executable script - cmd.sh # To select the backend for job scheduler - path.sh # Setup script for environment variables - conf/ # Containing Configuration files - steps/ # The steps scripts from Kaldi - utils/ # The utils scripts from Kaldi tts1/ # TTS recipe ...\"]},\"16\":{\"h\":\"Execution of example scripts\",\"t\":[\"Move to an example directory under the egs directory. We prepare several major ASR benchmarks including WSJ, CHiME-4, and TED. The following directory is an example of performing ASR experiment with the CMU Census Database (AN4) recipe.\",\"$ cd egs/an4/asr1\",\"Once move to the directory, then, execute the following main script with a chainer backend:\",\"$ ./run.sh --backend chainer\",\"or execute the following main script with a pytorch backend:\",\"$ ./run.sh --backend pytorch\",\"With this main script, you can perform a full procedure of ASR experiments including\",\"Data download\",\"Data preparation (Kaldi style)\",\"Feature extraction (Kaldi style)\",\"Dictionary and JSON format data preparation\",\"Training based on chainer or pytorch.\",\"Recognition and scoring\"]},\"17\":{\"h\":\"Logging\",\"t\":[\"The training progress (loss and accuracy for training and validation data) can be monitored with the following command\",\"$ tail -f exp/${expdir}/train.log\",\"When we use ./run.sh --verbose 0 (--verbose 0 is default in most recipes), it gives you the following information\",\"epoch iteration main/loss main/loss_ctc main/loss_att validation/main/loss validation/main/loss_ctc validation/main/loss_att main/acc validation/main/acc elapsed_time eps : : 6 89700 63.7861 83.8041 43.768 0.731425 136184 1e-08 6 89800 71.5186 93.9897 49.0475 0.72843 136320 1e-08 6 89900 72.1616 94.3773 49.9459 0.730052 136473 1e-08 7 90000 64.2985 84.4583 44.1386 72.506 94.9823 50.0296 0.740617 0.72476 137936 1e-08 7 90100 81.6931 106.74 56.6462 0.733486 138049 1e-08 7 90200 74.6084 97.5268 51.6901 0.731593 138175 1e-08 total [#################.................................] 35.54% this epoch [#####.............................................] 10.84% 91300 iter, 7 epoch / 20 epochs 0.71428 iters/sec. Estimated time to finish: 2 days, 16:23:34.613215.\",\"Note that the an4 recipe uses --verbose 1 as default since this recipe is often used for a debugging purpose.\",\"In addition Tensorboard events are automatically logged in the tensorboard/${expname} folder. Therefore, when you install Tensorboard, you can easily compare several experiments by using\",\"$ tensorboard --logdir tensorboard\",\"and connecting to the given address (default : localhost:6006). This will provide the following information: Note that we would not include the installation of Tensorboard to simplify our installation process. Please install it manually (pip install tensorflow; pip install tensorboard) when you want to use Tensorboard.\"]},\"18\":{\"h\":\"Change options in run.sh\",\"t\":[\"We rely on utils/parse_options.sh to paser command line arguments in shell script and it's used in run.sh:\",\"e.g. If the script has ngpu option\",\"#!/usr/bin/env bash # run.sh ngpu=1 . utils/parse_options.sh echo ${ngpu}\",\"Then you can change the value as following:\",\"$ ./run.sh --ngpu 2 echo 2\"]},\"19\":{\"h\":\"Use of GPU\",\"t\":[\"Training: If you want to use GPUs in your experiment, please set --ngpu option in run.sh appropriately, e.g.,\",\" # use single gpu $ ./run.sh --ngpu 1 # use multi-gpu $ ./run.sh --ngpu 3 # if you want to specify gpus, set CUDA_VISIBLE_DEVICES as follows # (Note that if you use slurm, this specification is not needed) $ CUDA_VISIBLE_DEVICES=0,1,2 ./run.sh --ngpu 3 # use cpu $ ./run.sh --ngpu 0\",\"Default setup uses a single GPU (--ngpu 1).\",\"ASR decoding: ESPnet also supports the GPU-based decoding for fast recognition.\",\"Please manually remove the following lines in run.sh:\",\"#### use CPU for decoding ngpu=0\",\"Set 1 or more values for --batchsize option in asr_recog.py to enable GPU decoding\",\"And execute the script (e.g., run.sh --stage 5 --ngpu 1)\",\"You'll achieve significant speed improvement by using the GPU decoding\"]},\"20\":{\"h\":\"ESPnet1 Transducer\",\"t\":[\"Important: If you encounter any issue related to Transducer loss, please open an issue in our fork of warp-transducer.\",\"ESPnet supports models trained with Transducer loss, aka Transducer models. To train such model, the following should be set in the training config:\",\"criterion: loss model-module: \\\"espnet.nets.pytorch_backend.e2e_asr_transducer:E2E\\\"\"]},\"21\":{\"h\":\"Architecture\",\"t\":[\"Several Transducer architectures are currently available in ESPnet:\",\"RNN-Transducer (default, e.g.: etype: blstm with dtype: lstm)\",\"Custom-Transducer (e.g.: etype: custom and dtype: custom)\",\"Mixed Custom/RNN-Transducer (e.g: etype: custom with dtype: lstm)\",\"The architecture specification is separated for the encoder and decoder part, and defined by the user through, respectively, etype and dtype in the training config. If custom is specified for either, a customizable architecture will be used for the corresponding part. Otherwise, an RNN-based architecture will be selected.\",\"Here, the custom architecture is a unique feature of the Transducer model in ESPnet. It was made available to add some flexibility in the architecture definition and ease the reproduction of some SOTA Transducer models mixing different layers types or parameters within the same model part (encoder or decoder). As such, the architecture definition is different compared to the RNN architecture :\",\"Each block (or layer) of the custom architecture should be specified individually through enc-block-arch or/and dec-block-arch parameters:\",\"# e.g: Conv-Transformer encoder etype: custom enc-block-arch: - type: conv1d idim: 80 odim: 32 kernel_size: [3, 7] stride: [1, 2] - type: conv1d idim: 32 odim: 32 kernel_size: 3 stride: 2 - type: conv1d idim: 32 odim: 384 kernel_size: 3 stride: 1 - type: transformer d_hidden: 384 d_ff: 1536 heads: 4\",\"Different block types are allowed for the custom encoder (tdnn, conformer or transformer) and the custom decoder (causal-conv1d or transformer). Each one has a set of mandatory and optional parameters :\",\"# 1D convolution (TDNN) block - type: conv1d idim: [Input dimension. (int)] odim: [Output dimension. (int)] kernel_size: [Size of the context window. (int or tuple)] stride (optional): [Stride of the sliding blocks. (int or tuple, default = 1)] dilation (optional): [Parameter to control the stride of elements within the neighborhood. (int or tuple, default = 1)] groups (optional): [Number of blocked connections from input channels to output channels. (int, default = 1) bias (optional): [Whether to add a learnable bias to the output. (bool, default = True)] use-relu (optional): [Whether to use a ReLU activation after convolution. (bool, default = True)] use-batchnorm: [Whether to use batch normalization after convolution. (bool, default = False)] dropout-rate (optional): [Dropout-rate for TDNN block. (float, default = 0.0)] # Transformer - type: transformer d_hidden: [Input/output dimension of Transformer block. (int)] d_ff: [Hidden dimension of the Feed-forward module. (int)] heads: [Number of heads in multi-head attention. (int)] dropout-rate (optional): [Dropout-rate for Transformer block. (float, default = 0.0)] pos-dropout-rate (optional): [Dropout-rate for positional encoding module. (float, default = 0.0)] att-dropout-rate (optional): [Dropout-rate for attention module. (float, default = 0.0)] # Conformer - type: conformer d_hidden: [Input/output dimension of Conformer block (int)] d_ff: [Hidden dimension of the Feed-forward module. (int)] heads: [Number of heads in multi-head attention. (int)] macaron_style: [Whether to use macaron style. (bool)] use_conv_mod: [Whether to use convolutional module. (bool)] conv_mod_kernel (required if use_conv_mod = True): [Number of kernel in convolutional module. (int)] dropout-rate (optional): [Dropout-rate for Transformer block. (float, default = 0.0)] pos-dropout-rate (optional): [Dropout-rate for positional encoding module. (float, default = 0.0)] att-dropout-rate (optional): [Dropout-rate for attention module. (float, default = 0.0)] # Causal Conv1d - type: causal-conv1d idim: [Input dimension. (int)] odim: [Output dimension. (int)] kernel_size: [Size of the context window. (int)] stride (optional): [Stride of the sliding blocks. (int, default = 1)] dilation (optional): [Parameter to control the stride of elements within the neighborhood. (int, default = 1)] groups (optional): [Number of blocked connections from input channels to output channels. (int, default = 1) bias (optional): [Whether to add a learnable bias to the output. (bool, default = True)] use-relu (optional): [Whether to use a ReLU activation after convolution. (bool, default = True)] use-batchnorm: [Whether to use batch normalization after convolution. (bool, default = False)] dropout-rate (optional): [Dropout-rate for TDNN block. (float, default = 0.0)]\",\"The defined architecture can be repeated by specifying the total number of blocks/layers in the architecture through enc-block-repeat or/and dec-block-repeat parameters:\",\"# e.g.: 2x (Causal-Conv1d + Transformer) decoder dtype: transformer dec-block-arch: - type: causal-conv1d idim: 256 odim: 256 kernel_size: 5 - type: transformer d_hidden: 256 d_ff: 256 heads: 4 dropout-rate: 0.1 att-dropout-rate: 0.4 dec-block-repeat: 2\"]},\"22\":{\"h\":\"Multi-task learning\",\"t\":[\"We also support multi-task learning with various auxiliary losses, such as: CTC, cross-entropy w/ label-smoothing (LM loss), auxiliary Transducer, and symmetric KL divergence. The four losses can be simultaneously trained with main Transducer loss to jointly optimize the total loss defined as:\",\"augmented Transducer training\",\"where the losses are respectively, in order: The main Transducer loss, the CTC loss, the auxiliary Transducer loss, the symmetric KL divergence loss, and the LM loss. Lambda values define their respective contribution to the overall loss. Additionally, each loss can be independently selected or omitted depending on the task.\",\"Each loss can be defined in the training config alongside its specific options, such as follow:\",\"# Transducer loss (L1) transducer-loss-weight: [Weight of the main Transducer loss (float)] # CTC loss (L2) use-ctc-loss: True ctc-loss-weight (optional): [Weight of the CTC loss. (float, default = 0.5)] ctc-loss-dropout-rate (optional): [Dropout rate for encoder output representation. (float, default = 0.0)] # Auxiliary Transducer loss (L3) use-aux-transducer-loss: True aux-transducer-loss-weight (optional): [Weight of the auxiliary Transducer loss. (float, default = 0.4)] aux-transducer-loss-enc-output-layers (required if use-aux-transducer-loss = True): [List of intermediate encoder layer IDs to compute auxiliary Transducer loss(es). (list)] aux-transducer-loss-mlp-dim (optional): [Hidden dimension for the MLP network. (int, default = 320)] aux-transducer-loss-mlp-dropout-rate: [Dropout rate for the MLP network. (float, default = 0.0)] # Symmetric KL divergence loss (L4) # Note: It can be only used in addition to the auxiliary Transducer loss. use-symm-kl-div-loss: True symm-kl-div-loss-weight (optional): [Weight of the symmetric KL divergence loss. (float, default = 0.2)] # LM loss (L5) use-lm-loss: True lm-loss-weight (optional): [Weight of the LM loss. (float, default = 0.2)] lm-loss-smoothing-rate: [Smoothing rate for LM loss. If > 0, label smoothing is enabled. (float, default = 0.0)]\"]},\"23\":{\"h\":\"Inference\",\"t\":[\"Various decoding algorithms are also available for Transducer by setting beam-size and search-type parameters in decode config.\",\"Greedy search constrained to one emission by timestep (beam-size: 1).\",\"Beam search algorithm without prefix search (beam-size: >1 and search-type: default).\",\"Time Synchronous Decoding [Saon et al., 2020] (beam-size: >1 and search-type: tsd).\",\"Alignment-Length Synchronous Decoding [Saon et al., 2020] (beam-size: >1 and search-type: alsd).\",\"N-step Constrained beam search modified from [Kim et al., 2020] (beam-size: >1 and search-type: default).\",\"modified Adaptive Expansion Search, based on [Kim et al., 2021] and NSC (beam-size: >1 and search-type: maes).\",\"The algorithms share two parameters to control beam size (beam-size) and final hypotheses normalization (score-norm-transducer). The specific parameters for each algorithm are:\",\"# Default beam search search-type: default # Time-synchronous decoding search-type: tsd max-sym-exp: [Number of maximum symbol expansions at each time step (int)] # Alignement-length decoding search-type: alsd u-max: [Maximum output sequence length (int)] # N-step Constrained beam search search-type: nsc nstep: [Number of maximum expansion steps at each time step (int)] # nstep = max-sym-exp + 1 (blank) prefix-alpha: [Maximum prefix length in prefix search (int)] # modified Adaptive Expansion Search search-type: maes nstep: [Number of maximum expansion steps at each time step (int, > 1)] prefix-alpha: [Maximum prefix length in prefix search (int)] expansion-gamma: [Number of additional candidates in expanded hypotheses selection (int)] expansion-beta: [Allowed logp difference for prune-by-value method (float, > 0)]\",\"Except for the default algorithm, the described parameters are used to control the performance and decoding speed. The optimal values for each parameter are task-dependent; a high value will typically increase decoding time to focus on performance while a low value will improve decoding time at the expense of performance.\"]},\"24\":{\"h\":\"Additional notes\",\"t\":[\"Similarly to training with CTC, Transducer does not output the validation accuracy. Thus, the optimum model is selected with its loss value (i.e., --recog_model model.loss.best).\",\"There are several differences between MTL and Transducer training/decoding options. The users should refer to espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py for an overview and espnet/espnet/nets/pytorch_backend/transducer/arguments for all possible arguments.\",\"FastEmit regularization [Yu et al., 2021] is available through --fastemit-lambda training parameter (default = 0.0).\",\"RNN-decoder pre-initialization using an LM is supported. Note that regular decoder keys are expected. The LM state dict keys (predictor.*) will be renamed according to AM state dict keys (dec.*).\",\"Transformer-decoder pre-initialization using a Transformer LM is not supported yet.\"]},\"25\":{\"h\":\"Changing the training configuration\",\"t\":[\"The default configurations for training and decoding are written in conf/train.yaml and conf/decode.yaml respectively. It can be overwritten by specific arguments: e.g.\",\"# e.g. asr_train.py --config conf/train.yaml --batch-size 24 # e.g.--config2 and --config3 are also provided and the latter option can overwrite the former. asr_train.py --config conf/train.yaml --config2 conf/new.yaml\",\"In this way, you need to edit run.sh and it might be inconvenient sometimes. Instead of giving arguments directly, we recommend you to modify the yaml file and give it to run.sh:\",\"# e.g. ./run.sh --train-config conf/train_modified.yaml # e.g. ./run.sh --train-config conf/train_modified.yaml --decode-config conf/decode_modified.yaml\",\"We also provide a utility to generate a yaml file from the input yaml file:\",\"# e.g. You can give any parameters as '-a key=value' and '-a' is repeatable. # This generates new file at 'conf/train_batch-size24_epochs10.yaml' ./run.sh --train-config $(change_yaml.py conf/train.yaml -a batch-size=24 -a epochs=10) # e.g. '-o' option specifies the output file name instead of auto named file. ./run.sh --train-config $(change_yaml.py conf/train.yaml -o conf/train2.yaml -a batch-size=24)\"]},\"26\":{\"h\":\"How to set minibatch\",\"t\":[\"From espnet v0.4.0, we have three options in --batch-count to specify minibatch size (see espnet.utils.batchfy for implementation);\",\"--batch-count seq --batch-seqs 32 --batch-seq-maxlen-in 800 --batch-seq-maxlen-out 150.\",\"This option is compatible to the old setting before v0.4.0. This counts the minibatch size as the number of sequences and reduces the size when the maximum length of the input or output sequences is greater than 800 or 150, respectively.\",\"--batch-count bin --batch-bins 100000.\",\"This creates the minibatch that has the maximum number of bins under 100 in the padded input/output minibatch tensor (i.e., max(ilen) * idim + max(olen) * odim). Basically, this option makes training iteration faster than --batch-count seq. If you already has the best --batch-seqs x config, try --batch-bins $((x * (mean(ilen) * idim + mean(olen) * odim))).\",\"--batch-count frame --batch-frames-in 800 --batch-frames-out 100 --batch-frames-inout 900.\",\"This creates the minibatch that has the maximum number of input, output and input+output frames under 800, 100 and 900, respectively. You can set one of --batch-frames-xxx partially. Like --batch-bins, this option makes training iteration faster than --batch-count seq. If you already has the best --batch-seqs x config, try --batch-frames-in $((x * (mean(ilen) * idim)) --batch-frames-out $((x * mean(olen) * odim)).\"]},\"27\":{\"h\":\"How to use finetuning\",\"t\":[\"ESPnet currently supports two finetuning operations: transfer learning and freezing. We expect the user to define the following options in its main training config (e.g.: conf/train*.yaml). If needed, they can be directly passed to (asr|tts|vc)_train.py by adding the prefix -- to the options.\"]},\"28\":{\"h\":\"Transfer learning\",\"t\":[\"Transfer learning option is split between encoder initialization (enc-init) and decoder initialization (dec-init). However, the same model can be specified for both options.\",\"Each option takes a snapshot path (e.g.: [espnet_model_path]/results/snapshot.ep.1) or model path (e.g.: [espnet_model_path]/results/model.loss.best) as argument.\",\"Additionally, a list of encoder and decoder modules (separated by a comma) can also be specified to control the modules to transfer with the options enc-init-mods and dec-init-mods.\",\"For each specified module, we only expect a partial match with the start of the target model module name. Thus, multiple modules can be specified with the same key if they share a common prefix.\",\" > Mandatory: `enc-init: /home/usr/espnet/egs/vivos/asr1/exp/train_nodev_pytorch_train/results/model.loss.best` -> specify a pre-trained model on VIVOS for transfer learning. > Example 1: `enc-init-mods: 'enc.'` -> transfer all encoder parameters. > Example 2: `enc-init-mods: 'enc.embed.,enc.0.'` -> transfer encoder embedding layer and first layer parameters.\"]},\"29\":{\"h\":\"Freezing\",\"t\":[\"Freezing option can be enabled with freeze-mods, (freeze_param in espnet2).\",\"The option take a list of model modules (separated by a comma) as argument. As previously, we do not expect a complete match for the specified modules.\",\" > Example 1: `freeze-mods: 'enc.embed.'` -> freeze encoder embedding layer parameters. > Example 2: `freeze-mods: 'dec.embed,dec.0.'` -> freeze decoder embedding layer and first layer parameters. > Example 3 (espnet2): `freeze_param: 'encoder.embed'` -> freeze encoder embedding layer parameters.\"]},\"30\":{\"h\":\"Important notes\",\"t\":[\"Given a pre-trained source model, the modules specified for transfer learning are expected to have the same parameters (i.e.: layers and units) as the target model modules.\",\"We also support initialization with a pre-trained RNN LM for the RNN-Transducer decoder.\",\"RNN models use different key names for encoder and decoder parts compared to Transformer, Conformer or Custom models: \",\"RNN model use enc. for encoder part and dec. for decoder part.\",\"Transformer/Conformer/Custom model use encoder. for encoder part and decoder. for decoder part.\"]},\"31\":{\"h\":\"Chainer and Pytorch backends\",\"t\":[\"Chainer\",\"Pytorch\",\"Performance\",\"◎\",\"◎\",\"Speed\",\"○\",\"◎\",\"Multi-GPU\",\"supported\",\"supported\",\"VGG-like encoder\",\"supported\",\"supported\",\"Transformer\",\"supported\",\"supported\",\"RNNLM integration\",\"supported\",\"supported\",\"#Attention types\",\"3 (no attention, dot, location)\",\"12 including variants of multihead\",\"TTS recipe support\",\"no support\",\"supported\"]},\"32\":{\"h\":\"Distributed training\",\"t\":[\"ESPnet2 provides some kinds of data-parallel distributed training.\",\"DP/DDP\",\"Single/Multi host\",\"Option\",\"Multi-processing with single host\",\"DistributedDataParallel\",\"Single\",\"--ngpu N-GPU --multiprocessing_distributed true\",\"Multi-threading with single host\",\"DataParallel\",\"Single\",\"--ngpu N-GPU --multiprocessing_distributed false\",\"Multi-processing with N-HOST jobs with N-GPU for each host (=N-HOSTxN-GPU nodes)\",\"DistributedDataParallel\",\"Multi\",\"--dist_world_size N-HOST --ngpu N-GPU --multiprocessing_distributed true\",\"Multi-threading with N-HOST jobs with N-GPU for each host (=N-HOSTxN-GPU nodes)\",\"DistributedDataParallel\",\"Multi\",\"--dist_world_size N-HOST --ngpu N-GPU --multiprocessing_distributed false\",\"N-NODE jobs with 1-GPU for each node\",\"DistributedDataParallel\",\"Single/Multi\",\"--dist_world_size N-NODE --ngpu 1\"]},\"33\":{\"h\":\"Examples\",\"t\":[\"Note: The behavior of batch size in ESPnet2 during multi-GPU training is different from that in ESPnet1. In ESPnet2, the total batch size is not changed regardless of the number of GPUs. Therefore, you need to manually increase the batch size if you increase the number of GPUs. Please refer to this doc for more information.\"]},\"34\":{\"h\":\"Single node with 4GPUs with distributed mode\",\"t\":[\"% python -m espnet2.bin.asr_train --ngpu 4 --multiprocessing_distributed true\",\"You can disable distributed mode and switch to threading based data parallel as follows:\",\"% python -m espnet2.bin.asr_train --ngpu 4 --multiprocessing_distributed false\",\"If you meet some errors with distributed mode, please try single gpu mode or multi-GPUs with --multiprocessing_distributed false before reporting the issue.\"]},\"35\":{\"h\":\"To enable sharded training\",\"t\":[\"We supports sharded training provided by fairscale\",\"% python -m espnet2.bin.asr_train --ngpu 4 --multiprocessing_distributed true --sharded_ddp true\",\"Note that the other features of fairscale are not supported now.\"]},\"36\":{\"h\":\"2Host and 2GPUs for each host with multiprocessing distributed mode\",\"t\":[\"Note that multiprocessing distributed mode assumes the same number of GPUs for each node.\",\"(host1) % python -m espnet2.bin.asr_train \\\\ --multiprocessing_distributed true \\\\ --ngpu 2 \\\\ --dist_rank 0 \\\\ --dist_world_size 2 \\\\ --dist_master_addr host1 \\\\ --dist_master_port &lt;any-free-port&gt; (host2) % python -m espnet2.bin.asr_train \\\\ --multiprocessing_distributed true \\\\ --ngpu 2 \\\\ --dist_rank 1 \\\\ --dist_world_size 2 \\\\ --dist_master_addr host1 \\\\ --dist_master_port &lt;any-free-port&gt;\"]},\"37\":{\"h\":\"RANK and WORLD_SIZE\",\"t\":[\"--dist_rank and --dist_world_size indicate RANK and WORLD_SIZE in terms of MPI; i.e., they indicate the id of each processe and the number of processes respectively. They can be also specified by the environment variables ${RANK} and ${WORLD_SIZE}.\"]},\"38\":{\"h\":\"About init method\",\"t\":[\"See: https://pytorch.org/docs/stable/distributed.html#tcp-initialization\",\"There are two ways to initialize and these methods can be interchanged in all examples.\",\"TCP initialization\",\"# These three are equivalent: --dist_master_addr &lt;rank0-host&gt; --dist_master_port &lt;any-free-port&gt; --dist_init_method \\\"tcp://&lt;rank0-host&gt;:&lt;any-free-port&gt;\\\" export MASTER_ADDR=&lt;rank0-host&gt; MASTER_PORT=&lt;any-free-port&gt;\",\"Shared file system initialization\",\"--dist_init_method \\\"file:///nfs/some/where/filename\\\"\",\"This initialization might be failed if the previous file is existing. I recommend you to use a random file name to avoid to reuse it. e.g.\",\"--dist_init_method \\\"file://$(pwd)/.dist_init_$(openssl rand -base64 12)\\\"\"]},\"39\":{\"h\":\"2Hosts which have 2GPUs and 1GPU respectively\",\"t\":[\"(host1) % python -m espnet2.bin.asr_train \\\\ --ngpu 1 \\\\ --multiprocessing_distributed false \\\\ --dist_rank 0 \\\\ --dist_world_size 3 \\\\ --dist_master_addr host1 \\\\ --dist_master_port &lt;any-free-port&gt; (host1) % python -m espnet2.bin.asr_train \\\\ --ngpu 1 \\\\ --multiprocessing_distributed false \\\\ --dist_rank 1 \\\\ --dist_world_size 3 \\\\ --dist_master_addr host1 \\\\ --dist_master_port &lt;any-free-port&gt; (host2) % python -m espnet2.bin.asr_train \\\\ --ngpu 1 \\\\ --multiprocessing_distributed false \\\\ --dist_rank 2 \\\\ --dist_world_size 3 \\\\ --dist_master_addr host1 \\\\ --dist_master_port &lt;any-free-port&gt;\"]},\"40\":{\"h\":\"2Hosts and 2GPUs for each node using with multiprocessing distributed\",\"t\":[\" % srun -c2 -N2 --gres gpu:2 \\\\ python -m espnet2.bin.asr_train --ngpu 2 --multiprocessing_distributed true \\\\ --dist_launcher slurm \\\\ --dist_init_method \\\"file://$(pwd)/.dist_init_$(openssl rand -base64 12)\\\"\",\"I recommend shared-file initialization in this case because the host will be determined after submitting the job, therefore we can't tell the free port number before.\"]},\"41\":{\"h\":\"5GPUs with 3nodes using\",\"t\":[\"(Not tested)\",\"% srun -n5 -N3 --gpus-per-task 1 \\\\ python -m espnet2.bin.asr_train --ngpu 1 --multiprocessing_distributed false \\\\ --dist_launcher slurm \\\\ --dist_init_method \\\"file://$(pwd)/.dist_init_$(openssl rand -base64 12)\\\"\"]},\"42\":{\"h\":\"2Hosts and 2GPUs for each node using with multiprocessing distributed\",\"t\":[\" % mpirun -np 2 -host host1,host2 \\\\ python -m espnet2.bin.asr_train --ngpu 2 --multiprocessing_distributed true \\\\ --dist_launcher mpi \\\\ --dist_init_method \\\"file://$(pwd)/.dist_init_$(openssl rand -base64 12)\\\"\"]},\"43\":{\"h\":\"\",\"t\":[\"Coming soon...\"]},\"44\":{\"h\":\"Troubleshooting for NCCL with Ethernet case\",\"t\":[\"NCCL WARN Connect to 192.168.1.51&lt;51890&gt; failed : No route to host\",\"Reason: Firewall?\",\"Need to free all ports?\",\"NCCL INFO Call to connect returned Connection refused, retrying\",\"Reason: NIC is found, but connection is refused?\",\"Set NCCL_SOCKET_IFNAME=&lt;appropriate_interface&gt;\",\"NCCL WARN Bootstrap : no socket interface found\",\"Reason: Any NIC are not found . (Maybe NCCL_SOCKET_IFNAME is incorrect)\",\"Set NCCL_SOCKET_IFNAME=&lt;appropriate_interface&gt;.\",\"NCCL WARN peer mapping resources exhausted\",\"???\",\"https://devtalk.nvidia.com/default/topic/970010/cuda-programming-and-performance/cuda-peer-resources-error-when-running-on-more-than-8-k80s-aws-p2-16xlarge-/post/4994583/#4994583\"]},\"45\":{\"h\":\"The rules of\",\"t\":[\"See: https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html\",\"The default value is NCCL_SOCKET_IFNAME=^lo,docker.\",\"Support two syntax: white list or black list\",\"White list e.g.: NCCL_SOCKET_IFNAME=eth,em\",\"It's enough to specify the prefix only. You don't need to set it as eth0.\",\"Blacklist e.g.: ^virbr,lo,docker.\",\"If multiple network interfaces are found in your environment, the first is selected. \",\"You can check your environment by ifconfig for example. https://www.cyberciti.biz/faq/linux-list-network-interfaces-names-command/\",\"Note that lo is the first normally, so lo must be filtered.\",\"My recommended setting for a non-virtual environment\",\"NCCL_SOCKET_IFNAME=en,eth,em,bond Or, NCCL_SOCKET_IFNAME=^lo,docker,virbr,vmnet,vboxnet,wl,ww,ppp\",\"The prefix of network interface name\",\"Note\",\"lo\",\"Loopback.\",\"eth\",\"Ethernet. Classically used.\",\"em\",\"Ethernet. Dell machine?\",\"en\",\"Ethernet (Used in recent Linux. e.g CentOS7)\",\"wlan\",\"Wireless\",\"wl\",\"Wireless LAN (Used in recent Linux)\",\"ww\",\"Wireless wan (Used in recent Linux)\",\"ib\",\"IP over IB\",\"bond\",\"Bonding of multiple ethernets\",\"virbr\",\"Virtual bridge\",\"docker,vmnet,vboxnet\",\"Virtual machine\",\"ppp\",\"Point to point\"]},\"46\":{\"h\":\"Converting audio file formats using format_wav_scp.py\",\"t\":[\"The format_wav_scp.py is an utility to convert the audio format of the files specified wav.scp and the format_wav_scp.sh is a shell script wrapping format_wav_scp.py. In the typical case, in the stage3 of the template recipe, format_wav_scp.sh is used to convert the audio file format of your original corpus to the audio format which you actually want to feed to the DNN model.\",\"format_wav_scp.py and format_wav_scp.sh has same function of generation wav.scp from wav.scp, but format_wav_scp.sh is different in that it has the capability of parallel processing.\",\"wav.scp -> [format_wav_scp.py] -> wav.scp wav.scp -> [format_wav_scp.sh] -> wav.scp\",\"Note that format_wav_scp.py dumps audio files with linear PCM with sint16 regardless the input audio format.\"]},\"47\":{\"h\":\"Quick usage\",\"t\":[\"At the first, you need to prepare a text file named as wav.scp:\",\"ID_a /some_where/a.wav ID_b /some_where2/b.wav ...\",\"ID_aand ID_b are the IDs which you can name arbitrarily to specify audio files. Note that we don't assume any directory stuctures for the audio files.\",\"# Please change directory before using our shell scripts cd egs2/some_corpus/some_task cmd=utils/run.pl nj=10 # Number of parallel jobs audio_format=flac # The audio codec of output files fs=16k # The sampling frequency of output files ref_channels=0 # If the input data has multiple channels and you want to use only a single channel in the file (please spicify the channel with 0-based number) ./scripts/audio/format_wav_scp.sh --nj \\\"${nj}\\\" --cmd \\\"${cmd}\\\" --audio_format \\\"${audio_format}\\\" --fs \\\"${fs}\\\" --ref_channels \\\"${ref_channels}\\\" somewhere/wav.scp output_dir # Then, you can find output_dir/wav.scp\",\"See also:\",\"About wav.scp: https://github.com/espnet/data_example\",\"About cmd: Using job scheduling system\"]},\"48\":{\"h\":\"Why is audio file formatting necessary?\",\"t\":[\"The audio data included in the corpus obtained from the source website are distributed in various audio file formats, i.e., the audio codec (wav of linear PCM, flac, mp3, DSD, u-law, a-lawor etc.), the sampling frequency (48khz, 44.1khz, 16khz, 8khz, or etc.), the bit depth (uint8, sint16, sint32, float20, float32 or etc.), the number of channels (monaural, stereo, or more than 2ch), the byter order(little endian or big endian).\",\"When you try to develop a new recipe with a corpus that is not yet prepared in our recipes, of course, you can also try to use the audio data as they are without any formatting. However, in a typical case, the configuration of our DNN model may assume the specific audio format, especially regarding the sampling frequency and the data precision. If you are conservative with your new recipe, we recommend converting them to the original recipe's audio format. For example, 16khz and sint16 audio is typically used in our ASR recipes.\"]},\"49\":{\"h\":\"The audio file formats supported in ESPnet2\",\"t\":[\"ESPnet adopts python soundifile for data loading, and, thus the supported audio codecs depend on libsndfile.\",\"You can check the supported audio codecs of soundfile with the following command:\",\"import soundfile print(soundfile.available_formats())\",\"Note that the wav.scp of Kaldi originally requires that the audio format is wav with pcm_s16le type, but wav.scp of ESPnet2 can handle all audio formats supported by soundfile. e.g. You can use flac format in wav.scp for the input/output of format_wav_scp.py.\",\"Depending on the situation, you may choose one of the following codecs:\",\"Codec\",\"Compression\",\"Maximum channnels\",\"Maximum sampling frequency\",\"Note\",\"wav (Microsoft wav with linear pcm)\",\"No\",\"1024\",\"-\",\"flac\",\"Lossless\",\"8\",\"192khz\",\"mp3\",\"Lossy\",\"2\",\"48khz\",\"The patent of MP3 has expired\",\"ogg (Vorbis)\",\"Lossy\",\"255\",\"192khz\",\"Segmentation fault happens\",\"By default, we select flac because flac can convert linear pcm files with compression rate of ~55 % without data loss. flac is helpful to reduce the IO load, especially, when training with a large amount of corpus. If you would like to change it to the other format, please use --audio_format option for run.sh.\",\"cd egs2/some_corpus/some_task ./run.sh --audio_format mp3\",\"Note that if the audio files in your corpus are disributed with lossy audio codec, such as MP3, it's better to keep the file format to avoid the duplication of the full corpus with the uncompressed format. If the input audio format type is exactly same as the output format, format_wav_scp.py avoid the gengeration of the output files and reuse the input files.\"]},\"50\":{\"h\":\"Use case\"},\"51\":{\"h\":\"Case1: Extract segmentations with long recoding\",\"t\":[\"Create wav.scp and segments with the format of The format is &lt;utterance_id&gt; &lt;wav_id&gt; &lt;start_time&gt; &lt;end_time&gt; (second unit).\",\"wav.scp:\",\"record_a a.wav ...\",\"segments:\",\"segment_a record_a 0.98 11.56 segment_a record_a 12.34 15.43 ...\",\"Then, you can extract the segments with:\",\"./scripts/audio/format_wav_scp.sh --segments segments wav.scp output_dir\"]},\"52\":{\"h\":\"Case2: Extract audio data from video codec / Use non supported format by soundfile\",\"t\":[\"ffmpeg is required. Create wav.scp as following:\",\"ID_a ffmpeg -i \\\"ID_a.mp4\\\" -f wav -af pan=\\\"1c|c0=c0\\\" -acodec pcm_s16le - | ID_b ffmpeg -i \\\"ID_b.mp4\\\" -f wav -af pan=\\\"1c|c0=c0\\\" -acodec pcm_s16le - | ...\",\"Note: -af pan is pan filter. \",\"&lt;num&gt;c| specifies &lt;num&gt; of output channels\",\"|c&lt;out-channel&gt;=c&lt;in-channel&gt; assigns &lt;in-channel&gt;th channel of input stream into &lt;out-channel&gt;th channel of output stream\",\"Caution: -map_channel option is deprecated and will be removed.\"]},\"53\":{\"h\":\"Case3: Convert NIST Sphere files to wav\",\"t\":[\"sph2pipe is required. Create wav.scp as following:\",\"ID_a sph2pipe -f wav -p -c 1 ID_a.sph | ID_b sph2pipe -f wav -p -c 1 ID_b.sph | ...\"]},\"54\":{\"h\":\"Case4: Using a mechanism for multi channels inputs\",\"t\":[\"If you are going to generate multi channels audio file from monaural audio files, create the following wav.scp:\",\"ID_a a1.wav a2.wav ...\",\"and run the following commands:\",\"./scripts/audio/format_wav_scp.sh --multi_columns_input true wav.scp output_dir\",\"Conversely, if you and going to monaural audio files from multi channels audio files\",\"./scripts/audio/format_wav_scp.sh --multi_columns_output true wav.scp output_dir\",\"Then, you can get wav.scp like the following file:\",\"ID_a output_dir/IDa-CH0.wav output_dir/ID_a-CH1.wav ...\"]},\"55\":{\"h\":\"Task class and data input system for training\"},\"56\":{\"h\":\"Task class\",\"t\":[\"In ESpnet1, we have too many duplicated python modules. One of the big purposes of ESPnet2 is to provide a common interface and enable us to focus more on the unique parts of each task.\",\"Task class is a common system to build training tools for each task, ASR, TTS, LM, etc. inspired by Fairseq Task idea. To build your task, only you have to do is just inheriting AbsTask class:\",\"from espnet2.tasks.abs_task import AbsTask from espnet2.train.abs_espnet_model import AbsESPnetModel class NewModel(ESPnetModel): def forward(self, input, target): (...) # loss: The loss of the task. Must be a scalar value. # stats: A dict object, used for logging and validation criterion # weight: A scalar value that is used for normalization of loss and stats values among each mini-batches. # In many cases, this value should be equal to the mini-batch-size return loss, stats, weight class NewTask(AbsTask): @classmethod def add_task_arguments(cls, parser): parser.add_arguments(...) (...) @classmethod def build_collate_fn(cls, args: argparse.Namespace) (...) @classmethod def build_preprocess_fn(cls, args, train): (...) @classmethod def required_data_names(cls, inference: bool = False): (...) @classmethod def optional_data_names(cls, inference: bool = False): (...) @classmethod def build_model(cls, args): return NewModel(...) if __name__ == \\\"__main__\\\": # Start training NewTask.main()\"]},\"57\":{\"h\":\"Data input system\",\"t\":[\"Espnet2 also provides a command line interface to describe the training corpus. On the contrary, unlike fairseq or training system such as pytorch-lightning, our Task class doesn't have an interface for building the dataset explicitly. This is because we aim at the task related to speech/text only, so we don't need such general system so far.\",\"The following is an example of the command lint arguments:\",\"python -m espnet2.bin.asr_train \\\\ --train_data_path_and_name_and_type=/some/path/tr/wav.scp,speech,sound \\\\ --train_data_path_and_name_and_type=/some/path/tr/token_int,text,text_int \\\\ --valid_data_path_and_name_and_type=/some/path/dev/wav.scp,speech,sound \\\\ --valid_data_path_and_name_and_type=/some/path/dev/token_int,text,text_int\",\"First of all, our mini-batch is always a dict object:\",\"# In training iteration for batch in iterator: # e.g. batch = {\\\"speech\\\": ..., \\\"text\\\": ...} # Forward model(**batch)\",\"Where the model is same as the model built by Task.build_model().\",\"You can flexibly construct this mini-batch object using --*_data_path_and_name_and_type. --*_data_path_and_name_and_type can be repeated as you need and each --*_data_path_and_name_and_type corresponds to an element in the mini-batch. Also, keep in mind that there is no distinction between input and target data.\",\"The argument of --train_data_path_and_name_and_type should be given as three values separated by commas, like &lt;file-path&gt;,&lt;key-name&gt;,&lt;file-format&gt;.\",\"key-name specify the key of dict\",\"file-path is a file/directory path for the data source.\",\"file-format indicates the format of file specified by file-path. e.g. sound, kaldi_ark, or etc.\"]},\"58\":{\"h\":\"file\",\"t\":[\"You can show the supported file format using --help option.\",\"python -m espnet2.bin.asr_train --help\",\"Almost all formats are referred as scp file according to Kaldi-ASR. scp is just a text file which has two columns for each line: The first indicates the sample id and the second is some value. e.g. file path, transcription, a sequence of numbers.\",\"format=npy\",\"sample_id_a /some/path/a.npy sample_id_b /some/path/b.npy\",\"format=sound\",\"sample_id_a /some/path/a.flac sample_id_b /some/path/a.wav\",\"format=kaldi_ark\",\"sample_id_a /some/path/a.ark:1234 sample_id_b /some/path/a.ark:5678\",\"format=text_int\",\"sample_id_a 10 2 4 4 sample_id_b 3 2 0 1 6 2\",\"format=text\",\"sample_id_a hello world sample_id_b It is rainy today\"]},\"59\":{\"h\":\"and\",\"t\":[\"Though an arbitrary dictionary can be created by this system, each task assumes that the specific key is given for a specific purpose. e.g. ASR Task requires speech and text keys and each value is used for input data and target data respectively. See again the methods of Task class: required_data_names() and optional_data_names().\",\"class NewTask(AbsTask): @classmethod def required_data_names(cls, inference: bool = False): if not inference: retval = (\\\"input\\\", \\\"target\\\") else: retval = (\\\"input\\\",) return retval @classmethod def optional_data_names(cls, inference: bool = False): retval = (\\\"auxially_feature\\\",) return retval\",\"required_data_names() determines the mandatory data names and optional_data_names() gives optional data. It means that the other names are allowed to given by command line arguments.\",\"# The following is the expected argument python -m new_task \\\\ --train_data_path_and_name_and_type=filepath,input,sometype \\\\ --train_data_path_and_name_and_type=filepath,target,sometype \\\\ --train_data_path_and_name_and_type=filepath,auxially_feature,sometype # The following raises an error python -m new_task \\\\ --train_data_path_and_name_and_type=filepath,unknown,sometype\",\"The intention of this system is just an assertion check, so if feel unnecessary, you can turn off this checking with --allow_variable_data_keys true.\",\"# Ignore assertion checking for data names python -m new_task \\\\ --train_data_path_and_name_and_type=filepath,unknown_name,sometype \\\\ --allow_variable_data_keys true\"]},\"60\":{\"h\":\"Customize for PyTorch data loader\",\"t\":[\"Task class has a method to customize collcate_fn:\",\"class NewTask(AbsTask): @classmethod def build_collate_fn(cls, args: argparse.Namespace): ...\",\"collcate_fn is an argument of torch.utils.data.DataLoader and it can modify the data which is received from data-loader. e.g.:\",\"def collcate_fn(data): # data is a list of the return value of Dataset class: modified_data = (...touch data) return modified_data from torch.utils.data import DataLoader data_loader = DataLoader(dataset, collcate_fn=collcate_fn) for modified_data in data_loader: ...\",\"The type of argument is determined by the input dataset class and our dataset is always espnet2.train.dataset.ESPnetDataset, which the return value is a tuple of sample id and a dict of tensor,\",\"batch = (\\\"sample_id\\\", {\\\"speech\\\": tensor, \\\"text\\\": tensor})\",\"Therefore, the type is a list of dict of tensor.\",\"data = [ (\\\"sample_id\\\", {\\\"speech\\\": tensor, \\\"text\\\": tensor}), (\\\"sample_id2\\\", {\\\"speech\\\": tensor, \\\"text\\\": tensor}), ... ]\",\"The return type of collate_fn is supposed to be a tuple of list and a dict of tensor in espnet2, so the collcate_fn for Task must transform the data type to it.\",\"for ids, batch in data_loader: model(**batch)\",\"We provide common collate_fn and this function can support many cases, so you might not need to customize it. This collate_fn is aware of variable sequence features for seq2seq task:\",\"The first axis of the sequence tensor from dataset must be length axis: e.g. (Length, Dim), (Length, Dim, Dim2), or (Length, ...)\",\"It's not necessary to make the lengths of each sample unified and they are stacked with zero-padding.\",\"The value of padding can be changed.\",\"from espnet2.train.collate_fn import CommonCollateFn @classmethod def build_collate_fn(cls, args): # float_pad_value is used for float-tensor and int_pad_value is used for int-tensor return CommonCollateFn(float_pad_value=0.0, int_pad_value=-1)\",\"Tensors which represent the length of each samples are also appended\",\"batch = {\\\"speech\\\": ..., \\\"speech_lengths\\\": ..., \\\"text\\\": ..., \\\"text_lengths\\\": ...}\",\"If the feature is not sequential data, this behavior can be disabled.\",\"python -m new_task --train_data_path_and_name_and_type=filepath,foo,npy\",\"@classmethod def build_collate_fn(cls, args): return CommonCollateFn(not_sequence=[\\\"foo\\\"])\"]},\"61\":{\"h\":\"Change the configuration for training\"},\"62\":{\"h\":\"Show usage\",\"t\":[\"There are two ways to show the command line options: --help and --print_config\",\"# Show the command line option python -m espnet2.bin.asr_train --help # Show the all configuration in yaml format python -m espnet2.bin.asr_train --print_config\",\"In this section, we use espnet2.bin.asr_train for an example, but the other training tools based on Task class have the same interface, so you can replace it to another command.\",\"Note that ESPnet2 always selects_ instead of - for the separation for the option name to avoid confusion.\",\"# Bad --batch-size # Good --batch_size\",\"A notable feature of --print_config is that it shows the configuration parsing with the given arguments dynamically: You can look up the parameters for a changeable class.\",\"% # Show parameters of Adam optimizer % python -m espnet2.bin.asr_train --optim adam --print_config ... optim: adam optim_conf: lr: 0.001 betas: - 0.9 - 0.999 eps: 1.0e-08 weight_decay: 0 amsgrad: false ... % # Show parameters of ReduceLROnPlateau scheduler % python -m espnet2.bin.asr_train --scheduler ReduceLROnPlateau --print_config ... scheduler: reducelronplateau scheduler_conf: mode: min factor: 0.1 patience: 10 verbose: false threshold: 0.0001 threshold_mode: rel cooldown: 0 min_lr: 0 eps: 1.0e-08 ...\"]},\"63\":{\"h\":\"Configuration file\",\"t\":[\"You can find the configuration files for DNN training in conf/train_*.yaml.\",\"ls conf/\",\"We adopt ConfigArgParse for this configuration system. The configuration in YAML format has an equivalent effect to the command line argument. e.g. The following two are equivalent:\",\"# config.yaml foo: 3 bar: 4\",\"python -m espnet2.bin.asr_train --config conf/config.yaml python -m espnet2.bin.asr_train --foo 3 --bar 4\"]},\"64\":{\"h\":\"Change the configuration for dict type value\",\"t\":[\"Some parameters are named as *_conf, e.g. optim_conf, decoder_conf and they has the dict type value. We also provide a way to configure the nested value in such a dict object.\",\"# e.g. Change parameters one by one python -m espnet2.bin.asr_train --optim_conf lr=0.1 --optim_conf rho=0.8 # e.g. Give the parameters in yaml format python -m espnet2.bin.asr_train --optim_conf \\\"{lr: 0.1, rho: 0.8}\\\"\"]},\"65\":{\"h\":\"Resume training process\",\"t\":[\"python -m espnet2.bin.asr_train --resume true\",\"The state of the training process is saved at the end of every epoch as checkpoint.pth and the training process can be resumed from the start of the next epoch. Checkpoint includes the following states.\",\"Model state\",\"Optimizer states\",\"Scheduler states\",\"Reporter state\",\"torch.cuda.amp state (from torch=1.6)\"]},\"66\":{\"h\":\"Transfer learning / Fine tuning using pretrained model\",\"t\":[\"Use --init_param &lt;file_path&gt;:&lt;src_key&gt;:&lt;dst_key&gt;:&lt;exclude_keys&gt;\",\"# Load all parameters python -m espnet2.bin.asr_train --init_param model.pth # Load only the parameters starting with \\\"decoder\\\" python -m espnet2.bin.asr_train --init_param model.pth:decoder # Load only the parameters starting with \\\"decoder\\\" and set it to model.decoder python -m espnet2.bin.asr_train --init_param model.pth:decoder:decoder # Set parameters to model.decoder python -m espnet2.bin.asr_train --init_param decoder.pth::decoder # Load all parameters excluding \\\"decoder.embed\\\" python -m espnet2.bin.asr_train --init_param model.pth:::decoder.embed # Load all parameters excluding \\\"encoder\\\" and \\\"decoder.embed\\\" python -m espnet2.bin.asr_train --init_param model.pth:::encoder,decoder.embed\"]},\"67\":{\"h\":\"Freeze parameters\",\"t\":[\"python -m espnet2.bin.asr_train --freeze_param encoder.enc encoder.decoder\"]},\"68\":{\"h\":\"Change logging interval\",\"t\":[\"The result in the middle state of the training will be shown by the specified number:\",\"python -m espnet2.bin.asr_train --log_interval 100\"]},\"69\":{\"h\":\"Change the number of iterations in each epoch\",\"t\":[\"By default, an epoch indicates using up whole data in the training corpus and the following steps will also run after training for every epoch:\",\"Validation\",\"Saving model and checkpoint\",\"Show result in the epoch\",\"Sometimes the validation after training with a whole corpus is too coarse if using large corpus. For that case, --num_iters_per_epoch can restrict the number of iteration of each epoch.\",\"python -m espnet2.bin.asr_train --num_iters_per_epoch 1000\",\"Note that the training process can't be resumed at the middle of an epoch because data iterators are stateless, but don't worry it! Our iterator is built at the start of each epoch and the random seed is fixed by the epoch number, just like:\",\"epoch_iter_factory = Task.build_epoch_iter_factory() for epoch in range(max_epoch): iterator = epoch_iter_factory.build_iter(epoch)\",\"Therefore, the training can be resumed at the start of the epoch.\"]},\"70\":{\"h\":\"Weights & Biases integration\",\"t\":[\"About Weights & Biases: https://docs.wandb.com/\",\"Installation and setup\",\"See: https://docs.wandb.com/quickstart\",\"wandb login\",\"Enable wandb\",\"python -m espnet2.bin.asr_train --use_wandb true\",\"and go to the shown URL.\",\"[Option] To use HTTPS PROXY\",\"export HTTPS_PROXY=...your proxy export CURL_CA_BUNDLE=your.pem export CURL_CA_BUNDLE= # Disable SSL certificate verification\"]},\"71\":{\"h\":\"Multi GPUs\",\"t\":[\"python -m espnet2.bin.asr_train --ngpu 2\",\"Just using CUDA_VISIBLE_DEVICES to specify the device number:\",\"CUDA_VISIBLE_DEVICES=2,3 python -m espnet2.bin.asr_train --ngpu 2\",\"About distributed training, see Distributed training.\"]},\"72\":{\"h\":\"The relation between mini-batch size and number of GPUs\",\"t\":[\"The batch-size can be changed as follows:\",\"# Change both of the batch_size for training and validation python -m espnet2.bin.asr_train --batch_size 20 # Change the batch_size for validation python -m espnet2.bin.asr_train --valid_batch_size 200\",\"The behavior for batch-size during multi-GPU training is different from that of ESPNet1.\",\"ESPNet1: The batch-size will be multiplied by the number of GPUs.\",\"python -m espnet.bin.asr_train --batch_size 10 --ngpu 2 # Actual batch_size is 20 and each GPU devices are assigned to 10\",\"ESPnet2: The batch-size is not changed regardless of the number of GPUs.\",\"Therefore, you should set a more number of batch-size than that of GPUs.\",\"python -m espnet.bin.asr_train --batch_size 10 --ngpu 2 # Actual batch_size is 10 and each GPU devices are assigned to 5\"]},\"73\":{\"h\":\"Change mini-batch type\",\"t\":[\"We adopt variable mini-batch size with considering the dimension of the input features to make the best use of the GPU memory.\",\"There are 6 types:\",\"batch_type\",\"Option to change batch-size\",\"Variable batch-size\",\"Requirement\",\"unsorted\",\"--batch_size\",\"No\",\"-\",\"sorted\",\"--batch_size\",\"No\",\"Length information of features\",\"folded\",\"--batch_size\",\"Yes\",\"Length information of features\",\"length\",\"--batch_bins\",\"Yes\",\"Length information of features\",\"numel\",\"--batch_bins\",\"Yes\",\"Shape information of features\",\"catbel\",\"--batch_size\",\"No\",\"-\",\"Note that --batch_size is ignored if --batch_type=length or --batch_type=numel.\"]},\"74\":{\"h\":\"\",\"t\":[\"This mode has nothing special feature and just creates constant-size mini-batches without any sorting by the length order. If you intend to use ESPnet as not Seq2Seq task, this type may be suitable.\",\"Unlike the other mode, this mode doesn't require the information of the feature dimension. In other words, it's not mandatory to prepare shape_file:\",\"python -m espnet.bin.asr_train \\\\ --batch_size 10 --batch_type unsorted \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --train_shape_file \\\"train.scp\\\" \\\\ --valid_shape_file \\\"valid.scp\\\"\",\"This system might seem strange and you might also feel --*_shape_file is verbose because the training corpus can be described totally only using --*_data_path_and_name_and_type.\",\"From the viewpoint of the implementation, we separate the data source for the Dataset and BatchSampler in the term of PyTorch and --*_data_path_and_name_and_type and --*_shape_file correspond to them respectively. From the viewpoint of the training strategy, because variable batch-size is supported according to the length/dimension of each feature, thus we need to prepare the shape information before training.\"]},\"75\":{\"h\":\"\",\"t\":[\"This mode creates constant-size mini-batches with sorting by the length order. This mode requires the information of the length.\",\"python -m espnet.bin.asr_train \\\\ --batch_size 10 --batch_type sorted \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_length.txt\\\" \\\\ --valid_shape_file \\\"valid_length.txt\\\"\",\"e.g. length.txt\",\"sample_id1 1230 sample_id2 156 sample_id3 890 ...\",\"Where the fist column indicates the sample id and the second is the length of the corresponding feature. You can see that shape file is input instead in our recipes.\",\"e.g. shape.txt\",\"sample_id1 1230,80 sample_id2 156,80 sample_id3 890,80 ...\",\"This file describes the full information of the feature shape; The first number is the length of the sequence and the second or later are the dimension of feature: Length,Dim1,Dim2,....\",\"Only the first number is referred for --batch_type sorted, --batch_type folded and --batch_type length, and the shape information is required only when --batch_type numel.\"]},\"76\":{\"h\":\"\",\"t\":[\"In ESPnet1, this mode is referred as seq.\",\"This mode creates mini-batch which has the size of base_batch_size // max_i(1 + L_i // f_i). Where L_i is the maximum length in the mini-batch for ith feature and f_i is the --fold length corresponding to the feature. This mode requires the information of length.\",\"python -m espnet.bin.asr_train \\\\ --batch_size 20 --batch_type folded \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_length.scp\\\" \\\\ --train_shape_file \\\"train_length2.scp\\\" \\\\ --valid_shape_file \\\"valid_length.scp\\\" \\\\ --valid_shape_file \\\"valid_length2.scp\\\" \\\\ --fold_length 5000 \\\\ --fold_length 300\",\"Note that the repeat number of *_shape_file must equal to the number of --fold_length, but you don't need to input same number of shape files as the number of data file. i.e. You can give it as follows:\",\"python -m espnet.bin.asr_train \\\\ --batch_size 20 --batch_type folded \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_length.txt\\\" \\\\ --valid_shape_file \\\"valid_length.txt\\\" \\\\ --fold_length 5000\",\"In this example, the length of the first feature is considered while the second can be ignored. This technique can be also applied for --batch_type length and --batch_type numel.\"]},\"77\":{\"h\":\"\",\"t\":[\"In ESPnet1, this mode is referred as frame.\",\"You need to specify --batch_bins to determine the mini-batch size instead of --batch_size. Each mini-batch has equal number of bins as possible counting by the total length in the mini-batch; i.e. bins = sum(len(feat) for feats in batch for feat in feats). This mode requires the information of length.\",\"python -m espnet.bin.asr_train \\\\ --batch_bins 10000 --batch_type length \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_length.txt\\\" \\\\ --train_shape_file \\\"train_length2.txt\\\" \\\\ --valid_shape_file \\\"valid_length.txt\\\" \\\\ --valid_shape_file \\\"valid_length2.txt\\\" \\\\\"]},\"78\":{\"h\":\"\",\"t\":[\"In ESPnet1, this mode is referred as bins.\",\"You need to specify --batch_bins to determine the mini-batch size instead of --batch_size. Each mini-batches has equal number of bins as possible counting by the total number of elements; i.e. bins = sum(numel(feat) for feats in batch for feat in feats), where numel returns the infinite product of the shape of each feature; shape[0] * shape[1] * ...\",\"python -m espnet.bin.asr_train \\\\ --batch_bins 200000 --batch_type numel \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_shape.txt\\\" \\\\ --train_shape_file \\\"train_shape2.txt\\\" \\\\ --valid_shape_file \\\"valid_shape.txt\\\" \\\\ --valid_shape_file \\\"valid_shape2.txt\\\"\"]},\"79\":{\"h\":\"\",\"t\":[\"This type of batch_type focuses on the case of classification tasks. It guarantees that within each mini-batch, all samples belong to different classes. --batch_size is used to determine the mini-batch size. This batch type does not go along with the default sequence iterator_type. It is instead designed to be used with category iterator_type. Therefore, instead of explicitely giving --batch_type catbel, it is more recommended to give --iterator_type category which will automatically set batch_type to catbel. It is also important to use a preprocessor that adjusts the sample duration to enable mini-batch construction. One example would be espnet2/train/preprocessor/SpkPreprocessor.\",\"python -m espnet.bin.spk_train \\\\ --batch_bins 256 --iterator_type category \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --train_shape_file \\\"train_shape.txt\\\" \\\\ --valid_shape_file \\\"valid_shape.txt\\\" \\\\\"]},\"80\":{\"h\":\"Gradient accumulating\",\"t\":[\"There are several ways to deal with larger model architectures than the capacity of your GPU device memory during training.\",\"Using a larger number of GPUs\",\"Using a half decision tensor\",\"Using torch.utils.checkpoint\",\"Gradient accumulating\",\"Gradient accumulating is a technique to handle larger mini-batch than available size.\",\"Split a mini-batch into several numbers and forward and backward for each piece and accumulate the gradients ony by one, while optimizer's updating is invoked every the number of forwarding just like following:\",\"# accum_grad is the number of pieces for i, batch in enumerate(iterator): loss = net(batch) (loss / accum_grad).backward() # Gradients are accumulated if i % accum_grad: optim.update() optim.zero_grads()\",\"Give --accum_grad &lt;int&gt; to use this option.\",\"python -m espnet.bin.asr_train --accum_grad 2\",\"The effective batch_size becomes almost same as accum_grad * batch_size except for:\",\"The random state\",\"Some statistical layers based on mini-batch e.g. BatchNormalization\",\"The case that the batch_size is not unified for each iteration.\"]},\"81\":{\"h\":\"Automatic Mixed Precision training\",\"t\":[\"python -m espnet.bin.asr_train --use_amp true\"]},\"82\":{\"h\":\"Reproducibility and determinization\",\"t\":[\"There are some possibilities to make training non-reproducible.\",\"Initialization of parameters that come from PyTorch/ESPnet version difference.\",\"Reducing order for float values during multi GPUs training. \",\"I don't know whether NCCL is deterministic or not.\",\"Random seed difference \",\"We fixed the random seed for each epoch.\",\"CuDNN or some non-deterministic operations for CUDA: See https://pytorch.org/docs/stable/notes/randomness.html\",\"By default, CuDNN performs deterministic mode in our training and it can be turned off by:\",\"python -m espnet.bin.asr_train --cudnn_deterministic false\"]},\"83\":{\"h\":\"ESPnet2 Tutorial\",\"t\":[\"We are planning a super major update, called ESPnet2. The developing status is still under construction yet, so please be very careful to use with understanding following cautions:\",\"There might be fatal bugs related to essential parts.\",\"We haven't achieved comparable results to espnet1 on each task yet.\"]},\"84\":{\"h\":\"Main changing from ESPnet1\",\"t\":[\"Chainer free\",\"Discarding Chainer completely.\",\"The development of Chainer is stopped at v7: https://chainer.org/announcement/2019/12/05/released-v7.html\",\"Kaldi free\",\"It's not mandatory to compile Kaldi.\",\"If you find some recipes requiring Kaldi mandatory, please report it. It should be dealt with as a bug in ESPnet2.\",\"We still support the features made by Kaldi optionally.\",\"We still follow Kaldi style. i.e. depending on utils/ of Kaldi.\",\"On the fly feature extraction & text preprocessing for training \",\"You don't need to create the feature file before training, but just input wave data directly.\",\"We support both raw wave input and extracted features.\",\"The preprocessing for text, tokenization to characters, or sentencepieces, can be also applied during training.\",\"Support self-supervised learning representations from s3prl\",\"Discarding the JSON format describing the training corpus. \",\"Why do we discard the JSON format? Because a dict object generated from a large JSON file requires much memory and it also takes much time to parse such a large JSON file.\",\"Support distributed data-parallel training (Not enough tested) \",\"Single node multi GPU training with DistributedDataParallel is also supported.\"]},\"85\":{\"h\":\"Recipes using ESPnet2\",\"t\":[\"You can find the new recipes in egs2:\",\"espnet/ # Python modules of espnet1 espnet2/ # Python modules of espnet2 egs/ # espnet1 recipes egs2/ # espnet2 recipes\",\"The usage of recipes is almost the same as that of ESPnet1.\",\"Change directory to the base directory\",\"# e.g. cd egs2/an4/asr1/\",\"an4 is a tiny corpus and can be freely obtained, so it might be suitable for this tutorial. You can perform any other recipes as the same way. e.g. wsj, librispeech, and etc.\",\"Keep in mind that all scripts should be ran at the level of egs2/*/{asr1,tts1,...}.\",\"# Doesn't work cd egs2/an4/ ./asr1/run.sh ./asr1/scripts/&lt;some-script&gt;.sh # Doesn't work cd egs2/an4/asr1/local/ ./data.sh # Work cd egs2/an4/asr1 ./run.sh ./scripts/&lt;some-script&gt;.sh\",\"Change the configuration Describing the directory structure as follows:\",\"egs2/an4/asr1/ - conf/ # Configuration files for training, inference, etc. - scripts/ # Bash utilities of espnet2 - pyscripts/ # Python utilities of espnet2 - steps/ # From Kaldi utilities - utils/ # From Kaldi utilities - db.sh # The directory path of each corpora - path.sh # Setup script for environment variables - cmd.sh # Configuration for your backend of job scheduler - run.sh # Entry point - asr.sh # Invoked by run.sh\",\"You need to modify db.sh for specifying your corpus before executing run.sh. For example, when you touch the recipe of egs2/wsj, you need to change the paths of WSJ0 and WSJ1 in db.sh.\",\"Some corpora can be freely obtained from the WEB and they are written as \\\"downloads/\\\" at the initial state. You can also change them to your corpus path if it's already downloaded.\",\"path.sh is used to set up the environment for run.sh. Note that the Python interpreter used for ESPnet is not the current Python of your terminal, but it's the Python which was installed at tools/. Thus you need to source path.sh to use this Python.\",\". path.sh python\",\"cmd.sh is used for specifying the backend of the job scheduler. If you don't have such a system in your local machine environment, you don't need to change anything about this file. See Using Job scheduling system\",\"Run run.sh\",\"./run.sh\",\"run.sh is an example script, which we often call as \\\"recipe\\\", to run all stages related to DNN experiments; data-preparation, training, and evaluation.\"]},\"86\":{\"h\":\"See training status\"},\"87\":{\"h\":\"Show the log file\",\"t\":[\"% tail -f exp/*_train_*/train.log [host] 2020-04-05 16:34:54,278 (trainer:192) INFO: 2/40epoch started. Estimated time to finish: 7 minutes and 58.63 seconds [host] 2020-04-05 16:34:56,315 (trainer:453) INFO: 2epoch:train:1-10batch: iter_time=0.006, forward_time=0.076, loss=50.873, los s_att=35.801, loss_ctc=65.945, acc=0.471, backward_time=0.072, optim_step_time=0.006, lr_0=1.000, train_time=0.203 [host] 2020-04-05 16:34:58,046 (trainer:453) INFO: 2epoch:train:11-20batch: iter_time=4.280e-05, forward_time=0.068, loss=44.369 , loss_att=28.776, loss_ctc=59.962, acc=0.506, backward_time=0.055, optim_step_time=0.006, lr_0=1.000, train_time=0.173\"]},\"88\":{\"h\":\"Show the training status in a image file\",\"t\":[\"# Accuracy plot # (eog is Eye of GNOME Image Viewer) eog exp/*_train_*/images/acc.img # Attention plot eog exp/*_train_*/att_ws/&lt;sample-id&gt;/&lt;param-name&gt;.img\"]},\"89\":{\"h\":\"Use tensorboard\",\"t\":[\"tensorboard --logdir exp/*_train_*/tensorboard/\"]},\"90\":{\"h\":\"How to parse command-line arguments in shell scripts?\",\"t\":[\"All shell scripts in espnet/espnet2 depend on utils/parse_options.sh to parase command line arguments.\",\"e.g. If the script has ngpu option\",\"#!/usr/bin/env bash # run.sh ngpu=1 . utils/parse_options.sh echo ${ngpu}\",\"Then you can change the value as follows:\",\"$ ./run.sh --ngpu 2 echo 2\",\"You can also show the help message:\",\"./run.sh --help\"]},\"91\":{\"h\":\"Start from a specified stage and stop at a specified stage\",\"t\":[\"The procedures in run.sh can be divided into some stages, e.g. data preparation, training, and evaluation. You can specify the starting stage and the stopping stage.\",\"./run.sh --stage 2 --stop-stage 6\",\"There are also some altenative otpions to skip specified stages:\",\"run.sh --skip_data_prep true # Skip data preparation stages. run.sh --skip_train true # Skip training stages. run.sh --skip_eval true # Skip decoding and evaluation stages. run.sh --skip_packing false --skip_upload_hf false # Enable packing and uploading to huggingface stages.\",\"Note that skip_upload and skip_upload_hf are true by default. Please change them to false when uploading your model.\"]},\"92\":{\"h\":\"Change the configuration for training\",\"t\":[\"Please keep in mind that run.sh is a wrapper script of several tools including DNN training command. You need to do one of the following two ways to change the training configuration.\",\"# Give a configuration file ./run.sh --asr_config conf/train_asr.yaml # Give arguments to \\\"espnet2/bin/asr_train.py\\\" directly ./run.sh --asr_args \\\"--foo arg --bar arg2\\\"\",\"e.g. To change learning rate for the LM training\",\"./run.sh --lm_args \\\"--optim_conf lr=0.1\\\"\",\"This is the case of ASR training and you need to replace it accordingly for the other task. e.g. For TTS\",\"./run.sh --tts_args \\\"--optim_conf lr=0.1\\\"\",\"See Change the configuration for training for more detail about the usage of training tools.\"]},\"93\":{\"h\":\"Change the number of parallel jobs\",\"t\":[\"./run.sh --nj 10 # Chnage the number of parallels for data preparation stages. ./run.sh --inference_nj 10 # Chnage the number of parallels for inference jobs.\",\"We also support submitting jobs to multiple hosts to accelerate your experiment: See Using Job scheduling system\"]},\"94\":{\"h\":\"Multi GPUs training and distributed training\",\"t\":[\"./run.sh --ngpu 4 # 4GPUs in a single node ./run.sh --ngpu 2 --num_nodes 2 # 2GPUs x 2nodes\",\"Note that you need to setup your environment correctly to use distributed training. See the following two:\",\"Distributed training\",\"Using Job scheduling system\"]},\"95\":{\"h\":\"Relationship between mini-batch size and number of GPUs\",\"t\":[\"The behavior of batch size in ESPnet2 during multi-GPU training is different from that in ESPnet1. In ESPnet2, the total batch size is not changed regardless of the number of GPUs. Therefore, you need to manually increase the batch size if you increase the number of GPUs. Please refer to this doc for more information.\"]},\"96\":{\"h\":\"Use specified experiment directory for evaluation\",\"t\":[\"If you already have trained a model, you may wonder how to give it to run.sh when you'll evaluate it later. By default the directory name is determined according to given options, asr_args, lm_args, or etc. You can overwrite it by --asr_exp and --lm_exp.\",\"# For ASR recipe ./run.sh --skip_data_prep true --skip_train true --asr_exp &lt;your_asr_exp_directory&gt; --lm_exp &lt;your_lm_exp_directory&gt; # For TTS recipe ./run.sh --skip_data_prep true --skip_train true --tts_exp &lt;your_tts_exp_directory&gt;\"]},\"97\":{\"h\":\"Evaluation without training using pretrained model\",\"t\":[\"./run.sh --download_model &lt;model_name&gt; --skip_train true\",\"You need to fill model_name by yourself. You can search for pretrained models on Hugging Face using the tag espnet\",\"(Deprecated: See the following link about our pretrain models: https://github.com/espnet/espnet_model_zoo)\"]},\"98\":{\"h\":\"Evaluation using OpenAI Whisper\",\"t\":[\"ESPnet2 provides a script to run inference and scoring using OpenAI's Whisper. This can be used to evaluate speech generation models. Here is an example:\",\"#!/usr/bin/env bash # Set bash to 'debug' mode, it will exit on : # -e 'error', -u 'undefined variable', -o ... 'error in pipeline', -x 'print commands', set -e set -u set -o pipefail whisper_tag=medium # whisper model tag, e.g., small, medium, large, etc cleaner=whisper_en hyp_cleaner=whisper_en nj=1 test_sets=\\\"test/WSJ/test_eval92\\\" # decode_options is used in Whisper model's transcribe method decode_options=\\\"{language: en, task: transcribe, temperature: 0, beam_size: 10, fp16: False}\\\" for x in ${test_sets}; do wavscp=dump/raw/${x}/wav.scp # path to wav.scp outdir=whisper-${whisper_tag}_outputs/${x} # path to save output gt_text=dump/raw/${x}/text # path to groundtruth text file (for scoring only) scripts/utils/evaluate_asr.sh \\\\ --whisper_tag ${whisper_tag} \\\\ --nj ${nj} \\\\ --gpu_inference true \\\\ --stage 2 \\\\ --stop_stage 3 \\\\ --cleaner ${cleaner} \\\\ --hyp_cleaner ${hyp_cleaner} \\\\ --decode_options \\\"${decode_options}\\\" \\\\ --gt_text ${gt_text} \\\\ ${wavscp} \\\\ ${outdir} done\"]},\"99\":{\"h\":\"Packing and sharing your trained model\",\"t\":[\"ESPnet encourages you to share your results using platforms like Hugging Face.\",\"For sharing your models, the last three stages of each task simplify this process. The model is packed into a zip file and uploaded to the selected platform (one or both).\",\"For Hugging Face, you need to first create a repository (&lt;my_repo&gt; = &lt;user_name&gt;/&lt;repo_name&gt;). Remember to install git-lfs before continuing. Then, execute run.sh as follows:\",\"./run.sh --stage &lt;packing stage&gt; --skip-packing false --skip-upload-hf false --hf-repo &lt;my_repo&gt;\",\"The stage number differs according to the task. Please read the task-specific shell script (e.g., asr1/asr.sh) to see the number to specify. The packed model can be uploaded to huggingface by setting the previously mentioned flags.\"]},\"100\":{\"h\":\"Usage of Self-Supervised Learning Representations as feature\",\"t\":[\"ESPnet supports self-supervised learning representations (SSLR) to replace traditional spectrum features. In some cases, SSLRs can boost the performance.\",\"To use SSLRs in your task, you need to make several modifications.\"]},\"101\":{\"h\":\"Prerequisite\",\"t\":[\"Install S3PRL by tools/installers/install_s3prl.sh.\",\"If HuBERT / Wav2Vec is needed, fairseq should be installed by tools/installers/install_fairseq.sh.\"]},\"102\":{\"h\":\"Usage\",\"t\":[\"To reduce the time used in collect_stats step, please specify --feats_normalize uttmvn in run.sh and pass it as arguments to asr.sh or other task-specific scripts. (Recommended)\",\"In the configuration file, specify the frontend and preencoder. Taking HuBERT as an example: The upstream name can be whatever supported in S3PRL. multilayer-feature=True means the final representation is a weighted-sum of all layers' hidden states from SSLR model.\",\"frontend: s3prl frontend_conf: frontend_conf: upstream: hubert_large_ll60k # Note: If the upstream is changed, please change the input_size in the preencoder. download_dir: ./hub multilayer_feature: True\",\"Here the preencoder is to reduce the input dimension to the encoder, to reduce the memory cost. The input_size depends on the upstream model, while the output_size can be set to any values.\",\"preencoder: linear preencoder_conf: input_size: 1024 # Note: If the upstream is changed, please change this value accordingly. output_size: 80\",\"Because the shift sizes of different upstream models are different, e.g. HuBERT and Wav2Vec2.0 have 20ms frameshift. Sometimes, the downsampling rate (input_layer) in the encoder configuration need to be changed. For example, using input_layer: conv2d2 will results in a total frameshift of 40ms, which is enough for some tasks.\"]},\"103\":{\"h\":\"Streaming ASR\",\"t\":[\"ESPnet supports streaming Transformer/Conformer ASR with blockwise synchronous beam search.\",\"For more details, please refer to the paper.\"]},\"104\":{\"h\":\"Training\",\"t\":[\"To achieve streaming ASR, please employ blockwise Transformer/Conformer encoder in the configuration file. Taking blockwise Transformer as an example: The encoder name can be contextual_block_transformer or contextual_block_conformer.\",\"encoder: contextual_block_transformer encoder_conf: block_size: 40 # block size for block processing hop_size: 16 # hop size for block processing look_ahead: 16 # look-ahead size for block processing init_average: true # whether to use average input as initial context ctx_pos_enc: true # whether to use positional encoding for the context vectors\"]},\"105\":{\"h\":\"Decoding\",\"t\":[\"To enable online decoding, the argument --use_streaming true should be added to run.sh.\",\"./run.sh --stage 12 --use_streaming true\"]},\"106\":{\"h\":\"FAQ\",\"t\":[\"Issue about 'NoneType' object has no attribute 'max' during training: Please make sure you employ forward_train function during traininig, check more details here.\",\"I successfully trained the model, but encountered the above issue during decoding: You may forget to specify --use_streaming true to select streaming inference.\"]},\"107\":{\"h\":\"Real-Time-Factor and Latency\",\"t\":[\"In order to calculate real-time-factor and (non-streaming) latency the script utils/calculate_rtf.py has been reworked and can now be used for both ESPnet1 and ESPnet2. The script calculates inference times based on time markers in the decoding log files and reports the average real-time-factor (RTF) and average latency over all decoded utterances. For ESPnet2, the script will automatically be run (see Limitations section below) after the decoding stage has finished but can also be run as a stand-alone script:\"]},\"108\":{\"h\":\"Usage\",\"t\":[\"usage: calculate_rtf.py [-h] [--log-dir LOG_DIR] [--log-name {decode,asr_inference}] [--input-shift INPUT_SHIFT] [--start-times-marker {input lengths,speech length}] [--end-times-marker {prediction,best hypo}] calculate real time factor (RTF) optional arguments: -h, --help show this help message and exit --log-dir LOG_DIR path to logging directory --log-name {decode,asr_inference} name of logfile, e.g., 'decode' (espnet1) and 'asr_inference' (espnet2) --input-shift INPUT_SHIFT shift of inputs in milliseconds --start-times-marker {input lengths,speech length} String marking start of decoding in logfile, e.g., 'input lengths' (espnet1) and 'speech length' (espnet2) --end-times-marker {prediction,best hypo} String marking end of decoding in logfile, e.g., 'prediction' (espnet1) and 'best hypo' (espnet2)\"]},\"109\":{\"h\":\"Notes\",\"t\":[\"Default settings still target ESPnet1 usage:\",\"--log-name 'decode' --input-shift 10.0 --start-times-marker 'input lengths' --end-times-marker 'prediction'\",\"For ESPnet2, other frame shifts than 10ms are possible via different front-end/feature configurations. So different to ESPnet1, which logs the input feature frames at a fixed 10ms frame shift, in ESPnet2 the number of speech samples is logged instead and the audio sample shift in milliseconds (1/sampleRate x 1000) needs to be specified for --input-shift parameter (see --input-shift 0.0625 in example below for 16000 Hz sample rate).\"]},\"110\":{\"h\":\"Example\",\"t\":[\"From espnet/egs2/librispeech/asr1 the following call runs the decoding stage with pretrained ESPnet2 model:\",\"./run.sh --stage 12 --use_streaming false --skip_data_prep true --skip_train true --download_model byan/librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0.001_sp\",\"Results for latency and rtf calculation on Librispeech test_clean subset can then be found in espnet/egs2/librispeech/asr1/exp/byan/librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0.001_sp/decode_asr_lm_lm_train_lm_transformer2_en_bpe5000_valid.loss.ave_asr_model_valid.acc.ave/test_clean/logdir/calculate_rtf.log file:\",\"# ../../../utils/calculate_rtf.py --log-dir exp/byan/librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0.001_sp/decode_as r_lm_lm_train_lm_transformer2_en_bpe5000_valid.loss.ave_asr_model_valid.acc.ave/test_clean/logdir --log-name asr_inference --input-shift 0.0625 --start-times- marker \\\"speech length\\\" --end-times-marker \\\"best hypo\\\" Total audio duration: 19452.481 [sec] Total decoding time: 137762.231 [sec] RTF: 7.082 Latency: 52581.004 [ms/sentence]\"]},\"111\":{\"h\":\"Limitations\",\"t\":[\"Only non-streaming inference mode is supported currently\",\"The decoding stage 12 in asr.sh automatically runs the rtf & latency calculation if \\\"asr_inference_tool == \\\"espnet2.bin.asr_inference\\\"; other inference tools like k2 & maskctc are still left to do\"]},\"112\":{\"h\":\"Transducer ASR\",\"t\":[\"Important: If you encounter any issue related to warp-transducer, please open an issue in our forked repo.\",\"ESPnet2 supports models trained with the (RNN-)Tranducer loss, aka Transducer models. Currently, two versions of these models exist within ESPnet2: one under asr and the other under asr_transducer. The first one is designed as a supplement of CTC-Attention ASR models while the second is designed independently and purely for the Transducer task. For that, we rely on ESPnetASRTransducerModel instead of ESPnetASRModel and a new task called ASRTransducerTask is used in place of ASRTask.\",\"For the user, it means two things. First, some features or modules may not be supported depending on the version used. Second, the usage of some common ASR features or modules may differ between the models. In addition, some core modules (e.g.: preencoder or postencoder) may be missing in the standalone version until validation.\",\"The following sections of this tutorial are dedicated to the introduction of the version under asr_transducer. Thus, the user should keep in mind that most features described here may not be available in the other version.\"]},\"113\":{\"h\":\"General usage\",\"t\":[\"To enable Transducer model training or decoding in your experiments, the following option should be supplied to asr.sh in your run.sh:\",\"asr.sh --asr_task asr_transducer [...]\",\"For Transducer loss computation during training, we rely by default on a fork of warp-transducer. The installation procedure is described here.\",\"Note: We made available FastEmit regularization [Yu et al., 2021] during loss computation. To enable it, fastemit_lambda need to be set in model_conf:\",\" model_conf: fastemit_lambda: Regularization parameter for FastEmit. (float, default = 0.0)\",\"Optionnaly, we also support training with the Pruned RNN-T loss [Kuang et al. 2022] made available in the k2 toolkit. To use it, the parameter use_k2_pruned_loss should be set to True in model_conf. From here, the loss computation can be controlled by setting the following parameters through k2_pruned_loss_args in model_conf:\",\"model_conf: use_k2_pruned_loss: True k2_pruned_loss_args: prune_range: How many tokens by frame are used compute the pruned loss. (int, default = 5) simple_loss_scaling: The weight to scale the simple loss after warm-up. (float, default = 0.5) lm_scale: The scale factor to smooth the LM part. (float, default = 0.0) am_scale: The scale factor to smooth the AM part. (float, default = 0.0) loss_type: Define the type of path to take for loss computation, either 'regular', 'smoothed' or 'constrained'. (str, default = \\\"regular\\\")\",\"Note: Because the number of tokens emitted by timestep can be restricted during training with this version, we also make available the parameter validation_nstep. It let the users apply similar constraints during the validation process, when reporting CER or/and WER:\",\"model_conf: validation_nstep: Maximum number of symbol expansions at each time step when reporting CER or/and WER using mAES.\",\"For more information, see section Inference and \\\"modified Adaptive Expansion Search\\\" algorithm.\"]},\"114\":{\"h\":\"Architecture\",\"t\":[\"The architecture is composed of three modules: encoder, decoder and joint network. Each module has one (or three) config(s) with various parameters in order to configure the internal parts. The following sections describe the mandatory and optional parameters for each module.\"]},\"115\":{\"h\":\"Encoder\",\"t\":[\"For the encoder, we propose a unique encoder type encapsulating the following blocks: Branchformer, Conformer, Conv 1D and E-Branchformer. It is similar to the custom encoder in ESPnet1, meaning we don't need to set the parameter encoder: [type] here. Instead, the encoder architecture is defined by three configurations passed to encoder_conf:\",\"input_conf (Dict): The configuration for the input block.\",\"main_conf (Dict): The main configuration for the parameters shared across all blocks.\",\"body_conf (List[Dict]): The list of configurations for each block of the encoder architecture but the input block.\",\"The first and second configurations are optional. If needed, the following parameters can be modified in each configuration:\",\"main_conf: pos_wise_act_type: Conformer position-wise feed-forward activation type. (str, default = \\\"swish\\\") conv_mod_act_type: Conformer convolution module activation type. (str, default = \\\"swish\\\") pos_enc_dropout_rate: Dropout rate for the positional encoding layer, if used. (float, default = 0.0) pos_enc_max_len: Positional encoding maximum length. (int, default = 5000) simplified_att_score: Whether to use simplified attention score computation. (bool, default = False) norm_type: X-former normalization module type. (str, default = \\\"layer_norm\\\") conv_mod_norm_type: Branchformer convolution module normalization type. (str, default = \\\"layer_norm\\\") after_norm_eps: Epsilon value for the final normalization module. (float, default = 1e-05 or 0.25 for BasicNorm) after_norm_partial: Partial value for the final normalization module, if norm_type = 'rms_norm'. (float, default = -1.0) blockdrop_rate: Probability threshold of dropping out each encoder block. (float, default = 0.0) # For more information on the parameters below, please refer to espnet2/asr_transducer/activation.py ftswish_threshold: Threshold value for FTSwish activation formulation. ftswish_mean_shift: Mean shifting value for FTSwish activation formulation. hardtanh_min_val: Minimum value of the linear region range for HardTanh activation. (float, default = -1.0) hardtanh_max_val: Maximum value of the linear region range for HardTanh. (float, default = 1.0) leakyrelu_neg_slope: Negative slope value for LeakyReLU activation formulation. smish_alpha: Alpha value for Smish variant activation fomulation. (float, default = 1.0) smish_beta: Beta value for Smish variant activation formulation. (float, default = 1.0) softplus_beta: Beta value for softplus activation formulation in Mish activation. (float, default = 1.0) softplus_threshold: Values above this revert to a linear function in Mish activation. (int, default = 20) swish_beta: Beta value for E-Swish activation formulation. (float, default = 20) input_conf: block_type: Input block type, either \\\"conv2d\\\" or \\\"vgg\\\". (str, default = \\\"conv2d\\\") conv_size: Convolution output size. For \\\"vgg\\\", the two convolution outputs can be controlled by passing a tuple. (int, default = 256) subsampling_factor: Subsampling factor of the input block, either 2 (only conv2d), 4 or 6. (int, default = 4)\",\"The only mandatory configuration is body_conf, defining the encoder body architecture block by block. Each block has its own set of mandatory and optional parameters depending on the type, defined by block_type:\",\"# Branchformer - block_type: branchformer hidden_size: Hidden (and output) dimension. (int) linear_size: Dimension of the Linear layers. (int) conv_mod_kernel_size: Size of the convolving kernel in the ConvolutionalSpatialGatingUnit module. (int) heads (optional): Number of heads in multi-head attention. (int, default = 4) norm_eps (optional): Epsilon value for the normalization module. (float, default = 1e-05 or 0.25 for BasicNorm) norm_partial (optional): Partial value for the normalization module, if norm_type = 'rms_norm'. (float, default = -1.0) conv_mod_norm_eps (optional): Epsilon value for ConvolutionalSpatialGatingUnit module normalization. (float, default = 1e-05 or 0.25 for BasicNorm) conv_mod_norm_partial (optional): Partial value for the ConvolutionalSpatialGatingUnit module normalization, if conv_norm_type = 'rms_norm'. (float, default = -1.0) dropout_rate (optional): Dropout rate for some intermediate layers. (float, default = 0.0) att_dropout_rate (optional): Dropout rate for the attention module. (float, default = 0.0) # Conformer - block_type: conformer hidden_size: Hidden (and output) dimension. (int) linear_size: Dimension of feed-forward module. (int) conv_mod_kernel_size: Size of the convolving kernel in the ConformerConvolution module. (int) heads (optional): Number of heads in multi-head attention. (int, default = 4) norm_eps (optional): Epsilon value for normalization module. (float, default = 1e-05 or 0.25 for BasicNorm) norm_partial (optional): Partial value for the normalization module, if norm_type = 'rms_norm'. (float, default = -1.0) conv_mod_norm_eps (optional): Epsilon value for Batchnorm1d in the ConformerConvolution module. (float, default = 1e-05) conv_mod_norm_momentum (optional): Momentum value for Batchnorm1d in the ConformerConvolution module. (float, default = 0.1) dropout_rate (optional): Dropout rate for some intermediate layers. (float, default = 0.0) att_dropout_rate (optional): Dropout rate for the attention module. (float, default = 0.0) pos_wise_dropout_rate (optional): Dropout rate for the position-wise feed-forward module. (float, default = 0.0) # Conv 1D - block_type: conv1d output_size: Output size. (int) kernel_size: Size of the convolving kernel. (int or Tuple) stride (optional): Stride of the sliding blocks. (int or tuple, default = 1) dilation (optional): Parameter to control the stride of elements within the neighborhood. (int or tuple, default = 1) groups (optional): Number of blocked connections from input channels to output channels. (int, default = 1) bias (optional): Whether to add a learnable bias to the output. (bool, default = True) relu (optional): Whether to use a ReLU activation after convolution. (bool, default = True) batch_norm: Whether to use batch normalization after convolution. (bool, default = False) dropout_rate (optional): Dropout rate for the Conv1d outputs. (float, default = 0.0) # E-Branchformer - block_type: ebranchformer hidden_size: Hidden (and output) dimension. (int) linear_size: Dimension of the feed-forward module and othger linear layers. (int) conv_mod_kernel_size: Size of the convolving kernel in the ConvolutionalSpatialGatingUnit module. (int) depthwise_conv_kernel_size: Size of the convolving kernel in the DepthwiseConvolution module. (int, default = conv_mod_kernel_size) heads (optional): Number of heads in multi-head attention. (int, default = 4) norm_eps (optional): Epsilon value for the normalization module. (float, default = 1e-05 or 0.25 for BasicNorm) norm_partial (optional): Partial value for the normalization module, if norm_type = 'rms_norm'. (float, default = -1.0) conv_mod_norm_eps (optional): Epsilon value for ConvolutionalSpatialGatingUnit module normalization. (float, default = 1e-05 or 0.25 for BasicNorm) conv_mod_norm_partial (optional): Partial value for the ConvolutionalSpatialGatingUnit module normalization, if conv_norm_type = 'rms_norm'. (float, default = -1.0) dropout_rate (optional): Dropout rate for some intermediate layers. (float, default = 0.0) att_dropout_rate (optional): Dropout rate for the attention module. (float, default = 0.0)\",\"In addition, each block has a parameter num_blocks to build N times the defined block (int, default = 1). This is useful if you want to use a group of blocks sharing the same parameters without writing each configuration.\",\"Example 1: conv 2D + 2x Conv 1D + 14x Conformer.\",\"encoder_conf: main_conf: pos_wise_act_type: swish pos_enc_dropout_rate: 0.1 conv_mod_act_type: swish input_conf: block_type: conv2d conv_size: 256 subsampling_factor: 4 body_conf: - block_type: conv1d output_size: 128 kernel_size: 3 - block_type: conv1d output_size: 256 kernel_size: 2 - block_type: conformer linear_size: 1024 hidden_size: 256 heads: 8 dropout_rate: 0.1 pos_wise_dropout_rate: 0.1 att_dropout_rate: 0.1 conv_mod_kernel_size: 31 num_blocks: 14\"]},\"116\":{\"h\":\"Decoder\",\"t\":[\"For the decoder, four types of blocks are available: stateless ('stateless'), RNN ('rnn'), MEGA ('mega') or RWKV ('rwkv'). Contrary to the encoder, the parameters are shared across the blocks, meaning we only define one block in the configuration. The type of the stack of blocks is defined by passing the corresponding type string to the parameter decoder. The internal parts are defined through the field decoder_conf containing the following controlable parameters:\",\"decoder_conf: embed_size: Size of the embedding layer (int, default = 256). num_blocks: Number of decoder blocks/layers (int, default = 4 for MEGA or 1 for RNN). rnn_type (RNN only): Type of RNN cells (int, default = \\\"lstm\\\"). hidden_size (RNN only): Size of the hidden layers (int, default = 256). block_size (MEGA/RWKV only): Size of the block's input/output (int, default = 512). linear_size (MEGA/RWKV only): Feed-Forward module hidden size (int, default = 1024). attention_size (RWKV only): Hidden-size of the attention module. (int, default = None). context_size (RWKV only): Context size for the WKV kernel module (int, default = 1024). qk_size (MEGA only): Shared query and key size for attention module (int, default = 128). v_size (MEGA only): Value size for attention module (int, default = 1024). chunk_size (MEGA only): Chunk size for attention computation (int, default = -1, i.e. full context). num_heads (MEGA only): Number of EMA heads (int, default = 4). rel_pos_bias (MEGA only): Type of relative position bias in attention module (str, default = \\\"simple\\\"). max_positions (MEGA only): Maximum number of position for RelativePositionBias (int, default = 2048). truncation_length (MEGA only): Maximum length for truncation in EMA module (int, default = None). normalization_type (MEGA/RWKV only): Normalization layer type (str, default = \\\"layer_norm\\\"). normalization_args (MEGA/RKWV only): Normalization layer arguments (dict, default = {}). activation_type (MEGA only): Activation function type (str, default = \\\"swish\\\"). activation_args (MEGA only): Activation function arguments (dict, default = {}). rescale_every (RWKV only): Whether to rescale input every N blocks during inference (int, default = 0) dropout_rate (excl. RWKV): Dropout rate for main block modules (float, default = 0.0). embed_dropout_rate: Dropout rate for embedding layer (float, default = 0.0). att_dropout_rate (MEGA/RWKV only): Dropout rate for the attention module. ema_dropout_rate (MEGA only): Dropout rate for the EMA module. ffn_dropout_rate (MEGA/RWKV only): Dropout rate for the feed-forward module.\",\"Example 1: RNN decoder.\",\"decoder: rnn decoder_conf: rnn_type: lstm num_layers: 2 embed_size: 256 hidden_size: 256 dropout_rate: 0.1 embed_dropout_rate: 0.1\",\"Example 2: MEGA decoder.\",\"decoder: mega decoder_conf: block_size: 256 linear_size: 2048 qk_size: 128 v_size: 1024 max_positions: 1024 num_heads: 4 rel_pos_bias_type: \\\"rotary\\\" chunk_size: 256 num_blocks: 6 dropout_rate: 0.1 ffn_dropout_rate: 0.1 att_dropout_rate: 0.1 embed_dropout_rate: 0.1\"]},\"117\":{\"h\":\"Joint network\",\"t\":[\"Currently, we only propose the standard joint network module composed of three linear layers and an activation function. The module definition is optional but the following parameters can be modified through the configuration parameter joint_network_conf:\",\"joint_network_conf: joint_space_size: Size of the joint space (int, default = 256). joint_act_type: Type of activation in the joint network (str, default = \\\"tanh\\\").\",\"The options related to the activation functions can also be modified through the parameters introduced in the Encoder section (See main_conf description).\"]},\"118\":{\"h\":\"Multi-task learning\",\"t\":[\"We also support multi-task learning with two auxiliary tasks: CTC and cross-entropy w/ label smoothing option (called LM loss here). The auxiliary tasks contribute to the overal task defined as:\",\"L_tot = (λ_trans x L_trans) + (λ_auxCTC x L_auxCTC) + (λ_auxLM x L_auxLM)\",\"where the losses (L_*) are respectively, in order: The Transducer loss, the CTC loss and the LM loss. Lambda values define their respective contribution to the total loss. Each task can be parameterized using the following options, passed to model_conf:\",\"model_conf: transducer_weight: Weight of the Transducer loss (float, default = 1.0) auxiliary_ctc_weight: Weight of the CTC loss. (float, default = 0.0) auxiliary_ctc_dropout_rate: Dropout rate for the CTC loss inputs. (float, default = 0.0) auxiliary_lm_loss_weight: Weight of the LM loss. (float, default = 0.2) auxiliary_lm_loss_smoothing: Smoothing rate for LM loss. If > 0, label smoothing is enabled. (float, default = 0.0)\",\"Note: We do not support other auxiliary tasks in ESPnet2 yet.\"]},\"119\":{\"h\":\"Inference\",\"t\":[\"Various decoding algorithms are also available for Transducer by setting search_type parameter in your decode config:\",\"Beam search algorithm without prefix search [Graves, 2012]. (search_type: default)\",\"Time Synchronous Decoding [Saon et al., 2020]. (search_type: tsd)\",\"Alignment-Length Synchronous Decoding [Saon et al., 2020]. (search_type: alsd)\",\"modified Adaptive Expansion Search, based on [Kim et al., 2021] and [Boyer et al., 2021]. (search_type: maes)\",\"The algorithms share two parameters to control the beam size (beam_size) and the partial/final hypotheses normalization (score_norm). In addition, three algorithms have specific parameters:\",\"# Time-synchronous decoding search_type: tsd max_sym_exp : Number of maximum symbol expansions at each time step. (int > 1, default = 3) # Alignement-Length Synchronous decoding search_type: alsd u_max: Maximum expected target sequence length. (int, default = 50) # modified Adaptive Expansion Search search_type: maes nstep: Number of maximum expansion steps at each time step (int, default = 2) expansion_gamma: Number of additional candidates in expanded hypotheses selection. (int, default = 2) expansion_beta: Allowed logp difference for prune-by-value method. (float, default = 2.3)\",\"Note: Except for the default algorithm, the described parameters are used to control the performance and decoding speed. The optimal values for each parameter are task-dependent; a high value will typically increase decoding time to focus on performance while a low value will improve decoding time at the expense of performance.\",\"Note 2: The algorithms in the standalone version are the same as the one in the other version.. However, due to design choices, some parts were reworked and minor optimizations were added in the same time.\"]},\"120\":{\"h\":\"Streaming\",\"t\":[\"To enable streaming capabilities for Transducer models, we support dynamic chunk training and chunk-by-chunk decoding as proposed in [Zhang et al., 2021]. Our implementation is based on the version proposed in Icefall, based itself on the original WeNet one.\",\"For a complete explanation on the different procedure and parameters, we refer the reader to the corresponding paper.\"]},\"121\":{\"h\":\"Training\",\"t\":[\"To train a streaming model, the parameter dynamic_chunk_training should be set to True in main_conf (See section Encoder. From here, the user has access to two parameters in order to control the dynamic chunk selection (short_chunk_threshold and short_chunk_size) and another one to control the left context in the causal convolution and the attention module (num_left_chunks).\",\"All these parameters can be configured through main_conf, introduced in the Encoder section:\",\"dynamic_chunk_training: Whether to train streaming model with dynamic chunks. (bool, default = False) short_chunk_threshold: Chunk length threshold (in percent) for dynamic chunk selection. (int, default = 0.75) short_chunk_size: Minimum number of frames during dynamic chunk training. (int, default = 25) num_left_chunks: The number of left chunks the attention module can see during training, where the actual size is defined by `short_chunk_threshold` and `short_chunk_size`. (int, default = 0, i.e. full context)\"]},\"122\":{\"h\":\"Decoding\",\"t\":[\"To perform chunk-by-chunk inference, the parameter streaming should be set to True in the decoding configuration (otherwise, offline decoding will be performed). Two parameters are available to control the decoding process:\",\" decoding_window: The input audio length, in milliseconds, to process during decoding. (int, default = 640) left_context: Number of previous frames (AFTER subsampling) the attention module can see in current chunk. (int, default = 32)\",\"Note: All search algorithms but ALSD are available with chunk-by-chunk inference.\"]},\"123\":{\"h\":\"FAQ\"},\"124\":{\"h\":\"How to add a new block type to the custom encoder?\",\"t\":[\"Provided paths are relative to the directory: espnet2/asr_transducer/encoder/\",\"Adding support to a new block type can be achieved in three main steps:\",\"Write your need block class in encoder/blocks/. The class should have the following methods: __init__(...), forward(...) (training + offline), chunk_forward(...) (online decoding), reset_streaming_cache(...) (online cache definition). For more details on implementing internal parts, we refer the user to the existing block definition and the Streaming section.\",\"In building.py, write a block constructor method and add a new condition in build_body_blocks(...) for your block type, calling the constructor method. If you need additional parameters to share across blocks, you can add them in build_main_parameters(...) and pass main_conf to your constructor.\",\"In validation.py, add new conditions to `validate_block_arguments(...) in order to set and validate the mandatory block parameters before building (if not already covered).\",\"For additional information or examples, please refer to the named files. If you need to add other classes related to the new block, they should be added within the block class or in modules/.\"]},\"125\":{\"h\":\"FAQ\"},\"126\":{\"h\":\"How to build espnet on a cloud machine such as GCP, AWS, etc.?\",\"t\":[\"Our documentation, Installation, assumes that some basic tools are already installed in your machine, gcc, make, etc., so you need also to install them if you don't have them. They are undocumented, but the configuration of our CI may help you because it also builds the environment from scratch with install.sh\"]},\"127\":{\"h\":\"ModuleNotFoundError: No module named 'espnet', or etc.\",\"t\":[\"Firstly, you definitely missed some installation processes. Please read Installation again before posting an issue. If you still have a problem, then please try to manual installation.\",\". tools/activate_python.sh pip install &lt;some-tools&gt; conda install &lt;some-tools&gt;\",\"If you need to install some packages not distributed in pypi, e.g. k2, try to use the installer scripts in espnet.\",\"cd tools ./installers/install_warp-transducer.sh\"]},\"128\":{\"h\":\"To detect the installation problem with a normal installation\",\"t\":[\"Check where your python is\",\"$ . tools/activate_python.sh $ which python # Normally, it should point to &lt;espnet-root&gt;/tools/venv\",\"Check the installation of espnet\",\"$ python >>> import espnet\"]},\"129\":{\"h\":\"ESPnet\"},\"130\":{\"h\":\"Citations\",\"t\":[\"@inproceedings{watanabe2018espnet, author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai}, title={{ESPnet}: End-to-End Speech Processing Toolkit}, year={2018}, booktitle={Proceedings of Interspeech}, pages={2207--2211}, doi={10.21437/Interspeech.2018-1456}, url={http://dx.doi.org/10.21437/Interspeech.2018-1456} } @inproceedings{hayashi2020espnet, title={{Espnet-TTS}: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit}, author={Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu}, booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={7654--7658}, year={2020}, organization={IEEE} } @inproceedings{inaguma-etal-2020-espnet, title = \\\"{ESP}net-{ST}: All-in-One Speech Translation Toolkit\\\", author = \\\"Inaguma, Hirofumi and Kiyono, Shun and Duh, Kevin and Karita, Shigeki and Yalta, Nelson and Hayashi, Tomoki and Watanabe, Shinji\\\", booktitle = \\\"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\\\", month = jul, year = \\\"2020\\\", address = \\\"Online\\\", publisher = \\\"Association for Computational Linguistics\\\", url = \\\"https://www.aclweb.org/anthology/2020.acl-demos.34\\\", pages = \\\"302--311\\\", } @article{hayashi2021espnet2, title={Espnet2-tts: Extending the edge of tts research}, author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji}, journal={arXiv preprint arXiv:2110.07840}, year={2021} } @inproceedings{li2020espnet, title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration}, author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe}, booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)}, pages={785--792}, year={2021}, organization={IEEE}, } @inproceedings{arora2021espnet, title={{ESPnet-SLU}: Advancing Spoken Language Understanding through ESPnet}, author={Arora, Siddhant and Dalmia, Siddharth and Denisov, Pavel and Chang, Xuankai and Ueda, Yushi and Peng, Yifan and Zhang, Yuekai and Kumar, Sujay and Ganesan, Karthik and Yan, Brian and others}, booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={7167--7171}, year={2022}, organization={IEEE} } @inproceedings{shi2022muskits, author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin}, title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis}, year={2022}, booktitle={Proceedings of Interspeech}, pages={4277-4281}, url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf} } @inproceedings{lu22c_interspeech, author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe}, title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}}, year=2022, booktitle={Proc. Interspeech 2022}, pages={5458--5462}, } @article{gao2022euro, title={{EURO}: {ESPnet} Unsupervised ASR Open-source Toolkit}, author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev}, journal={arXiv preprint arXiv:2211.17196}, year={2022} } @article{peng2023reproducing, title={Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data}, author={Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and others}, journal={arXiv preprint arXiv:2309.13876}, year={2023} }\"]},\"131\":{\"h\":\"Installation\"},\"132\":{\"h\":\"Requirements\",\"t\":[\"Python 3.7+\",\"gcc 4.9+ for PyTorch1.10.2+\",\"(If you'll use an anaconda environment at the installation step2, the following packages are installed using conda, so you can skip them.)\",\"cmake3 for some extensions\",\"# For Ubuntu $ sudo apt-get install cmake\",\"sox\",\"# For Ubuntu $ sudo apt-get install sox # For CentOS $ sudo yum install sox\",\"flac (This is not required when installing, but used in some recipes)\",\"# For Ubuntu $ sudo apt-get install flac # For CentOS $ sudo yum install flac\"]},\"133\":{\"h\":\"Supported Linux distributions and other requirements\",\"t\":[\"We support the following Linux distributions with CI. If you want to build your own Linux by yourself, please also check our CI configurations to prepare the appropriate environments.\",\"ubuntu18\",\"centos7\",\"debian11\",\"Windows10 (installation only) \",\"We can conduct complete experiments based on WSL-2 (Ubuntu 20.04). See the link and #4909 for details (Thanks, @Bereket-Desbele!)\",\"MacOS12 (installation only)\"]},\"134\":{\"h\":\"Step 1) [Optional] Install Kaldi\",\"t\":[\"If you use ESPnet1 (under egs/), you must compile Kaldi.\",\"If you use ESPnet2 (under egs2/), You can skip the installation of Kaldi.\",\"Click to compile Kaldi...\",\"Related links:\",\"Kaldi Github\",\"Kaldi Documentation\",\"Downloading and installing Kaldi\",\"The build process (how Kaldi is compiled)\",\"Kaldi INSTALL\",\"Kaldi's requirements:\",\"OS: Ubuntu, CentOS, MacOSX, Windows, Cygwin, etc.\",\"GCC >= 4.7\",\"Git clone Kaldi\",\"$ cd &lt;any-place&gt; $ git clone https://github.com/kaldi-asr/kaldi\",\"Install tools\",\"$ cd &lt;kaldi-root&gt;/tools $ make -j &lt;NUM-CPU&gt;\",\"Select BLAS library from ATLAS, OpenBLAS, or MKL\",\"OpenBLAS\",\"$ cd &lt;kaldi-root&gt;/tools $ ./extras/install_openblas.sh\",\"MKL (You need sudo privilege)\",\"$ cd &lt;kaldi-root&gt;/tools $ sudo ./extras/install_mkl.sh\",\"ATLAS (You need sudo privilege)\",\"# Ubuntu $ sudo apt-get install libatlas-base-dev\",\"Compile Kaldi & install\",\"$ cd &lt;kaldi-root&gt;/src # [By default MKL is used] ESPnet uses only a feature extractor, so you can disable CUDA $ ./configure --use-cuda=no # [With OpenBLAS] # $ ./configure --openblas-root=../tools/OpenBLAS/install --use-cuda=no # If you'll use CUDA # ./configure --cudatk-dir=/usr/local/cuda-10.0 $ make -j clean depend; make -j &lt;NUM-CPU&gt;\",\"We also have prebuilt Kaldi binaries.\"]},\"135\":{\"h\":\"Step 2) Installation ESPnet\",\"t\":[\"Git clone ESPnet\",\"$ cd &lt;any-place&gt; $ git clone https://github.com/espnet/espnet\",\"[Optional] Put compiled Kaldi under espnet/tools\",\"If you have compiled Kaldi at Step 1, put it under tools.\",\"$ cd &lt;espnet-root&gt;/tools $ ln -s &lt;kaldi-root&gt; .\",\"Setup Python environment\",\"You must create &lt;espnet-root&gt;/tools/activate_python.sh to specify the Python interpreter used in espnet recipes. (To understand how ESPnet specifies Python, see path.sh for example.)\",\"We also have some scripts to generate tools/activate_python.sh.\",\"Option A) Setup conda environment\",\"$ cd &lt;espnet-root&gt;/tools $ ./setup_anaconda.sh [output-dir-name|default=venv] [conda-env-name|default=root] [python-version|default=none] # e.g. $ ./setup_anaconda.sh miniconda espnet 3.8\",\"This script tries to create a new miniconda if the output directory doesn't exist. If you already have conda and you'll use it, then,\",\"$ cd &lt;espnet-root&gt;/tools $ CONDA_ROOT=${${CONDA_PREFIX}/../.. # CONDA_PREFIX is an environment variable set by ${CONDA_ROOT}/etc/profile.d/conda.sh $ ./setup_anaconda.sh ${CONDA_ROOT} [conda-env-name] [python-version] # e.g. $ ./setup_anaconda.sh ${CONDA_ROOT} espnet 3.8\",\"Option B) Setup venv from the system Python\",\"$ cd &lt;espnet-root&gt;/tools $ ./setup_venv.sh $(command -v python3)\",\"Option C) Setup system Python environment\",\"$ cd &lt;espnet-root&gt;/tools $ ./setup_python.sh $(command -v python3)\",\"Option D) Without setting the Python environment\",\"Option C and Option D are almost the same. This option might be suitable for Google colab.\",\"$ cd &lt;espnet-root&gt;/tools $ rm -f activate_python.sh && touch activate_python.sh\",\"Install ESPnet\",\"$ cd &lt;espnet-root&gt;/tools $ make\",\"The Makefile tries to install ESPnet and all dependencies, including PyTorch. You can also specify the PyTorch version, for example:\",\"$ cd &lt;espnet-root&gt;/tools $ make TH_VERSION=1.10.1\",\"Note that the CUDA version is derived from nvcc command. If you'd like to specify the other CUDA version, you need to give CUDA_VERSION.\",\"$ cd &lt;espnet-root&gt;/tools $ make TH_VERSION=1.10.1 CUDA_VERSION=11.3\",\"If you don't have nvcc command, packages are installed for CPU mode by default. If you'll turn it on manually, give CPU_ONLY option.\",\"$ cd &lt;espnet-root&gt;/tools $ make CPU_ONLY=0\"]},\"136\":{\"h\":\"Step 3) [Optional] Custom tool installation\",\"t\":[\"Some packages used only for specific tasks, e.g., Transducer ASR, Japanese TTS, etc. are not installed by default, so if you meet some installation error when running these recipes, you need to install them optionally.\",\"e.g.\",\"To install Warp Transducer\",\"cd &lt;espnet-root&gt;/tools cuda_root=&lt;cuda-root&gt; # e.g. &lt;cuda-root&gt; = /usr/local/cuda bash -c \\\". activate_python.sh; . ./setup_cuda_env.sh $cuda_root; ./installers/install_warp-transducer.sh\\\"\",\"To install PyOpenJTalk\",\"cd &lt;espnet-root&gt;/tools bash -c \\\". activate_python.sh; ./installers/install_pyopenjtalk.sh\\\"\",\"To install a module using pip: e.g. to install ipython\",\"cd &lt;espnet-root&gt;/tools bash -c \\\". activate_python.sh; pip install ipython\\\"\",\"In addition to the python libraries, you can also install several non-python libraries in the conda environment, e.g.,\",\"cd &lt;espnet-root&gt;/tools bash -c \\\". activate_python.sh; conda install -c anaconda cmake\\\"\"]},\"137\":{\"h\":\"Check installation\",\"t\":[\"You can check whether your installation is successfully finished by\",\"cd &lt;espnet-root&gt;/tools bash -c \\\". ./activate_python.sh; . ./extra_path.sh; python3 check_install.py\\\"\",\"Note that this check is always called in the last stage of the above installation.\"]},\"138\":{\"h\":\"Notebook\",\"t\":[\"Jupyter notebooks for course demos and tutorials.\"]},\"139\":{\"h\":\"ESPnet1\\\\n\",\"t\":[\"pretrained.md\",\"asr_library.md\",\"tts_recipe.md\",\"asr_recipe.md\",\"st_demo.md\",\"tts_realtime_demo.md\"]},\"140\":{\"h\":\"ESPnet2\\\\n\",\"t\":[\"general_tutorial.md\",\"assignment5_st.md\",\"s2st_demo.md\",\"assignment7_se.md\",\"assignment4_ssl.md\",\"assignment3_spk.md\",\"assignment6_slu.md\",\"assignment8_tts.md\",\"assignment0_data-prep.md\",\"assignment1_espnet-tutorial.md\",\"recipe_tutorial.md\",\"new_task_tutorial.md\",\"streaming_asr_demo.md\",\"asr_realtime_demo.md\",\"asr_transfer_learning_demo.md\",\"onnx_conversion_demo.md\",\"2pass_slu_demo.md\",\"tts_realtime_demo.md\",\"se_demo_for_waspaa_2021.md\",\"se_demo.md\"]},\"141\":{\"h\":\"Using job scheduling system\",\"t\":[\"Our recipes support some Job scheduling systems, SGE, PBS/Torque, and Slurm, according to Parallelization in Kaldi. By default, the job runs at local machine. If there are any Job scheduling systems in your environment, you can submit more number of Jobs with multiple machines.\",\"Please ask the administrator to install it if you have multiple machines.\"]},\"142\":{\"h\":\"Select Job scheduler\",\"t\":[\"cmd.sh is a configuration file and it's used by run.sh to set some shell variables. These shell variables should be set as one of following perl scripts:\",\"cmd\",\"Backend\",\"configuration file\",\"run.pl\",\"Local machine (default)\",\"-\",\"queue.pl\",\"Sun grid engine, or grid endine like tool\",\"conf/queue.conf\",\"slurm.pl\",\"Slurm\",\"conf/slurm.conf\",\"pbs.pl\",\"PBS/Torque\",\"conf/pbs.conf\",\"ssh.pl\",\"SSH\",\".queue/machines\"]},\"143\":{\"h\":\"Usage of run.pl\",\"t\":[\"run.pl, queue.pl, slurm.pl, pbs.pl and ssh.pl have a unified interface, therefore we can assign any one of them to ${cmd} in the shell script:\",\"nj=4 ${cmd} JOB=1:${nj} JOB.log echo JOB\",\"JOB=1:${nj} indicates the parallelization, which is known as \\\"array-job\\\", with ${nj} number of jobs. JOB.log is a destination of the stdout and stderr from jobs. The string of JOB will be changed to the job number if it's included in the log file name or command line arguments. i.e. The following commands are almost equivalent to the above:\",\"echo 1 &> 1.log & echo 2 &> 2.log & echo 3 &> 3.log & echo 4 &> 4.log & wait\"]},\"144\":{\"h\":\"Configuration\",\"t\":[\"You also need to modify the configuration file for a specific job scheduler to change command-line options to submit jobs e.g. queue setting, resource request, etc.\",\"The following text is an example of conf/queue.conf.\",\"# Default configuration command qsub -v PATH -cwd -S /bin/bash -j y -l arch=*64* option mem=* -l mem_free=$0,ram_free=$0 option mem=0 # Do not add anything to qsub_opts option num_threads=* -pe smp $0 option num_threads=1 # Do not add anything to qsub_opts option max_jobs_run=* -tc $0 default gpu=0 option gpu=0 option gpu=* -l gpu=$0 -q g.q\",\"Note that the queue/partition name, -q g.q, is an example, so you must change it to the existing queue/partition in your cluster.\",\"You can't use the specific options depending on each system in our scripts, e.g. you can't use -q option for queue.pl directly. Instead, you can use --mem, --num_threads, --max_jobs_run, and --gpu in this case.\",\"Take a look at the following:\",\"option gpu=* -l gpu=$0 -q g.q\",\"This line means that the optional argument specified by the second column, gpu=*, will be converted to the options after it: -l gpu=$0 -q g.q:\",\"queue.pl --gpu 2\",\"will be converted to\",\"qsub -v PATH -cwd -S /bin/bash -j y -l arch=*64* -l gpu=2 -q g.q\",\"You can also add a new option for your system using this syntax.\",\"option foo=* --bar $0\"]},\"145\":{\"h\":\"Common usages\"},\"146\":{\"h\":\"ESPnet1\",\"t\":[\"Please first check ESPnet1 tutorial\"]},\"147\":{\"h\":\"ESPnet2\",\"t\":[\"Please first check ESPnet2 tutorial\"]},\"148\":{\"h\":\"Multiple GPU TIPs\",\"t\":[\"Note that if you want to use multiple GPUs, the installation of nccl is required before setup.\",\"Currently, espnet1 only supports multiple GPU training within a single node. The distributed setup across multiple nodes is only supported in espnet2.\",\"We don't support multiple GPU inference. Instead, please split the recognition task for multiple jobs and distribute these split jobs to multiple GPUs.\",\"If you cannot get enough speed improvement with multiple GPUs, you should first check the GPU usage by nvidia-smi. If the GPU-Util percentage is low, the bottleneck will come from disk access. You can apply data prefetching by --n-iter-processes 2 in your run.sh to mitigate the problem. Note that this data prefetching consumes a lot of CPU memory, so please be careful when you increase the number of processes.\",\"The behavior of batch size in ESPnet2 during multi-GPU training is different from that in ESPnet1. In ESPnet2, the total batch size is not changed regardless of the number of GPUs. Therefore, you need to manually increase the batch size if you increase the number of GPUs. Please refer to this doc for more information.\"]},\"149\":{\"h\":\"Start from the middle stage or stop at the specified stage\",\"t\":[\"run.sh has multiple stages, including data preparation, training, etc., so you may likely want to start from the specified stage if some stages failed for some reason, for example.\",\"You can start from the specified stage as follows and stop the process at the specified stage:\",\"# Start from 3rd stage and stop at 5th stage $ ./run.sh --stage 3 --stop-stage 5\"]},\"150\":{\"h\":\"CTC, attention, and hybrid CTC/attention\",\"t\":[\"ESPnet can easily switch the model's training/decoding mode from CTC, attention, and hybrid CTC/attention.\",\"Each mode can be trained by specifying mtlalpha (espnet1) ctc_weight (espnet2):\",\"espnet1\",\"# hybrid CTC/attention (default) mtlalpha: 0.3 # CTC mtlalpha: 1.0 # attention mtlalpha: 0.0\",\"espnet2\",\"# hybrid CTC/attention (default) model_conf: ctc_weight: 0.3 # CTC model_conf: ctc_weight: 1.0 # attention model_conf: ctc_weight: 0.0\",\"Decoding for each mode can be done using the following decoding configurations:\",\"espnet1\",\"# hybrid CTC/attention (default) ctc-weight: 0.3 beam-size: 10 # CTC ctc-weight: 1.0 ## for best path decoding api: v1 # default setting (can be omitted) ## for prefix search decoding w/ beam search api: v2 beam-size: 10 # attention ctc-weight: 0.0 beam-size: 10 maxlenratio: 0.8 minlenratio: 0.3\",\"espnet2\",\"# hybrid CTC/attention (default) ctc_weight: 0.3 beam_size: 10 # CTC ctc_weight: 1.0 beam_size: 10 # attention ctc_weight: 0.0 beam_size: 10 maxlenratio: 0.8 minlenratio: 0.3\",\"The CTC mode does not compute the validation accuracy, and the optimum model is selected with its loss value, e.g.,\",\"espnet1\",\"best_model_criterion: - - valid - cer_ctc - min\",\"espnet2\",\"./run.sh --recog_model model.loss.best\",\"The pure attention mode requires setting the maximum and minimum hypothesis length (--maxlenratio and --minlenratio) appropriately. In general, if you have more insertion errors, you can decrease the maxlenratio value, while if you have more deletion errors, you can increase the minlenratio value. Note that the optimum values depend on the ratio of the input frame and output label lengths, which are changed for each language and each BPE unit.\",\"Negative maxlenratio can be used to set the constant maximum hypothesis length independently from the number of input frames. If maxlenratio is set to -1, the decoding will always stop after the first output, which can be used to emulate the utterance classification tasks. This is suitable for some spoken language understanding and speaker identification tasks.\",\"About the effectiveness of hybrid CTC/attention during training and recognition, see [2] and [3]. For example, hybrid CTC/attention is not sensitive to the above maximum and minimum hypothesis heuristics.\"]},\"151\":{\"h\":\"ESPnet Notebook Information\"},\"152\":{\"h\":\"What's new\",\"t\":[\"2023.06.01: new course materials used in speech processing course, spring 2023.\"]},\"153\":{\"h\":\"ESPnet2\"},\"154\":{\"h\":\"Demo\"},\"155\":{\"h\":\"ASR (Speech recognition)\",\"t\":[\"asr_realtime_demo.ipynb: ASR realtime inference with various pre-trained models.\",\"asr_transfer_learning_demo.ipynb: Demo on how to use pre-trained ASR models for fine-tuning.\",\"streaming_asr_demo.ipynb: Streaming ASR realtime inference with pre-trained models.\"]},\"156\":{\"h\":\"SE (Speech enhancement/separation)\",\"t\":[\"se_demo.ipynb: Speech enhancement/separation inference with various pre-trained models.\",\"se_demo_for_waspaa_2021.ipynb: WASPAA2021 version of ESPnet-SE demo.\"]},\"157\":{\"h\":\"SLU (Spoken language understanding)\",\"t\":[\"2pass_slu_demo.ipynb: Two pass spoken language understanding pre-trained model examples.\"]},\"158\":{\"h\":\"TTS (Text-to-speech)\",\"t\":[\"tts_realtime_demo.ipynb: TTS realtime inference with various pre-trained models.\"]},\"159\":{\"h\":\"Others (Other utilities)\",\"t\":[\"onnx_conversion_demo.ipynb: How to convert ESPnet models into ONNX format.\"]},\"160\":{\"h\":\"Course\"},\"161\":{\"h\":\"CMU SpeechProcessing Spring2023\",\"t\":[\"assignment0_data-prep.ipynb: Course assignment on how to prepare ESPnet-format data.\",\"assignment1_espnet-tutorial.ipynb: A simplified version of previous year's new task tutorial.\",\"assignemnt3_spk.ipynb: Examples of using ESPnet to extract speaker embeddings and conduct speaker recognition.\",\"assignment4_ssl.ipynb: Exploration on using self-supervised speech representation to ESPnet ASR training.\",\"assignment5_st.ipynb: Examples of state-of-the-art speech translation models in ESPnet.\",\"assignment6_slu.ipynb: Examples of state-of-the-art spoken language understanding models in ESPnet.\",\"assignment7_se.ipynb: Examples of state-of-the-art speech enhancement/separation in ESPnet.\",\"assignment8_tts.ipynb: A student version of espnet2-tts realtime demonstration.\",\"s2st_demo.ipynb: An example of existing speech-to-speech translation model for ESPnet.\"]},\"162\":{\"h\":\"CMU SpeechRecognition Fall2022\",\"t\":[\"recipe_tutorial.ipynb: A general tutorial of stage-by-stage explanation of ESPnet2 recipes (with new functions).\",\"new_task_tutorial.ipynb: A tutorial on how to add new models/tasks to ESPnet framework.\"]},\"163\":{\"h\":\"CMU SpeecRecognition Fall2021\",\"t\":[\"general_tutorial.ipynb: A general tutorial of stage-by-stage explanation of ESPnet2 recipes.\"]},\"164\":{\"h\":\"ESPnet1\",\"t\":[\"asr_library.ipynb: Speech recognition library explanation with network training.\",\"asr_recipe.ipynb: Speech recognition recipe explanation.\",\"pretrained.ipynb: Tutorial on how to use pre-trained models.\",\"st_demo.ipynb: Speech translation demonstration with a TTS model to achieve speech-to-speech translation.\",\"tts_realtime_demo.ipynb: TTS demonstration with different pre-trained TTS models.\",\"tts_recipe.ipynb: Stage explanation for TTS recipes.\"]},\"165\":{\"h\":\"ESPnet1 Demo\",\"t\":[\"pretrained.md\",\"asr_library.md\",\"tts_recipe.md\",\"asr_recipe.md\",\"st_demo.md\",\"README.md\",\"tts_realtime_demo.md\"]},\"166\":{\"h\":\"Speech Recognition (Library)\",\"t\":[\"This example shows you a practical ASR example using ESPnet as a command line interface and library.\",\"See also\",\"run in colab\",\"documetation https://espnet.github.io/espnet/\",\"github https://github.com/espnet\",\"Author: Shigeki Karita\"]},\"167\":{\"h\":\"Installation\",\"t\":[\"ESPnet depends on Kaldi ASR toolkit and Warp-CTC. This cell will take a few minutes.\",\"# TODO(karita): put these lines in ./espnet/tools/setup_colab.sh # OS setup !sudo apt-get install bc tree !cat /etc/os-release # espnet setup !git clone https://github.com/espnet/espnet !cd espnet; pip install -e . !mkdir espnet/tools/venv/bin; touch espnet/tools/venv/bin/activate # warp ctc setup !git clone https://github.com/espnet/warp-ctc -b pytorch-1.1 !cd warp-ctc && mkdir build && cd build && cmake .. && make -j4 !cd warp-ctc/pytorch_binding && python setup.py install # kaldi setup !cd ./espnet/tools; git clone https://github.com/kaldi-asr/kaldi !echo \\\"\\\" > ./espnet/tools/kaldi/tools/extras/check_dependencies.sh # ignore check !chmod +x ./espnet/tools/kaldi/tools/extras/check_dependencies.sh !cd ./espnet/tools/kaldi/tools; make sph2pipe sclite !rm -rf espnet/tools/kaldi/tools/python ![ ! -e ubuntu16-featbin.tar.gz ] && wget https://18-198329952-gh.circle-artifacts.com/0/home/circleci/repo/ubuntu16-featbin.tar.gz !tar -xf ./ubuntu16-featbin.tar.gz !cp featbin/* espnet/tools/kaldi/src/featbin/\"]},\"168\":{\"h\":\"ESPnet data preparation\",\"t\":[\"You can use the end-to-end script run.sh for reproducing systems reported in espnet/egs/*/asr1/RESULTS.md. Typically, we organize run.sh with several stages:\",\"Data download (if available)\",\"Kaldi-style data preparation\",\"Dump useful data for traning (e.g., JSON, HDF5, etc)\",\"Lanuage model training\",\"ASR model training\",\"Decoding and evaluation\",\"For example, if you add --stop-stage 2, you can stop the script before neural network training.\",\"!cd espnet/egs/an4/asr1; ./run.sh --ngpu 1 --stop-stage 2\"]},\"169\":{\"h\":\"Kaldi-style directories\",\"t\":[\"Always we organize each recipe placed in egs/xxx/asr1 in Kaldi way. For example, the important directories are:\",\"conf/: kaldi configurations, e.g., speech feature\",\"data/: almost raw data prepared by Kaldi\",\"exp/: intermidiate files through experiments, e.g., log files, model parameters\",\"fbank/: speech feature binary files, e.g., ark, scp\",\"dump/: ESPnet meta data for tranining, e.g., json, hdf5\",\"local/: corpus specific data preparation scripts\",\"steps/, utils/: Kaldi's helper scripts\",\"!tree -L 1 !ls data/train\"]},\"170\":{\"h\":\"ESPnet as a library\",\"t\":[\"Here we use ESPnet as a library to create a simple Python snippet for speech recognition. ESPnet 's training script'asr_train.py has three parts:\",\"Load train/dev dataset\",\"Create minibatches\",\"Build neural networks\",\"Update neural networks by iterating datasets\",\"Let's implement these procedures from scratch!\"]},\"171\":{\"h\":\"Load train/dev dataset (1/4)\",\"t\":[\"First, we will check how run.sh organized the JSON files and load the pair of the speech feature and its transcription.\",\"import json import matplotlib.pyplot as plt import kaldiio root = \\\"espnet/egs/an4/asr1\\\" with open(root + \\\"/dump/train_nodev/deltafalse/data.json\\\", \\\"r\\\") as f: train_json = json.load(f)[\\\"utts\\\"] with open(root + \\\"/dump/train_dev/deltafalse/data.json\\\", \\\"r\\\") as f: dev_json = json.load(f)[\\\"utts\\\"] # the first training data for speech recognition key, info = next(iter(train_json.items())) # plot the 80-dim fbank + 3-dim pitch speech feature fbank = kaldiio.load_mat(info[\\\"input\\\"][0][\\\"feat\\\"]) plt.matshow(fbank.T[::-1]) plt.title(key + \\\": \\\" + info[\\\"output\\\"][0][\\\"text\\\"]) # print the key-value pair key, info\"]},\"172\":{\"h\":\"Create minibatches (2/4)\",\"t\":[\"To parallelize neural network training, we create minibatches that containes several sequence pairs by splitting datasets.\",\"from espnet.utils.training.batchfy import make_batchset batch_size = 32 trainset = make_batchset(train_json, batch_size) devset = make_batchset(dev_json, batch_size) assert len(devset[0]) == batch_size devset[0][:3]\"]},\"173\":{\"h\":\"Build neural networks (3/4)\",\"t\":[\"For simplicity, we use a predefined model: Transformer.\",\"NOTE: You can also use your custom model in command line tools as asr_train.py --model-module your_module:YourModel\",\"import argparse from espnet.bin.asr_train import get_parser from espnet.nets.pytorch_backend.e2e_asr import E2E parser = get_parser() parser = E2E.add_arguments(parser) config = parser.parse_args([ \\\"--mtlalpha\\\", \\\"0.0\\\", # weight for cross entropy and CTC loss \\\"--outdir\\\", \\\"out\\\", \\\"--dict\\\", \\\"\\\"]) # TODO: allow no arg idim = info[\\\"input\\\"][0][\\\"shape\\\"][1] odim = info[\\\"output\\\"][0][\\\"shape\\\"][1] setattr(config, \\\"char_list\\\", []) model = E2E(idim, odim, config) model\"]},\"174\":{\"h\":\"Update neural networks by iterating datasets (4/4)\",\"t\":[\"Finaly, we got the training part.\",\"import numpy import torch from torch.nn.utils.rnn import pad_sequence from torch.nn.utils.clip_grad import clip_grad_norm_ from torch.utils.data import DataLoader def collate(minibatch): fbanks = [] tokens = [] for key, info in minibatch[0]: fbanks.append(torch.tensor(kaldiio.load_mat(info[\\\"input\\\"][0][\\\"feat\\\"]))) tokens.append(torch.tensor([int(s) for s in info[\\\"output\\\"][0][\\\"tokenid\\\"].split()])) ilens = torch.tensor([x.shape[0] for x in fbanks]) return pad_sequence(fbanks, batch_first=True), ilens, pad_sequence(tokens, batch_first=True) train_loader = DataLoader(trainset, collate_fn=collate, shuffle=True, pin_memory=True) dev_loader = DataLoader(devset, collate_fn=collate, pin_memory=True) model.cuda() optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98)) n_iter = len(trainset) n_epoch = 10 total_iter = n_iter * n_epoch train_acc = [] valid_acc = [] for epoch in range(n_epoch): # training acc = [] model.train() for data in train_loader: loss = model(*[d.cuda() for d in data]) optim.zero_grad() loss.backward() acc.append(model.acc) norm = clip_grad_norm_(model.parameters(), 10.0) optim.step() train_acc.append(numpy.mean(acc)) # validation acc = [] model.eval() for data in dev_loader: model(*[d.cuda() for d in data]) acc.append(model.acc) valid_acc.append(numpy.mean(acc)) print(f\\\"epoch: {epoch}, train acc: {train_acc[-1]:.3f}, dev acc: {valid_acc[-1]:.3f}\\\")\",\"import matplotlib.pyplot as plt plt.plot(range(len(train_acc)), train_acc, label=\\\"train acc\\\") plt.plot(range(len(valid_acc)), valid_acc, label=\\\"dev acc\\\") plt.grid() plt.legend()\",\"torch.save(model.state_dict(), \\\"best.pt\\\")\"]},\"175\":{\"h\":\"Recognize speech\",\"t\":[\"import json import matplotlib.pyplot as plt import kaldiio from espnet.bin.asr_recog import get_parser # load data root = \\\"espnet/egs/an4/asr1\\\" with open(root + \\\"/dump/test/deltafalse/data.json\\\", \\\"r\\\") as f: test_json = json.load(f)[\\\"utts\\\"] key, info = list(test_json.items())[10] # plot the 80-dim fbank + 3-dim pitch speech feature fbank = kaldiio.load_mat(info[\\\"input\\\"][0][\\\"feat\\\"]) plt.matshow(fbank.T[::-1]) plt.title(key + \\\": \\\" + info[\\\"output\\\"][0][\\\"text\\\"]) # load token dict with open(root + \\\"/data/lang_1char/train_nodev_units.txt\\\", \\\"r\\\") as f: token_list = [entry.split()[0] for entry in f] token_list.insert(0, '&lt;blank&gt;') token_list.append('&lt;eos&gt;') # recognize speech parser = get_parser() args = parser.parse_args([ \\\"--beam-size\\\", \\\"1\\\", \\\"--ctc-weight\\\", \\\"0\\\", \\\"--result-label\\\", \\\"out.json\\\", \\\"--model\\\", \\\"\\\" ]) model.cpu() model.eval() def to_str(result): return \\\"\\\".join(token_list[y] for y in result[0][\\\"yseq\\\"]) \\\\ .replace('&lt;eos&gt;', \\\"\\\").replace('&lt;space&gt;', \\\" \\\").replace('&lt;blank&gt;', \\\"\\\") print(\\\"groundtruth:\\\", info[\\\"output\\\"][0][\\\"text\\\"]) print(\\\"prediction: \\\", to_str(model.recognize(fbank, args, token_list)))\"]},\"176\":{\"h\":\"Speech Recognition (Recipe)\",\"t\":[\"Author: Shigeki Karita\",\"July 29 2019\",\"ESPnet Hackathon 2019 @Tokyo\"]},\"177\":{\"h\":\"Abstract\",\"t\":[\"This example shows you a practical ASR example using ESPnet as a command line interface, and also as a library.\",\"See also\",\"documetation https://espnet.github.io/espnet/\",\"github https://github.com/espnet\"]},\"178\":{\"h\":\"Installation\",\"t\":[\"ESPnet depends on Kaldi ASR toolkit and Warp-CTC. This will take a few minutes.\",\"# OS setup !sudo apt-get install bc tree !cat /etc/os-release # espnet setup !git clone https://github.com/espnet/espnet !cd espnet; pip install -e . !mkdir -p espnet/tools/venv/bin; touch espnet/tools/venv/bin/activate # warp ctc setup !git clone https://github.com/espnet/warp-ctc -b pytorch-1.1 !cd warp-ctc && mkdir build && cd build && cmake .. && make -j4 !cd warp-ctc/pytorch_binding && python setup.py install # kaldi setup !cd ./espnet/tools; git clone https://github.com/kaldi-asr/kaldi !echo \\\"\\\" > ./espnet/tools/kaldi/tools/extras/check_dependencies.sh # ignore check !chmod +x ./espnet/tools/kaldi/tools/extras/check_dependencies.sh !cd ./espnet/tools/kaldi/tools; make sph2pipe sclite !rm -rf espnet/tools/kaldi/tools/python ![ ! -e ubuntu16-featbin.tar.gz ] && wget https://18-198329952-gh.circle-artifacts.com/0/home/circleci/repo/ubuntu16-featbin.tar.gz !tar -xf ./ubuntu16-featbin.tar.gz !cp featbin/* espnet/tools/kaldi/src/featbin/\"]},\"179\":{\"h\":\"ESPnet command line usage (espnet/egs/xxx)\",\"t\":[\"You can use the end-to-end script run.sh for reproducing systems reported in espnet/egs/*/asr1/RESULTS.md. Typically, we organize run.sh with several stages:\",\"Data download (if available)\",\"Kaldi-style data preparation\",\"Save python-friendly data (e.g., JSON, HDF5, etc)\",\"Language model training\",\"ASR model training\",\"Decoding and evaluation\",\"!ls espnet/egs\"]},\"180\":{\"h\":\"Stage 0 - 2 Data preparation\",\"t\":[\"For example, if you add --stop-stage 2, you can stop the script before neural network training.\",\"!cd espnet/egs/an4/asr1; ./run.sh --ngpu 1 --stop-stage 2\"]},\"181\":{\"h\":\"Kaldi-style directory structure\",\"t\":[\"Always we organize each recipe placed in egs/xxx/asr1 in Kaldi way:\",\"conf/: kaldi configurations, e.g., speech feature\",\"data/: almost raw data prepared by Kaldi\",\"exp/: intermidiate files through experiments, e.g., log files, model parameters\",\"fbank/: speech feature binary files, e.g., ark, scp\",\"dump/: ESPnet meta data for tranining, e.g., json, hdf5\",\"local/: corpus specific data preparation scripts\",\"steps/, utils/: Kaldi's helper scripts\",\"!tree -L 1 espnet/egs/an4/asr1\"]},\"182\":{\"h\":\"TIPS: essential files in data preparation\",\"t\":[\"To create a new recipe, all you need is stage 1 that creates key-value pair files:\",\"speechdata/xxx/wav.scp\",\"textdata/xxx/text\"]},\"183\":{\"h\":\"raw speech file list\",\"t\":[\"!head espnet/egs/an4/asr1/data/train/wav.scp\"]},\"184\":{\"h\":\"raw text list\",\"t\":[\"!head espnet/egs/an4/asr1/data/train/text\"]},\"185\":{\"h\":\"TIPS: explore datasets with data.json\",\"t\":[\"To explore datasets easily, ESPnet stores metadata dump/xxx/data.json in the stage 2.\",\"import json import matplotlib.pyplot as plt import kaldiio # load 10-th speech/text in data.json root = \\\"espnet/egs/an4/asr1\\\" with open(root + \\\"/dump/test/deltafalse/data.json\\\", \\\"r\\\") as f: test_json = json.load(f)[\\\"utts\\\"] key, info = list(test_json.items())[10] # plot the speech feature fbank = kaldiio.load_mat(info[\\\"input\\\"][0][\\\"feat\\\"]) plt.matshow(fbank.T[::-1]) plt.title(key + \\\": \\\" + info[\\\"output\\\"][0][\\\"text\\\"]) # print the key-value pair key, info\"]},\"186\":{\"h\":\"Stage 3 - 4 NN Training\",\"t\":[\"Let's go to the most interesting part...\",\"!tail espnet/egs/an4/asr1/conf/train_mtlalpha1.0.yaml\",\"!cd espnet/egs/an4/asr1; ./run.sh --ngpu 1 --stage 3 --stop-stage 4 --train-config ./conf/train_mtlalpha1.0.yaml\"]},\"187\":{\"h\":\"TIPS: change_yaml.py\",\"t\":[\"You can tweak YAML config by $(change_yaml.py xxx.yaml -a yyy=zzz)\",\"!cd espnet/egs/an4/asr1; source path.sh; \\\\ ./run.sh --ngpu 1 --stage 4 --stop-stage 4 \\\\ --train-config $(change_yaml.py ./conf/train_mtlalpha1.0.yaml -a eunits=100)\"]},\"188\":{\"h\":\"TIPS: tensorboard\",\"t\":[\"You can easily monitor effects of the config by tensorboard\"]},\"189\":{\"h\":\"Decoding and evaluation\",\"t\":[\"decode config (change_yaml.py also works)\",\"!cat espnet/egs/an4/asr1/conf/decode_ctcweight1.0.yaml\"]},\"190\":{\"h\":\"Command line usage\",\"t\":[\"!cd espnet/egs/an4/asr1; ./run.sh --stage 5\"]},\"191\":{\"h\":\"ASR result as\",\"t\":[\"!head -n20 espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha1.0/decode_test_decode_ctcweight1.0_lm_word100/data.json\"]},\"192\":{\"h\":\"Recognize speech from python\",\"t\":[\"Let's use ESPnet as a library and the trained model:\",\"!ls espnet/egs/an4/asr1/exp/train_nodev_pytorch_train_mtlalpha1.0/results\"]},\"193\":{\"h\":\"recap: load speech from data.json\",\"t\":[\"import json import matplotlib.pyplot as plt import kaldiio # load 10-th speech/text in data.json root = \\\"espnet/egs/an4/asr1\\\" with open(root + \\\"/dump/test/deltafalse/data.json\\\", \\\"r\\\") as f: test_json = json.load(f)[\\\"utts\\\"] key, info = list(test_json.items())[10] # plot the speech feature fbank = kaldiio.load_mat(info[\\\"input\\\"][0][\\\"feat\\\"]) plt.matshow(fbank.T[::-1]) plt.title(key + \\\": \\\" + info[\\\"output\\\"][0][\\\"text\\\"])\"]},\"194\":{\"h\":\"load model\",\"t\":[\"import json import torch import argparse from espnet.bin.asr_recog import get_parser from espnet.nets.pytorch_backend.e2e_asr import E2E root = \\\"espnet/egs/an4/asr1\\\" model_dir = root + \\\"/exp/train_nodev_pytorch_train_mtlalpha1.0/results\\\" # load model with open(model_dir + \\\"/model.json\\\", \\\"r\\\") as f: idim, odim, conf = json.load(f) model = E2E(idim, odim, argparse.Namespace(**conf)) model.load_state_dict(torch.load(model_dir + \\\"/model.loss.best\\\")) model.cpu().eval() # load token dict with open(root + \\\"/data/lang_1char/train_nodev_units.txt\\\", \\\"r\\\") as f: token_list = [entry.split()[0] for entry in f] token_list.insert(0, '&lt;blank&gt;') token_list.append('&lt;eos&gt;') # recognize speech parser = get_parser() args = parser.parse_args([\\\"--beam-size\\\", \\\"2\\\", \\\"--ctc-weight\\\", \\\"1.0\\\", \\\"--result-label\\\", \\\"out.json\\\", \\\"--model\\\", \\\"\\\"]) result = model.recognize(fbank, args, token_list) s = \\\"\\\".join(conf[\\\"char_list\\\"][y] for y in result[0][\\\"yseq\\\"]).replace('&lt;eos&gt;', \\\"\\\").replace('&lt;space&gt;', \\\" \\\").replace('&lt;blank&gt;', \\\"\\\") print(\\\"groundtruth:\\\", info[\\\"output\\\"][0][\\\"text\\\"]) print(\\\"prediction: \\\", s)\",\"import os import kaldiio from IPython.display import Audio try: d = os.getcwd() os.chdir(root) sr, wav = kaldiio.load_scp(\\\"data/test/wav.scp\\\")[key] finally: os.chdir(d) Audio(wav, rate=sr)\"]},\"195\":{\"h\":\"Pretrained Model\",\"t\":[\"This is the example notebook of how-to-recognize and -synthesize speech using the ESPnet models.\",\"See also:\",\"Tutorial: https://github.com/espnet/espnet/blob/master/doc/tutorial.md\",\"Github: https://github.com/espnet\",\"Author: Takenori Yoshimura\",\"Last update: 2019/07/28\"]},\"196\":{\"h\":\"Setup envrionment\",\"t\":[\"Let's setup the environmet for the demonstration. It takes around 10 minues. Please keep waiting for a while.\",\"# OS setup !sudo apt-get install bc tree sox !cat /etc/os-release # espnet setup !git clone https://github.com/espnet/espnet !cd espnet; pip install -e . # warp ctc setup !git clone https://github.com/espnet/warp-ctc -b pytorch-1.1 !cd warp-ctc && mkdir build && cd build && cmake .. && make -j !cd warp-ctc/pytorch_binding && python setup.py install # kaldi setup !cd /content/espnet/tools; git clone https://github.com/kaldi-asr/kaldi !echo \\\"\\\" > ./espnet/tools/kaldi/tools/extras/check_dependencies.sh !chmod +x ./espnet/tools/kaldi/tools/extras/check_dependencies.sh !cd ./espnet/tools/kaldi/tools; make sph2pipe sclite !rm -rf espnet/tools/kaldi/tools/python !wget https://18-198329952-gh.circle-artifacts.com/0/home/circleci/repo/ubuntu16-featbin.tar.gz !tar -xf ./ubuntu16-featbin.tar.gz !cp featbin/* espnet/tools/kaldi/src/featbin/ # sentencepiece setup !cd espnet/tools; make sentencepiece.done # make dummy activate !mkdir -p espnet/tools/venv/bin !touch espnet/tools/venv/bin/activate\"]},\"197\":{\"h\":\"Recognize speech using pretrained models\",\"t\":[\"Let's recognize 7-minutes long audio speech as an example. Go to a recipe directory and run recog_wav.sh at the directory.\",\"Available models are summarized here.\",\"!cd espnet/egs/tedlium2/asr1; bash ../../../utils/recog_wav.sh --models tedlium2.tacotron2.v1\",\"You can see the progress of the recognition.\",\"!cat espnet/egs/tedlium2/asr1/decode/TomWujec_2010U/log/decode.log\",\"You can change E2E model, language model, decoding parameters, etc. For the detail, see recog_wav.sh.\",\"!cat espnet/utils/recog_wav.sh\"]},\"198\":{\"h\":\"Synthesize speech using pretrained models\",\"t\":[\"Let's synthesize speech using an E2E model. Go to a recipe directory and run synth_wav.sh at the directory.\",\"Available models are summarized here.\",\"!cd espnet/egs/ljspeech/tts1; \\\\ echo \\\"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\\\" > example.txt; \\\\ bash ../../../utils/synth_wav.sh --models ljspeech.tacotron2.v1 example.txt\",\"Let's listen the synthesized speech!\",\"from google.colab import files files.download('espnet/egs/ljspeech/tts1/decode/example/wav/example.wav')\",\"You can change E2E model, decoding parameters, etc. For the detail, see synth_wav.sh.\",\"!cat espnet/utils/synth_wav.sh\",\"We have a web storage to put your good trained models. If you want, please contact Shinji Watanabe <shinjiw@ieee.org>.\"]},\"199\":{\"h\":\"ESPnet Speech Translation Demonstration\",\"t\":[\"Open In Colab\",\"See also\",\"ESPnet: https://github.com/espnet/espnet\",\"ESPnet documentation: https://espnet.github.io/espnet/\",\"TTS demo: https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb\",\"Author: Shigeki Karita\"]},\"200\":{\"h\":\"Install\",\"t\":[\"It takes around 3 minutes. Please keep waiting for a while.\",\"# OS setup !cat /etc/os-release !apt-get install -qq bc tree sox # espnet and moses setup !git clone -q https://github.com/ShigekiKarita/espnet.git !pip install -q torch==1.1 !cd espnet; git checkout c0466d9a356c1a33f671a546426d7bc33b5b17e8; pip install -q -e . !cd espnet/tools/; make moses.done # download pre-compiled warp-ctc and kaldi tools !espnet/utils/download_from_google_drive.sh \\\\ \\\"https://drive.google.com/open?id=13Y4tSygc8WtqzvAVGK_vRV9GlV7TRC0w\\\" espnet/tools tar.gz > /dev/null # make dummy activate !mkdir -p espnet/tools/venv/bin && touch espnet/tools/venv/bin/activate !echo \\\"setup done.\\\"\"]},\"201\":{\"h\":\"Spanish speech -> English text translation\",\"t\":[\"This audio says \\\"yo soy José.\\\"\",\"from IPython.display import display, Audio display(Audio(\\\"/content/espnet/test_utils/st_test.wav\\\", rate=16000))\",\"Let's translate this into English text by our pretrained Transformer ST model trained on the Fisher-CALLHOME Spanish dataset.\",\"# move on the recipe directory import os os.chdir(\\\"/content/espnet/egs/fisher_callhome_spanish/st1\\\") !../../../utils/translate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en ../../../test_utils/st_test.wav | tee /content/translated.txt\",\"As seen above, we successfully obtained the result: \\\"Translated text: yes i'm jose\\\"!\"]},\"202\":{\"h\":\"English translated text-to-speech synthesis\",\"t\":[\"Now let's generate an English speech from the translated text using a pretrained ESPnet-TTS model.\",\"!sed -n 's/Translated text://p' /content/translated.txt | tr '[:lower:]' '[:upper:]' | tee /content/translated_sed.txt !../../../utils/synth_wav.sh /content/translated_sed.txt\",\"import matplotlib.pyplot as plt import kaldiio fbank = next(iter(kaldiio.load_scp(\\\"decode/translated_sed/outputs/feats.scp\\\").values())) plt.matshow(fbank.T)\",\"from IPython.display import display, Audio display(Audio(\\\"decode/translated_sed/wav_wnv/translated_sed_gen.wav\\\"))\",\"Successfully, it says \\\"Yes I'm Jose\\\"! For more TTS demo, visit https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb\"]},\"203\":{\"h\":\"Check decoding log\",\"t\":[\"After the translation, you will find &lt;decode_dir&gt;/&lt;wav name&gt;/result.json for details;\",\"!cat decode/st_test/result.json\",\"and &lt;decode_dir&gt;/&lt;wav name&gt;/log/decode.log for runtime log;\",\"!cat decode/st_test/log/decode.log\",\"Let's calculate real-time factor (RTF) of the ST decoding from the decode.log\",\"from dateutil import parser from subprocess import PIPE, run # calc input duration (seconds) input_sec = float(run([\\\"soxi\\\", \\\"-D\\\", \\\"/content/espnet/test_utils/st_test.wav\\\"], stdout=PIPE).stdout) # calc NN decoding time with open(\\\"decode/st_test/log/decode.log\\\", \\\"r\\\") as f: times = [parser.parse(x.split(\\\"(\\\")[0]) for x in f if \\\"e2e_st_transformer\\\" in x] decode_sec = (times[-1] - times[0]).total_seconds() # get real-time factor (RTF) print(\\\"Input duration:\\\\t\\\", input_sec, \\\"sec\\\") print(\\\"NN decoding:\\\\t\\\", decode_sec, \\\"sec\\\") print(\\\"Real-time factor:\\\\t\\\", decode_sec / input_sec)\",\"As you can see above, ESPnet-ST can translate speech faster than the input (it should be RTF < 1.0).\"]},\"204\":{\"h\":\"Training ST models from scratch\",\"t\":[\"We provide Kaldi-style recipes for ST as well as ASR and TTS as all-in-one bash script run.sh:\",\"!cd /content/espnet/egs/must_c/st1/ && ./run.sh --must-c /content\",\"However, it takes too much time to finish downloading the dataset. So we cancel the cell above.\"]},\"205\":{\"h\":\"Details of ESPnet tools\",\"t\":[\"!../../../utils/translate_wav.sh --help\",\"!../../../utils/synth_wav.sh --help\"]},\"206\":{\"h\":\"ESPnet real time E2E-TTS demonstration\",\"t\":[\"Open In Colab\",\"This notebook provides a demonstration of the realtime E2E-TTS using ESPnet-TTS and ParallelWaveGAN (+ MelGAN).\",\"ESPnet: https://github.com/espnet/espnet\",\"ParallelWaveGAN: https://github.com/kan-bayashi/ParallelWaveGAN\",\"Author: Tomoki Hayashi (@kan-bayashi)\"]},\"207\":{\"h\":\"Install\",\"t\":[\"# install minimal components !pip install -q parallel_wavegan PyYaml unidecode ConfigArgparse g2p_en espnet_tts_frontend !pip install --upgrade --no-cache-dir gdown !git clone -q https://github.com/espnet/espnet.git !cd espnet && git fetch && git checkout -b v.0.9.1 refs/tags/v.0.9.1\"]},\"208\":{\"h\":\"English demo\"},\"209\":{\"h\":\"Download pretrained feature generation model\",\"t\":[\"You can select one from three models. Please only run the seletected model cells.\"]},\"210\":{\"h\":\"(a) Tacotron2\",\"t\":[\"# download pretrained model import os if not os.path.exists(\\\"downloads/en/tacotron2\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1lFfeyewyOsxaNO-DEWy9iSz6qB9ZS1UR downloads/en/tacotron2 tar.gz # set path trans_type = \\\"phn\\\" dict_path = \\\"downloads/en/tacotron2/data/lang_1phn/phn_train_no_dev_units.txt\\\" model_path = \\\"downloads/en/tacotron2/exp/phn_train_no_dev_pytorch_train_pytorch_tacotron2.v3/results/model.last1.avg.best\\\" print(\\\"sucessfully finished download.\\\")\"]},\"211\":{\"h\":\"(b) Transformer\",\"t\":[\"# download pretrained model import os if not os.path.exists(\\\"downloads/en/transformer\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1z8KSOWVBjK-_Ws4RxVN4NTx-Buy03-7c downloads/en/transformer tar.gz # set path trans_type = \\\"phn\\\" dict_path = \\\"downloads/en/transformer/data/lang_1phn/phn_train_no_dev_units.txt\\\" model_path = \\\"downloads/en/transformer/exp/phn_train_no_dev_pytorch_train_pytorch_transformer.v3.single/results/model.last1.avg.best\\\" print(\\\"sucessfully finished download.\\\")\"]},\"212\":{\"h\":\"(c) FastSpeech\",\"t\":[\"# download pretrained model import os if not os.path.exists(\\\"downloads/en/fastspeech\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1P9I4qag8wAcJiTCPawt6WCKBqUfJFtFp downloads/en/fastspeech tar.gz # set path trans_type = \\\"phn\\\" dict_path = \\\"downloads/en/fastspeech/data/lang_1phn/phn_train_no_dev_units.txt\\\" model_path = \\\"downloads/en/fastspeech/exp/phn_train_no_dev_pytorch_train_tacotron2.v3_fastspeech.v4.single/results/model.last1.avg.best\\\" print(\\\"Sucessfully finished download.\\\")\"]},\"213\":{\"h\":\"Download pretrained vocoder model\",\"t\":[\"You can select one from two models. Please only run the seletected model cells.\"]},\"214\":{\"h\":\"(a) Parallel WaveGAN\",\"t\":[\"# download pretrained model import os if not os.path.exists(\\\"downloads/en/parallel_wavegan\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1Grn7X9wD35UcDJ5F7chwdTqTa4U7DeVB downloads/en/parallel_wavegan tar.gz # set path vocoder_path = \\\"downloads/en/parallel_wavegan/ljspeech.parallel_wavegan.v2/checkpoint-400000steps.pkl\\\" print(\\\"Sucessfully finished download.\\\")\"]},\"215\":{\"h\":\"(b) MelGAN\",\"t\":[\"# download pretrained model import os if not os.path.exists(\\\"downloads/en/melgan\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1_a8faVA5OGCzIcJNw4blQYjfG4oA9VEt downloads/en/melgan tar.gz # set path vocoder_path = \\\"downloads/en/melgan/train_nodev_ljspeech_melgan.v3.long/checkpoint-4000000steps.pkl\\\" print(\\\"Sucessfully finished download.\\\")\"]},\"216\":{\"h\":\"(c) Multi-band MelGAN\",\"t\":[\"This is an EXPERIMENTAL model.\",\"# download pretrained model import os if not os.path.exists(\\\"downloads/en/mb-melgan\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1rGG5y15uy4WZ-lJy8NPVTkmB_6VhC20V downloads/en/mb-melgan tar.gz # set path vocoder_path = \\\"downloads/en/mb-melgan/train_nodev_ljspeech_multi_band_melgan.v1/checkpoint-1000000steps.pkl\\\" print(\\\"Sucessfully finished download.\\\")\"]},\"217\":{\"h\":\"Setup\",\"t\":[\"# add path import sys sys.path.append(\\\"espnet\\\") # define device import torch device = torch.device(\\\"cuda\\\") # define E2E-TTS model from argparse import Namespace from espnet.asr.asr_utils import get_model_conf from espnet.asr.asr_utils import torch_load from espnet.utils.dynamic_import import dynamic_import idim, odim, train_args = get_model_conf(model_path) model_class = dynamic_import(train_args.model_module) model = model_class(idim, odim, train_args) torch_load(model_path, model) model = model.eval().to(device) inference_args = Namespace(**{ \\\"threshold\\\": 0.5,\\\"minlenratio\\\": 0.0, \\\"maxlenratio\\\": 10.0, # Only for Tacotron 2 \\\"use_attention_constraint\\\": True, \\\"backward_window\\\": 1,\\\"forward_window\\\":3, # Only for fastspeech (lower than 1.0 is faster speech, higher than 1.0 is slower speech) \\\"fastspeech_alpha\\\": 1.0, }) # define neural vocoder from parallel_wavegan.utils import load_model fs = 22050 vocoder = load_model(vocoder_path) vocoder.remove_weight_norm() vocoder = vocoder.eval().to(device) # define text frontend from tacotron_cleaner.cleaners import custom_english_cleaners from g2p_en import G2p with open(dict_path) as f: lines = f.readlines() lines = [line.replace(\\\"\\\\n\\\", \\\"\\\").split(\\\" \\\") for line in lines] char_to_id = {c: int(i) for c, i in lines} g2p = G2p() def frontend(text): \\\"\\\"\\\"Clean text and then convert to id sequence.\\\"\\\"\\\" text = custom_english_cleaners(text) if trans_type == \\\"phn\\\": text = filter(lambda s: s != \\\" \\\", g2p(text)) text = \\\" \\\".join(text) print(f\\\"Cleaned text: {text}\\\") charseq = text.split(\\\" \\\") else: print(f\\\"Cleaned text: {text}\\\") charseq = list(text) idseq = [] for c in charseq: if c.isspace(): idseq += [char_to_id['&lt;space&gt;']] elif c not in char_to_id.keys(): idseq += [char_to_id['&lt;unk&gt;']] else: idseq += [char_to_id[c]] idseq += [idim - 1] # &lt;eos&gt; return torch.LongTensor(idseq).view(-1).to(device) import nltk nltk.download('punkt') print(\\\"Now ready to synthesize!\\\")\"]},\"218\":{\"h\":\"Synthesis\",\"t\":[\"import time print(\\\"Input your favorite sentence in English!\\\") input_text = input() with torch.no_grad(): start = time.time() x = frontend(input_text) c, _, _ = model.inference(x, inference_args) y = vocoder.inference(c) rtf = (time.time() - start) / (len(y) / fs) print(f\\\"RTF = {rtf:5f}\\\") from IPython.display import display, Audio display(Audio(y.view(-1).cpu().numpy(), rate=fs))\"]},\"219\":{\"h\":\"Japanese demo\"},\"220\":{\"h\":\"Install Japanese dependencies\",\"t\":[\"!pip install pyopenjtalk\"]},\"221\":{\"h\":\"Download pretrained models\",\"t\":[\"Here we select Tacotron2 or Transformer. The vocoder model is Parallel WaveGAN.\"]},\"222\":{\"h\":\"(a) Tacotron 2\",\"t\":[\"# download pretrained models import os if not os.path.exists(\\\"downloads/jp/tacotron2\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM downloads/jp/tacotron2 tar.gz !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1kp5M4VvmagDmYckFJa78WGqh1drb_P9t downloads/jp/tacotron2 tar.gz # set path dict_path = \\\"downloads/jp/tacotron2/data/lang_1phn/train_no_dev_units.txt\\\" model_path = \\\"downloads/jp/tacotron2/exp/train_no_dev_pytorch_train_pytorch_tacotron2_phn/results/model.last1.avg.best\\\" vocoder_path = \\\"downloads/jp/tacotron2/jsut.parallel_wavegan.v1/checkpoint-400000steps.pkl\\\" print(\\\"sucessfully finished download.\\\")\"]},\"223\":{\"h\":\"(b) Transformer\",\"t\":[\"# download pretrained models import os if not os.path.exists(\\\"downloads/jp/transformer\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM downloads/jp/transformer tar.gz !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD downloads/jp/transformer tar.gz # set path dict_path = \\\"downloads/jp/transformer/data/lang_1phn/train_no_dev_units.txt\\\" model_path = \\\"downloads/jp/transformer/exp/train_no_dev_pytorch_train_pytorch_transformer_phn/results/model.last1.avg.best\\\" vocoder_path = \\\"downloads/jp/transformer/jsut.parallel_wavegan.v1/checkpoint-400000steps.pkl\\\" print(\\\"sucessfully finished download.\\\")\"]},\"224\":{\"h\":\"Setup\",\"t\":[\"# add path import sys sys.path.append(\\\"espnet\\\") # define device import torch device = torch.device(\\\"cuda\\\") # define E2E-TTS model from argparse import Namespace from espnet.asr.asr_utils import get_model_conf from espnet.asr.asr_utils import torch_load from espnet.utils.dynamic_import import dynamic_import idim, odim, train_args = get_model_conf(model_path) model_class = dynamic_import(train_args.model_module) model = model_class(idim, odim, train_args) torch_load(model_path, model) model = model.eval().to(device) inference_args = Namespace(**{\\\"threshold\\\": 0.5, \\\"minlenratio\\\": 0.0, \\\"maxlenratio\\\": 10.0}) # define neural vocoder from parallel_wavegan.utils import load_model fs = 24000 vocoder = load_model(vocoder_path) vocoder.remove_weight_norm() vocoder = vocoder.eval().to(device) # define text frontend import pyopenjtalk with open(dict_path) as f: lines = f.readlines() lines = [line.replace(\\\"\\\\n\\\", \\\"\\\").split(\\\" \\\") for line in lines] char_to_id = {c: int(i) for c, i in lines} def frontend(text): \\\"\\\"\\\"Clean text and then convert to id sequence.\\\"\\\"\\\" text = pyopenjtalk.g2p(text, kana=False) print(f\\\"Cleaned text: {text}\\\") charseq = text.split(\\\" \\\") idseq = [] for c in charseq: if c.isspace(): idseq += [char_to_id['&lt;space&gt;']] elif c not in char_to_id.keys(): idseq += [char_to_id['&lt;unk&gt;']] else: idseq += [char_to_id[c]] idseq += [idim - 1] # &lt;eos&gt; return torch.LongTensor(idseq).view(-1).to(device) frontend(\\\"初回の辞書のインストールが必要です\\\") print(\\\"Now ready to synthesize!\\\")\"]},\"225\":{\"h\":\"Synthesis\",\"t\":[\"import time print(\\\"日本語で好きな文章を入力してください\\\") input_text = input() with torch.no_grad(): start = time.time() x = frontend(input_text) c, _, _ = model.inference(x, inference_args) y = vocoder.inference(c) rtf = (time.time() - start) / (len(y) / fs) print(f\\\"RTF = {rtf:5f}\\\") from IPython.display import display, Audio display(Audio(y.view(-1).cpu().numpy(), rate=fs))\"]},\"226\":{\"h\":\"Mandarin demo\",\"t\":[\"IMPORTANT NOTE: The author cannot understand Mandarin. The text front-end part might have some bugs.\"]},\"227\":{\"h\":\"Install Mandarin dependencies\",\"t\":[\"!pip install pypinyin\"]},\"228\":{\"h\":\"Download pretrained models\",\"t\":[\"You can select Transformer or FastSpeech.\"]},\"229\":{\"h\":\"(a) Transformer\",\"t\":[\"# download pretrained models import os if not os.path.exists(\\\"downloads/zh/transformer\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy downloads/zh/transformer tar.gz !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1bTSygvonv5TS6-iuYsOIUWpN2atGnyhZ downloads/zh/transformer tar.gz # set path dict_path = \\\"downloads/zh/transformer/data/lang_phn/train_no_dev_units.txt\\\" model_path = \\\"downloads/zh/transformer/exp/train_no_dev_pytorch_train_pytorch_transformer.v1.single/results/model.last1.avg.best\\\" vocoder_path = \\\"downloads/zh/transformer/csmsc.parallel_wavegan.v1/checkpoint-400000steps.pkl\\\" print(\\\"sucessfully finished download.\\\")\"]},\"230\":{\"h\":\"(b) FastSpeech\",\"t\":[\"# download pretrained models import os if not os.path.exists(\\\"downloads/zh/fastspeech\\\"): !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy downloads/zh/fastspeech tar.gz !./espnet/utils/download_from_google_drive.sh \\\\ https://drive.google.com/open?id=1T8thxkAxjGFPXPWPTcKLvHnd6lG0-82R downloads/zh/fastspeech tar.gz # set path dict_path = \\\"downloads/zh/fastspeech/data/lang_phn/train_no_dev_units.txt\\\" model_path = \\\"downloads/zh/fastspeech/exp/train_no_dev_pytorch_train_fastspeech.v3.single/results/model.last1.avg.best\\\" vocoder_path = \\\"downloads/zh/fastspeech/csmsc.parallel_wavegan.v1/checkpoint-400000steps.pkl\\\" print(\\\"sucessfully finished download.\\\")\"]},\"231\":{\"h\":\"Setup\",\"t\":[\"# add path import sys sys.path.append(\\\"espnet\\\") # define device import torch device = torch.device(\\\"cuda\\\") # define E2E-TTS model from argparse import Namespace from espnet.asr.asr_utils import get_model_conf from espnet.asr.asr_utils import torch_load from espnet.utils.dynamic_import import dynamic_import idim, odim, train_args = get_model_conf(model_path) model_class = dynamic_import(train_args.model_module) model = model_class(idim, odim, train_args) torch_load(model_path, model) model = model.eval().to(device) inference_args = Namespace(**{\\\"threshold\\\": 0.5, \\\"minlenratio\\\": 0.0, \\\"maxlenratio\\\": 10.0}) # define neural vocoder from parallel_wavegan.utils import load_model fs = 24000 vocoder = load_model(vocoder_path) vocoder.remove_weight_norm() vocoder = vocoder.eval().to(device) # define text frontend from pypinyin import pinyin, Style from pypinyin.style._utils import get_initials, get_finals with open(dict_path) as f: lines = f.readlines() lines = [line.replace(\\\"\\\\n\\\", \\\"\\\").split(\\\" \\\") for line in lines] char_to_id = {c: int(i) for c, i in lines} def frontend(text): \\\"\\\"\\\"Clean text and then convert to id sequence.\\\"\\\"\\\" text = pinyin(text, style=Style.TONE3) text = [c[0] for c in text] print(f\\\"Cleaned text: {text}\\\") idseq = [] for x in text: c_init = get_initials(x, strict=True) c_final = get_finals(x, strict=True) for c in [c_init, c_final]: if len(c) == 0: continue c = c.replace(\\\"ü\\\", \\\"v\\\") c = c.replace(\\\"ui\\\", \\\"uei\\\") c = c.replace(\\\"un\\\", \\\"uen\\\") c = c.replace(\\\"iu\\\", \\\"iou\\\") # Special rule: \\\"e5n\\\" -> \\\"en5\\\" if \\\"5\\\" in c: c = c.replace(\\\"5\\\", \\\"\\\") + \\\"5\\\" if c not in char_to_id.keys(): print(f\\\"WARN: {c} is not included in dict.\\\") idseq += [char_to_id['&lt;unk&gt;']] else: idseq += [char_to_id[c]] idseq += [idim - 1] # &lt;eos&gt; return torch.LongTensor(idseq).view(-1).to(device) print(\\\"now ready to synthesize!\\\")\"]},\"232\":{\"h\":\"Synthesis\",\"t\":[\"import time print(\\\"請用中文輸入您喜歡的句子!\\\") input_text = input() with torch.no_grad(): start = time.time() x = frontend(input_text) c, _, _ = model.inference(x, inference_args) y = vocoder.inference(c) rtf = (time.time() - start) / (len(y) / fs) print(f\\\"RTF = {rtf:5f}\\\") from IPython.display import display, Audio display(Audio(y.view(-1).cpu().numpy(), rate=fs))\"]},\"233\":{\"h\":\"Text-to-Speech (Recipe)\",\"t\":[\"This is the example notebook of how-to-run the ESPnet TTS recipe using an4 dataset. You can understand the overview of TTS recipe through this notebook within an hour!\",\"See also:\",\"Documentaion: https://espnet.github.io/espnet\",\"Github: https://github.com/espnet\",\"Author: Tomoki Hayashi\",\"Last update: 2019/07/25\"]},\"234\":{\"h\":\"Setup envrionment\",\"t\":[\"First, let's setup the environmet to run the recipe. It take around 10 minues. Please keep waiting for a while.\",\"# OS setup !sudo apt-get install bc tree !cat /etc/os-release # espnet setup !git clone https://github.com/espnet/espnet !cd espnet; pip install -e . # warp ctc setup !git clone https://github.com/espnet/warp-ctc -b pytorch-1.1 !cd warp-ctc && mkdir build && cd build && cmake .. && make -j !cd warp-ctc/pytorch_binding && python setup.py install # kaldi setup !cd /content/espnet/tools; git clone https://github.com/kaldi-asr/kaldi !echo \\\"\\\" > ./espnet/tools/kaldi/tools/extras/check_dependencies.sh # ignore check !chmod +x ./espnet/tools/kaldi/tools/extras/check_dependencies.sh !cd ./espnet/tools/kaldi/tools; make sph2pipe sclite !rm -rf espnet/tools/kaldi/tools/python !wget https://18-198329952-gh.circle-artifacts.com/0/home/circleci/repo/ubuntu16-featbin.tar.gz !tar -xf ./ubuntu16-featbin.tar.gz # take a few minutes !cp featbin/* espnet/tools/kaldi/src/featbin/ # make dummy activate !mkdir -p espnet/tools/venv/bin !touch espnet/tools/venv/bin/activate\"]},\"235\":{\"h\":\"Run the recipe\",\"t\":[\"Now ready to run the recipe! We use the most simplest recipe egs/an4/tts1 as an example.\",\"Unfortunately, egs/an4/tts1 is too small to generate reasonable speech. But you can understand the flow or TTS recipe through this recipe since all of the TTS recipes has the exactly same flow.\",\"# Let's go to an4 recipe! import os os.chdir(\\\"/content/espnet/egs/an4/tts1\\\")\",\"Before running the recipe, let us check the recipe structure.\",\"!tree -L 1\",\"Each recipe has the same structure and files.\",\"run.sh: Main script of the recipe. Once you run this script, all of the processing will be conducted from data download, preparation, feature extraction, training, and decoding.\",\"cmd.sh: Command configuration source file about how-to-run each processing. You can modify this script if you want to run the script through job control system e.g. Slurm or Torque.\",\"path.sh: Path configuration source file. Basically, we do not have to touch.\",\"conf/: Directory containing configuration files.\",\"local/: Directory containing the recipe-specific scripts e.g. data preparation.\",\"steps/ and utils/: Directory containing kaldi tools.\",\"Main script run.sh consists of several stages:\",\"stage -1: Download data if the data is available online.\",\"stage 0: Prepare data to make kaldi-stype data directory.\",\"stage 1: Extract feature vector, calculate statistics, and perform normalization.\",\"stage 2: Prepare a dictionary and make json files for training.\",\"stage 3: Train the E2E-TTS network.\",\"stage 4: Decode mel-spectrogram using the trained network.\",\"stage 5: Generate a waveform from a generated mel-spectrogram using Griffin-Lim.\",\"Currently, we support the following networks:\",\"Tacotron2: Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions\",\"Transformer: Neural Speech Synthesis with Transformer Network\",\"FastSpeech: FastSpeech: Fast, Robust and Controllable Text to Speech\",\"Let us check each stage step-by-step via --stage and --stop_stage options!\"]},\"236\":{\"h\":\"Stage -1: Data download\",\"t\":[\"This stage downloads dataset if the dataset is available online.\",\"!./run.sh --stage -1 --stop_stage -1\",\"!tree -L 1 !ls downloads/\",\"You can see downloads directory is cretead, which containing donwloaded an4 dataset.\"]},\"237\":{\"h\":\"Stage 0: Data preparation\",\"t\":[\"This stage creates kaldi-style data directories.\",\"!./run.sh --stage 0 --stop_stage 0\",\"!tree -L 1 data\",\"Through the data preparation stage, kaldi-style data directories will be created. Here, data/train/ is corresponding to training set, and data/test is corresponding to evaluation set. Each directory has the same following files:\",\"!ls data/*\",\"The above four files are all we have to prepare to create new recipes. Let's check each file.\",\"!head -n 3 data/train/{wav.scp,text,utt2spk,spk2utt}\",\"Each file contains the following information:\",\"wav.scp: List of audio path. Each line has &lt;utt_id&gt; &lt;wavfile_path or command pipe&gt;. &lt;utt_id&gt; must be unique.\",\"text: List of transcriptions. Each line has &lt;utt_id&gt; &lt;transcription&gt;. In the case of TTS, we assume that &lt;transcription&gt; is cleaned.\",\"utt2spk: List of correspondence table between utterances and speakers. Each line has &lt;utt_id&gt; &lt;speaker_id&gt;.\",\"spk2utt: List of correspondence table between speakers and utterances. Each lien has &lt;speaker_id&gt; &lt;utt_id&gt; ... &lt;utt_id&gt; . This file can be automatically created from utt2spk.\",\"In the ESPnet, speaker information is not used for any processing. Therefore, utt2spk and spk2utt can be a dummy.\"]},\"238\":{\"h\":\"Stage 1: Feature extration\",\"t\":[\"This stage performs the following processing:\",\"Mel-spectrogram extraction\",\"Data split into training and validation set\",\"Statistics (mean and variance) calculation\",\"Normalization\",\"!./run.sh --stage 1 --stop_stage 1 --nj 4\",\"Raw filterbanks are saved in fbank/ directory with ark/scp format.\",\"!ls fbank\",\".ark is binary file and .scp contain the correspondence between &lt;utt_id&gt; and &lt;path_in_ark&gt;. Since feature extraction can be performed for split small sets in parallel, raw_fbank is split into raw_fbank_*.{1..N}.{scp,ark}.\",\"!head -n 3 fbank/raw_fbank_train.1.scp\",\"These files can be loaded in python via kaldiio as follows:\",\"import kaldiio import matplotlib.pyplot as plt # load scp file scp_dict = kaldiio.load_scp(\\\"fbank/raw_fbank_train.1.scp\\\") for key in scp_dict: plt.imshow(scp_dict[key].T[::-1]) plt.title(key) plt.colorbar() plt.show() break # load ark file ark_generator = kaldiio.load_ark(\\\"fbank/raw_fbank_train.1.ark\\\") for key, array in ark_generator: plt.imshow(array.T[::-1]) plt.title(key) plt.colorbar() plt.show() break\",\"After raw mel-spectrogram extraction, some files are added in data/train/.feats.scp is concatenated scp file of fbank/raw_fbank_train.{1..N}.scp.utt2num_frames has the number of feature frames of each &lt;utt_id&gt;.\",\"!ls data/train !head -n 3 data/train/{feats.scp,utt2num_frames}\",\"And data/train/ directory is split into two directory:\",\"data/train_nodev/: data directory for training\",\"data/train_dev/: data directory for validation\",\"!ls data !ls data/train_*\",\"You can find cmvn.ark in data/train_nodev, which is the calculated statistics file. This file also can be loaded in python via kaldiio.\",\"# load cmvn.ark file (Be careful not load_ark, but load_mat) cmvn = kaldiio.load_mat(\\\"data/train_nodev/cmvn.ark\\\") # cmvn consists of mean and variance, the last dimension of mean represents the number of frames. print(\\\"cmvn shape = \\\"+ str(cmvn.shape)) # calculate mean and variance mu = cmvn[0, :-1] / cmvn[0, -1] var = cmvn[1, :-1] / cmvn[0, -1] # show mean print(\\\"mean = \\\" + str(mu)) print(\\\"variance = \\\" + str(var))\",\"Normalzed features for training, validation and evaluation set are dumped in dump/{train_nodev,train_dev,test}/. There ark and scp can be loaded as the same as the above procedure.\",\"!ls dump/*\"]},\"239\":{\"h\":\"Stage 2: Dictionary and json preparation\",\"t\":[\"This stage creates dictrionary from data/train_nodev/text and makes json file for training.\",\"!./run.sh --stage 2 --stop_stage 2\",\"Dictrionary file will be created in data/lang_1char/. Dictionary file consists of &lt;token&gt;&lt;token index&gt;. Here, &lt;token index&gt; starts from 1 because 0 is used as padding index.\",\"!ls data !cat data/lang_1char/train_nodev_units.txt\",\"Json file will be created for training / validation /evaludation sets and they are saved as dump/{train_nodev,train_dev,test}/data.json.\",\"!ls dump/*/*.json\",\"Each json file contains all of the information in the data directory.\",\"!head -n 27 dump/train_nodev/data.json\",\"\\\"shape\\\": Shape of the input or output sequence. Here input shape [63, 80] represents the number of frames = 63 and the dimension of mel-spectrogram = 80.\",\"\\\"text\\\": Original transcription.\",\"\\\"token\\\": Token sequence of original transcription.\",\"\\\"tokenid\\\" Token id sequence of original transcription, which is converted using the dictionary.\",\"Now ready to start training!\"]},\"240\":{\"h\":\"Stage 3: Network training\",\"t\":[\"This stage performs training of the network. Network training configurations are written as .yaml format file. Let us check the default cofiguration conf/train_pytroch_tacotron2.yaml.\",\"!cat conf/train_pytorch_tacotron2.yaml\",\"You can modify this configuration file to change the hyperparameters. Here, let's change the number of epochs for this demonstration.\",\"# TODO(kan-bayashi): Change here to use change_yaml.py !cat conf/train_pytorch_tacotron2.yaml | sed -e \\\"s/epochs: 50/epochs: 3/g\\\" > conf/train_pytorch_tacotron2_sample.yaml !cat conf/train_pytorch_tacotron2_sample.yaml\",\"Let's train the network. You can specify the config file via --train_config option. It takes several minutes.\",\"!./run.sh --stage 3 --stop_stage 3 --train_config conf/train_pytorch_tacotron2_sample.yaml --verbose 1\",\"You can see the training log in exp/train_*/train.log.\",\"The models are saved in exp/train_*/results/ directory.\",\"!ls exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/{results,results/att_ws}\",\"exp/train_*/results/*.png are the figures of training curve.\",\"from IPython.display import Image, display_png print(\\\"all loss curve\\\") display_png(Image(\\\"exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/results/all_loss.png\\\")) print(\\\"l1 loss curve\\\") display_png(Image(\\\"exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/results/l1_loss.png\\\")) print(\\\"mse loss curve\\\") display_png(Image(\\\"exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/results/mse_loss.png\\\")) print(\\\"bce loss curve\\\") display_png(Image(\\\"exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/results/bce_loss.png\\\"))\",\"exp/train_*/results/att_ws/.png are the figures of attention weights in each epoch.\",\"print(\\\"Attention weights of initial epoch\\\") display_png(Image(\\\"exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/results/att_ws/fash-cen1-b.ep.1.png\\\"))\",\"exp/train_*/results/model.loss.best contains only the model parameters. On the other hand, exp/train_*/results/snapshot contains the model parameters, optimizer states, and iterator states. So you can restart from the training by specifying the snapshot file with --resume option.\",\"# resume training from snapshot.ep.2 !./run.sh --stage 3 --stop_stage 3 --train_config conf/train_pytorch_tacotron2_sample.yaml --resume exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/results/snapshot.ep.2 --verbose 1\",\"!cat exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/train.log\",\"Also, we support tensorboard. You can see the training log through tensorboard.\",\"%load_ext tensorboard %tensorboard --logdir tensorboard/train_nodev_pytorch_train_pytorch_tacotron2_sample/\"]},\"241\":{\"h\":\"Stage 4: Network decoding\",\"t\":[\"This stage performs decoding using the trained model to generate mel-spectrogram from a given text.\",\"!./run.sh --stage 4 --stop_stage 4 --nj 8 --train_config conf/train_pytorch_tacotron2_sample.yaml\",\"Generated features are saved as ark/scp format.\",\"!ls exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/outputs_model.loss.best_decode/*\",\"We can specify the model or snapshot to be used for decoding via --model.\",\"!./run.sh --stage 4 --stop_stage 4 --nj 8 --train_config conf/train_pytorch_tacotron2_sample.yaml --model snapshot.ep.2\",\"!ls exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/outputs_snapshot.ep.2_decode/*\"]},\"242\":{\"h\":\"Stage 5: Waveform synthesis\",\"t\":[\"Finally, in this stage, we generate waveform using Grrifin-Lim algorithm. First, we perform de-normalization to convert the generated mel-spectrogram into the original scale. Then we apply Grrifin-Lim algorithm to restore phase components and apply inverse STFT to generate waveforms.\",\"!./run.sh --stage 5 --stop_stage 5 --nj 8 --train_config conf/train_pytorch_tacotron2_sample.yaml --griffin_lim_iters 50\",\"Generated wav files are saved in exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/outputs_model.loss.best_decode_denorm/*/wav\",\"!ls exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/outputs_model.loss.best_decode_denorm/*/wav\",\"!tree -L 3\"]},\"243\":{\"h\":\"NEXT step\",\"t\":[\"Try pretrained model to generate speech.\",\"Try a large single speaker dataset recipe egs/ljspeech/tts1.\",\"Try a large multi-speaker recipe egs/libritts/tts1.\",\"Make the original recipe using your own dataset.\"]},\"244\":{\"h\":\"ESPnet2 Demo\",\"t\":[\"general_tutorial.md\",\"assignment5_st.md\",\"s2st_demo.md\",\"assignment7_se.md\",\"assignment4_ssl.md\",\"assignment3_spk.md\",\"assignment6_slu.md\",\"assignment8_tts.md\",\"assignment0_data-prep.md\",\"assignment1_espnet-tutorial.md\",\"recipe_tutorial.md\",\"new_task_tutorial.md\",\"README.md\",\"streaming_asr_demo.md\",\"asr_realtime_demo.md\",\"asr_transfer_learning_demo.md\",\"onnx_conversion_demo.md\",\"2pass_slu_demo.md\",\"tts_realtime_demo.md\",\"se_demo_for_waspaa_2021.md\",\"se_demo.md\"]},\"245\":{\"h\":\"asr_align.py\",\"t\":[\"<!-- _asr_align.py -->\",\"Align text to audio using CTC segmentation.using a pre-trained speech recognition model.\",\"usage: asr_align.py [-h] [--config CONFIG] [--ngpu NGPU] [--dtype {float16,float32,float64}] [--backend {pytorch}] [--debugmode DEBUGMODE] [--verbose VERBOSE] [--preprocess-conf PREPROCESS_CONF] [--data-json DATA_JSON] [--utt-text UTT_TEXT] --model MODEL [--model-conf MODEL_CONF] [--num-encs NUM_ENCS] [--subsampling-factor SUBSAMPLING_FACTOR] [--frame-duration FRAME_DURATION] [--min-window-size MIN_WINDOW_SIZE] [--max-window-size MAX_WINDOW_SIZE] [--use-dict-blank USE_DICT_BLANK] [--set-blank SET_BLANK] [--gratis-blank GRATIS_BLANK] [--replace-spaces-with-blanks REPLACE_SPACES_WITH_BLANKS] [--scoring-length SCORING_LENGTH] --output OUTPUT\"]},\"246\":{\"h\":\"Named Arguments\"},\"247\":{\"h\":\"asr_enhance.py\",\"t\":[\"<!-- _asr_enhance.py -->\",\"Enhance noisy speech for speech recognition\",\"usage: asr_enhance.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--backend {chainer,pytorch}] [--debugmode DEBUGMODE] [--seed SEED] [--verbose VERBOSE] [--batchsize BATCHSIZE] [--preprocess-conf PREPROCESS_CONF] [--recog-json RECOG_JSON] --model MODEL [--model-conf MODEL_CONF] [--enh-wspecifier ENH_WSPECIFIER] [--enh-filetype {mat,hdf5,sound.hdf5,sound}] [--fs FS] [--keep-length KEEP_LENGTH] [--image-dir IMAGE_DIR] [--num-images NUM_IMAGES] [--apply-istft APPLY_ISTFT] [--istft-win-length ISTFT_WIN_LENGTH] [--istft-n-shift ISTFT_N_SHIFT] [--istft-window ISTFT_WINDOW]\"]},\"248\":{\"h\":\"Named Arguments\"},\"249\":{\"h\":\"asr_recog.py\",\"t\":[\"<!-- _asr_recog.py -->\",\"Transcribe text from speech using a speech recognition model on one CPU or GPU\",\"usage: asr_recog.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--dtype {float16,float32,float64}] [--backend {chainer,pytorch}] [--debugmode DEBUGMODE] [--seed SEED] [--verbose VERBOSE] [--batchsize BATCHSIZE] [--preprocess-conf PREPROCESS_CONF] [--api {v1,v2}] [--recog-json RECOG_JSON] --result-label RESULT_LABEL --model MODEL [--model-conf MODEL_CONF] [--num-spkrs {1,2}] [--num-encs NUM_ENCS] [--nbest NBEST] [--beam-size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc-weight CTC_WEIGHT] [--weights-ctc-dec WEIGHTS_CTC_DEC] [--ctc-window-margin CTC_WINDOW_MARGIN] [--search-type {default,nsc,tsd,alsd,maes}] [--nstep NSTEP] [--prefix-alpha PREFIX_ALPHA] [--max-sym-exp MAX_SYM_EXP] [--u-max U_MAX] [--expansion-gamma EXPANSION_GAMMA] [--expansion-beta EXPANSION_BETA] [--score-norm [SCORE_NORM]] [--softmax-temperature SOFTMAX_TEMPERATURE] [--rnnlm RNNLM] [--rnnlm-conf RNNLM_CONF] [--word-rnnlm WORD_RNNLM] [--word-rnnlm-conf WORD_RNNLM_CONF] [--word-dict WORD_DICT] [--lm-weight LM_WEIGHT] [--ngram-model NGRAM_MODEL] [--ngram-weight NGRAM_WEIGHT] [--ngram-scorer {full,part}] [--streaming-mode {window,segment}] [--streaming-window STREAMING_WINDOW] [--streaming-min-blank-dur STREAMING_MIN_BLANK_DUR] [--streaming-onset-margin STREAMING_ONSET_MARGIN] [--streaming-offset-margin STREAMING_OFFSET_MARGIN] [--maskctc-n-iterations MASKCTC_N_ITERATIONS] [--maskctc-probability-threshold MASKCTC_PROBABILITY_THRESHOLD] [--quantize-config [QUANTIZE_CONFIG [QUANTIZE_CONFIG ...]]] [--quantize-dtype {float16,qint8}] [--quantize-asr-model QUANTIZE_ASR_MODEL] [--quantize-lm-model QUANTIZE_LM_MODEL]\"]},\"250\":{\"h\":\"Named Arguments\"},\"251\":{\"h\":\"asr_train.py\",\"t\":[\"<!-- _asr_train.py -->\",\"Train an automatic speech recognition (ASR) model on one CPU, one or multiple GPUs\",\"usage: asr_train.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--use-ddp] [--train-dtype {float16,float32,float64,O0,O1,O2,O3}] [--backend {chainer,pytorch}] --outdir OUTDIR [--debugmode DEBUGMODE] --dict DICT [--seed SEED] [--debugdir DEBUGDIR] [--resume [RESUME]] [--minibatches MINIBATCHES] [--verbose VERBOSE] [--tensorboard-dir [TENSORBOARD_DIR]] [--report-interval-iters REPORT_INTERVAL_ITERS] [--save-interval-iters SAVE_INTERVAL_ITERS] [--train-json TRAIN_JSON] [--valid-json VALID_JSON] [--model-module MODEL_MODULE] [--num-encs NUM_ENCS] [--ctc_type {builtin,gtnctc,cudnnctc}] [--mtlalpha MTLALPHA] [--lsm-weight LSM_WEIGHT] [--report-cer] [--report-wer] [--nbest NBEST] [--beam-size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc-weight CTC_WEIGHT] [--rnnlm RNNLM] [--rnnlm-conf RNNLM_CONF] [--lm-weight LM_WEIGHT] [--sym-space SYM_SPACE] [--sym-blank SYM_BLANK] [--sortagrad [SORTAGRAD]] [--batch-count {auto,seq,bin,frame}] [--batch-size BATCH_SIZE] [--batch-bins BATCH_BINS] [--batch-frames-in BATCH_FRAMES_IN] [--batch-frames-out BATCH_FRAMES_OUT] [--batch-frames-inout BATCH_FRAMES_INOUT] [--maxlen-in ML] [--maxlen-out ML] [--n-iter-processes N_ITER_PROCESSES] [--preprocess-conf [PREPROCESS_CONF]] [--opt {adadelta,adam,noam}] [--accum-grad ACCUM_GRAD] [--eps EPS] [--eps-decay EPS_DECAY] [--weight-decay WEIGHT_DECAY] [--criterion {loss,loss_eps_decay_only,acc}] [--threshold THRESHOLD] [--epochs EPOCHS] [--early-stop-criterion [EARLY_STOP_CRITERION]] [--patience [PATIENCE]] [--grad-clip GRAD_CLIP] [--num-save-attention NUM_SAVE_ATTENTION] [--num-save-ctc NUM_SAVE_CTC] [--grad-noise GRAD_NOISE] [--num-spkrs {1,2}] [--context-residual [CONTEXT_RESIDUAL]] [--enc-init ENC_INIT] [--enc-init-mods ENC_INIT_MODS] [--dec-init DEC_INIT] [--dec-init-mods DEC_INIT_MODS] [--freeze-mods FREEZE_MODS] [--use-frontend USE_FRONTEND] [--use-wpe USE_WPE] [--wtype {lstm,blstm,lstmp,blstmp,vgglstmp,vggblstmp,vgglstm,vggblstm,gru,bgru,grup,bgrup,vgggrup,vggbgrup,vgggru,vggbgru}] [--wlayers WLAYERS] [--wunits WUNITS] [--wprojs WPROJS] [--wdropout-rate WDROPOUT_RATE] [--wpe-taps WPE_TAPS] [--wpe-delay WPE_DELAY] [--use-dnn-mask-for-wpe USE_DNN_MASK_FOR_WPE] [--use-beamformer USE_BEAMFORMER] [--btype {lstm,blstm,lstmp,blstmp,vgglstmp,vggblstmp,vgglstm,vggblstm,gru,bgru,grup,bgrup,vgggrup,vggbgrup,vgggru,vggbgru}] [--blayers BLAYERS] [--bunits BUNITS] [--bprojs BPROJS] [--badim BADIM] [--bnmask BNMASK] [--ref-channel REF_CHANNEL] [--bdropout-rate BDROPOUT_RATE] [--stats-file STATS_FILE] [--apply-uttmvn APPLY_UTTMVN] [--uttmvn-norm-means UTTMVN_NORM_MEANS] [--uttmvn-norm-vars UTTMVN_NORM_VARS] [--fbank-fs FBANK_FS] [--n-mels N_MELS] [--fbank-fmin FBANK_FMIN] [--fbank-fmax FBANK_FMAX]\"]},\"252\":{\"h\":\"Named Arguments\"},\"253\":{\"h\":\"lm_train.py\",\"t\":[\"<!-- _lm_train.py -->\",\"Train a new language model on one CPU or one GPU\",\"usage: lm_train.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--train-dtype {float16,float32,float64,O0,O1,O2,O3}] [--backend {chainer,pytorch}] --outdir OUTDIR [--debugmode DEBUGMODE] --dict DICT [--seed SEED] [--resume [RESUME]] [--verbose VERBOSE] [--tensorboard-dir [TENSORBOARD_DIR]] [--report-interval-iters REPORT_INTERVAL_ITERS] --train-label TRAIN_LABEL --valid-label VALID_LABEL [--test-label TEST_LABEL] [--dump-hdf5-path DUMP_HDF5_PATH] [--opt OPT] [--sortagrad [SORTAGRAD]] [--batchsize BATCHSIZE] [--accum-grad ACCUM_GRAD] [--epoch EPOCH] [--early-stop-criterion [EARLY_STOP_CRITERION]] [--patience [PATIENCE]] [--schedulers SCHEDULERS] [--gradclip GRADCLIP] [--maxlen MAXLEN] [--model-module MODEL_MODULE]\"]},\"254\":{\"h\":\"Named Arguments\"},\"255\":{\"h\":\"mt_train.py\",\"t\":[\"<!-- _mt_train.py -->\",\"Train a neural machine translation (NMT) model on one CPU, one or multiple GPUs\",\"usage: mt_train.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--train-dtype {float16,float32,float64,O0,O1,O2,O3}] [--backend {chainer,pytorch}] --outdir OUTDIR [--debugmode DEBUGMODE] --dict DICT [--seed SEED] [--debugdir DEBUGDIR] [--resume [RESUME]] [--minibatches MINIBATCHES] [--verbose VERBOSE] [--tensorboard-dir [TENSORBOARD_DIR]] [--report-interval-iters REPORT_INTERVAL_ITERS] [--save-interval-iters SAVE_INTERVAL_ITERS] [--train-json TRAIN_JSON] [--valid-json VALID_JSON] [--model-module MODEL_MODULE] [--lsm-weight LSM_WEIGHT] [--report-bleu] [--nbest NBEST] [--beam-size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--rnnlm RNNLM] [--rnnlm-conf RNNLM_CONF] [--lm-weight LM_WEIGHT] [--sym-space SYM_SPACE] [--sym-blank SYM_BLANK] [--sortagrad [SORTAGRAD]] [--batch-count {auto,seq,bin,frame}] [--batch-size BATCH_SIZE] [--batch-bins BATCH_BINS] [--batch-frames-in BATCH_FRAMES_IN] [--batch-frames-out BATCH_FRAMES_OUT] [--batch-frames-inout BATCH_FRAMES_INOUT] [--maxlen-in ML] [--maxlen-out ML] [--n-iter-processes N_ITER_PROCESSES] [--opt {adadelta,adam,noam}] [--accum-grad ACCUM_GRAD] [--eps EPS] [--eps-decay EPS_DECAY] [--lr LR] [--lr-decay LR_DECAY] [--weight-decay WEIGHT_DECAY] [--criterion {loss,acc}] [--threshold THRESHOLD] [--epochs EPOCHS] [--early-stop-criterion [EARLY_STOP_CRITERION]] [--patience [PATIENCE]] [--grad-clip GRAD_CLIP] [--num-save-attention NUM_SAVE_ATTENTION] [--context-residual [CONTEXT_RESIDUAL]] [--tie-src-tgt-embedding [TIE_SRC_TGT_EMBEDDING]] [--tie-classifier [TIE_CLASSIFIER]] [--enc-init [ENC_INIT]] [--enc-init-mods ENC_INIT_MODS] [--dec-init [DEC_INIT]] [--dec-init-mods DEC_INIT_MODS] [--multilingual MULTILINGUAL] [--replace-sos REPLACE_SOS]\"]},\"256\":{\"h\":\"Named Arguments\"},\"257\":{\"h\":\"mt_trans.py\",\"t\":[\"<!-- _mt_trans.py -->\",\"Translate text from speech using a speech translation model on one CPU or GPU\",\"usage: mt_trans.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--dtype {float16,float32,float64}] [--backend {chainer,pytorch}] [--debugmode DEBUGMODE] [--seed SEED] [--verbose VERBOSE] [--batchsize BATCHSIZE] [--preprocess-conf PREPROCESS_CONF] [--api {v1,v2}] [--trans-json TRANS_JSON] --result-label RESULT_LABEL --model MODEL [--model-conf MODEL_CONF] [--nbest NBEST] [--beam-size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--tgt-lang TGT_LANG]\"]},\"258\":{\"h\":\"Named Arguments\"},\"259\":{\"h\":\"st_train.py\",\"t\":[\"<!-- _st_train.py -->\",\"Train a speech translation (ST) model on one CPU, one or multiple GPUs\",\"usage: st_train.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--train-dtype {float16,float32,float64,O0,O1,O2,O3}] [--backend {chainer,pytorch}] --outdir OUTDIR [--debugmode DEBUGMODE] --dict DICT [--seed SEED] [--debugdir DEBUGDIR] [--resume [RESUME]] [--minibatches MINIBATCHES] [--verbose VERBOSE] [--tensorboard-dir [TENSORBOARD_DIR]] [--report-interval-iters REPORT_INTERVAL_ITERS] [--save-interval-iters SAVE_INTERVAL_ITERS] [--train-json TRAIN_JSON] [--valid-json VALID_JSON] [--model-module MODEL_MODULE] [--ctc_type {builtin,gtnctc,cudnnctc}] [--mtlalpha MTLALPHA] [--asr-weight ASR_WEIGHT] [--mt-weight MT_WEIGHT] [--lsm-weight LSM_WEIGHT] [--report-cer] [--report-wer] [--report-bleu] [--nbest NBEST] [--beam-size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--rnnlm RNNLM] [--rnnlm-conf RNNLM_CONF] [--lm-weight LM_WEIGHT] [--sym-space SYM_SPACE] [--sym-blank SYM_BLANK] [--sortagrad [SORTAGRAD]] [--batch-count {auto,seq,bin,frame}] [--batch-size BATCH_SIZE] [--batch-bins BATCH_BINS] [--batch-frames-in BATCH_FRAMES_IN] [--batch-frames-out BATCH_FRAMES_OUT] [--batch-frames-inout BATCH_FRAMES_INOUT] [--maxlen-in ML] [--maxlen-out ML] [--n-iter-processes N_ITER_PROCESSES] [--preprocess-conf [PREPROCESS_CONF]] [--opt {adadelta,adam,noam}] [--accum-grad ACCUM_GRAD] [--eps EPS] [--eps-decay EPS_DECAY] [--lr LR] [--lr-decay LR_DECAY] [--weight-decay WEIGHT_DECAY] [--criterion {loss,acc}] [--threshold THRESHOLD] [--epochs EPOCHS] [--early-stop-criterion [EARLY_STOP_CRITERION]] [--patience [PATIENCE]] [--grad-clip GRAD_CLIP] [--num-save-attention NUM_SAVE_ATTENTION] [--num-save-ctc NUM_SAVE_CTC] [--grad-noise GRAD_NOISE] [--context-residual [CONTEXT_RESIDUAL]] [--enc-init [ENC_INIT]] [--enc-init-mods ENC_INIT_MODS] [--dec-init [DEC_INIT]] [--dec-init-mods DEC_INIT_MODS] [--multilingual MULTILINGUAL] [--replace-sos REPLACE_SOS] [--stats-file STATS_FILE] [--apply-uttmvn APPLY_UTTMVN] [--uttmvn-norm-means UTTMVN_NORM_MEANS] [--uttmvn-norm-vars UTTMVN_NORM_VARS] [--fbank-fs FBANK_FS] [--n-mels N_MELS] [--fbank-fmin FBANK_FMIN] [--fbank-fmax FBANK_FMAX]\"]},\"260\":{\"h\":\"Named Arguments\"},\"261\":{\"h\":\"st_trans.py\",\"t\":[\"<!-- _st_trans.py -->\",\"Translate text from speech using a speech translation model on one CPU or GPU\",\"usage: st_trans.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--dtype {float16,float32,float64}] [--backend {chainer,pytorch}] [--debugmode DEBUGMODE] [--seed SEED] [--verbose VERBOSE] [--batchsize BATCHSIZE] [--preprocess-conf PREPROCESS_CONF] [--api {v1,v2}] [--trans-json TRANS_JSON] --result-label RESULT_LABEL --model MODEL [--nbest NBEST] [--beam-size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--tgt-lang TGT_LANG]\"]},\"262\":{\"h\":\"Named Arguments\"},\"263\":{\"h\":\"tts_decode.py\",\"t\":[\"<!-- _tts_decode.py -->\",\"Synthesize speech from text using a TTS model on one CPU\",\"usage: tts_decode.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--backend {chainer,pytorch}] [--debugmode DEBUGMODE] [--seed SEED] --out OUT [--verbose VERBOSE] [--preprocess-conf PREPROCESS_CONF] --json JSON --model MODEL [--model-conf MODEL_CONF] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--threshold THRESHOLD] [--use-att-constraint USE_ATT_CONSTRAINT] [--backward-window BACKWARD_WINDOW] [--forward-window FORWARD_WINDOW] [--fastspeech-alpha FASTSPEECH_ALPHA] [--save-durations SAVE_DURATIONS] [--save-focus-rates SAVE_FOCUS_RATES]\"]},\"264\":{\"h\":\"Named Arguments\"},\"265\":{\"h\":\"tts_train.py\",\"t\":[\"<!-- _tts_train.py -->\",\"Train a new text-to-speech (TTS) model on one CPU, one or multiple GPUs\",\"usage: tts_train.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--backend {chainer,pytorch}] --outdir OUTDIR [--debugmode DEBUGMODE] [--seed SEED] [--resume [RESUME]] [--minibatches MINIBATCHES] [--verbose VERBOSE] [--tensorboard-dir [TENSORBOARD_DIR]] [--eval-interval-epochs EVAL_INTERVAL_EPOCHS] [--save-interval-epochs SAVE_INTERVAL_EPOCHS] [--report-interval-iters REPORT_INTERVAL_ITERS] --train-json TRAIN_JSON --valid-json VALID_JSON [--model-module MODEL_MODULE] [--sortagrad [SORTAGRAD]] [--batch-sort-key [{shuffle,output,input}]] [--batch-count {auto,seq,bin,frame}] [--batch-size BATCH_SIZE] [--batch-bins BATCH_BINS] [--batch-frames-in BATCH_FRAMES_IN] [--batch-frames-out BATCH_FRAMES_OUT] [--batch-frames-inout BATCH_FRAMES_INOUT] [--maxlen-in ML] [--maxlen-out ML] [--num-iter-processes NUM_ITER_PROCESSES] [--preprocess-conf PREPROCESS_CONF] [--use-speaker-embedding USE_SPEAKER_EMBEDDING] [--use-second-target USE_SECOND_TARGET] [--opt {adam,noam}] [--accum-grad ACCUM_GRAD] [--lr LR] [--eps EPS] [--weight-decay WEIGHT_DECAY] [--epochs EPOCHS] [--early-stop-criterion [EARLY_STOP_CRITERION]] [--patience [PATIENCE]] [--grad-clip GRAD_CLIP] [--num-save-attention NUM_SAVE_ATTENTION] [--keep-all-data-on-mem KEEP_ALL_DATA_ON_MEM] [--enc-init ENC_INIT] [--enc-init-mods ENC_INIT_MODS] [--dec-init DEC_INIT] [--dec-init-mods DEC_INIT_MODS] [--freeze-mods FREEZE_MODS]\"]},\"266\":{\"h\":\"Named Arguments\"},\"267\":{\"h\":\"vc_decode.py\",\"t\":[\"<!-- _vc_decode.py -->\",\"Converting speech using a VC model on one CPU\",\"usage: vc_decode.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--backend {chainer,pytorch}] [--debugmode DEBUGMODE] [--seed SEED] --out OUT [--verbose VERBOSE] [--preprocess-conf PREPROCESS_CONF] --json JSON --model MODEL [--model-conf MODEL_CONF] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--threshold THRESHOLD] [--use-att-constraint USE_ATT_CONSTRAINT] [--backward-window BACKWARD_WINDOW] [--forward-window FORWARD_WINDOW] [--save-durations SAVE_DURATIONS] [--save-focus-rates SAVE_FOCUS_RATES]\"]},\"268\":{\"h\":\"Named Arguments\"},\"269\":{\"h\":\"vc_train.py\",\"t\":[\"<!-- _vc_train.py -->\",\"Train a new voice conversion (VC) model on one CPU, one or multiple GPUs\",\"usage: vc_train.py [-h] [--config CONFIG] [--config2 CONFIG2] [--config3 CONFIG3] [--ngpu NGPU] [--backend {chainer,pytorch}] --outdir OUTDIR [--debugmode DEBUGMODE] [--seed SEED] [--resume [RESUME]] [--minibatches MINIBATCHES] [--verbose VERBOSE] [--tensorboard-dir [TENSORBOARD_DIR]] [--eval-interval-epochs EVAL_INTERVAL_EPOCHS] [--save-interval-epochs SAVE_INTERVAL_EPOCHS] [--report-interval-iters REPORT_INTERVAL_ITERS] [--srcspk SRCSPK] [--trgspk TRGSPK] --train-json TRAIN_JSON --valid-json VALID_JSON [--model-module MODEL_MODULE] [--sortagrad [SORTAGRAD]] [--batch-sort-key [{shuffle,output,input}]] [--batch-count {auto,seq,bin,frame}] [--batch-size BATCH_SIZE] [--batch-bins BATCH_BINS] [--batch-frames-in BATCH_FRAMES_IN] [--batch-frames-out BATCH_FRAMES_OUT] [--batch-frames-inout BATCH_FRAMES_INOUT] [--maxlen-in ML] [--maxlen-out ML] [--num-iter-processes NUM_ITER_PROCESSES] [--preprocess-conf PREPROCESS_CONF] [--use-speaker-embedding USE_SPEAKER_EMBEDDING] [--use-second-target USE_SECOND_TARGET] [--opt {adam,noam,lamb}] [--accum-grad ACCUM_GRAD] [--lr LR] [--eps EPS] [--weight-decay WEIGHT_DECAY] [--epochs EPOCHS] [--early-stop-criterion [EARLY_STOP_CRITERION]] [--patience [PATIENCE]] [--grad-clip GRAD_CLIP] [--num-save-attention NUM_SAVE_ATTENTION] [--keep-all-data-on-mem KEEP_ALL_DATA_ON_MEM] [--enc-init ENC_INIT] [--enc-init-mods ENC_INIT_MODS] [--dec-init DEC_INIT] [--dec-init-mods DEC_INIT_MODS] [--freeze-mods FREEZE_MODS]\"]},\"270\":{\"h\":\"Named Arguments\"},\"271\":{\"h\":\"spm_decode\",\"t\":[\"usage: spm_decode [-h] --model MODEL [--input INPUT] [--input_format {piece,id}] optional arguments: --model MODEL sentencepiece model to use for decoding --input INPUT input file to decode --input_format {piece,id}\"]},\"272\":{\"h\":\"spm_encode\",\"t\":[\"usage: spm_encode [-h] --model MODEL [--inputs INPUTS [INPUTS ...]] [--outputs OUTPUTS [OUTPUTS ...]] [--output_format {piece,id}] [--min-len N] [--max-len N] optional arguments: --model MODEL sentencepiece model to use for encoding --inputs INPUTS [INPUTS ...] input files to filter/encode --outputs OUTPUTS [OUTPUTS ...] path to save encoded outputs --output_format {piece,id} --min-len N filter sentence pairs with fewer than N tokens --max-len N filter sentence pairs with more than N tokens\"]},\"273\":{\"h\":\"asr_align_wav.sh\",\"t\":[\"No help found.\"]},\"274\":{\"h\":\"clean_corpus.sh\",\"t\":[\"Usage: clean_corpus.sh [options] &lt;data-dir&gt; &lt;langs&gt; e.g.: clean_corpus.sh data/train \\\"en de\\\" Options: --maxframes # number of maximum input frame length --maxchars # number of maximum character length --utt_extra_files # extra text files for target sequence --no_feat # set to True for MT recipe\"]},\"275\":{\"h\":\"convert_fbank.sh\",\"t\":[\"Usage: convert_fbank.sh [options] &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;fbank-dir&gt;] ] e.g.: convert_fbank.sh data/train exp/griffin_lim/train wav Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;fbank-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --fs &lt;fs&gt; # sampling rate --fmax &lt;fmax&gt; # maximum frequency --fmin &lt;fmin&gt; # minimum frequency --n_fft &lt;n_fft&gt; # number of FFT points (default=1024) --n_shift &lt;n_shift&gt; # shift size in point (default=256) --win_length &lt;win_length&gt; # window length in point (default=) --n_mels &lt;n_mels&gt; # number of mel basis (default=80) --iters &lt;iters&gt; # number of Griffin-lim iterations (default=64) --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs.\"]},\"276\":{\"h\":\"data2json.sh\",\"t\":[\"Usage: data2json.sh &lt;data-dir&gt; &lt;dict&gt; e.g. data2json.sh data/train data/lang_1char/train_units.txt Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs. --feat &lt;feat-scp&gt; # feat.scp or feat1.scp,feat2.scp,... --oov &lt;oov-word&gt; # Default: &lt;unk&gt; --out &lt;outputfile&gt; # If omitted, write in stdout --filetype &lt;mat|hdf5|sound.hdf5&gt; # Specify the format of feats file --preprocess-conf &lt;json&gt; # Apply preprocess to feats when creating shape.scp --verbose &lt;num&gt; # Default: 0\"]},\"277\":{\"h\":\"download_from_google_drive.sh\",\"t\":[\"Usage: download_from_google_drive.sh &lt;share-url&gt; [&lt;download_dir&gt; &lt;file_ext&gt;] e.g.: download_from_google_drive.sh https://drive.google.com/open?id=1zF88bRNbJhw9hNBq3NrDg8vnGGibREmg downloads zip Options: &lt;download_dir&gt;: directory to save downloaded file. (Default=downloads) &lt;file_ext&gt;: file extension of the file to be downloaded. (Default=zip)\"]},\"278\":{\"h\":\"dump.sh\",\"t\":[\"Usage: dump.sh &lt;scp&gt; &lt;cmvnark&gt; &lt;logdir&gt; &lt;dumpdir&gt;\"]},\"279\":{\"h\":\"dump_pcm.sh\",\"t\":[\"Usage: dump_pcm.sh [options] &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;pcm-dir&gt;] ] e.g.: dump_pcm.sh data/train exp/dump_pcm/train pcm Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;pcm-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs. --write-utt2num-frames &lt;true|false&gt; # If true, write utt2num_frames file. --filetype &lt;mat|hdf5|sound.hdf5&gt; # Specify the format of feats file\"]},\"280\":{\"h\":\"eval_source_separation.sh\",\"t\":[\"Usage: eval_source_separation.sh reffiles enffiles &lt;dir&gt; e.g. eval_source_separation.sh reference.scp enhanced.scp outdir And also supporting multiple sources: e.g. eval_source_separation.sh \\\"ref1.scp,ref2.scp\\\" \\\"enh1.scp,enh2.scp\\\" outdir Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs.\"]},\"281\":{\"h\":\"feat_to_shape.sh\",\"t\":[\"Usage: feat_to_shape.sh [options] &lt;input-scp&gt; &lt;output-scp&gt; [&lt;log-dir&gt;] e.g.: feat_to_shape.sh data/train/feats.scp data/train/shape.scp data/train/log Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs. --filetype &lt;mat|hdf5|sound.hdf5&gt; # Specify the format of feats file --preprocess-conf &lt;json&gt; # Apply preprocess to feats when creating shape.scp --verbose &lt;num&gt; # Default: 0\"]},\"282\":{\"h\":\"generate_wav.sh\",\"t\":[\"Usage: generate_wav.sh [options] &lt;model-path&gt; &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;fbank-dir&gt;] ] Example: generate_wav.sh ljspeech.wavenet.ns.v1/checkpoint-1000000.pkl data/train exp/wavenet_vocoder/train wav Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;fbank-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --fs &lt;fs&gt; # sampling rate (default=22050) --n_fft &lt;n_fft&gt; # number of FFT points (default=1024) --n_shift &lt;n_shift&gt; # shift size in point (default=256) --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs.\"]},\"283\":{\"h\":\"make_fbank.sh\",\"t\":[\"Usage: make_fbank.sh [options] &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;fbank-dir&gt;] ] e.g.: make_fbank.sh data/train exp/make_fbank/train mfcc Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;fbank-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs. --filetype &lt;mat|hdf5|sound.hdf5&gt; # Specify the format of feats file\"]},\"284\":{\"h\":\"make_stft.sh\",\"t\":[\"Usage: make_stft.sh [options] &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;stft-dir&gt;] ] e.g.: make_stft.sh data/train exp/make_stft/train stft Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;stft-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs. --filetype &lt;mat|hdf5|sound.hdf5&gt; # Specify the format of feats file\"]},\"285\":{\"h\":\"pack_model.sh\",\"t\":[\"Usage: pack_model.sh --lm &lt;lm&gt; --dict &lt;dict&gt; &lt;tr_conf&gt; &lt;dec_conf&gt; &lt;cmvn&gt; &lt;e2e&gt;, for example: &lt;lm&gt;: exp/train_rnnlm/rnnlm.model.best &lt;dict&gt;: data/lang_char &lt;tr_conf&gt;: conf/train.yaml &lt;dec_conf&gt;: conf/decode.yaml &lt;cmvn&gt;: data/tr_it/cmvn.ark &lt;e2e&gt;: exp/tr_it_pytorch_train/results/model.last10.avg.best\"]},\"286\":{\"h\":\"recog_wav.sh\",\"t\":[\"Usage: recog_wav.sh [options] &lt;wav_file&gt; Options: --backend &lt;chainer|pytorch&gt; # chainer or pytorch (Default: pytorch) --ngpu &lt;ngpu&gt; # Number of GPUs (Default: 0) --decode_dir &lt;directory_name&gt; # Name of directory to store decoding temporary data --models &lt;model_name&gt; # Model name (e.g. tedlium2.transformer.v1) --cmvn &lt;path&gt; # Location of cmvn.ark --lang_model &lt;path&gt; # Location of language model --recog_model &lt;path&gt; # Location of E2E model --decode_config &lt;path&gt; # Location of configuration file --api &lt;api_version&gt; # API version (v1 or v2, available in only pytorch backend) Example: # Record audio from microphone input as example.wav rec -c 1 -r 16000 example.wav trim 0 5 # Decode using model name recog_wav.sh --models tedlium2.transformer.v1 example.wav # Decode with streaming mode (only RNN with API v1 is supported) recog_wav.sh --models tedlium2.rnn.v2 --api v1 example.wav # Decode using model file recog_wav.sh --cmvn cmvn.ark --lang_model rnnlm.model.best --recog_model model.acc.best --decode_config conf/decode.yaml example.wav # Decode with GPU (require batchsize > 0 in configuration file) recog_wav.sh --ngpu 1 example.wav Available models: - tedlium2.rnn.v1 - tedlium2.rnn.v2 - tedlium2.transformer.v1 - tedlium3.transformer.v1 - librispeech.transformer.v1 - librispeech.transformer.v1.transformerlm.v1 - commonvoice.transformer.v1 - csj.transformer.v1\"]},\"287\":{\"h\":\"reduce_data_dir.sh\",\"t\":[\"usage: reduce_data_dir.sh srcdir turnlist destdir\"]},\"288\":{\"h\":\"remove_longshortdata.sh\",\"t\":[\"usage: remove_longshortdata.sh olddatadir newdatadir\"]},\"289\":{\"h\":\"score_bleu.sh\",\"t\":[\"No help found.\"]},\"290\":{\"h\":\"score_sclite.sh\",\"t\":[\"Usage: score_sclite.sh &lt;data-dir&gt; &lt;dict&gt;\"]},\"291\":{\"h\":\"score_sclite_case.sh\",\"t\":[\"No help found.\"]},\"292\":{\"h\":\"score_sclite_wo_dict.sh\",\"t\":[\"Usage: score_sclite_wo_dict.sh &lt;data-dir&gt;\"]},\"293\":{\"h\":\"show_result.sh\",\"t\":[\"No help found.\"]},\"294\":{\"h\":\"speed_perturb.sh\",\"t\":[\"Usage: speed_perturb.sh [options] &lt;data-dir&gt; &lt;destination-dir&gt; &lt;fbankdir&gt; e.g.: speed_perturb.sh data/train en de Options: --cases # target case information (e.g., lc.rm, lc, tc) --speeds # speed used in speed perturbation (e.g., 0.9. 1.0, 1.1) --langs # all languages (source + target) --write_utt2num_frames # write utt2num_frames in steps/make_fbank_pitch.sh --cmd &lt;run.pl|queue.pl <queue opts&gt;> # how to run jobs --nj &lt;nj&gt; # number of parallel jobs\"]},\"295\":{\"h\":\"synth_wav.sh\",\"t\":[\"Usage: $ synth_wav.sh &lt;text&gt; Note: This code does not include text frontend part. Please clean the input text manually. Also, you need to modify feature configuration according to the model. Default setting is for ljspeech models, so if you want to use other pretrained models, please modify the parameters by yourself. For our provided models, you can find them in the tables at https://github.com/espnet/espnet#tts-demo. If you are beginner, instead of this script, I strongly recommend trying the following colab notebook at first, which includes all of the procedure from text frontend, feature generation, and waveform generation. https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb Example: # make text file and then generate it # (for the default model, ljspeech, we use upper-case char sequence as the input) echo \\\"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\\\" > example.txt synth_wav.sh example.txt # also you can use multiple text echo \\\"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\\\" > example.txt echo \\\"TEXT TO SPEECH IS A TECHQNIQUE TO CONVERT TEXT INTO SPEECH.\\\" >> example.txt synth_wav.sh example.txt # you can specify the pretrained models synth_wav.sh --models ljspeech.transformer.v3 example.txt # also you can specify vocoder model synth_wav.sh --vocoder_models ljspeech.wavenet.mol.v2 example.txt Available models: - ljspeech.tacotron2.v1 - ljspeech.tacotron2.v2 - ljspeech.tacotron2.v3 - ljspeech.transformer.v1 - ljspeech.transformer.v2 - ljspeech.transformer.v3 - ljspeech.fastspeech.v1 - ljspeech.fastspeech.v2 - ljspeech.fastspeech.v3 - libritts.tacotron2.v1 - libritts.transformer.v1 - jsut.transformer.v1 - jsut.tacotron2.v1 - csmsc.transformer.v1 - csmsc.fastspeech.v3 Available vocoder models: - ljspeech.wavenet.softmax.ns.v1 - ljspeech.wavenet.mol.v1 - ljspeech.parallel_wavegan.v1 - libritts.wavenet.mol.v1 - jsut.wavenet.mol.v1 - jsut.parallel_wavegan.v1 - csmsc.wavenet.mol.v1 - csmsc.parallel_wavegan.v1 Model details: | Model name | Lang | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Input type | | ----------------------- | ---- | ------- | -------------- | ---------------------- | ---------- | | ljspeech.tacotron2.v1 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.tacotron2.v2 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.tacotron2.v3 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.transformer.v1 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.transformer.v2 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.transformer.v3 | EN | 22.05k | None | 1024 / 256 / None | phn | | ljspeech.fastspeech.v1 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.fastspeech.v2 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.fastspeech.v3 | EN | 22.05k | None | 1024 / 256 / None | phn | | libritts.tacotron2.v1 | EN | 24k | 80-7600 | 1024 / 256 / None | char | | libritts.transformer.v1 | EN | 24k | 80-7600 | 1024 / 256 / None | char | | jsut.tacotron2 | JP | 24k | 80-7600 | 2048 / 300 / 1200 | phn | | jsut.transformer | JP | 24k | 80-7600 | 2048 / 300 / 1200 | phn | | csmsc.transformer.v1 | ZH | 24k | 80-7600 | 2048 / 300 / 1200 | pinyin | | csmsc.fastspeech.v3 | ZH | 24k | 80-7600 | 2048 / 300 / 1200 | pinyin | Vocoder model details: | Model name | Lang | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Model type | | ------------------------------ | ---- | ------- | -------------- | ---------------------- | ---------------- | | ljspeech.wavenet.softmax.ns.v1 | EN | 22.05k | None | 1024 / 256 / None | Softmax WaveNet | | ljspeech.wavenet.mol.v1 | EN | 22.05k | None | 1024 / 256 / None | MoL WaveNet | | ljspeech.parallel_wavegan.v1 | EN | 22.05k | None | 1024 / 256 / None | Parallel WaveGAN | | libritts.wavenet.mol.v1 | EN | 24k | None | 1024 / 256 / None | MoL WaveNet | | jsut.wavenet.mol.v1 | JP | 24k | 80-7600 | 2048 / 300 / 1200 | MoL WaveNet | | jsut.parallel_wavegan.v1 | JP | 24k | 80-7600 | 2048 / 300 / 1200 | Parallel WaveGAN | | csmsc.wavenet.mol.v1 | ZH | 24k | 80-7600 | 2048 / 300 / 1200 | MoL WaveNet | | csmsc.parallel_wavegan.v1 | ZH | 24k | 80-7600 | 2048 / 300 / 1200 | Parallel WaveGAN |\"]},\"296\":{\"h\":\"translate_wav.sh\",\"t\":[\"Usage: translate_wav.sh [options] &lt;wav_file&gt; Options: --ngpu &lt;ngpu&gt; # Number of GPUs (Default: 0) --decode_dir &lt;directory_name&gt; # Name of directory to store decoding temporary data --models &lt;model_name&gt; # Model name (e.g. must_c.transformer.v1.en-fr) --cmvn &lt;path&gt; # Location of cmvn.ark --trans_model &lt;path&gt; # Location of E2E model --decode_config &lt;path&gt; # Location of configuration file --api &lt;api_version&gt; # API version (v1 or v2) Example: # Record audio from microphone input as example.wav rec -c 1 -r 16000 example.wav trim 0 5 # Decode using model name translate_wav.sh --models must_c.transformer.v1.en-fr example.wav # Decode using model file translate_wav.sh --cmvn cmvn.ark --trans_model model.acc.best --decode_config conf/decode.yaml example.wav # Decode with GPU (require batchsize > 0 in configuration file) translate_wav.sh --ngpu 1 example.wav Available models: - must_c.transformer.v1.en-fr - fisher_callhome_spanish.transformer.v1.es-en\"]},\"297\":{\"h\":\"trim_silence.sh\",\"t\":[\"Usage: trim_silence.sh [options] &lt;data-dir&gt; &lt;log-dir&gt; e.g.: trim_silence.sh data/train exp/trim_silence/train Options: --fs &lt;fs&gt; # sampling frequency (default=16000) --win_length &lt;win_length&gt; # window length in point (default=1024) --shift_length &lt;shift_length&gt; # shift length in point (default=256) --threshold &lt;threshold&gt; # power threshold in db (default=60) --min_silence &lt;sec&gt; # minimum silence length in sec (default=0.01) --normalize &lt;bit&gt; # audio bit (default=16) --cmd &lt;cmd&gt; # how to run jobs (default=run.pl) --nj &lt;nj&gt; # number of parallel jobs (default=32)\"]},\"298\":{\"h\":\"update_json.sh\",\"t\":[\"Usage: update_json.sh &lt;json&gt; &lt;data-dir&gt; &lt;dict&gt; e.g. update_json.sh data/train data/lang_1char/train_units.txt Options: --oov &lt;oov-word&gt; # Default: &lt;unk&gt; --verbose &lt;num&gt; # Default: 0\"]},\"299\":{\"h\":\"aggregate_stats_dirs.py\",\"t\":[\"<!-- _aggregate_stats_dirs.py -->\",\"Aggregate statistics directories into one directory\",\"usage: aggregate_stats_dirs.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] [--skip_sum_stats] [--input_dir INPUT_DIR] --output_dir OUTPUT_DIR\"]},\"300\":{\"h\":\"Named Arguments\"},\"301\":{\"h\":\"asr_align.py\",\"t\":[\"<!-- _asr_align.py -->\",\"ASR Decoding\",\"usage: asr_align.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] [--ngpu NGPU] [--dtype {float16,float32,float64}] --asr_train_config ASR_TRAIN_CONFIG --asr_model_file ASR_MODEL_FILE [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--fs FS] [--min_window_size MIN_WINDOW_SIZE] [--max_window_size MAX_WINDOW_SIZE] [--set_blank SET_BLANK] [--gratis_blank GRATIS_BLANK] [--replace_spaces_with_blanks REPLACE_SPACES_WITH_BLANKS] [--scoring_length SCORING_LENGTH] [--time_stamps {auto,fixed}] [--text_converter {tokenize,classic}] [--kaldi_style_text KALDI_STYLE_TEXT] [--print_utt_text PRINT_UTT_TEXT] [--print_utt_score PRINT_UTT_SCORE] -a AUDIO -t TEXT [-o OUTPUT]\"]},\"302\":{\"h\":\"Named Arguments\"},\"303\":{\"h\":\"Model configuration related\"},\"304\":{\"h\":\"Text converter related\"},\"305\":{\"h\":\"CTC segmentation related\"},\"306\":{\"h\":\"Input/output arguments\"},\"307\":{\"h\":\"asr_inference.py\",\"t\":[\"<!-- _asr_inference.py -->\",\"ASR Decoding\",\"usage: asr_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--asr_train_config ASR_TRAIN_CONFIG] [--asr_model_file ASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--enh_s2t_task ENH_S2T_TASK] [--multi_asr MULTI_ASR] [--quantize_asr_model QUANTIZE_ASR_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES [QUANTIZE_MODULES ...]]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--streaming STREAMING] [--hugging_face_decoder HUGGING_FACE_DECODER] [--hugging_face_decoder_conf HUGGING_FACE_DECODER_CONF] [--transducer_conf TRANSDUCER_CONF] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--time_sync TIME_SYNC] [--lang_prompt_token LANG_PROMPT_TOKEN] [--nlp_prompt_token NLP_PROMPT_TOKEN] [--prompt_token_file PROMPT_TOKEN_FILE] [--normalize_length NORMALIZE_LENGTH] [--partial_ar PARTIAL_AR] [--threshold_probability THRESHOLD_PROBABILITY] [--max_seq_len MAX_SEQ_LEN] [--max_mask_parallel MAX_MASK_PARALLEL]\"]},\"308\":{\"h\":\"Named Arguments\"},\"309\":{\"h\":\"Input data related\"},\"310\":{\"h\":\"The model configuration related\"},\"311\":{\"h\":\"Quantization related\"},\"312\":{\"h\":\"Beam-search related\"},\"313\":{\"h\":\"Text converter related\"},\"314\":{\"h\":\"Partially AR related\"},\"315\":{\"h\":\"asr_inference_k2.py\",\"t\":[\"<!-- _asr_inference_k2.py -->\",\"ASR Decoding\",\"usage: asr_inference_k2.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--asr_train_config ASR_TRAIN_CONFIG] [--asr_model_file ASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--streaming STREAMING] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--is_ctc_decoding IS_CTC_DECODING] [--use_nbest_rescoring USE_NBEST_RESCORING] [--num_paths NUM_PATHS] [--nbest_batch_size NBEST_BATCH_SIZE] [--nll_batch_size NLL_BATCH_SIZE] [--k2_config K2_CONFIG]\"]},\"316\":{\"h\":\"Named Arguments\"},\"317\":{\"h\":\"Input data related\"},\"318\":{\"h\":\"The model configuration related\"},\"319\":{\"h\":\"Beam-search related\"},\"320\":{\"h\":\"Text converter related\"},\"321\":{\"h\":\"asr_inference_maskctc.py\",\"t\":[\"<!-- _asr_inference_maskctc.py -->\",\"ASR Decoding\",\"usage: asr_inference_maskctc.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] --asr_train_config ASR_TRAIN_CONFIG --asr_model_file ASR_MODEL_FILE [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--maskctc_n_iterations MASKCTC_N_ITERATIONS] [--maskctc_threshold_probability MASKCTC_THRESHOLD_PROBABILITY] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL]\"]},\"322\":{\"h\":\"Named Arguments\"},\"323\":{\"h\":\"Input data related\"},\"324\":{\"h\":\"The model configuration related\"},\"325\":{\"h\":\"Decoding related\"},\"326\":{\"h\":\"Text converter related\"},\"327\":{\"h\":\"asr_inference_streaming.py\",\"t\":[\"<!-- _asr_inference_streaming.py -->\",\"ASR Decoding\",\"usage: asr_inference_streaming.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--sim_chunk_length SIM_CHUNK_LENGTH] --asr_train_config ASR_TRAIN_CONFIG --asr_model_file ASR_MODEL_FILE [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--disable_repetition_detection DISABLE_REPETITION_DETECTION] [--encoded_feat_length_limit ENCODED_FEAT_LENGTH_LIMIT] [--decoder_text_length_limit DECODER_TEXT_LENGTH_LIMIT] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--normalize_length NORMALIZE_LENGTH]\"]},\"328\":{\"h\":\"Named Arguments\"},\"329\":{\"h\":\"Input data related\"},\"330\":{\"h\":\"The model configuration related\"},\"331\":{\"h\":\"Beam-search related\"},\"332\":{\"h\":\"Text converter related\"},\"333\":{\"h\":\"asr_transducer_inference.py\",\"t\":[\"<!-- _asr_transducer_inference.py -->\",\"ASR Transducer Decoding\",\"usage: asr_transducer_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--asr_train_config ASR_TRAIN_CONFIG] [--asr_model_file ASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--lm_weight LM_WEIGHT] [--beam_search_config BEAM_SEARCH_CONFIG] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--quantize_asr_model QUANTIZE_ASR_MODEL] [--quantize_modules [QUANTIZE_MODULES [QUANTIZE_MODULES ...]]] [--quantize_dtype {float16,qint8}] [--streaming STREAMING] [--decoding_window DECODING_WINDOW] [--left_context LEFT_CONTEXT] [--display_hypotheses DISPLAY_HYPOTHESES]\"]},\"334\":{\"h\":\"Named Arguments\"},\"335\":{\"h\":\"Input data related\"},\"336\":{\"h\":\"The model configuration related\"},\"337\":{\"h\":\"Beam-search related\"},\"338\":{\"h\":\"Text converter related\"},\"339\":{\"h\":\"asvspoof_inference.py\",\"t\":[\"<!-- _asvspoof_inference.py -->\",\"ASVSpoof Decoding\",\"usage: asvspoof_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] [--batch_size BATCH_SIZE] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--asvspoof_train_config ASVSPOOF_TRAIN_CONFIG] [--asvspoof_model_file ASVSPOOF_MODEL_FILE]\"]},\"340\":{\"h\":\"Named Arguments\"},\"341\":{\"h\":\"Input data related\"},\"342\":{\"h\":\"The model configuration related\"},\"343\":{\"h\":\"diar_inference.py\",\"t\":[\"<!-- _diar_inference.py -->\",\"Speaker Diarization inference\",\"usage: diar_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--fs FS] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--segment_size SEGMENT_SIZE] [--hop_size HOP_SIZE] [--show_progressbar SHOW_PROGRESSBAR] [--num_spk NUM_SPK] [--enh_s2t_task ENH_S2T_TASK] [--normalize_segment_scale NORMALIZE_SEGMENT_SCALE] [--normalize_output_wav NORMALIZE_OUTPUT_WAV] [--multiply_diar_result MULTIPLY_DIAR_RESULT]\"]},\"344\":{\"h\":\"Named Arguments\"},\"345\":{\"h\":\"Input data related\"},\"346\":{\"h\":\"The model configuration related\"},\"347\":{\"h\":\"Data loading related\"},\"348\":{\"h\":\"Diarize speech related\"},\"349\":{\"h\":\"Enh + Diar related\"},\"350\":{\"h\":\"enh_inference.py\",\"t\":[\"<!-- _enh_inference.py -->\",\"Frontend inference\",\"usage: enh_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--fs FS] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--normalize_output_wav NORMALIZE_OUTPUT_WAV] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--inference_config INFERENCE_CONFIG] [--enh_s2t_task ENH_S2T_TASK] [--batch_size BATCH_SIZE] [--segment_size SEGMENT_SIZE] [--hop_size HOP_SIZE] [--normalize_segment_scale NORMALIZE_SEGMENT_SCALE] [--show_progressbar SHOW_PROGRESSBAR] [--ref_channel REF_CHANNEL]\"]},\"351\":{\"h\":\"Named Arguments\"},\"352\":{\"h\":\"Input data related\"},\"353\":{\"h\":\"Output data related\"},\"354\":{\"h\":\"The model configuration related\"},\"355\":{\"h\":\"Data loading related\"},\"356\":{\"h\":\"SeparateSpeech related\"},\"357\":{\"h\":\"enh_inference_streaming.py\",\"t\":[\"<!-- _enh_inference_streaming.py -->\",\"Frontend inference\",\"usage: enh_inference_streaming.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--fs FS] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--inference_config INFERENCE_CONFIG] [--enh_s2t_task ENH_S2T_TASK] [--batch_size BATCH_SIZE] [--ref_channel REF_CHANNEL]\"]},\"358\":{\"h\":\"Named Arguments\"},\"359\":{\"h\":\"Input data related\"},\"360\":{\"h\":\"The model configuration related\"},\"361\":{\"h\":\"Data loading related\"},\"362\":{\"h\":\"SeparateSpeech related\"},\"363\":{\"h\":\"enh_scoring.py\",\"t\":[\"<!-- _enh_scoring.py -->\",\"Frontend inference\",\"usage: enh_scoring.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--dtype {float16,float32,float64}] --ref_scp REF_SCP --inf_scp INF_SCP [--key_file KEY_FILE] [--ref_channel REF_CHANNEL] [--flexible_numspk FLEXIBLE_NUMSPK] [--is_tse IS_TSE] [--use_dnsmos USE_DNSMOS] [--dnsmos_mode {local,web}] [--dnsmos_auth_key DNSMOS_AUTH_KEY] [--dnsmos_use_gpu DNSMOS_USE_GPU] [--dnsmos_convert_to_torch DNSMOS_CONVERT_TO_TORCH] [--dnsmos_primary_model DNSMOS_PRIMARY_MODEL] [--dnsmos_p808_model DNSMOS_P808_MODEL] [--use_pesq USE_PESQ]\"]},\"364\":{\"h\":\"Named Arguments\"},\"365\":{\"h\":\"Input data related\"},\"366\":{\"h\":\"DNSMOS related\"},\"367\":{\"h\":\"PESQ related\"},\"368\":{\"h\":\"enh_tse_inference.py\",\"t\":[\"<!-- _enh_tse_inference.py -->\",\"Frontend inference\",\"usage: enh_tse_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--fs FS] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--normalize_output_wav NORMALIZE_OUTPUT_WAV] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--inference_config INFERENCE_CONFIG] [--batch_size BATCH_SIZE] [--segment_size SEGMENT_SIZE] [--hop_size HOP_SIZE] [--normalize_segment_scale NORMALIZE_SEGMENT_SCALE] [--show_progressbar SHOW_PROGRESSBAR] [--ref_channel REF_CHANNEL]\"]},\"369\":{\"h\":\"Named Arguments\"},\"370\":{\"h\":\"Input data related\"},\"371\":{\"h\":\"Output data related\"},\"372\":{\"h\":\"The model configuration related\"},\"373\":{\"h\":\"Data loading related\"},\"374\":{\"h\":\"SeparateSpeech related\"},\"375\":{\"h\":\"hugging_face_export_vocabulary.py\",\"t\":[\"<!-- _hugging_face_export_vocabulary.py -->\",\"Export Hugging Face vocabulary\",\"usage: hugging_face_export_vocabulary.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output OUTPUT --model_name_or_path MODEL_NAME_OR_PATH [--add_symbol ADD_SYMBOL]\"]},\"376\":{\"h\":\"Named Arguments\"},\"377\":{\"h\":\"launch.py\",\"t\":[\"<!-- _launch.py -->\",\"Launch distributed process with appropriate options.\",\"usage: launch.py [-h] [--cmd CMD] [--log LOG] [--max_num_log_files MAX_NUM_LOG_FILES] [--ngpu NGPU] [--num_nodes NUM_NODES | --host HOST] [--envfile ENVFILE] [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED] [--master_port MASTER_PORT] [--master_addr MASTER_ADDR] [--init_file_prefix INIT_FILE_PREFIX] args [args ...]\"]},\"378\":{\"h\":\"Positional Arguments\"},\"379\":{\"h\":\"Named Arguments\"},\"380\":{\"h\":\"lm_calc_perplexity.py\",\"t\":[\"<!-- _lm_calc_perplexity.py -->\",\"Calc perplexity\",\"usage: lm_calc_perplexity.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] [--log_base LOG_BASE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE]\"]},\"381\":{\"h\":\"Named Arguments\"},\"382\":{\"h\":\"Input data related\"},\"383\":{\"h\":\"The model configuration related\"},\"384\":{\"h\":\"lm_inference.py\",\"t\":[\"<!-- _lm_inference.py -->\",\"LM Decoding (conditional generation)\",\"usage: lm_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES [QUANTIZE_MODULES ...]]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlen MAXLEN] [--minlen MINLEN] [--ngram_weight NGRAM_WEIGHT] [--token_type {char,word,bpe,None}] [--bpemodel BPEMODEL]\"]},\"385\":{\"h\":\"Named Arguments\"},\"386\":{\"h\":\"Input data related\"},\"387\":{\"h\":\"The model configuration related\"},\"388\":{\"h\":\"Quantization related\"},\"389\":{\"h\":\"Beam-search related\"},\"390\":{\"h\":\"Text converter related\"},\"391\":{\"h\":\"mt_inference.py\",\"t\":[\"<!-- _mt_inference.py -->\",\"MT Decoding\",\"usage: mt_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--mt_train_config MT_TRAIN_CONFIG] [--mt_model_file MT_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--normalize_length NORMALIZE_LENGTH]\"]},\"392\":{\"h\":\"Named Arguments\"},\"393\":{\"h\":\"Input data related\"},\"394\":{\"h\":\"The model configuration related\"},\"395\":{\"h\":\"Beam-search related\"},\"396\":{\"h\":\"Text converter related\"},\"397\":{\"h\":\"pack.py\",\"t\":[\"<!-- _pack.py -->\",\"Pack input files to archive format\",\"usage: pack.py [-h] {asr,st,tts,enh,diar,svs,enh_s2t,ssl,s2st,s2t,spk} ...\"]},\"398\":{\"h\":\"Sub-commands\",\"t\":[\"asr\",\"Undocumented\",\"pack.py asr [-h] --outpath OUTPATH [--asr_train_config ASR_TRAIN_CONFIG] [--lm_train_config LM_TRAIN_CONFIG] [--asr_model_file ASR_MODEL_FILE] [--lm_file LM_FILE] [--option OPTION]\",\"Named Arguments\",\"st\",\"Undocumented\",\"pack.py st [-h] --outpath OUTPATH [--st_train_config ST_TRAIN_CONFIG] [--st_model_file ST_MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"tts\",\"Undocumented\",\"pack.py tts [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"enh\",\"Undocumented\",\"pack.py enh [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"diar\",\"Undocumented\",\"pack.py diar [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"svs\",\"Undocumented\",\"pack.py svs [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"enh _s2t \",\"Undocumented\",\"pack.py enh_s2t [-h] --outpath OUTPATH [--enh_s2t_train_config ENH_S2T_TRAIN_CONFIG] [--lm_train_config LM_TRAIN_CONFIG] [--enh_s2t_model_file ENH_S2T_MODEL_FILE] [--lm_file LM_FILE] [--option OPTION]\",\"Named Arguments\",\"ssl\",\"Undocumented\",\"pack.py ssl [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"s2st\",\"Undocumented\",\"pack.py s2st [-h] --outpath OUTPATH [--s2st_train_config S2ST_TRAIN_CONFIG] [--s2st_model_file S2ST_MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"s2t\",\"Undocumented\",\"pack.py s2t [-h] --outpath OUTPATH [--s2t_train_config S2T_TRAIN_CONFIG] [--lm_train_config LM_TRAIN_CONFIG] [--s2t_model_file S2T_MODEL_FILE] [--lm_file LM_FILE] [--option OPTION]\",\"Named Arguments\",\"spk\",\"Undocumented\",\"pack.py spk [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\"]},\"399\":{\"h\":\"s2st_inference.py\",\"t\":[\"<!-- _s2st_inference.py -->\",\"S2ST inference\",\"usage: s2st_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--st_subtask_maxlenratio ST_SUBTASK_MAXLENRATIO] [--st_subtask_minlenratio ST_SUBTASK_MINLENRATIO] [--threshold THRESHOLD] [--use_att_constraint USE_ATT_CONSTRAINT] [--backward_window BACKWARD_WINDOW] [--forward_window FORWARD_WINDOW] [--use_teacher_forcing USE_TEACHER_FORCING] [--always_fix_seed ALWAYS_FIX_SEED] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--st_subtask_nbest ST_SUBTASK_NBEST] [--st_subtask_beam_size ST_SUBTASK_BEAM_SIZE] [--st_subtask_penalty ST_SUBTASK_PENALTY] [--vocoder_config VOCODER_CONFIG] [--vocoder_file VOCODER_FILE] [--vocoder_tag VOCODER_TAG] [--st_subtask_token_type {char,bpe,None}] [--st_subtask_bpemodel ST_SUBTASK_BPEMODEL] [--normalize_length NORMALIZE_LENGTH]\"]},\"400\":{\"h\":\"Named Arguments\"},\"401\":{\"h\":\"Input data related\"},\"402\":{\"h\":\"The model configuration related\"},\"403\":{\"h\":\"Decoding related\"},\"404\":{\"h\":\"Spectrogram-based generation related\"},\"405\":{\"h\":\"Beam-search (discrete unit/multi-pass) related\"},\"406\":{\"h\":\"Vocoder related\"},\"407\":{\"h\":\"Text converter related\"},\"408\":{\"h\":\"s2t_inference.py\",\"t\":[\"<!-- _s2t_inference.py -->\",\"S2T Decoding\",\"usage: s2t_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--s2t_train_config S2T_TRAIN_CONFIG] [--s2t_model_file S2T_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--lang_sym LANG_SYM] [--task_sym TASK_SYM] [--predict_time PREDICT_TIME] [--quantize_s2t_model QUANTIZE_S2T_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES [QUANTIZE_MODULES ...]]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--normalize_length NORMALIZE_LENGTH] [--token_type {char,bpe,word,None}] [--bpemodel BPEMODEL] [--partial_ar PARTIAL_AR] [--threshold_probability THRESHOLD_PROBABILITY] [--max_seq_len MAX_SEQ_LEN] [--max_mask_parallel MAX_MASK_PARALLEL]\"]},\"409\":{\"h\":\"Named Arguments\"},\"410\":{\"h\":\"Input data related\"},\"411\":{\"h\":\"Model configuration related\"},\"412\":{\"h\":\"Quantization related\"},\"413\":{\"h\":\"Beam-search related\"},\"414\":{\"h\":\"Text converter related\"},\"415\":{\"h\":\"Partially AR related\"},\"416\":{\"h\":\"s2t_inference_language.py\",\"t\":[\"<!-- _s2t_inference_language.py -->\",\"S2T Decoding\",\"usage: s2t_inference_language.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--s2t_train_config S2T_TRAIN_CONFIG] [--s2t_model_file S2T_MODEL_FILE] [--model_tag MODEL_TAG] [--first_lang_sym FIRST_LANG_SYM] [--last_lang_sym LAST_LANG_SYM] [--quantize_s2t_model QUANTIZE_S2T_MODEL] [--quantize_modules [QUANTIZE_MODULES [QUANTIZE_MODULES ...]]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST]\"]},\"417\":{\"h\":\"Named Arguments\"},\"418\":{\"h\":\"Input data related\"},\"419\":{\"h\":\"Model configuration related\"},\"420\":{\"h\":\"Quantization related\"},\"421\":{\"h\":\"Beam-search related\"},\"422\":{\"h\":\"slu_inference.py\",\"t\":[\"<!-- _slu_inference.py -->\",\"ASR Decoding\",\"usage: slu_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--slu_train_config SLU_TRAIN_CONFIG] [--slu_model_file SLU_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--quantize_asr_model QUANTIZE_ASR_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES [QUANTIZE_MODULES ...]]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--streaming STREAMING] [--transducer_conf TRANSDUCER_CONF] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--normalize_length NORMALIZE_LENGTH]\"]},\"423\":{\"h\":\"Named Arguments\"},\"424\":{\"h\":\"Input data related\"},\"425\":{\"h\":\"The model configuration related\"},\"426\":{\"h\":\"Quantization related\"},\"427\":{\"h\":\"Beam-search related\"},\"428\":{\"h\":\"Text converter related\"},\"429\":{\"h\":\"spk_embed_extract.py\",\"t\":[\"<!-- _spk_embed_extract.py -->\",\"speaker embedding extraction\",\"usage: spk_embed_extract.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--batch_type {unsorted,sorted,folded,length,numel}] [--batch_bins BATCH_BINS] [--valid_batch_bins VALID_BATCH_BINS] [--valid_batch_type {unsorted,sorted,folded,length,numel,None}] [--max_cache_size MAX_CACHE_SIZE] [--max_cache_fd MAX_CACHE_FD] [--allow_multi_rates ALLOW_MULTI_RATES] [--valid_max_cache_size VALID_MAX_CACHE_SIZE] [--shape_file SHAPE_FILE] [--input_size INPUT_SIZE] [--num_cohort_spk NUM_COHORT_SPK] [--num_utt_per_spk NUM_UTT_PER_SPK] [--utt_select_sec UTT_SELECT_SEC] [--average_spk AVERAGE_SPK] [--adaptive_cohort_size ADAPTIVE_COHORT_SIZE] [--qmf_dur_thresh QMF_DUR_THRESH] [--qmf_num_trial_per_condition QMF_NUM_TRIAL_PER_CONDITION] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--average_embd AVERAGE_EMBD] [--train_dtype {float16,float32,float64}] [--use_amp USE_AMP] [--no_forward_run NO_FORWARD_RUN] [--sort_in_batch {descending,ascending}] [--sort_batch {descending,ascending}] [--drop_last_iter DROP_LAST_ITER] [--spk_train_config SPK_TRAIN_CONFIG] [--spk_model_file SPK_MODEL_FILE] [--model_tag MODEL_TAG] [--dist_backend DIST_BACKEND] [--dist_init_method DIST_INIT_METHOD] [--dist_world_size DIST_WORLD_SIZE] [--dist_rank DIST_RANK] [--local_rank LOCAL_RANK] [--dist_master_addr DIST_MASTER_ADDR] [--dist_master_port DIST_MASTER_PORT] [--dist_launcher {slurm,mpi,None}] [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED] [--unused_parameters UNUSED_PARAMETERS] [--sharded_ddp SHARDED_DDP] [--use_matplotlib USE_MATPLOTLIB] [--use_tensorboard USE_TENSORBOARD] [--create_graph_in_tensorboard CREATE_GRAPH_IN_TENSORBOARD] [--use_wandb USE_WANDB] [--wandb_project WANDB_PROJECT] [--wandb_id WANDB_ID] [--wandb_entity WANDB_ENTITY] [--wandb_name WANDB_NAME] [--wandb_model_log_interval WANDB_MODEL_LOG_INTERVAL] [--detect_anomaly DETECT_ANOMALY] [--use_lora USE_LORA] [--save_lora_only SAVE_LORA_ONLY] [--lora_conf LORA_CONF] [--cudnn_enabled CUDNN_ENABLED] [--cudnn_benchmark CUDNN_BENCHMARK] [--cudnn_deterministic CUDNN_DETERMINISTIC] [--valid_batch_size VALID_BATCH_SIZE] [--target_duration TARGET_DURATION] [--num_eval NUM_EVAL] [--fold_length FOLD_LENGTH] [--use_preprocessor USE_PREPROCESSOR]\"]},\"430\":{\"h\":\"Named Arguments\"},\"431\":{\"h\":\"Input data related\"},\"432\":{\"h\":\"The model configuration related\"},\"433\":{\"h\":\"distributed training related\"},\"434\":{\"h\":\"trainer initialization related\"},\"435\":{\"h\":\"cudnn mode related\"},\"436\":{\"h\":\"The inference hyperparameter related\"},\"437\":{\"h\":\"spk_inference.py\",\"t\":[\"<!-- _spk_inference.py -->\",\"Speaker Embedding Extraction\",\"usage: spk_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--batch_size BATCH_SIZE] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG]\"]},\"438\":{\"h\":\"Named Arguments\"},\"439\":{\"h\":\"Input data related\"},\"440\":{\"h\":\"The model configuration related\"},\"441\":{\"h\":\"split_scps.py\",\"t\":[\"<!-- _split_scps.py -->\",\"Split scp files\",\"usage: split_scps.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --scps SCPS [SCPS ...] [--names NAMES [NAMES ...]] [--num_splits NUM_SPLITS] --output_dir OUTPUT_DIR\"]},\"442\":{\"h\":\"Named Arguments\"},\"443\":{\"h\":\"st_inference.py\",\"t\":[\"<!-- _st_inference.py -->\",\"ST Decoding\",\"usage: st_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--st_train_config ST_TRAIN_CONFIG] [--st_model_file ST_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--src_lm_train_config SRC_LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--src_lm_file SRC_LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--src_word_lm_train_config SRC_WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--src_word_lm_file SRC_WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--src_ngram_file SRC_NGRAM_FILE] [--model_tag MODEL_TAG] [--enh_s2t_task ENH_S2T_TASK] [--batch_size BATCH_SIZE] [--nbest NBEST] [--asr_nbest ASR_NBEST] [--beam_size BEAM_SIZE] [--asr_beam_size ASR_BEAM_SIZE] [--penalty PENALTY] [--asr_penalty ASR_PENALTY] [--maxlenratio MAXLENRATIO] [--asr_maxlenratio ASR_MAXLENRATIO] [--minlenratio MINLENRATIO] [--asr_minlenratio ASR_MINLENRATIO] [--lm_weight LM_WEIGHT] [--asr_lm_weight ASR_LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--asr_ngram_weight ASR_NGRAM_WEIGHT] [--ctc_weight CTC_WEIGHT] [--asr_ctc_weight ASR_CTC_WEIGHT] [--transducer_conf TRANSDUCER_CONF] [--token_type {char,bpe,None}] [--src_token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--src_bpemodel SRC_BPEMODEL] [--ctc_greedy CTC_GREEDY] [--hugging_face_decoder HUGGING_FACE_DECODER] [--hugging_face_decoder_max_length HUGGING_FACE_DECODER_MAX_LENGTH] [--normalize_length NORMALIZE_LENGTH]\"]},\"444\":{\"h\":\"Named Arguments\"},\"445\":{\"h\":\"Input data related\"},\"446\":{\"h\":\"The model configuration related\"},\"447\":{\"h\":\"Beam-search related\"},\"448\":{\"h\":\"Text converter related\"},\"449\":{\"h\":\"st_inference_streaming.py\",\"t\":[\"<!-- _st_inference_streaming.py -->\",\"ST Decoding\",\"usage: st_inference_streaming.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--sim_chunk_length SIM_CHUNK_LENGTH] --st_train_config ST_TRAIN_CONFIG --st_model_file ST_MODEL_FILE [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--disable_repetition_detection DISABLE_REPETITION_DETECTION] [--encoded_feat_length_limit ENCODED_FEAT_LENGTH_LIMIT] [--decoder_text_length_limit DECODER_TEXT_LENGTH_LIMIT] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--time_sync TIME_SYNC] [--incremental_decode INCREMENTAL_DECODE] [--blank_penalty BLANK_PENALTY] [--hold_n HOLD_N] [--transducer_conf TRANSDUCER_CONF] [--hugging_face_decoder HUGGING_FACE_DECODER] [--normalize_length NORMALIZE_LENGTH]\"]},\"450\":{\"h\":\"Named Arguments\"},\"451\":{\"h\":\"Input data related\"},\"452\":{\"h\":\"The model configuration related\"},\"453\":{\"h\":\"Beam-search related\"},\"454\":{\"h\":\"Text converter related\"},\"455\":{\"h\":\"svs_inference.py\",\"t\":[\"<!-- _svs_inference.py -->\",\"SVS Decode\",\"usage: svs_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--use_teacher_forcing USE_TEACHER_FORCING] [--noise_scale NOISE_SCALE] [--noise_scale_dur NOISE_SCALE_DUR] [--vocoder_checkpoint VOCODER_CHECKPOINT] [--vocoder_config VOCODER_CONFIG] [--svs_task SVS_TASK]\"]},\"456\":{\"h\":\"Named Arguments\"},\"457\":{\"h\":\"Input data related\"},\"458\":{\"h\":\"The model configuration related\"},\"459\":{\"h\":\"Decoding related\"},\"460\":{\"h\":\"Vocoder related\"},\"461\":{\"h\":\"tokenize_text.py\",\"t\":[\"<!-- _tokenize_text.py -->\",\"Tokenize texts\",\"usage: tokenize_text.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --input INPUT --output OUTPUT [--field FIELD] [--token_type {char,bpe,word,phn}] [--delimiter DELIMITER] [--space_symbol SPACE_SYMBOL] [--bpemodel BPEMODEL] [--non_linguistic_symbols NON_LINGUISTIC_SYMBOLS] [--remove_non_linguistic_symbols REMOVE_NON_LINGUISTIC_SYMBOLS] [--cleaner {None,tacotron,jaconv,vietnamese,korean_cleaner,whisper_en,whisper_basic}] [--g2p {None,g2p_en,g2p_en_no_space,pyopenjtalk,pyopenjtalk_kana,pyopenjtalk_accent,pyopenjtalk_accent_with_pause,pyopenjtalk_prosody,pypinyin_g2p,pypinyin_g2p_phone,pypinyin_g2p_phone_without_prosody,espeak_ng_arabic,espeak_ng_german,espeak_ng_french,espeak_ng_spanish,espeak_ng_russian,espeak_ng_greek,espeak_ng_finnish,espeak_ng_hungarian,espeak_ng_dutch,espeak_ng_english_us_vits,espeak_ng_hindi,espeak_ng_italian,espeak_ng_ukrainian,espeak_ng_polish,g2pk,g2pk_no_space,g2pk_explicit_space,korean_jaso,korean_jaso_no_space,g2p_is}] [--write_vocabulary WRITE_VOCABULARY] [--vocabulary_size VOCABULARY_SIZE] [--cutoff CUTOFF] [--add_symbol ADD_SYMBOL] [--add_nonsplit_symbol ADD_NONSPLIT_SYMBOL]\"]},\"462\":{\"h\":\"Named Arguments\"},\"463\":{\"h\":\"write_vocabulary mode related\"},\"464\":{\"h\":\"tts2_inference.py\",\"t\":[\"<!-- _tts2_inference.py -->\",\"TTS inference\",\"usage: tts2_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--threshold THRESHOLD] [--use_att_constraint USE_ATT_CONSTRAINT] [--backward_window BACKWARD_WINDOW] [--forward_window FORWARD_WINDOW] [--use_teacher_forcing USE_TEACHER_FORCING] [--speed_control_alpha SPEED_CONTROL_ALPHA] [--noise_scale NOISE_SCALE] [--noise_scale_dur NOISE_SCALE_DUR] [--always_fix_seed ALWAYS_FIX_SEED] [--vocoder_config VOCODER_CONFIG] [--vocoder_file VOCODER_FILE] [--vocoder_tag VOCODER_TAG]\"]},\"465\":{\"h\":\"Named Arguments\"},\"466\":{\"h\":\"Input data related\"},\"467\":{\"h\":\"The model configuration related\"},\"468\":{\"h\":\"Decoding related\"},\"469\":{\"h\":\"Vocoder related\"},\"470\":{\"h\":\"tts_inference.py\",\"t\":[\"<!-- _tts_inference.py -->\",\"TTS inference\",\"usage: tts_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--threshold THRESHOLD] [--use_att_constraint USE_ATT_CONSTRAINT] [--backward_window BACKWARD_WINDOW] [--forward_window FORWARD_WINDOW] [--use_teacher_forcing USE_TEACHER_FORCING] [--speed_control_alpha SPEED_CONTROL_ALPHA] [--noise_scale NOISE_SCALE] [--noise_scale_dur NOISE_SCALE_DUR] [--always_fix_seed ALWAYS_FIX_SEED] [--vocoder_config VOCODER_CONFIG] [--vocoder_file VOCODER_FILE] [--vocoder_tag VOCODER_TAG]\"]},\"471\":{\"h\":\"Named Arguments\"},\"472\":{\"h\":\"Input data related\"},\"473\":{\"h\":\"The model configuration related\"},\"474\":{\"h\":\"Decoding related\"},\"475\":{\"h\":\"Vocoder related\"},\"476\":{\"h\":\"uasr_extract_feature.py\",\"t\":[\"<!-- _uasr_extract_feature.py -->\",\"UASR Decoding\",\"usage: uasr_extract_feature.py [-h] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--uasr_train_config UASR_TRAIN_CONFIG] [--uasr_model_file UASR_MODEL_FILE] [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--ngpu NGPU] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] [--dtype {float16,float32,float64}] [--dset DSET] [--output_dir OUTPUT_DIR] [--log_level {ERROR,WARNING,INFO,DEBUG,NOTSET}]\"]},\"477\":{\"h\":\"Named Arguments\"},\"478\":{\"h\":\"uasr_inference.py\",\"t\":[\"<!-- _uasr_inference.py -->\",\"UASR Decoding\",\"usage: uasr_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--uasr_train_config UASR_TRAIN_CONFIG] [--uasr_model_file UASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--quantize_uasr_model QUANTIZE_UASR_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES [QUANTIZE_MODULES ...]]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL]\"]},\"479\":{\"h\":\"Named Arguments\"},\"480\":{\"h\":\"Input data related\"},\"481\":{\"h\":\"The model configuration related\"},\"482\":{\"h\":\"Quantization related\"},\"483\":{\"h\":\"Beam-search related\"},\"484\":{\"h\":\"Text converter related\"},\"485\":{\"h\":\"uasr_inference_k2.py\",\"t\":[\"<!-- _uasr_inference_k2.py -->\",\"UASR Decoding\",\"usage: uasr_inference_k2.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--uasr_train_config UASR_TRAIN_CONFIG] [--uasr_model_file UASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--streaming STREAMING] [--token_type {phn,word}] [--bpemodel BPEMODEL] [--is_ctc_decoding IS_CTC_DECODING] [--use_nbest_rescoring USE_NBEST_RESCORING] [--num_paths NUM_PATHS] [--nbest_batch_size NBEST_BATCH_SIZE] [--nll_batch_size NLL_BATCH_SIZE] [--decoding_graph DECODING_GRAPH] [--word_token_list WORD_TOKEN_LIST] [--k2_config K2_CONFIG]\"]},\"486\":{\"h\":\"Named Arguments\"},\"487\":{\"h\":\"Input data related\"},\"488\":{\"h\":\"The model configuration related\"},\"489\":{\"h\":\"Beam-search related\"},\"490\":{\"h\":\"Text converter related\"},\"491\":{\"h\":\"whisper_export_vocabulary.py\",\"t\":[\"<!-- _whisper_export_vocabulary.py -->\",\"Export Whisper vocabulary\",\"usage: whisper_export_vocabulary.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output OUTPUT --whisper_model WHISPER_MODEL [--add_token_file_name ADD_TOKEN_FILE_NAME] [--whisper_language WHISPER_LANGUAGE] [--whisper_task WHISPER_TASK] [--sot_asr SOT_ASR] [--speaker_change_symbol SPEAKER_CHANGE_SYMBOL]\"]},\"492\":{\"h\":\"Named Arguments\"},\"493\":{\"h\":\"addjson.py\",\"t\":[\"<!-- _addjson.py -->\",\"add multiple json values to an input or output value\",\"usage: addjson.py [-h] [-i IS_INPUT] [--verbose VERBOSE] jsons [jsons ...]\"]},\"494\":{\"h\":\"Positional Arguments\"},\"495\":{\"h\":\"Named Arguments\"},\"496\":{\"h\":\"apply-cmvn.py\",\"t\":[\"<!-- _apply-cmvn.py -->\",\"apply mean-variance normalization to files\",\"usage: apply-cmvn.py [-h] [--verbose VERBOSE] [--in-filetype {mat,hdf5,sound.hdf5,sound}] [--stats-filetype {mat,hdf5,npy}] [--out-filetype {mat,hdf5}] [--norm-means NORM_MEANS] [--norm-vars NORM_VARS] [--reverse REVERSE] [--spk2utt SPK2UTT] [--utt2spk UTT2SPK] [--write-num-frames WRITE_NUM_FRAMES] [--compress COMPRESS] [--compression-method COMPRESSION_METHOD] stats_rspecifier_or_rxfilename rspecifier wspecifier\"]},\"497\":{\"h\":\"Positional Arguments\"},\"498\":{\"h\":\"Named Arguments\"},\"499\":{\"h\":\"average_checkpoints.py\",\"t\":[\"<!-- _average_checkpoints.py -->\",\"average models from snapshot\",\"usage: average_checkpoints.py [-h] --snapshots SNAPSHOTS [SNAPSHOTS ...] --out OUT [--num NUM] [--backend BACKEND] [--log [LOG]] [--metric [{acc,bleu,cer,cer_ctc,loss,perplexity}]] [--max-epoch [MAX_EPOCH]]\"]},\"500\":{\"h\":\"Named Arguments\"},\"501\":{\"h\":\"calculate_rtf.py\",\"t\":[\"<!-- _calculate_rtf.py -->\",\"calculate real time factor (RTF)\",\"usage: calculate_rtf.py [-h] [--log-dir LOG_DIR] [--log-name {decode,asr_inference}] [--input-shift INPUT_SHIFT] [--start-times-marker {input lengths,speech length}] [--end-times-marker {prediction,best hypo}] [--inf-num INF_NUM]\"]},\"502\":{\"h\":\"Named Arguments\"},\"503\":{\"h\":\"change_yaml.py\",\"t\":[\"<!-- _change_yaml.py -->\",\"change specified attributes of a YAML file\",\"usage: change_yaml.py [-h] [-o OUTYAML | --outdir OUTDIR] [-a ARG] [-d DELETE] [inyaml]\"]},\"504\":{\"h\":\"Positional Arguments\"},\"505\":{\"h\":\"Named Arguments\"},\"506\":{\"h\":\"compute-cmvn-stats.py\",\"t\":[\"<!-- _compute-cmvn-stats.py -->\",\"Compute cepstral mean and variance normalization statisticsIf wspecifier provided: per-utterance by default, or per-speaker ifspk2utt option provided; if wxfilename: global\",\"usage: compute-cmvn-stats.py [-h] [--spk2utt SPK2UTT] [--verbose VERBOSE] [--in-filetype {mat,hdf5,sound.hdf5,sound}] [--out-filetype {mat,hdf5,npy}] [--preprocess-conf PREPROCESS_CONF] rspecifier wspecifier_or_wxfilename\"]},\"507\":{\"h\":\"Positional Arguments\"},\"508\":{\"h\":\"Named Arguments\"},\"509\":{\"h\":\"compute-fbank-feats.py\",\"t\":[\"<!-- _compute-fbank-feats.py -->\",\"compute FBANK feature from WAV\",\"usage: compute-fbank-feats.py [-h] [--fs FS] [--fmax [FMAX]] [--fmin [FMIN]] [--n_mels N_MELS] [--n_fft N_FFT] [--n_shift N_SHIFT] [--win_length [WIN_LENGTH]] [--window {hann,hamming}] [--write-num-frames WRITE_NUM_FRAMES] [--filetype {mat,hdf5}] [--compress COMPRESS] [--compression-method COMPRESSION_METHOD] [--verbose VERBOSE] [--normalize {1,16,24,32}] [--segments SEGMENTS] rspecifier wspecifier\"]},\"510\":{\"h\":\"Positional Arguments\"},\"511\":{\"h\":\"Named Arguments\"},\"512\":{\"h\":\"compute-stft-feats.py\",\"t\":[\"<!-- _compute-stft-feats.py -->\",\"compute STFT feature from WAV\",\"usage: compute-stft-feats.py [-h] [--fs FS] [--n_fft N_FFT] [--n_shift N_SHIFT] [--win_length [WIN_LENGTH]] [--window {hann,hamming}] [--write-num-frames WRITE_NUM_FRAMES] [--filetype {mat,hdf5}] [--compress COMPRESS] [--compression-method COMPRESSION_METHOD] [--verbose VERBOSE] [--normalize {1,16,24,32}] [--segments SEGMENTS] rspecifier wspecifier\"]},\"513\":{\"h\":\"Positional Arguments\"},\"514\":{\"h\":\"Named Arguments\"},\"515\":{\"h\":\"concat_json_multiref.py\",\"t\":[\"<!-- _concat_json_multiref.py -->\",\"concatenate multiple json files for data augmentation\",\"usage: concat_json_multiref.py [-h] jsons [jsons ...]\"]},\"516\":{\"h\":\"Positional Arguments\"},\"517\":{\"h\":\"concatjson.py\",\"t\":[\"<!-- _concatjson.py -->\",\"concatenate json files\",\"usage: concatjson.py [-h] jsons [jsons ...]\"]},\"518\":{\"h\":\"Positional Arguments\"},\"519\":{\"h\":\"convert_fbank_to_wav.py\",\"t\":[\"<!-- _convert_fbank_to_wav.py -->\",\"convert FBANK to WAV using Griffin-Lim algorithm\",\"usage: convert_fbank_to_wav.py [-h] [--fs FS] [--fmax [FMAX]] [--fmin [FMIN]] [--n_fft N_FFT] [--n_shift N_SHIFT] [--win_length [WIN_LENGTH]] [--n_mels [N_MELS]] [--window {hann,hamming}] [--iters ITERS] [--filetype {mat,hdf5}] rspecifier outdir\"]},\"520\":{\"h\":\"Positional Arguments\"},\"521\":{\"h\":\"Named Arguments\"},\"522\":{\"h\":\"copy-feats.py\",\"t\":[\"<!-- _copy-feats.py -->\",\"copy feature with preprocessing\",\"usage: copy-feats.py [-h] [--verbose VERBOSE] [--in-filetype {mat,hdf5,sound.hdf5,sound}] [--out-filetype {mat,hdf5,sound.hdf5,sound}] [--write-num-frames WRITE_NUM_FRAMES] [--compress COMPRESS] [--compression-method COMPRESSION_METHOD] [--preprocess-conf PREPROCESS_CONF] rspecifier wspecifier\"]},\"523\":{\"h\":\"Positional Arguments\"},\"524\":{\"h\":\"Named Arguments\"},\"525\":{\"h\":\"dump-pcm.py\",\"t\":[\"<!-- _dump-pcm.py -->\",\"dump PCM files from a WAV scp file\",\"usage: dump-pcm.py [-h] [--write-num-frames WRITE_NUM_FRAMES] [--filetype {mat,hdf5,sound.hdf5,sound}] [--format FORMAT] [--compress COMPRESS] [--compression-method COMPRESSION_METHOD] [--verbose VERBOSE] [--normalize {1,16,24,32}] [--preprocess-conf PREPROCESS_CONF] [--keep-length KEEP_LENGTH] [--segments SEGMENTS] rspecifier wspecifier\"]},\"526\":{\"h\":\"Positional Arguments\"},\"527\":{\"h\":\"Named Arguments\"},\"528\":{\"h\":\"eval-source-separation.py\",\"t\":[\"<!-- _eval-source-separation.py -->\",\"Evaluate enhanced speech. e.g. /hdd/doc/vuepress/espnet_page/tools/venv/bin/sphinx-build –ref ref.scp –enh enh.scp –outdir outputdiror /hdd/doc/vuepress/espnet_page/tools/venv/bin/sphinx-build –ref ref.scp ref2.scp –enh enh.scp enh2.scp –outdir outputdir\",\"usage: eval-source-separation.py [-h] [--verbose VERBOSE] --ref REFFILES [REFFILES ...] --enh ENHFILES [ENHFILES ...] --outdir OUTDIR [--keylist KEYLIST] [--evaltypes {SDR,STOI,ESTOI,PESQ} [{SDR,STOI,ESTOI,PESQ} ...]] [--permutation PERMUTATION] [--bss-eval-images BSS_EVAL_IMAGES] [--bss-eval-version {v3,v4}]\"]},\"529\":{\"h\":\"Named Arguments\"},\"530\":{\"h\":\"eval_perm_free_error.py\",\"t\":[\"<!-- _eval_perm_free_error.py -->\",\"evaluate permutation-free error\",\"usage: eval_perm_free_error.py [-h] [--num-spkrs NUM_SPKRS] results [results ...]\"]},\"531\":{\"h\":\"Positional Arguments\"},\"532\":{\"h\":\"Named Arguments\"},\"533\":{\"h\":\"feat-to-shape.py\",\"t\":[\"<!-- _feat-to-shape.py -->\",\"convert feature to its shape\",\"usage: feat-to-shape.py [-h] [--verbose VERBOSE] [--filetype {mat,hdf5,sound.hdf5,sound}] [--preprocess-conf PREPROCESS_CONF] rspecifier [out]\"]},\"534\":{\"h\":\"Positional Arguments\"},\"535\":{\"h\":\"Named Arguments\"},\"536\":{\"h\":\"feats2npy.py\",\"t\":[\"<!-- _feats2npy.py -->\",\"Convet kaldi-style features to numpy arrays\",\"usage: feats2npy.py [-h] scp_file out_dir\"]},\"537\":{\"h\":\"Positional Arguments\"},\"538\":{\"h\":\"filt.py\",\"t\":[\"<!-- _filt.py -->\",\"filter words in a text file\",\"usage: filt.py [-h] [--exclude] filt infile\"]},\"539\":{\"h\":\"Positional Arguments\"},\"540\":{\"h\":\"Named Arguments\"},\"541\":{\"h\":\"generate_wav_from_fbank.py\",\"t\":[\"<!-- _generate_wav_from_fbank.py -->\",\"generate wav from FBANK using wavenet vocoder\",\"usage: generate_wav_from_fbank.py [-h] [--fs FS] [--n_fft N_FFT] [--n_shift N_SHIFT] [--model MODEL] [--filetype {mat,hdf5}] rspecifier outdir\"]},\"542\":{\"h\":\"Positional Arguments\"},\"543\":{\"h\":\"Named Arguments\"},\"544\":{\"h\":\"get_yaml.py\",\"t\":[\"<!-- _get_yaml.py -->\",\"get a specified attribute from a YAML file\",\"usage: get_yaml.py [-h] inyaml attr\"]},\"545\":{\"h\":\"Positional Arguments\"},\"546\":{\"h\":\"json2sctm.py\",\"t\":[\"<!-- _json2sctm.py -->\",\"convert json to sctm\",\"usage: json2sctm.py [-h] [--num-spkrs [NUM_SPKRS]] [--refs [REFS [REFS ...]]] [--hyps [HYPS [HYPS ...]]] [--orig-stm [ORIG_STM]] [--stm STM [STM ...]] [--ctm CTM [CTM ...]] [--bpe [BPE]] [json] dict\"]},\"547\":{\"h\":\"Positional Arguments\"},\"548\":{\"h\":\"Named Arguments\"},\"549\":{\"h\":\"json2text.py\",\"t\":[\"<!-- _json2text.py -->\",\"convert ASR recognized json to text\",\"usage: json2text.py [-h] json dict ref hyp\"]},\"550\":{\"h\":\"Positional Arguments\"},\"551\":{\"h\":\"json2trn.py\",\"t\":[\"<!-- _json2trn.py -->\",\"convert a json to a transcription file with a token dictionary\",\"usage: json2trn.py [-h] [--num-spkrs NUM_SPKRS] [--refs REFS [REFS ...]] [--hyps HYPS [HYPS ...]] json dict\"]},\"552\":{\"h\":\"Positional Arguments\"},\"553\":{\"h\":\"Named Arguments\"},\"554\":{\"h\":\"json2trn_mt.py\",\"t\":[\"<!-- _json2trn_mt.py -->\",\"convert json to machine translation transcription\",\"usage: json2trn_mt.py [-h] [--refs REFS [REFS ...]] [--hyps HYPS [HYPS ...]] [--srcs SRCS [SRCS ...]] [--dict-src [DICT_SRC]] json dict\"]},\"555\":{\"h\":\"Positional Arguments\"},\"556\":{\"h\":\"Named Arguments\"},\"557\":{\"h\":\"json2trn_wo_dict.py\",\"t\":[\"<!-- _json2trn_wo_dict.py -->\",\"convert a json to a transcription file with a token dictionary\",\"usage: json2trn_wo_dict.py [-h] [--num-spkrs NUM_SPKRS] [--refs REFS [REFS ...]] [--hyps HYPS [HYPS ...]] json\"]},\"558\":{\"h\":\"Positional Arguments\"},\"559\":{\"h\":\"Named Arguments\"},\"560\":{\"h\":\"make_pair_json.py\",\"t\":[\"<!-- _make_pair_json.py -->\",\"Merge source and target data.json files into one json file.\",\"usage: make_pair_json.py [-h] [--src-json SRC_JSON] [--trg-json TRG_JSON] [--num_utts NUM_UTTS] [--verbose VERBOSE] [--out OUT]\"]},\"561\":{\"h\":\"Named Arguments\"},\"562\":{\"h\":\"mcd_calculate.py\",\"t\":[\"<!-- _mcd_calculate.py -->\",\"calculate MCD.\",\"usage: mcd_calculate.py [-h] --wavdir WAVDIR --gtwavdir GTWAVDIR [--mcep_dim MCEP_DIM] [--mcep_alpha MCEP_ALPHA] [--fftl FFTL] [--shiftms SHIFTMS] --f0min F0MIN --f0max F0MAX [--n_jobs N_JOBS]\"]},\"563\":{\"h\":\"Named Arguments\"},\"564\":{\"h\":\"merge_scp2json.py\",\"t\":[\"<!-- _merge_scp2json.py -->\",\"Given each file paths with such format as <key>:<file>:<type>. type> can be omitted and the default is “str”. e.g. /hdd/doc/vuepress/espnet_page/tools/venv/bin/sphinx-build –input-scps feat:data/feats.scp shape:data/utt2feat_shape:shape –input-scps feat:data/feats2.scp shape:data/utt2feat2_shape:shape –output-scps text:data/text shape:data/utt2text_shape:shape –scps utt2spk:data/utt2spk\",\"usage: merge_scp2json.py [-h] [--input-scps [INPUT_SCPS [INPUT_SCPS ...]]] [--output-scps [OUTPUT_SCPS [OUTPUT_SCPS ...]]] [--scps SCPS [SCPS ...]] [--verbose VERBOSE] [--allow-one-column ALLOW_ONE_COLUMN] [--out OUT]\"]},\"565\":{\"h\":\"Named Arguments\"},\"566\":{\"h\":\"mergejson.py\",\"t\":[\"<!-- _mergejson.py -->\",\"merge json files\",\"usage: mergejson.py [-h] [--input-jsons INPUT_JSONS [INPUT_JSONS ...]] [--output-jsons OUTPUT_JSONS [OUTPUT_JSONS ...]] [--jsons JSONS [JSONS ...]] [--verbose VERBOSE] [-O OUTPUT]\"]},\"567\":{\"h\":\"Named Arguments\"},\"568\":{\"h\":\"mix-mono-wav-scp.py\",\"t\":[\"<!-- _mix-mono-wav-scp.py -->\",\"Mixing wav.scp files into a multi-channel wav.scp using sox.\",\"usage: mix-mono-wav-scp.py [-h] scp [scp ...] [out]\"]},\"569\":{\"h\":\"Positional Arguments\"},\"570\":{\"h\":\"result2json.py\",\"t\":[\"<!-- _result2json.py -->\",\"convert sclite’s result.txt file to json\",\"usage: result2json.py [-h] [--key KEY]\"]},\"571\":{\"h\":\"Named Arguments\"},\"572\":{\"h\":\"score_lang_id.py\",\"t\":[\"<!-- _score_lang_id.py -->\",\"language identification scoring\",\"usage: score_lang_id.py [-h] --ref REF --hyp HYP [--out OUT]\"]},\"573\":{\"h\":\"Named Arguments\"},\"574\":{\"h\":\"scp2json.py\",\"t\":[\"<!-- _scp2json.py -->\",\"convert scp to json\",\"usage: scp2json.py [-h] [--key KEY]\"]},\"575\":{\"h\":\"Named Arguments\"},\"576\":{\"h\":\"splitjson.py\",\"t\":[\"<!-- _splitjson.py -->\",\"split a json file for parallel processing\",\"usage: splitjson.py [-h] [--parts PARTS] json\"]},\"577\":{\"h\":\"Positional Arguments\"},\"578\":{\"h\":\"Named Arguments\"},\"579\":{\"h\":\"text2token.py\",\"t\":[\"<!-- _text2token.py -->\",\"convert raw text to tokenized text\",\"usage: text2token.py [-h] [--nchar NCHAR] [--skip-ncols SKIP_NCOLS] [--space SPACE] [--non-lang-syms NON_LANG_SYMS] [--trans_type {char,phn}] [text]\"]},\"580\":{\"h\":\"Positional Arguments\"},\"581\":{\"h\":\"Named Arguments\"},\"582\":{\"h\":\"text2vocabulary.py\",\"t\":[\"<!-- _text2vocabulary.py -->\",\"create a vocabulary file from text files\",\"usage: text2vocabulary.py [-h] [--output OUTPUT] [--cutoff CUTOFF] [--vocabsize VOCABSIZE] [text_files [text_files ...]]\"]},\"583\":{\"h\":\"Positional Arguments\"},\"584\":{\"h\":\"Named Arguments\"},\"585\":{\"h\":\"trim_silence.py\",\"t\":[\"<!-- _trim_silence.py -->\",\"Trim slience with simple power thresholding and make segments file.\",\"usage: trim_silence.py [-h] [--fs FS] [--threshold THRESHOLD] [--win_length WIN_LENGTH] [--shift_length SHIFT_LENGTH] [--min_silence MIN_SILENCE] [--figdir FIGDIR] [--verbose VERBOSE] [--normalize {1,16,24,32}] rspecifier wspecifier\"]},\"586\":{\"h\":\"Positional Arguments\"},\"587\":{\"h\":\"Named Arguments\"},\"588\":{\"h\":\"trn2ctm.py\",\"t\":[\"<!-- _trn2ctm.py -->\",\"convert trn to ctm\",\"usage: trn2ctm.py [-h] [trn] [ctm]\"]},\"589\":{\"h\":\"Positional Arguments\"},\"590\":{\"h\":\"trn2stm.py\",\"t\":[\"<!-- _trn2stm.py -->\",\"convert trn to stm\",\"usage: trn2stm.py [-h] [--orig-stm [ORIG_STM]] [trn] [stm]\"]},\"591\":{\"h\":\"Positional Arguments\"},\"592\":{\"h\":\"Named Arguments\"},\"593\":{\"h\":\"espnet.distributed.pytorch_backend.launch.MainProcessError\",\"t\":[\"class espnet.distributed.pytorch_backend.launch.MainProcessError(*, signal_no)\",\"Bases: ProcessError\",\"An error happened from main process.\",\"Initialize error class.\",\"property signal_no\",\"Return signal number which stops main process.\"]},\"594\":{\"h\":\"espnet.distributed.pytorch_backend.launch.WorkerError\",\"t\":[\"class espnet.distributed.pytorch_backend.launch.WorkerError(*, msg, exitcode, worker_id)\",\"Bases: ProcessError\",\"An error happened within each worker.\",\"Initialize error class.\",\"property exitcode\",\"Return exitcode from worker process.\",\"property worker_id\",\"Return worker ID related to a process causes this error.\"]},\"595\":{\"h\":\"espnet.distributed.pytorch_backend.launch.free_port\",\"t\":[\"espnet.distributed.pytorch_backend.launch.free_port()\",\"Find free port using bind().\",\"There are some interval between finding this port and using it and the other process might catch the port by that time. Thus it is not guaranteed that the port is really empty.\"]},\"596\":{\"h\":\"espnet.distributed.pytorch_backend.launch.launch\",\"t\":[\"espnet.distributed.pytorch_backend.launch.launch(func, args, nprocs, master_addr='localhost', master_port=None)\",\"Launch processes with a given function and given arguments.\"]},\"597\":{\"h\":\"NOTE\",\"t\":[\"Current implementaiton supports only single node case.\"]},\"598\":{\"h\":\"espnet.distributed.pytorch_backend.launch.set_start_method\",\"t\":[\"espnet.distributed.pytorch_backend.launch.set_start_method(method)\",\"Set multiprocess start method.\"]},\"599\":{\"h\":\"espnet.mt.pytorch_backend.mt.CustomConverter\",\"t\":[\"class espnet.mt.pytorch_backend.mt.CustomConverter\",\"Bases: object\",\"Custom batch converter for Pytorch.\",\"Construct a CustomConverter object.\"]},\"600\":{\"h\":\"espnet.mt.mt_utils.add_results_to_json\",\"t\":[\"<!-- _espnet.mt.mt_utils.add_results_to_json -->\",\"espnet.mt.mt_utils.add_results_to_json(js, nbest_hyps, char_list)\",\"Add N-best results to json.\",\"Parameters:\",\"js (dict) – groundtruth utterance dict\",\"nbest_hyps (list) – list of hypothesis\",\"char_list (list) – list of characters\",\"Returns: N-best results added utterance dict\"]},\"601\":{\"h\":\"espnet.mt.mt_utils.parse_hypothesis\",\"t\":[\"<!-- _espnet.mt.mt_utils.parse_hypothesis -->\",\"espnet.mt.mt_utils.parse_hypothesis(hyp, char_list)\",\"Parse hypothesis.\",\"Parameters:\",\"hyp (list) – recognition hypothesis\",\"char_list (list) – list of characters\",\"Returns: recognition text string\",\"Returns: recognition token string\",\"Returns: recognition tokenid string\"]},\"602\":{\"h\":\"espnet.mt.pytorch_backend.mt.train\",\"t\":[\"<!-- _espnet.mt.pytorch_backend.mt.train -->\",\"espnet.mt.pytorch_backend.mt.train(args)\",\"Train with the given args.\",\"Parameters:args (namespace) – The program arguments.\"]},\"603\":{\"h\":\"espnet.mt.pytorch_backend.mt.trans\",\"t\":[\"<!-- _espnet.mt.pytorch_backend.mt.trans -->\",\"espnet.mt.pytorch_backend.mt.trans(args)\",\"Decode with the given args.\",\"Parameters:args (namespace) – The program arguments.\"]},\"604\":{\"h\":\"espnet.lm.chainer_backend.lm.BPTTUpdater\",\"t\":[\"<!-- _espnet.lm.chainer_backend.lm.BPTTUpdater -->\",\"class espnet.lm.chainer_backend.lm.BPTTUpdater(train_iter, optimizer, schedulers, device, accum_grad)\",\"Bases: StandardUpdater\",\"An updater for a chainer LM\",\":param chainer.dataset.Iterator train_iter : The train iterator :param optimizer: :param schedulers: :param int device : The device id :param int accum_grad :\",\"update_core()\"]},\"605\":{\"h\":\"espnet.lm.chainer_backend.lm.ClassifierWithState\",\"t\":[\"class espnet.lm.chainer_backend.lm.ClassifierWithState(predictor, lossfun=<function softmax_cross_entropy>, label_key=-1)\",\"Bases: Chain\",\"A wrapper for a chainer RNNLM\",\":param link.Chain predictor : The RNNLM :param function lossfun: The loss function to use :param int/str label_key:\",\"final(state)\",\"Predict final log probabilities for given state using the predictor\",\":param state : the state :return log probability vector :rtype cupy/numpy array\",\"predict(state, x)\",\"Predict log probabilities for given state and input x using the predictor\",\":param state : the state :param x : the input :return a tuple (state, log prob vector) :rtype cupy/numpy array\"]},\"606\":{\"h\":\"espnet.lm.chainer_backend.lm.DefaultRNNLM\",\"t\":[\"<!-- _espnet.lm.chainer_backend.lm.DefaultRNNLM -->\",\"class espnet.lm.chainer_backend.lm.DefaultRNNLM(**links: Link)\",\"Bases: LMInterface, Chain\",\"Default RNNLM wrapper to compute reduce framewise loss values.\",\"Parameters:\",\"n_vocab (int) – The size of the vocabulary\",\"args (argparse.Namespace) – configurations. see add_arguments\",\"static add_arguments(parser)\",\"Add arguments to command line argument parser.\"]},\"607\":{\"h\":\"espnet.lm.chainer_backend.lm.LMEvaluator\",\"t\":[\"<!-- _espnet.lm.chainer_backend.lm.LMEvaluator -->\",\"class espnet.lm.chainer_backend.lm.LMEvaluator(val_iter, eval_model, device)\",\"Bases: BaseEvaluator\",\"A custom evaluator for a chainer LM\",\":param chainer.dataset.Iterator val_iter : The validation iterator :param eval_model : The model to evaluate :param int device : The device id to use\",\"evaluate()\",\"Evaluates the model and returns a result dictionary.\",\"This method runs the evaluation loop over the validation dataset. It accumulates the reported values to DictSummary and returns a dictionary whose values are means computed by the summary.\",\"Note that this function assumes that the main iterator raises StopIteration or code in the evaluation loop raises an exception. So, if this assumption is not held, the function could be caught in an infinite loop.\",\"Users can override this method to customize the evaluation routine.\"]},\"608\":{\"h\":\"NOTE\",\"t\":[\"This method encloses eval_func calls with function.no_backprop_mode() context, so all calculations using FunctionNodes inside eval_func do not make computational graphs. It is for reducing the memory consumption.\",\"Returns: Result dictionary. This dictionary is further reported via report() without specifying any observer.\",\"Return type: dict\"]},\"609\":{\"h\":\"espnet.lm.chainer_backend.extlm.LookAheadWordLM\",\"t\":[\"class espnet.lm.chainer_backend.extlm.LookAheadWordLM(wordlm, word_dict, subword_dict, oov_penalty=0.0001, open_vocab=True)\",\"Bases: Chain\",\"final(state)\",\"logzero = -10000000000.0\",\"zero = 1e-10\"]},\"610\":{\"h\":\"espnet.lm.lm_utils.MakeSymlinkToBestModel\",\"t\":[\"<!-- _espnet.lm.lm_utils.MakeSymlinkToBestModel -->\",\"class espnet.lm.lm_utils.MakeSymlinkToBestModel(key, prefix='model', suffix='best')\",\"Bases: Extension\",\"Extension that makes a symbolic link to the best model\",\"Parameters:\",\"key (str) – Key of value\",\"prefix (str) – Prefix of model files and link target\",\"suffix (str) – Suffix of link target\",\"serialize(serializer)\",\"Serializes the extension state.\",\"It is called when a trainer that owns this extension is serialized. It serializes nothing by default.\"]},\"611\":{\"h\":\"espnet.lm.chainer_backend.extlm.MultiLevelLM\",\"t\":[\"class espnet.lm.chainer_backend.extlm.MultiLevelLM(wordlm, subwordlm, word_dict, subword_dict, subwordlm_weight=0.8, oov_penalty=1.0, open_vocab=True)\",\"Bases: Chain\",\"final(state)\",\"logzero = -10000000000.0\",\"zero = 1e-10\"]},\"612\":{\"h\":\"espnet.lm.lm_utils.ParallelSentenceIterator\",\"t\":[\"class espnet.lm.lm_utils.ParallelSentenceIterator(dataset, batch_size, max_length=0, sos=0, eos=0, repeat=True, shuffle=True)\",\"Bases: Iterator\",\"Dataset iterator to create a batch of sentences.\",\"This iterator returns a pair of sentences, where one token is shifted between the sentences like ‘<sos> w1 w2 w3’ and ‘w1 w2 w3 <eos>’ Sentence batches are made in order of longer sentences, and then randomly shuffled.\",\"property epoch_detail\",\"property previous_epoch_detail\",\"serialize(serializer)\",\"Serializes the internal state of the iterator.\",\"This is a method to support the serializer protocol of Chainer.\"]},\"613\":{\"h\":\"NOTE\",\"t\":[\"It should only serialize the internal state that changes over the iteration. It should not serialize what is set manually by users such as the batch size.\",\"start_shuffle()\"]},\"614\":{\"h\":\"espnet.lm.chainer_backend.lm.RNNLM\",\"t\":[\"<!-- _espnet.lm.chainer_backend.lm.RNNLM -->\",\"class espnet.lm.chainer_backend.lm.RNNLM(n_vocab, n_layers, n_units, typ='lstm')\",\"Bases: Chain\",\"A chainer RNNLM\",\"Parameters:\",\"n_vocab (int) – The size of the vocabulary\",\"n_layers (int) – The number of layers to create\",\"n_units (int) – The number of units per layer\",\"type (str) – The RNN type\"]},\"615\":{\"h\":\"espnet.lm.pytorch_backend.lm.Reporter\",\"t\":[\"<!-- _espnet.lm.pytorch_backend.lm.Reporter -->\",\"class espnet.lm.pytorch_backend.lm.Reporter(**links: Link)\",\"Bases: Chain\",\"Dummy module to use chainer’s trainer.\",\"report(loss)\",\"Report nothing.\"]},\"616\":{\"h\":\"espnet.lm.lm_utils.compute_perplexity\",\"t\":[\"<!-- _espnet.lm.lm_utils.compute_perplexity -->\",\"espnet.lm.lm_utils.compute_perplexity(result)\",\"Computes and add the perplexity to the LogReport\",\"Parameters:result (dict) – The current observations\"]},\"617\":{\"h\":\"espnet.lm.pytorch_backend.lm.concat_examples\",\"t\":[\"espnet.lm.pytorch_backend.lm.concat_examples(batch, device=None, padding=None)\",\"Concat examples in minibatch.\",\"Parameters:\",\"batch (np.ndarray) – The batch to concatenate\",\"device (int) – The device to send to\",\"padding (Tuple *[*int *,*int]) – The padding to use\",\"Returns: (inputs, targets)\",\":rtype (torch.Tensor, torch.Tensor)\"]},\"618\":{\"h\":\"espnet.lm.lm_utils.count_tokens\",\"t\":[\"<!-- _espnet.lm.lm_utils.count_tokens -->\",\"espnet.lm.lm_utils.count_tokens(data, unk_id=None)\",\"Count tokens and oovs in token ID sequences.\",\"Parameters:\",\"data (list *[*np.ndarray]) – list of token ID sequences\",\"unk_id (int) – ID of unknown token\",\"Returns: tuple of number of token occurrences and number of oov tokens\",\"Return type: tuple\"]},\"619\":{\"h\":\"espnet.lm.lm_utils.load_dataset\",\"t\":[\"<!-- _espnet.lm.lm_utils.load_dataset -->\",\"espnet.lm.lm_utils.load_dataset(path, label_dict, outdir=None)\",\"Load and save HDF5 that contains a dataset and stats for LM\",\"Parameters:\",\"path (str) – The path of an input text dataset file\",\"label_dict (dict *[*str,int]) – dictionary that maps token label string to its ID number\",\"outdir (str) – The path of an output dir\",\"Returns: Tuple of : token IDs in np.int32 converted by read_tokens the number of tokens by count_tokens, and the number of OOVs by count_tokens\",\"Return type: tuple[list[np.ndarray], int, int]\"]},\"620\":{\"h\":\"espnet.lm.lm_utils.make_lexical_tree\",\"t\":[\"<!-- _espnet.lm.lm_utils.make_lexical_tree -->\",\"espnet.lm.lm_utils.make_lexical_tree(word_dict, subword_dict, word_unk)\",\"Make a lexical tree to compute word-level probabilities\"]},\"621\":{\"h\":\"espnet.lm.lm_utils.read_tokens\",\"t\":[\"<!-- _espnet.lm.lm_utils.read_tokens -->\",\"espnet.lm.lm_utils.read_tokens(filename, label_dict)\",\"Read tokens as a sequence of sentences\",\":param str filename : The name of the input file :param dict label_dict : dictionary that maps token label string to its ID number :return list of ID sequences :rtype list\"]},\"622\":{\"h\":\"espnet.lm.chainer_backend.lm.train\",\"t\":[\"<!-- _espnet.lm.chainer_backend.lm.train -->\",\"espnet.lm.chainer_backend.lm.train(args)\",\"Train with the given args\",\"Parameters:args (Namespace) – The program arguments\"]},\"623\":{\"h\":\"espnet.asr.asr_utils.CompareValueTrigger\",\"t\":[\"<!-- _espnet.asr.asr_utils.CompareValueTrigger -->\",\"class espnet.asr.asr_utils.CompareValueTrigger(key, compare_fn, trigger=(1, 'epoch'))\",\"Bases: object\",\"Trigger invoked when key value getting bigger or lower than before.\",\"Parameters:\",\"key (str) – Key of value.\",\"compare_fn ( *(*float,float)-> bool) – Function to compare the values.\",\"trigger (tuple *(*int,str)) – Trigger that decide the comparison interval.\"]},\"624\":{\"h\":\"espnet.asr.pytorch_backend.asr.CustomConverter\",\"t\":[\"class espnet.asr.pytorch_backend.asr.CustomConverter(subsampling_factor=1, dtype=torch.float32)\",\"Bases: object\",\"Custom batch converter for Pytorch.\",\"Parameters:\",\"subsampling_factor (int) – The subsampling factor.\",\"dtype (torch.dtype) – Data type to convert.\",\"Construct a CustomConverter object.\"]},\"625\":{\"h\":\"espnet.asr.pytorch_backend.asr.CustomConverterMulEnc\",\"t\":[\"class espnet.asr.pytorch_backend.asr.CustomConverterMulEnc(subsampling_factors=[1, 1], dtype=torch.float32)\",\"Bases: object\",\"Custom batch converter for Pytorch in multi-encoder case.\",\"Parameters:\",\"subsampling_factors (list) – List of subsampling factors for each encoder.\",\"dtype (torch.dtype) – Data type to convert.\",\"Initialize the converter.\"]},\"626\":{\"h\":\"espnet.asr.pytorch_backend.asr.CustomEvaluator\",\"t\":[\"class espnet.asr.pytorch_backend.asr.CustomEvaluator(model, iterator, target, device, ngpu=None, use_ddp=False)\",\"Bases: BaseEvaluator\",\"Custom Evaluator for Pytorch.\",\"Parameters:\",\"model (torch.nn.Module) – The model to evaluate.\",\"iterator (chainer.dataset.Iterator) – The train iterator.\",\"target (link|dict *[*str,link]) – Link object or a dictionary of links to evaluate. If this is just a link object, the link is registered by the name 'main'.\",\"device (torch.device) – The device used.\",\"ngpu (int) – The number of GPUs.\",\"use_ddp (bool) – The flag to use DDP.\",\"evaluate()\",\"Main evaluate routine for CustomEvaluator.\"]},\"627\":{\"h\":\"espnet.asr.pytorch_backend.asr.CustomUpdater\",\"t\":[\"class espnet.asr.pytorch_backend.asr.CustomUpdater(model, grad_clip_threshold, train_iter, optimizer, device, ngpu, grad_noise=False, accum_grad=1, use_apex=False, use_ddp=False)\",\"Bases: StandardUpdater\",\"Custom Updater for Pytorch.\",\"Parameters:\",\"model (torch.nn.Module) – The model to update.\",\"grad_clip_threshold (float) – The gradient clipping value to use.\",\"train_iter (chainer.dataset.Iterator) – The training iterator.\",\"optimizer (torch.optim.optimizer) – The training optimizer.\",\"device (torch.device) – The device to use.\",\"ngpu (int) – The number of gpus to use.\",\"use_apex (bool) – The flag to use Apex in backprop.\",\"use_ddp (bool) – The flag to use DDP for multi-GPU training.\",\"update()\",\"Updates the parameters of the target model.\",\"This method implements an update formula for the training task, including data loading, forward/backward computations, and actual updates of parameters.\",\"This method is called once at each iteration of the training loop.\",\"update_core()\",\"Main update routine of the CustomUpdater.\"]},\"628\":{\"h\":\"espnet.asr.pytorch_backend.asr.DistributedDictSummary\",\"t\":[\"class espnet.asr.pytorch_backend.asr.DistributedDictSummary(device=None)\",\"Bases: object\",\"Distributed version of DictSummary.\",\"This implementation is based on an official implementation below. https://github.com/chainer/chainer/blob/v6.7.0/chainer/reporter.py\",\"To gather stats information from all processes and calculate exact mean values, this class is running AllReduce operation in compute_mean().\",\"add(d)\",\"compute_mean()\"]},\"629\":{\"h\":\"espnet.asr.asr_mix_utils.PlotAttentionReport\",\"t\":[\"class espnet.asr.asr_mix_utils.PlotAttentionReport(att_vis_fn, data, outdir, converter, device, reverse=False)\",\"Bases: Extension\",\"Plot attention reporter.\",\"Parameters:\",\"att_vis_fn (espnet.nets.*_backend.e2e_asr.calculate_all_attentions) – Function of attention visualization.\",\"data (list *[*tuple *(*str,dict *[*str,dict *[*str,Any]])]) – List json utt key items.\",\"outdir (str) – Directory to save figures.\",\"converter (espnet.asr.*_backend.asr.CustomConverter) – CustomConverter object. Function to convert data.\",\"device (torch.device) – The destination device to send tensor.\",\"reverse (bool) – If True, input and output length are reversed.\",\"Initialize PlotAttentionReport.\",\"draw_attention_plot(att_w)\",\"Visualize attention weights matrix.\",\"Parameters:att_w (Tensor) – Attention weight matrix.\",\"Returns: pyplot object with attention matrix image.\",\"Return type: matplotlib.pyplot\",\"get_attention_weight(idx, att_w, spkr_idx)\",\"Transform attention weight in regard to self.reverse.\",\"get_attention_weights()\",\"Return attention weights.\",\"Returns: attention weights. It’s shape would be : differ from bachend.dtype=float * pytorch-> 1) multi-head case => (B, H, Lmax, Tmax). 2) \",\"other case => (B, Lmax, Tmax).\",\"chainer-> attention weights (B, Lmax, Tmax).\",\"Return type: arr_ws_sd (numpy.ndarray)\",\"log_attentions(logger, step)\",\"Add image files of attention matrix to tensorboard.\"]},\"630\":{\"h\":\"espnet.asr.asr_utils.adadelta_eps_decay\",\"t\":[\"<!-- _espnet.asr.asr_utils.adadelta_eps_decay -->\",\"espnet.asr.asr_utils.adadelta_eps_decay(eps_decay)\",\"Extension to perform adadelta eps decay.\",\"Parameters:eps_decay (float) – Decay rate of eps.\",\"Returns: An extension function.\"]},\"631\":{\"h\":\"espnet.asr.asr_utils.adam_lr_decay\",\"t\":[\"<!-- _espnet.asr.asr_utils.adam_lr_decay -->\",\"espnet.asr.asr_utils.adam_lr_decay(eps_decay)\",\"Extension to perform adam lr decay.\",\"Parameters:eps_decay (float) – Decay rate of lr.\",\"Returns: An extension function.\"]},\"632\":{\"h\":\"espnet.asr.asr_utils.add_gradient_noise\",\"t\":[\"<!-- _espnet.asr.asr_utils.add_gradient_noise -->\",\"espnet.asr.asr_utils.add_gradient_noise(model, iteration, duration=100, eta=1.0, scale_factor=0.55)\",\"Adds noise from a standard normal distribution to the gradients.\",\"The standard deviation (sigma) is controlled by the three hyper-parameters below. sigma goes to zero (no noise) with more iterations.\",\"Parameters:\",\"model (torch.nn.model) – Model.\",\"iteration (int) – Number of iterations.\",\"duration (int) – Number of durations to control the interval of the sigma change.\",\"eta (float) – The magnitude of sigma.\",\"scale_factor (float) – The scale of sigma.\"]},\"633\":{\"h\":\"espnet.asr.asr_utils.add_results_to_json\",\"t\":[\"<!-- _espnet.asr.asr_utils.add_results_to_json -->\",\"espnet.asr.asr_utils.add_results_to_json(js, nbest_hyps, char_list)\",\"Add N-best results to json.\",\"Parameters:\",\"js (dict *[*str,Any]) – Groundtruth utterance dict.\",\"nbest_hyps_sd (list *[*dict *[*str,Any]]) – List of hypothesis for multi_speakers: nutts x nspkrs.\",\"char_list (list *[*str]) – List of characters.\",\"Returns: N-best results added utterance dict.\",\"Return type: dict[str, Any]\"]},\"634\":{\"h\":\"espnet.asr.asr_utils.chainer_load\",\"t\":[\"<!-- _espnet.asr.asr_utils.chainer_load -->\",\"espnet.asr.asr_utils.chainer_load(path, model)\",\"Load chainer model parameters.\",\"Parameters:\",\"path (str) – Model path or snapshot file path to be loaded.\",\"model (chainer.Chain) – Chainer model.\"]},\"635\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.create_transducer_compatible_state_dict\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.create_transducer_compatible_state_dict(model_state_dict, encoder_type, encoder_units)\",\"Create a compatible transducer model state dict for transfer learning.\",\"If RNN encoder modules from a non-Transducer model are found in the pre-trained model state dict, the corresponding modules keys are renamed for compatibility.\",\"Parameters:\",\"model_state_dict (Dict) – Pre-trained model state dict\",\"encoder_type (str) – Type of pre-trained encoder.\",\"encoder_units (int) – Number of encoder units in pre-trained model.\",\"Returns: Transducer compatible pre-trained model state dict.\",\"Return type: new_state_dict (Dict)\"]},\"636\":{\"h\":\"espnet.asr.pytorch_backend.asr.enhance\",\"t\":[\"<!-- _espnet.asr.pytorch_backend.asr.enhance -->\",\"espnet.asr.pytorch_backend.asr.enhance(args)\",\"Dumping enhanced speech and mask.\",\"Parameters:args (namespace) – The program arguments.\"]},\"637\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.filter_modules\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.filter_modules(model_state_dict, modules)\",\"Filter non-matched modules in model state dict.\",\"Parameters:\",\"model_state_dict (Dict) – Pre-trained model state dict.\",\"modules (List) – Specified module(s) to transfer.\",\"Returns: Filtered module list.\",\"Return type: new_mods (List)\"]},\"638\":{\"h\":\"espnet.asr.asr_utils.format_mulenc_args\",\"t\":[\"<!-- _espnet.asr.asr_utils.format_mulenc_args -->\",\"espnet.asr.asr_utils.format_mulenc_args(args)\",\"Format args for multi-encoder setup.\",\"It deals with following situations: (when args.num_encs=2):\",\"args.elayers = None -> args.elayers = [4, 4];\",\"args.elayers = 4 -> args.elayers = [4, 4];\",\"args.elayers = [4, 4, 4] -> args.elayers = [4, 4].\"]},\"639\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.freeze_modules\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.freeze_modules(model, modules)\",\"Freeze model parameters according to modules list.\",\"Parameters:\",\"model (torch.nn.Module) – Main model.\",\"modules (List) – Specified module(s) to freeze.\",\"Returns: Updated main model. model_params (filter): Filtered model parameters.\",\"Return type: model (torch.nn.Module)\"]},\"640\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.get_lm_state_dict\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.get_lm_state_dict(lm_state_dict)\",\"Create compatible ASR decoder state dict from LM state dict.\",\"Parameters:lm_state_dict (Dict) – Pre-trained LM state dict.\",\"Returns: State dict with compatible key names.\",\"Return type: new_state_dict (Dict)\"]},\"641\":{\"h\":\"espnet.asr.asr_utils.get_model_conf\",\"t\":[\"<!-- _espnet.asr.asr_utils.get_model_conf -->\",\"espnet.asr.asr_utils.get_model_conf(model_path, conf_path=None)\",\"Get model config information by reading a model config file (model.json).\",\"Parameters:\",\"model_path (str) – Model path.\",\"conf_path (str) – Optional model config path.\",\"Returns: Config information loaded from json file.\",\"Return type: list[int, int, dict[str, Any]]\"]},\"642\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.get_partial_state_dict\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.get_partial_state_dict(model_state_dict, modules)\",\"Create state dict with specified modules matching input model modules.\",\"Parameters:\",\"model_state_dict (Dict) – Pre-trained model state dict.\",\"modules (Dict) – Specified module(s) to transfer.\",\"Returns: State dict with specified modules weights.\",\"Return type: new_state_dict (Dict)\"]},\"643\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.get_trained_model_state_dict\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.get_trained_model_state_dict(model_path, new_is_transducer)\",\"Extract the trained model state dict for pre-initialization.\",\"Parameters:\",\"model_path (str) – Path to trained model.\",\"new_is_transducer (bool) – Whether the new model is Transducer-based.\",\"Returns: Trained model state dict.\",\"Return type: (Dict)\"]},\"644\":{\"h\":\"espnet.asr.pytorch_backend.asr.is_writable_process\",\"t\":[\"espnet.asr.pytorch_backend.asr.is_writable_process(args, worldsize, rank, localrank)\"]},\"645\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.load_trained_model\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.load_trained_model(model_path, training=True)\",\"Load the trained model for recognition.\",\"Parameters:\",\"model_path (str) – Path to model.***.best\",\"training (bool) – Training mode specification for transducer model.\",\"Returns: Trained model. train_args (Namespace): Trained model arguments.\",\"Return type: model (torch.nn.Module)\"]},\"646\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.load_trained_modules\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.load_trained_modules(idim, odim, args, interface=<class 'espnet.nets.asr_interface.ASRInterface'>)\",\"Load ASR/MT/TTS model with pre-trained weights for specified modules.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"Namespace (args) – Model arguments.\",\"interface (ASRInterface|MTInterface|TTSInterface) – Model interface.\",\"Returns: Model with pre-initialized weights.\",\"Return type: main_model (torch.nn.Module)\"]},\"647\":{\"h\":\"espnet.asr.asr_utils.parse_hypothesis\",\"t\":[\"<!-- _espnet.asr.asr_utils.parse_hypothesis -->\",\"espnet.asr.asr_utils.parse_hypothesis(hyp, char_list)\",\"Parse hypothesis.\",\"Parameters:\",\"hyp (list *[*dict *[*str,Any]]) – Recognition hypothesis.\",\"char_list (list *[*str]) – List of characters.\",\"Returns: tuple(str, str, str, float)\"]},\"648\":{\"h\":\"espnet.asr.asr_utils.plot_spectrogram\",\"t\":[\"<!-- _espnet.asr.asr_utils.plot_spectrogram -->\",\"espnet.asr.asr_utils.plot_spectrogram(plt, spec, mode='db', fs=None, frame_shift=None, bottom=True, left=True, right=True, top=False, labelbottom=True, labelleft=True, labelright=True, labeltop=False, cmap='inferno')\",\"Plot spectrogram using matplotlib.\",\"Parameters:\",\"plt (matplotlib.pyplot) – pyplot object.\",\"spec (numpy.ndarray) – Input stft (Freq, Time)\",\"mode (str) – db or linear.\",\"fs (int) – Sample frequency. To convert y-axis to kHz unit.\",\"frame_shift (int) – The frame shift of stft. To convert x-axis to second unit.\",\"bottom (bool) – Whether to draw the respective ticks.\",\"left (bool) –\",\"right (bool) –\",\"top (bool) –\",\"labelbottom (bool) – Whether to draw the respective tick labels.\",\"labelleft (bool) –\",\"labelright (bool) –\",\"labeltop (bool) –\",\"cmap (str) – Colormap defined in matplotlib.\"]},\"649\":{\"h\":\"espnet.asr.chainer_backend.asr.recog\",\"t\":[\"<!-- _espnet.asr.chainer_backend.asr.recog -->\",\"espnet.asr.chainer_backend.asr.recog(args)\",\"Decode with the given args.\",\"Parameters:args (namespace) – The program arguments.\"]},\"650\":{\"h\":\"espnet.asr.pytorch_backend.recog.recog_v2\",\"t\":[\"<!-- _espnet.asr.pytorch_backend.recog.recog_v2 -->\",\"espnet.asr.pytorch_backend.recog.recog_v2(args)\",\"Decode with custom models that implements ScorerInterface.\",\"Notes\",\"The previous backend espnet.asr.pytorch_backend.asr.recog only supports E2E and RNNLM\",\"Parameters:args (namespace) – The program arguments.\",\":param See py:func:espnet.bin.asr_recog.get_parser for details:\"]},\"651\":{\"h\":\"espnet.asr.asr_utils.restore_snapshot\",\"t\":[\"<!-- _espnet.asr.asr_utils.restore_snapshot -->\",\"espnet.asr.asr_utils.restore_snapshot(model, snapshot, load_fn=None)\",\"Extension to restore snapshot.\",\"Returns: An extension function.\"]},\"652\":{\"h\":\"espnet.asr.asr_utils.snapshot_object\",\"t\":[\"<!-- _espnet.asr.asr_utils.snapshot_object -->\",\"espnet.asr.asr_utils.snapshot_object(target, filename)\",\"Returns a trainer extension to take snapshots of a given object.\",\"Parameters:\",\"target (model) – Object to serialize.\",\"filename (str) – Name of the file into which the object is serialized.It can be a format string, where the trainer object is passed to the :meth: str.format method. For example, 'snapshot_{.updater.iteration}' is converted to 'snapshot_10000' at the 10,000th iteration.\",\"Returns: An extension function.\"]},\"653\":{\"h\":\"espnet.asr.asr_utils.torch_load\",\"t\":[\"<!-- _espnet.asr.asr_utils.torch_load -->\",\"espnet.asr.asr_utils.torch_load(path, model)\",\"Load torch model states.\",\"Parameters:\",\"path (str) – Model path or snapshot file path to be loaded.\",\"model (torch.nn.Module) – Torch model.\"]},\"654\":{\"h\":\"espnet.asr.asr_utils.torch_resume\",\"t\":[\"<!-- _espnet.asr.asr_utils.torch_resume -->\",\"espnet.asr.asr_utils.torch_resume(snapshot_path, trainer)\",\"Resume from snapshot for pytorch.\",\"Parameters:\",\"snapshot_path (str) – Snapshot file path.\",\"trainer (chainer.training.Trainer) – Chainer’s trainer instance.\"]},\"655\":{\"h\":\"espnet.asr.asr_utils.torch_save\",\"t\":[\"<!-- _espnet.asr.asr_utils.torch_save -->\",\"espnet.asr.asr_utils.torch_save(path, model)\",\"Save torch model states.\",\"Parameters:\",\"path (str) – Model path to be saved.\",\"model (torch.nn.Module) – Torch model.\"]},\"656\":{\"h\":\"espnet.asr.asr_utils.torch_snapshot\",\"t\":[\"<!-- _espnet.asr.asr_utils.torch_snapshot -->\",\"espnet.asr.asr_utils.torch_snapshot(savefun=<function save>, filename='snapshot.ep.{.updater.epoch}')\",\"Extension to take snapshot of the trainer for pytorch.\",\"Returns: An extension function.\"]},\"657\":{\"h\":\"espnet.asr.chainer_backend.asr.train\",\"t\":[\"<!-- _espnet.asr.chainer_backend.asr.train -->\",\"espnet.asr.chainer_backend.asr.train(args)\",\"Train with the given args.\",\"Parameters:args (namespace) – The program arguments.\"]},\"658\":{\"h\":\"espnet.asr.pytorch_backend.asr_init.transfer_verification\",\"t\":[\"espnet.asr.pytorch_backend.asr_init.transfer_verification(model_state_dict, partial_state_dict, modules)\",\"Verify tuples (key, shape) for input model modules match specified modules.\",\"Parameters:\",\"model_state_dict (Dict) – Main model state dict.\",\"partial_state_dict (Dict) – Pre-trained model state dict.\",\"modules (List) – Specified module(s) to transfer.\",\"Returns: Whether transfer learning is allowed.\",\"Return type: (bool)\"]},\"659\":{\"h\":\"espnet.optimizer.pytorch.AdadeltaFactory\",\"t\":[\"<!-- _espnet.optimizer.pytorch.AdadeltaFactory -->\",\"class espnet.optimizer.pytorch.AdadeltaFactory\",\"Bases: OptimizerFactoryInterface\",\"Adadelta factory.\",\"static add_arguments(parser: ArgumentParser)\",\"Register args.\",\"static from_args(target, args: Namespace)\",\"Initialize optimizer from argparse Namespace.\",\"Parameters:\",\"target – for pytorch model.parameters(), for chainer model\",\"args (argparse.Namespace) – parsed command-line args\"]},\"660\":{\"h\":\"espnet.optimizer.pytorch.AdamFactory\",\"t\":[\"<!-- _espnet.optimizer.pytorch.AdamFactory -->\",\"class espnet.optimizer.pytorch.AdamFactory\",\"Bases: OptimizerFactoryInterface\",\"Adam factory.\",\"static add_arguments(parser: ArgumentParser)\",\"Register args.\",\"static from_args(target, args: Namespace)\",\"Initialize optimizer from argparse Namespace.\",\"Parameters:\",\"target – for pytorch model.parameters(), for chainer model\",\"args (argparse.Namespace) – parsed command-line args\"]},\"661\":{\"h\":\"espnet.optimizer.factory.OptimizerFactoryInterface\",\"t\":[\"class espnet.optimizer.factory.OptimizerFactoryInterface\",\"Bases: object\",\"Optimizer adaptor.\",\"static add_arguments(parser: ArgumentParser)\",\"Register args.\",\"classmethod build(target, **kwargs)\",\"Initialize optimizer with python-level args.\",\"Parameters:target – for pytorch model.parameters(), for chainer model\",\"Returns: new Optimizer\",\"static from_args(target, args: Namespace)\",\"Initialize optimizer from argparse Namespace.\",\"Parameters:\",\"target – for pytorch model.parameters(), for chainer model\",\"args (argparse.Namespace) – parsed command-line args\"]},\"662\":{\"h\":\"espnet.optimizer.pytorch.SGDFactory\",\"t\":[\"<!-- _espnet.optimizer.pytorch.SGDFactory -->\",\"class espnet.optimizer.pytorch.SGDFactory\",\"Bases: OptimizerFactoryInterface\",\"SGD factory.\",\"static add_arguments(parser: ArgumentParser)\",\"Register args.\",\"static from_args(target, args: Namespace)\",\"Initialize optimizer from argparse Namespace.\",\"Parameters:\",\"target – for pytorch model.parameters(), for chainer model\",\"args (argparse.Namespace) – parsed command-line args\"]},\"663\":{\"h\":\"espnet.optimizer.parser.adadelta\",\"t\":[\"<!-- _espnet.optimizer.parser.adadelta -->\",\"espnet.optimizer.parser.adadelta(parser)\",\"Add arguments.\"]},\"664\":{\"h\":\"espnet.optimizer.parser.adam\",\"t\":[\"<!-- _espnet.optimizer.parser.adam -->\",\"espnet.optimizer.parser.adam(parser)\",\"Add arguments.\"]},\"665\":{\"h\":\"espnet.optimizer.factory.dynamic_import_optimizer\",\"t\":[\"espnet.optimizer.factory.dynamic_import_optimizer(name: str, backend: str)\",\"Import optimizer class dynamically.\",\"Parameters:\",\"name (str) – alias name or dynamic import syntax module:class\",\"backend (str) – backend name e.g., chainer or pytorch\",\"Returns: OptimizerFactoryInterface or FunctionalOptimizerAdaptor\"]},\"666\":{\"h\":\"espnet.optimizer.parser.sgd\",\"t\":[\"<!-- _espnet.optimizer.parser.sgd -->\",\"espnet.optimizer.parser.sgd(parser)\",\"Add arguments.\"]},\"667\":{\"h\":\"espnet.scheduler.chainer.ChainerScheduler\",\"t\":[\"<!-- _espnet.scheduler.chainer.ChainerScheduler -->\",\"class espnet.scheduler.chainer.ChainerScheduler(schedulers: List[SchedulerInterface], optimizer: Optimizer)\",\"Bases: object\",\"Chainer optimizer scheduler.\",\"Initialize class.\",\"step(n_iter: int)\",\"Update optimizer by scheduling.\"]},\"668\":{\"h\":\"espnet.scheduler.scheduler.CyclicCosineScheduler\",\"t\":[\"class espnet.scheduler.scheduler.CyclicCosineScheduler(key: str, args: Namespace)\",\"Bases: SchedulerInterface\",\"Cyclic cosine annealing.\",\"Parameters:\",\"cosine_warmup (int) – number of warmup iterations.\",\"cosine_total (int) – number of total annealing iterations.\",\"Notes\",\"Proposed in https://openreview.net/pdf?id=BJYwwY9ll (and https://arxiv.org/pdf/1608.03983.pdf). Used in the GPT2 config of Megatron-LM https://github.com/NVIDIA/Megatron-LM\",\"Initialize class.\",\"alias = 'cosine'\",\"scale(n_iter)\",\"Scale of lr.\"]},\"669\":{\"h\":\"espnet.scheduler.scheduler.NoScheduler\",\"t\":[\"<!-- _espnet.scheduler.scheduler.NoScheduler -->\",\"class espnet.scheduler.scheduler.NoScheduler(key: str, args: Namespace)\",\"Bases: SchedulerInterface\",\"Scheduler which does nothing.\",\"Initialize class.\",\"alias = 'none'\",\"scale(n_iter)\",\"Scale of lr.\"]},\"670\":{\"h\":\"espnet.scheduler.scheduler.NoamScheduler\",\"t\":[\"<!-- _espnet.scheduler.scheduler.NoamScheduler -->\",\"class espnet.scheduler.scheduler.NoamScheduler(key, args)\",\"Bases: SchedulerInterface\",\"Warmup + InverseSqrt decay scheduler.\",\"Parameters:noam_warmup (int) – number of warmup iterations.\",\"Initialize class.\",\"alias = 'noam'\",\"scale(step)\",\"Scale of lr.\"]},\"671\":{\"h\":\"espnet.scheduler.pytorch.PyTorchScheduler\",\"t\":[\"<!-- _espnet.scheduler.pytorch.PyTorchScheduler -->\",\"class espnet.scheduler.pytorch.PyTorchScheduler(schedulers: List[SchedulerInterface], optimizer: Optimizer)\",\"Bases: object\",\"PyTorch optimizer scheduler.\",\"Initialize class.\",\"step(n_iter: int)\",\"Update optimizer by scheduling.\"]},\"672\":{\"h\":\"espnet.scheduler.scheduler.SchedulerInterface\",\"t\":[\"class espnet.scheduler.scheduler.SchedulerInterface(key: str, args: Namespace)\",\"Bases: object\",\"Scheduler interface.\",\"Initialize class.\",\"classmethod add_arguments(key: str, parser: ArgumentParser)\",\"Add arguments for CLI.\",\"alias = ''\",\"classmethod build(key: str, **kwargs)\",\"Initialize this class with python-level args.\",\"Parameters:key (str) – key of hyper parameter\",\"Returns: A new instance of LMInterface.\",\"Return type: LMinterface\",\"get_arg(name)\",\"Get argument without prefix.\",\"scale(n_iter: int)\",\"Scale at n_iter.\",\"Parameters:n_iter (int) – number of current iterations.\",\"Returns: current scale of learning rate.\",\"Return type: float\"]},\"673\":{\"h\":\"espnet.scheduler.scheduler._PrefixParser\",\"t\":[\"<!-- _espnet.scheduler.scheduler._PrefixParser -->\",\"class espnet.scheduler.scheduler._PrefixParser(parser, prefix)\",\"Bases: object\",\"add_argument(name, **kwargs)\"]},\"674\":{\"h\":\"espnet.scheduler.scheduler.dynamic_import_scheduler\",\"t\":[\"espnet.scheduler.scheduler.dynamic_import_scheduler(module)\",\"Import Scheduler class dynamically.\",\"Parameters:module (str) – module_name:class_name or alias in SCHEDULER_DICT\",\"Returns: Scheduler class\",\"Return type: type\"]},\"675\":{\"h\":\"espnet.scheduler.scheduler.register_scheduler\",\"t\":[\"espnet.scheduler.scheduler.register_scheduler(cls)\",\"Register scheduler.\"]},\"676\":{\"h\":\"espnet.nets.asr_interface.ASRInterface\",\"t\":[\"<!-- _espnet.nets.asr_interface.ASRInterface -->\",\"class espnet.nets.asr_interface.ASRInterface\",\"Bases: object\",\"ASR Interface for ESPnet model implementation.\",\"static add_arguments(parser)\",\"Add arguments to parser.\",\"property attention_plot_class\",\"Get attention plot class.\",\"classmethod build(idim: int, odim: int, **kwargs)\",\"Initialize this class with python-level args.\",\"Parameters:\",\"idim (int) – The number of an input feature dim.\",\"odim (int) – The number of output vocab.\",\"Returns: A new instance of ASRInterface.\",\"Return type: ASRinterface\",\"calculate_all_attentions(xs, ilens, ys)\",\"Calculate attention.\",\"Parameters:\",\"xs (list) – list of padded input sequences [(T1, idim), (T2, idim), …]\",\"ilens (ndarray) – batch of lengths of input sequences (B)\",\"ys (list) – list of character id sequence tensor [(L1), (L2), (L3), …]\",\"Returns: attention weights (B, Lmax, Tmax)\",\"Return type: float ndarray\",\"calculate_all_ctc_probs(xs, ilens, ys)\",\"Calculate CTC probability.\",\"Parameters:\",\"xs_pad (list) – list of padded input sequences [(T1, idim), (T2, idim), …]\",\"ilens (ndarray) – batch of lengths of input sequences (B)\",\"ys (list) – list of character id sequence tensor [(L1), (L2), (L3), …]\",\"Returns: CTC probabilities (B, Tmax, vocab)\",\"Return type: float ndarray\",\"property ctc_plot_class\",\"Get CTC plot class.\",\"encode(feat)\",\"Encode feature in beam_search (optional).\",\"Parameters:x (numpy.ndarray) – input feature (T, D)\",\"Returns: encoded feature (T, D)\",\"Return type: torch.Tensor for pytorch, chainer.Variable for chainer\",\"forward(xs, ilens, ys)\",\"Compute loss for training.\",\"Parameters:\",\"xs – For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable\",\"ilens – batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int\",\"ys – For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable\",\"Returns: loss value\",\"Return type: torch.Tensor for pytorch, chainer.Variable for chainer\",\"get_total_subsampling_factor()\",\"Get total subsampling factor.\",\"recognize(x, recog_args, char_list=None, rnnlm=None)\",\"Recognize x for evaluation.\",\"Parameters:\",\"x (ndarray) – input acouctic feature (B, T, D) or (T, D)\",\"recog_args (namespace) – argment namespace contraining options\",\"char_list (list) – list of characters\",\"rnnlm (torch.nn.Module) – language model module\",\"Returns: N-best decoding results\",\"Return type: list\",\"recognize_batch(x, recog_args, char_list=None, rnnlm=None)\",\"Beam search implementation for batch.\",\"Parameters:\",\"x (torch.Tensor) – encoder hidden state sequences (B, Tmax, Henc)\",\"recog_args (namespace) – argument namespace containing options\",\"char_list (list) – list of characters\",\"rnnlm (torch.nn.Module) – language model module\",\"Returns: N-best decoding results\",\"Return type: list\",\"scorers()\",\"Get scorers for beam_search (optional).\",\"Returns: dict of ScorerInterface objects\",\"Return type: dict[str, ScorerInterface]\"]},\"677\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttAdd\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttAdd(eprojs, dunits, att_dim, han_mode=False)\",\"Bases: Module\",\"Additive attention\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0)\",\"AttAdd forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – dummy (does not use)\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"reset states\",\"training : bool\"]},\"678\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttCov\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttCov(eprojs, dunits, att_dim, han_mode=False)\",\"Bases: Module\",\"Coverage mechanism attention\",\"Reference: Get To The Point: Summarization with Pointer-Generator Network : (https://arxiv.org/abs/1704.04368)\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev_list, scaling=2.0)\",\"AttCov forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev_list (list) – list of previous attention weight\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weights\",\"Return type: list\",\"reset()\",\"reset states\",\"training : bool\"]},\"679\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttCovLoc\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttCovLoc(eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Coverage mechanism location aware attention\",\"This attention is a combination of coverage and location-aware attentions.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev_list, scaling=2.0)\",\"AttCovLoc forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev_list (list) – list of previous attention weight\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weights\",\"Return type: list\",\"reset()\",\"reset states\",\"training : bool\"]},\"680\":{\"h\":\"espnet.nets.chainer_backend.rnn.attentions.AttDot\",\"t\":[\"class espnet.nets.chainer_backend.rnn.attentions.AttDot(eprojs, dunits, att_dim)\",\"Bases: Chain\",\"Compute attention based on dot product.\",\"Parameters:\",\"eprojs (int|None) – Dimension of input vectors from encoder.\",\"dunits (int|None) – Dimension of input vectors for decoder.\",\"att_dim (int) – Dimension of input vectors for attention.\",\"reset()\",\"Reset states.\"]},\"681\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttForward\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttForward(eprojs, dunits, att_dim, aconv_chans, aconv_filts)\",\"Bases: Module\",\"Forward attention module.\",\"Reference: Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\",\"(https://arxiv.org/pdf/1807.06736.pdf)\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=1.0, last_attended_idx=None, backward_window=1, forward_window=3)\",\"Calculate AttForward forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – attention weights of previous step\",\"scaling (float) – scaling parameter before applying softmax\",\"last_attended_idx (int) – index of the inputs of the last attended\",\"backward_window (int) – backward window size in attention constraint\",\"forward_window (int) – forward window size in attetion constraint\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"reset states\",\"training : bool\"]},\"682\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttForwardTA\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttForwardTA(eunits, dunits, att_dim, aconv_chans, aconv_filts, odim)\",\"Bases: Module\",\"Forward attention with transition agent module.\",\"Reference: Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\",\"(https://arxiv.org/pdf/1807.06736.pdf)\",\"Parameters:\",\"eunits (int) – # units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"odim (int) – output dimension\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, out_prev, scaling=1.0, last_attended_idx=None, backward_window=1, forward_window=3)\",\"Calculate AttForwardTA forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B, Tmax, eunits)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B, dunits)\",\"att_prev (torch.Tensor) – attention weights of previous step\",\"out_prev (torch.Tensor) – decoder outputs of previous step (B, odim)\",\"scaling (float) – scaling parameter before applying softmax\",\"last_attended_idx (int) – index of the inputs of the last attended\",\"backward_window (int) – backward window size in attention constraint\",\"forward_window (int) – forward window size in attetion constraint\",\"Returns: attention weighted encoder state (B, dunits)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B, Tmax)\",\"Return type: torch.Tensor\",\"reset()\",\"training : bool\"]},\"683\":{\"h\":\"espnet.nets.chainer_backend.rnn.attentions.AttLoc\",\"t\":[\"class espnet.nets.chainer_backend.rnn.attentions.AttLoc(eprojs, dunits, att_dim, aconv_chans, aconv_filts)\",\"Bases: Chain\",\"Compute location-based attention.\",\"Parameters:\",\"eprojs (int|None) – Dimension of input vectors from encoder.\",\"dunits (int|None) – Dimension of input vectors for decoder.\",\"att_dim (int) – Dimension of input vectors for attention.\",\"aconv_chans (int) – Number of channels of output arrays from convolutional layer.\",\"aconv_filts (int) – Size of filters of convolutional layer.\",\"reset()\",\"Reset states.\"]},\"684\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttLoc2D\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttLoc2D(eprojs, dunits, att_dim, att_win, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"2D location-aware attention\",\"This attention is an extended version of location aware attention. It take not only one frame before attention weights, but also earlier frames into account.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"att_win (int) – attention window size (default=5)\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0)\",\"AttLoc2D forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – previous attention weight (B x att_win x T_max)\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x att_win x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"reset states\",\"training : bool\"]},\"685\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttLocRec\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttLocRec(eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"location-aware recurrent attention\",\"This attention is an extended version of location aware attention. With the use of RNN, it take the effect of the history of attention weights into account.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev_states, scaling=2.0)\",\"AttLocRec forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev_states (tuple) – previous attention weight and lstm states ((B, T_max), ((B, att_dim), (B, att_dim)))\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights and lstm states (w, (hx, cx)) ((B, T_max), ((B, att_dim), (B, att_dim)))\",\"Return type: tuple\",\"reset()\",\"reset states\",\"training : bool\"]},\"686\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd(eprojs, dunits, aheads, att_dim_k, att_dim_v, han_mode=False)\",\"Bases: Module\",\"Multi head additive attention\",\"Reference: Attention is all you need : (https://arxiv.org/abs/1706.03762)\",\"This attention is multi head attention using additive attention for each head.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"att_dim_k (int) – dimension k in multi head attention\",\"att_dim_v (int) – dimension v in multi head attention\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, **kwargs)\",\"AttMultiHeadAdd forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – dummy (does not use)\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weight (B x T_max) * aheads\",\"Return type: list\",\"reset()\",\"reset states\",\"training : bool\"]},\"687\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadDot\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadDot(eprojs, dunits, aheads, att_dim_k, att_dim_v, han_mode=False)\",\"Bases: Module\",\"Multi head dot product attention\",\"Reference: Attention is all you need : (https://arxiv.org/abs/1706.03762)\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"att_dim_k (int) – dimension k in multi head attention\",\"att_dim_v (int) – dimension v in multi head attention\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, **kwargs)\",\"AttMultiHeadDot forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – dummy (does not use)\",\"Returns: attention weighted encoder state (B x D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weight (B x T_max) * aheads\",\"Return type: list\",\"reset()\",\"reset states\",\"training : bool\"]},\"688\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadLoc\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadLoc(eprojs, dunits, aheads, att_dim_k, att_dim_v, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Multi head location based attention\",\"Reference: Attention is all you need : (https://arxiv.org/abs/1706.03762)\",\"This attention is multi head attention using location-aware attention for each head.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"att_dim_k (int) – dimension k in multi head attention\",\"att_dim_v (int) – dimension v in multi head attention\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0, **kwargs)\",\"AttMultiHeadLoc forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – list of previous attention weight (B x T_max) * aheads\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B x D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weight (B x T_max) * aheads\",\"Return type: list\",\"reset()\",\"reset states\",\"training : bool\"]},\"689\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadMultiResLoc\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadMultiResLoc(eprojs, dunits, aheads, att_dim_k, att_dim_v, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Multi head multi resolution location based attention\",\"Reference: Attention is all you need : (https://arxiv.org/abs/1706.03762)\",\"This attention is multi head attention using location-aware attention for each head. Furthermore, it uses different filter size for each head.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"att_dim_k (int) – dimension k in multi head attention\",\"att_dim_v (int) – dimension v in multi head attention\",\"aconv_chans (int) – maximum # channels of attention convolution each head use #ch = aconv_chans * (head + 1) / aheads e.g. aheads=4, aconv_chans=100 => filter size = 25, 50, 75, 100\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, **kwargs)\",\"AttMultiHeadMultiResLoc forward\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – list of previous attention weight (B x T_max) * aheads\",\"Returns: attention weighted encoder state (B x D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weight (B x T_max) * aheads\",\"Return type: list\",\"reset()\",\"reset states\",\"training : bool\"]},\"690\":{\"h\":\"espnet.nets.pytorch_backend.frontends.dnn_beamformer.AttentionReference\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.dnn_beamformer.AttentionReference(bidim, att_dim)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(psd_in: ComplexTensor, ilens: LongTensor, scaling: float = 2.0)\",\"The forward function\",\"Parameters:\",\"psd_in (ComplexTensor) – (B, F, C, C)\",\"ilens (torch.Tensor) – (B,)\",\"scaling (float) –\",\"Returns: (B, C) ilens (torch.Tensor): (B,)\",\"Return type: u (torch.Tensor)\",\"training : bool\"]},\"691\":{\"h\":\"espnet.nets.batch_beam_search.BatchBeamSearch\",\"t\":[\"class espnet.nets.batch_beam_search.BatchBeamSearch(scorers: Dict[str, ScorerInterface], weights: Dict[str, float], beam_size: int, vocab_size: int, sos: int, eos: int, token_list: List[str] | None = None, pre_beam_ratio: float = 1.5, pre_beam_score_key: str | None = None, return_hs: bool = False, hyp_primer: List[int] | None = None, normalize_length: bool = False)\",\"Bases: BeamSearch\",\"Batch beam search implementation.\",\"Initialize beam search.\",\"Parameters:\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"token_list (list *[*str]) – List of tokens for debug log\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"return_hs (bool) – Whether to return hidden intermediates\",\"normalize_length (bool) – If true, select the best ended hypotheses based on length-normalized scores rather than the accumulated scores\",\"batch_beam(weighted_scores: Tensor, ids: Tensor)\",\"Batch-compute topk full token ids and partial token ids.\",\"Parameters:\",\"weighted_scores (torch.Tensor) – The weighted sum scores for each tokens. Its shape is (n_beam, self.vocab_size).\",\"ids (torch.Tensor) – The partial token ids to compute topk. Its shape is (n_beam, self.pre_beam_size).\",\"Returns: The topk full (prev_hyp, new_token) ids and partial (prev_hyp, new_token) ids. Their shapes are all (self.beam_size,)\",\"Return type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\",\"batchfy(hyps: List[Hypothesis])\",\"Convert list to batch.\",\"init_hyp(x: Tensor)\",\"Get an initial hypothesis data.\",\"Parameters:x (torch.Tensor) – The encoder output feature\",\"Returns: The initial hypothesis.\",\"Return type:Hypothesis\",\"merge_states(states: Any, part_states: Any, part_idx: int)\",\"Merge states for new hypothesis.\",\"Parameters:\",\"states – states of self.full_scorers\",\"part_states – states of self.part_scorers\",\"part_idx (int) – The new token id for part_scores\",\"Returns: The new score dict. : Its keys are names of self.full_scorers and self.part_scorers. Its values are states of the scorers.\",\"Return type: Dict[str, torch.Tensor]\",\"post_process(i: int, maxlen: int, minlen: int, maxlenratio: float, running_hyps: BatchHypothesis, ended_hyps: List[Hypothesis])\",\"Perform post-processing of beam search iterations.\",\"Parameters:\",\"i (int) – The length of hypothesis tokens.\",\"maxlen (int) – The maximum length of tokens in beam search.\",\"maxlenratio (int) – The maximum length ratio in beam search.\",\"running_hyps (BatchHypothesis) – The running hypotheses in beam search.\",\"ended_hyps (List[Hypothesis]) – The ended hypotheses in beam search.\",\"Returns: The new running hypotheses.\",\"Return type:BatchHypothesis\",\"score_full(hyp: BatchHypothesis, x: Tensor, pre_x: Tensor | None = None)\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"score_partial(hyp: BatchHypothesis, ids: Tensor, x: Tensor, pre_x: Tensor | None = None)\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"ids (torch.Tensor) – 2D tensor of new partial tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"search(running_hyps: BatchHypothesis, x: Tensor, pre_x: Tensor | None = None)\",\"Search new tokens for running hypotheses and encoded speech x.\",\"Parameters:\",\"running_hyps (BatchHypothesis) – Running hypotheses on beam\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attention (T, D)\",\"Returns: Best sorted hypotheses\",\"Return type:BatchHypothesis\",\"training : bool\",\"unbatchfy(batch_hyps: BatchHypothesis)\",\"Revert batch to list.\"]},\"692\":{\"h\":\"espnet.nets.batch_beam_search_online.BatchBeamSearchOnline\",\"t\":[\"class espnet.nets.batch_beam_search_online.BatchBeamSearchOnline(*args, block_size=40, hop_size=16, look_ahead=16, disable_repetition_detection=False, encoded_feat_length_limit=0, decoder_text_length_limit=0, incremental_decode=False, time_sync=False, ctc=None, hold_n=0, transducer_conf=None, joint_network=None, **kwargs)\",\"Bases: BatchBeamSearch\",\"Online beam search implementation.\",\"This simulates streaming decoding. It requires encoded features of entire utterance and extracts block by block from it as it shoud be done in streaming processing. This is based on Tsunoo et al, “STREAMING TRANSFORMER ASR WITH BLOCKWISE SYNCHRONOUS BEAM SEARCH” (https://arxiv.org/abs/2006.14941).\",\"Initialize beam search.\",\"assemble_hyps(ended_hyps)\",\"Assemble the hypotheses.\",\"extend(x: Tensor, hyps: Hypothesis)\",\"Extend probabilities and states with more encoded chunks.\",\"Parameters:\",\"x (torch.Tensor) – The extended encoder output feature\",\"hyps (Hypothesis) – Current list of hypothesis\",\"Returns: The extended hypothesis\",\"Return type:Hypothesis\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0, is_final: bool = True)\",\"Perform beam search.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\",\"minlenratio (float) – Input length ratio to obtain min output length.\",\"Returns: N-best decoding results\",\"Return type: list[Hypothesis]\",\"process_one_block(h, is_final, maxlen, minlen, maxlenratio)\",\"Recognize one block.\",\"process_one_block_time_sync(h, is_final, maxlen, maxlenratio)\",\"Recognize one block w/ time sync.\",\"reset()\",\"Reset parameters.\",\"score_full(hyp: BatchHypothesis, x: Tensor, pre_x: Tensor | None = None)\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attention (T, D)\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"training : bool\"]},\"693\":{\"h\":\"espnet.nets.batch_beam_search_online_sim.BatchBeamSearchOnlineSim\",\"t\":[\"class espnet.nets.batch_beam_search_online_sim.BatchBeamSearchOnlineSim(scorers: Dict[str, ScorerInterface], weights: Dict[str, float], beam_size: int, vocab_size: int, sos: int, eos: int, token_list: List[str] | None = None, pre_beam_ratio: float = 1.5, pre_beam_score_key: str | None = None, return_hs: bool = False, hyp_primer: List[int] | None = None, normalize_length: bool = False)\",\"Bases: BatchBeamSearch\",\"Online beam search implementation.\",\"This simulates streaming decoding. It requires encoded features of entire utterance and extracts block by block from it as it shoud be done in streaming processing. This is based on Tsunoo et al, “STREAMING TRANSFORMER ASR WITH BLOCKWISE SYNCHRONOUS BEAM SEARCH” (https://arxiv.org/abs/2006.14941).\",\"Initialize beam search.\",\"Parameters:\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"token_list (list *[*str]) – List of tokens for debug log\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"return_hs (bool) – Whether to return hidden intermediates\",\"normalize_length (bool) – If true, select the best ended hypotheses based on length-normalized scores rather than the accumulated scores\",\"extend(x: Tensor, hyps: Hypothesis)\",\"Extend probabilities and states with more encoded chunks.\",\"Parameters:\",\"x (torch.Tensor) – The extended encoder output feature\",\"hyps (Hypothesis) – Current list of hypothesis\",\"Returns: The extended hypothesis\",\"Return type:Hypothesis\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0)\",\"Perform beam search.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\",\"minlenratio (float) – Input length ratio to obtain min output length.\",\"Returns: N-best decoding results\",\"Return type: list[Hypothesis]\",\"set_block_size(block_size: int)\",\"Set block size for streaming decoding.\",\"Parameters:block_size (int) – The block size of encoder\",\"set_hop_size(hop_size: int)\",\"Set hop size for streaming decoding.\",\"Parameters:hop_size (int) – The hop size of encoder\",\"set_look_ahead(look_ahead: int)\",\"Set look ahead size for streaming decoding.\",\"Parameters:look_ahead (int) – The look ahead size of encoder\",\"set_streaming_config(asr_config: str)\",\"Set config file for streaming decoding.\",\"Parameters:asr_config (str) – The config file for asr training\",\"training : bool\"]},\"694\":{\"h\":\"espnet.nets.batch_beam_search.BatchHypothesis\",\"t\":[\"class espnet.nets.batch_beam_search.BatchHypothesis(yseq: Tensor = tensor([]), score: Tensor = tensor([]), length: Tensor = tensor([]), scores: Dict[str, Tensor] = {}, states: Dict[str, Dict] = {}, hs: List[Tensor] = [])\",\"Bases: tuple\",\"Batchfied/Vectorized hypothesis data type.\",\"Create new instance of BatchHypothesis(yseq, score, length, scores, states, hs)\",\"hs : List[Tensor]\",\"Alias for field number 5\",\"length : Tensor\",\"Alias for field number 2\",\"score : Tensor\",\"Alias for field number 1\",\"scores : Dict[str, Tensor]\",\"Alias for field number 3\",\"states : Dict[str, Dict]\",\"Alias for field number 4\",\"yseq : Tensor\",\"Alias for field number 0\"]},\"695\":{\"h\":\"espnet.nets.scorer_interface.BatchPartialScorerInterface\",\"t\":[\"class espnet.nets.scorer_interface.BatchPartialScorerInterface\",\"Bases: BatchScorerInterface, PartialScorerInterface\",\"Batch partial scorer interface for beam search.\",\"batch_score_partial(ys: Tensor, next_tokens: Tensor, states: List[Any], xs: Tensor)\",\"Score new token (required).\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"next_tokens (torch.Tensor) – torch.int64 tokens to score (n_batch, n_token).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of a score tensor for ys that has a shape (n_batch, n_vocab) and next states for ys\",\"Return type: tuple[torch.Tensor, Any]\"]},\"696\":{\"h\":\"espnet.nets.scorer_interface.BatchScorerInterface\",\"t\":[\"class espnet.nets.scorer_interface.BatchScorerInterface\",\"Bases: ScorerInterface\",\"Batch scorer interface.\",\"batch_init_state(x: Tensor)\",\"Get an initial state for decoding (optional).\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch (required).\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\"]},\"697\":{\"h\":\"espnet.nets.beam_search.BeamSearch\",\"t\":[\"<!-- _espnet.nets.beam_search.BeamSearch -->\",\"class espnet.nets.beam_search.BeamSearch(scorers: Dict[str, ScorerInterface], weights: Dict[str, float], beam_size: int, vocab_size: int, sos: int, eos: int, token_list: List[str] | None = None, pre_beam_ratio: float = 1.5, pre_beam_score_key: str | None = None, return_hs: bool = False, hyp_primer: List[int] | None = None, normalize_length: bool = False)\",\"Bases: Module\",\"Beam search implementation.\",\"Initialize beam search.\",\"Parameters:\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"token_list (list *[*str]) – List of tokens for debug log\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"return_hs (bool) – Whether to return hidden intermediates\",\"normalize_length (bool) – If true, select the best ended hypotheses based on length-normalized scores rather than the accumulated scores\",\"static append_token(xs: Tensor, x: int)\",\"Append new token to prefix tokens.\",\"Parameters:\",\"xs (torch.Tensor) – The prefix token\",\"x (int) – The new token to append\",\"Returns: New tensor contains: xs + [x] with xs.dtype and xs.device\",\"Return type: torch.Tensor\",\"beam(weighted_scores: Tensor, ids: Tensor)\",\"Compute topk full token ids and partial token ids.\",\"Parameters:\",\"weighted_scores (torch.Tensor) – The weighted sum scores for each tokens.\",\"` (Its shape is) –\",\"ids (torch.Tensor) – The partial token ids to compute topk\",\"Returns: The topk full token ids and partial token ids. Their shapes are (self.beam_size,)\",\"Return type: Tuple[torch.Tensor, torch.Tensor]\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0, pre_x: Tensor | None = None)\",\"Perform beam search.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths If maxlenratio<0.0, its absolute value is interpreted as a constant max output length.\",\"minlenratio (float) – Input length ratio to obtain min output length. If minlenratio<0.0, its absolute value is interpreted as a constant min output length.\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: N-best decoding results\",\"Return type: list[Hypothesis]\",\"init_hyp(x: Tensor)\",\"Get an initial hypothesis data.\",\"Parameters:x (torch.Tensor) – The encoder output feature\",\"Returns: The initial hypothesis.\",\"Return type:Hypothesis\",\"static merge_scores(prev_scores: Dict[str, float], next_full_scores: Dict[str, Tensor], full_idx: int, next_part_scores: Dict[str, Tensor], part_idx: int)\",\"Merge scores for new hypothesis.\",\"Parameters:\",\"prev_scores (Dict *[*str,float]) – The previous hypothesis scores by self.scorers\",\"next_full_scores (Dict *[*str,torch.Tensor]) – scores by self.full_scorers\",\"full_idx (int) – The next token id for next_full_scores\",\"next_part_scores (Dict *[*str,torch.Tensor]) – scores of partial tokens by self.part_scorers\",\"part_idx (int) – The new token id for next_part_scores\",\"Returns: The new score dict. : Its keys are names of self.full_scorers and self.part_scorers. Its values are scalar tensors by the scorers.\",\"Return type: Dict[str, torch.Tensor]\",\"merge_states(states: Any, part_states: Any, part_idx: int)\",\"Merge states for new hypothesis.\",\"Parameters:\",\"states – states of self.full_scorers\",\"part_states – states of self.part_scorers\",\"part_idx (int) – The new token id for part_scores\",\"Returns: The new score dict. : Its keys are names of self.full_scorers and self.part_scorers. Its values are states of the scorers.\",\"Return type: Dict[str, torch.Tensor]\",\"post_process(i: int, maxlen: int, minlen: int, maxlenratio: float, running_hyps: List[Hypothesis], ended_hyps: List[Hypothesis])\",\"Perform post-processing of beam search iterations.\",\"Parameters:\",\"i (int) – The length of hypothesis tokens.\",\"maxlen (int) – The maximum length of tokens in beam search.\",\"maxlenratio (int) – The maximum length ratio in beam search.\",\"running_hyps (List[Hypothesis]) – The running hypotheses in beam search.\",\"ended_hyps (List[Hypothesis]) – The ended hypotheses in beam search.\",\"Returns: The new running hypotheses.\",\"Return type: List[Hypothesis]\",\"score_full(hyp: Hypothesis, x: Tensor, pre_x: Tensor | None = None)\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"score_partial(hyp: Hypothesis, ids: Tensor, x: Tensor)\",\"Score new hypothesis by self.part_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"ids (torch.Tensor) – 1D tensor of new partial tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"Returns: Tuple of : score dict of hyp that has string keys of self.part_scorers and tensor score values of shape: (len(ids),), and state dict that has string keys and state values of self.part_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"search(running_hyps: List[Hypothesis], x: Tensor, pre_x: Tensor | None = None)\",\"Search new tokens for running hypotheses and encoded speech x.\",\"Parameters:\",\"running_hyps (List[Hypothesis]) – Running hypotheses on beam\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: Best sorted hypotheses\",\"Return type: List[Hypotheses]\",\"set_hyp_primer(hyp_primer: List[int] | None = None)\",\"Set the primer sequence for decoding.\",\"Used for OpenAI Whisper models.\",\"training : bool\"]},\"698\":{\"h\":\"espnet.nets.beam_search_timesync.BeamSearchTimeSync\",\"t\":[\"class espnet.nets.beam_search_timesync.BeamSearchTimeSync(sos: int, beam_size: int, scorers: ~typing.Dict[str, ~espnet.nets.scorer_interface.ScorerInterface], weights: ~typing.Dict[str, float], token_list=<class 'dict'>, pre_beam_ratio: float = 1.5, blank: int = 0, force_lid: bool = False, temp: float = 1.0)\",\"Bases: Module\",\"Time synchronous beam search algorithm.\",\"Initialize beam search.\",\"Parameters:\",\"beam_size – num hyps\",\"sos – sos index\",\"ctc – CTC module\",\"pre_beam_ratio – pre_beam_ratio * beam_size = pre_beam pre_beam is used to select candidates from vocab to extend hypotheses\",\"decoder – decoder ScorerInterface\",\"ctc_weight – ctc_weight\",\"blank – blank index\",\"cached_score(h: Tuple[int], cache: dict, scorer: ScorerInterface)\",\"Retrieve decoder/LM scores which may be cached.\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0)\",\"Perform beam search.\",\"Parameters:enc_output (torch.Tensor) –\",\"Returns: list[Hypothesis]\",\"joint_score(hyps: Any, ctc_score_dp: Any)\",\"Calculate joint score for hyps.\",\"reset(enc_output: Tensor)\",\"Reset object for a new utterance.\",\"time_step(p_ctc: Any, ctc_score_dp: Any, hyps: Any)\",\"Execute a single time step.\",\"training : bool\"]},\"699\":{\"h\":\"espnet.nets.beam_search_timesync_streaming.BeamSearchTimeSyncStreaming\",\"t\":[\"class espnet.nets.beam_search_timesync_streaming.BeamSearchTimeSyncStreaming(sos: int, beam_size: int, scorers: ~typing.Dict[str, ~espnet.nets.scorer_interface.ScorerInterface], weights: ~typing.Dict[str, float], token_list=<class 'dict'>, pre_beam_ratio: float = 1.5, blank: int = 0, hold_n: int = 0)\",\"Bases: Module\",\"Time synchronous beam search algorithm.\",\"Initialize beam search.\",\"Parameters:\",\"beam_size – num hyps\",\"sos – sos index\",\"ctc – CTC module\",\"pre_beam_ratio – pre_beam_ratio * beam_size = pre_beam pre_beam is used to select candidates from vocab to extend hypotheses\",\"decoder – decoder ScorerInterface\",\"ctc_weight – ctc_weight\",\"blank – blank index\",\"cached_score(h: Tuple[int], cache: dict, scorer: ScorerInterface, recompute_cache: bool = False)\",\"Retrieve decoder/LM scores which may be cached.\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0, start_idx: int = 0, is_final: bool = False, incremental_decode: bool = False)\",\"Perform beam search.\",\"Parameters:enc_output (torch.Tensor) –\",\"Returns: list[Hypothesis]\",\"joint_score(hyps: Any, ctc_score_dp: Any, recompute_cache: bool = False)\",\"Calculate joint score for hyps.\",\"reset(enc_output: Tensor)\",\"Reset object for a new utterance.\",\"time_step(p_ctc: Any, ctc_score_dp: Any, hyps: Any, recompute_cache: bool = False)\",\"Execute a single time step.\",\"training : bool\"]},\"700\":{\"h\":\"espnet.nets.beam_search_transducer.BeamSearchTransducer\",\"t\":[\"class espnet.nets.beam_search_transducer.BeamSearchTransducer(decoder: RNNDecoder | CustomDecoder, joint_network: JointNetwork, beam_size: int, lm: Module | None = None, lm_weight: float = 0.1, search_type: str = 'default', max_sym_exp: int = 2, u_max: int = 50, nstep: int = 1, prefix_alpha: int = 1, expansion_gamma: int = 2.3, expansion_beta: int = 2, score_norm: bool = True, softmax_temperature: float = 1.0, nbest: int = 1, quantization: bool = False)\",\"Bases: object\",\"Beam search implementation for Transducer.\",\"Initialize Transducer search module.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint network module.\",\"beam_size – Beam size.\",\"lm – LM class.\",\"lm_weight – LM weight for soft fusion.\",\"search_type – Search algorithm to use during inference.\",\"max_sym_exp – Number of maximum symbol expansions at each time step. (TSD)\",\"u_max – Maximum output sequence length. (ALSD)\",\"nstep – Number of maximum expansion steps at each time step. (NSC/mAES)\",\"prefix_alpha – Maximum prefix length in prefix search. (NSC/mAES)\",\"expansion_beta – Number of additional candidates for expanded hypotheses selection. (mAES)\",\"expansion_gamma – Allowed logp difference for prune-by-value method. (mAES)\",\"score_norm – Normalize final scores by length. (“default”)\",\"softmax_temperature – Penalization term for softmax function.\",\"nbest – Number of final hypothesis.\",\"quantization – Whether dynamic quantization is used.\",\"align_length_sync_decoding(enc_out: Tensor)\",\"Alignment-length synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:h – Encoder output sequences. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"default_beam_search(enc_out: Tensor)\",\"Beam search implementation.\",\"Modified from https://arxiv.org/pdf/1211.3711.pdf\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"greedy_search(enc_out: Tensor)\",\"Greedy search implementation.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: 1-best hypotheses.\",\"Return type: hyp\",\"modified_adaptive_expansion_search(enc_out: Tensor)\",\"It’s the modified Adaptive Expansion Search (mAES) implementation.\",\"Based on/modified from https://ieeexplore.ieee.org/document/9250505 and NSC.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"nsc_beam_search(enc_out: Tensor)\",\"N-step constrained beam search implementation.\",\"Based on/Modified from https://arxiv.org/pdf/2002.03577.pdf. Please reference ESPnet (b-flo, PR #2444) for any usage outside ESPnet until further modifications.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"prefix_search(hyps: List[ExtendedHypothesis], enc_out_t: Tensor)\",\"Prefix search for NSC and mAES strategies.\",\"Based on https://arxiv.org/pdf/1211.3711.pdf\",\"sort_nbest(hyps: List[Hypothesis] | List[ExtendedHypothesis])\",\"Sort hypotheses by score or score given sequence length.\",\"Parameters:hyps – Hypothesis.\",\"Returns: Sorted hypothesis.\",\"Return type: hyps\",\"time_sync_decoding(enc_out: Tensor)\",\"Time synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\"]},\"701\":{\"h\":\"espnet.nets.pytorch_backend.tacotron2.cbhg.CBHG\",\"t\":[\"class espnet.nets.pytorch_backend.tacotron2.cbhg.CBHG(idim, odim, conv_bank_layers=8, conv_bank_chans=128, conv_proj_filts=3, conv_proj_chans=256, highway_layers=4, highway_units=128, gru_units=256)\",\"Bases: Module\",\"CBHG module to convert log Mel-filterbanks to linear spectrogram.\",\"This is a module of CBHG introduced in Tacotron: Towards End-to-End Speech Synthesis. The CBHG converts the sequence of log Mel-filterbanks into linear spectrogram.\",\"Initialize CBHG module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"conv_bank_layers (int,optional) – The number of convolution bank layers.\",\"conv_bank_chans (int,optional) – The number of channels in convolution bank.\",\"conv_proj_filts (int,optional) – Kernel size of convolutional projection layer.\",\"conv_proj_chans (int,optional) – The number of channels in convolutional projection layer.\",\"highway_layers (int,optional) – The number of highway network layers.\",\"highway_units (int,optional) – The number of highway network units.\",\"gru_units (int,optional) – The number of GRU units (for both directions).\",\"forward(xs, ilens)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of the padded sequences of inputs (B, Tmax, idim).\",\"ilens (LongTensor) – Batch of lengths of each input sequence (B,).\",\"Returns: Batch of the padded sequence of outputs (B, Tmax, odim). LongTensor: Batch of lengths of each output sequence (B,).\",\"Return type: Tensor\",\"inference(x)\",\"Inference.\",\"Parameters:x (Tensor) – The sequences of inputs (T, idim).\",\"Returns: The sequence of outputs (T, odim).\",\"Return type: Tensor\",\"training : bool\"]},\"702\":{\"h\":\"espnet.nets.pytorch_backend.tacotron2.cbhg.CBHGLoss\",\"t\":[\"class espnet.nets.pytorch_backend.tacotron2.cbhg.CBHGLoss(use_masking=True)\",\"Bases: Module\",\"Loss function module for CBHG.\",\"Initialize CBHG loss module.\",\"Parameters:use_masking (bool) – Whether to mask padded part in loss calculation.\",\"forward(cbhg_outs, spcs, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"cbhg_outs (Tensor) – Batch of CBHG outputs (B, Lmax, spc_dim).\",\"spcs (Tensor) – Batch of groundtruth of spectrogram (B, Lmax, spc_dim).\",\"olens (LongTensor) – Batch of the lengths of each sequence (B,).\",\"Returns: L1 loss value Tensor: Mean square error loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"703\":{\"h\":\"espnet.nets.chainer_backend.transformer.ctc.CTC\",\"t\":[\"class espnet.nets.chainer_backend.transformer.ctc.CTC(odim, eprojs, dropout_rate)\",\"Bases: Chain\",\"Chainer implementation of ctc layer.\",\"Parameters:\",\"odim (int) – The output dimension.\",\"eprojs (int|None) – Dimension of input vectors from encoder.\",\"dropout_rate (float) – Dropout rate.\",\"Initialize CTC.\",\"log_softmax(hs)\",\"Log_softmax of frame activations.\",\"Parameters:hs (listofchainer.Variable|N-dimension array) – Input variable from encoder.\",\"Returns: A n-dimension float array.\",\"Return type: chainer.Variable\"]},\"704\":{\"h\":\"espnet.nets.ctc_prefix_score.CTCPrefixScore\",\"t\":[\"class espnet.nets.ctc_prefix_score.CTCPrefixScore(x, blank, eos, xp)\",\"Bases: object\",\"Compute CTC label sequence scores\",\"which is based on Algorithm 2 in WATANABE et al. “HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,” but extended to efficiently compute the probablities of multiple labels simultaneously\",\"initial_state()\",\"Obtain an initial CTC state\",\"Returns: CTC state\"]},\"705\":{\"h\":\"espnet.nets.ctc_prefix_score.CTCPrefixScoreTH\",\"t\":[\"class espnet.nets.ctc_prefix_score.CTCPrefixScoreTH(x, xlens, blank, eos, margin=0)\",\"Bases: object\",\"Batch processing of CTCPrefixScore\",\"which is based on Algorithm 2 in WATANABE et al. “HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,” but extended to efficiently compute the label probablities for multiple hypotheses simultaneously See also Seki et al. “Vectorized Beam Search for CTC-Attention-Based Speech Recognition,” In INTERSPEECH (pp. 3825-3829), 2019.\",\"Construct CTC prefix scorer\",\"Parameters:\",\"x (torch.Tensor) – input label posterior sequences (B, T, O)\",\"xlens (torch.Tensor) – input lengths (B,)\",\"blank (int) – blank label id\",\"eos (int) – end-of-sequence id\",\"margin (int) – margin parameter for windowing (0 means no windowing)\",\"extend_prob(x)\",\"Extend CTC prob.\",\"Parameters:x (torch.Tensor) – input label posterior sequences (B, T, O)\",\"extend_state(state)\",\"Compute CTC prefix state.\",\":param state : CTC state :return ctc_state\",\"index_select_state(state, best_ids)\",\"Select CTC states according to best ids\",\":param state : CTC state :param best_ids : index numbers selected by beam pruning (B, W) :return selected_state\"]},\"706\":{\"h\":\"espnet.nets.scorers.ctc.CTCPrefixScorer\",\"t\":[\"<!-- _espnet.nets.scorers.ctc.CTCPrefixScorer -->\",\"class espnet.nets.scorers.ctc.CTCPrefixScorer(ctc: Module, eos: int)\",\"Bases: BatchPartialScorerInterface\",\"Decoder interface wrapper for CTCPrefixScore.\",\"Initialize class.\",\"Parameters:\",\"ctc (torch.nn.Module) – The CTC implementation. For example, espnet.nets.pytorch_backend.ctc.CTC\",\"eos (int) – The end-of-sequence id.\",\"batch_init_state(x: Tensor)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"batch_score_partial(y, ids, state, x)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D prefix token\",\"ids (torch.Tensor) – torch.int64 next token to score\",\"state – decoder state for prefix tokens\",\"x (torch.Tensor) – 2D encoder feature that generates ys\",\"Returns: Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"extend_prob(x: Tensor)\",\"Extend probs for decoding.\",\"This extension is for streaming decoding as in Eq (14) in https://arxiv.org/abs/2006.14941\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"extend_state(state)\",\"Extend state for decoding.\",\"This extension is for streaming decoding as in Eq (14) in https://arxiv.org/abs/2006.14941\",\"Parameters:state – The states of hyps\",\"Returns: exteded state\",\"init_state(x: Tensor)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"score_partial(y, ids, state, x)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D prefix token\",\"next_tokens (torch.Tensor) – torch.int64 next token to score\",\"state – decoder state for prefix tokens\",\"x (torch.Tensor) – 2D encoder feature that generates ys\",\"Returns: Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"select_state(state, i, new_id=None)\",\"Select state with relative ids in the main beam search.\",\"Parameters:\",\"state – Decoder state for prefix tokens\",\"i (int) – Index to select a state in the main beam search\",\"new_id (int) – New label id to select a state if necessary\",\"Returns: pruned state\",\"Return type: state\"]},\"707\":{\"h\":\"espnet.nets.beam_search_timesync_streaming.CacheItem\",\"t\":[\"class espnet.nets.beam_search_timesync_streaming.CacheItem(state: Any, scores: Any, log_sum: float)\",\"Bases: object\",\"For caching attentional decoder and LM states.\",\"log_sum : float\",\"scores : Any\",\"state : Any\"]},\"708\":{\"h\":\"espnet.nets.pytorch_backend.wavenet.CausalConv1d\",\"t\":[\"class espnet.nets.pytorch_backend.wavenet.CausalConv1d(in_channels, out_channels, kernel_size, dilation=1, bias=True)\",\"Bases: Module\",\"1D dilated causal convolution.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor with the shape (B, in_channels, T).\",\"Returns: Tensor with the shape (B, out_channels, T)\",\"Return type: Tensor\",\"training : bool\"]},\"709\":{\"h\":\"espnet.nets.chainer_backend.asr_interface.ChainerASRInterface\",\"t\":[\"class espnet.nets.chainer_backend.asr_interface.ChainerASRInterface(**links: Link)\",\"Bases: ASRInterface, Chain\",\"ASR Interface for ESPnet model implementation.\",\"static custom_converter(*args, **kw)\",\"Get customconverter of the model (Chainer only).\",\"static custom_parallel_updater(*args, **kw)\",\"Get custom_parallel_updater of the model (Chainer only).\",\"static custom_updater(*args, **kw)\",\"Get custom_updater of the model (Chainer only).\",\"get_total_subsampling_factor()\",\"Get total subsampling factor.\"]},\"710\":{\"h\":\"espnet.nets.pytorch_backend.lm.default.ClassifierWithState\",\"t\":[\"class espnet.nets.pytorch_backend.lm.default.ClassifierWithState(predictor, lossfun=CrossEntropyLoss(), label_key=-1)\",\"Bases: Module\",\"A wrapper for pytorch RNNLM.\",\"Initialize class.\",\":param torch.nn.Module predictor : The RNNLM :param function lossfun : The loss function to use :param int/str label_key :\",\"buff_predict(state, x, n)\",\"Predict new tokens from buffered inputs.\",\"final(state, index=None)\",\"Predict final log probabilities for given state using the predictor.\",\"Parameters:state – The state\",\":return The final log probabilities :rtype torch.Tensor\",\"forward(state, *args, **kwargs)\",\"Compute the loss value for an input and label pair.\",\"Notes\",\"It also computes accuracy and stores it to the attribute. When label_key is int, the corresponding element in args is treated as ground truth labels. And when it is str, the element in kwargs is used. The all elements of args and kwargs except the groundtruth labels are features. It feeds features to the predictor and compare the result with ground truth labels.\",\":param torch.Tensor state : the LM state :param list[torch.Tensor] args : Input minibatch :param dict[torch.Tensor] kwargs : Input minibatch :return loss value :rtype torch.Tensor\",\"predict(state, x)\",\"Predict log probabilities for given state and input x using the predictor.\",\":param torch.Tensor state : The current state :param torch.Tensor x : The input :return a tuple (new state, log prob vector) :rtype (torch.Tensor, torch.Tensor)\",\"training : bool\"]},\"711\":{\"h\":\"espnet.nets.pytorch_backend.conformer.contextual_block_encoder_layer.ContextualBlockEncoderLayer\",\"t\":[\"class espnet.nets.pytorch_backend.conformer.contextual_block_encoder_layer.ContextualBlockEncoderLayer(size, self_attn, feed_forward, feed_forward_macaron, conv_module, dropout_rate, total_layer_num, normalize_before=True, concat_after=False)\",\"Bases: Module\",\"Contexutal Block Encoder layer module.\",\"Parameters:\",\"size (int) – Input dimension.\",\"self_attn (torch.nn.Module) – Self-attention module instance. MultiHeadedAttention or RelPositionMultiHeadedAttention instance can be used as the argument.\",\"feed_forward (torch.nn.Module) – Feed-forward module instance. PositionwiseFeedForward, MultiLayeredConv1d, or Conv1dLinear instance can be used as the argument.\",\"feed_forward_macaron (torch.nn.Module) – Additional feed-forward module instance. PositionwiseFeedForward, MultiLayeredConv1d, or Conv1dLinear instance can be used as the argument.\",\"conv_module (torch.nn.Module) – Convolution module instance. ConvlutionModule instance can be used as the argument.\",\"dropout_rate (float) – Dropout rate.\",\"total_layer_num (int) – Total number of layers\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"Construct an EncoderLayer object.\",\"forward(x, mask, infer_mode=False, past_ctx=None, next_ctx=None, is_short_segment=False, layer_idx=0, cache=None)\",\"Calculate forward propagation.\",\"forward_infer(x, mask, past_ctx=None, next_ctx=None, is_short_segment=False, layer_idx=0, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (torch.Tensor) – Input tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, 1, time).\",\"past_ctx (torch.Tensor) – Previous contexutal vector\",\"next_ctx (torch.Tensor) – Next contexutal vector\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, 1, time). cur_ctx (torch.Tensor): Current contexutal vector next_ctx (torch.Tensor): Next contexutal vector layer_idx (int): layer index number\",\"Return type: torch.Tensor\",\"forward_train(x, mask, past_ctx=None, next_ctx=None, layer_idx=0, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (torch.Tensor) – Input tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, time).\",\"past_ctx (torch.Tensor) – Previous contexutal vector\",\"next_ctx (torch.Tensor) – Next contexutal vector\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, time). cur_ctx (torch.Tensor): Current contexutal vector next_ctx (torch.Tensor): Next contexutal vector layer_idx (int): layer index number\",\"Return type: torch.Tensor\",\"training : bool\"]},\"712\":{\"h\":\"espnet.nets.pytorch_backend.transducer.conv1d_nets.Conv1d\",\"t\":[\"class espnet.nets.pytorch_backend.transducer.conv1d_nets.Conv1d(idim: int, odim: int, kernel_size: int | Tuple, stride: int | Tuple = 1, dilation: int | Tuple = 1, groups: int | Tuple = 1, bias: bool = True, batch_norm: bool = False, relu: bool = True, dropout_rate: float = 0.0)\",\"Bases: Module\",\"1D convolution module for custom encoder.\",\"Parameters:\",\"idim – Input dimension.\",\"odim – Output dimension.\",\"kernel_size – Size of the convolving kernel.\",\"stride – Stride of the convolution.\",\"dilation – Spacing between the kernel points.\",\"groups – Number of blocked connections from input channels to output channels.\",\"bias – Whether to add a learnable bias to the output.\",\"batch_norm – Whether to use batch normalization after convolution.\",\"relu – Whether to use a ReLU activation after convolution.\",\"dropout_rate – Dropout rate.\",\"Construct a Conv1d module object.\",\"create_new_mask(mask: Tensor)\",\"Create new mask.\",\"Parameters:mask – Mask of input sequences. (B, 1, T)\",\"Returns: Mask of output sequences. (B, 1, sub(T))\",\"Return type: mask\",\"create_new_pos_embed(pos_embed: Tensor)\",\"Create new positional embedding vector.\",\"Parameters:pos_embed – Input sequences positional embedding. (B, 2 * (T - 1), D_att)\",\"Returns: Output sequences positional embedding. : (B, 2 * (sub(T) - 1), D_att)\",\"Return type: pos_embed\",\"forward(sequence: Tensor | Tuple[Tensor, Tensor], mask: Tensor)\",\"Forward ConvEncoderLayer module object.\",\"Parameters:\",\"sequence –\",\"Input sequences. (B, T, D_in)\",\"or (B, T, D_in), (B, 2 * (T - 1), D_att)\",\"mask – Mask of input sequences. (B, 1, T)\",\"Returns: Output sequences. : (B, sub(T), D_out) : or (B, sub(T), D_out), (B, 2 * (sub(T) - 1), D_att)\",\"mask: Mask of output sequences. (B, 1, sub(T))\",\"Return type: sequence\",\"training : bool\"]},\"713\":{\"h\":\"espnet.nets.pytorch_backend.transformer.multi_layer_conv.Conv1dLinear\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.multi_layer_conv.Conv1dLinear(in_chans, hidden_chans, kernel_size, dropout_rate)\",\"Bases: Module\",\"Conv1D + Linear for Transformer block.\",\"A variant of MultiLayeredConv1d, which replaces second conv-layer to linear.\",\"Initialize Conv1dLinear module.\",\"Parameters:\",\"in_chans (int) – Number of input channels.\",\"hidden_chans (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size of conv1d.\",\"dropout_rate (float) – Dropout rate.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Batch of input tensors (B, T, in_chans).\",\"Returns: Batch of output tensors (B, T, hidden_chans).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"714\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling1\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling1(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 1D subsampling.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv1dSubsampling1 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 2.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 2.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"715\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling2\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling2(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 1D subsampling (to 1/2 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv1dSubsampling2 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 2.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 2.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"716\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling3\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling3(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 1D subsampling (to 1/3 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv1dSubsampling3 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 2.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 2.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"717\":{\"h\":\"espnet.nets.chainer_backend.transformer.subsampling.Conv2dSubsampling\",\"t\":[\"class espnet.nets.chainer_backend.transformer.subsampling.Conv2dSubsampling(channels, idim, dims, dropout=0.1, initialW=None, initial_bias=None)\",\"Bases: Chain\",\"Convolutional 2D subsampling (to 1/4 length).\",\"Parameters:\",\"idim (int) – input dim\",\"odim (int) – output dim\",\"dropout_rate (flaot) – dropout rate\",\"Initialize Conv2dSubsampling.\",\"forward(xs, ilens)\",\"Subsample x.\",\"Parameters:x (chainer.Variable) – input tensor\",\"Returns: subsampled x and mask\"]},\"718\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling1\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling1(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Similar to Conv2dSubsampling module, but without any subsampling performed.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling1 object.\",\"forward(x, x_mask)\",\"Pass x through 2 Conv2d layers without subsampling.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim). : where time’ = time - 4.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’). : where time’ = time - 4.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"719\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling2\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling2(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 2D subsampling (to 1/2 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling2 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 2.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 2.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"720\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling6\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling6(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 2D subsampling (to 1/6 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling6 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 6.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 6.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"721\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling8\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling8(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 2D subsampling (to 1/8 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling8 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 8.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 8.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"722\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling_without_posenc.Conv2dSubsamplingWOPosEnc\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling_without_posenc.Conv2dSubsamplingWOPosEnc(idim, odim, dropout_rate, kernels, strides)\",\"Bases: Module\",\"Convolutional 2D subsampling.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"kernels (list) – kernel sizes\",\"strides (list) – stride sizes\",\"Construct an Conv2dSubsamplingWOPosEnc object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 4.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 4.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"723\":{\"h\":\"espnet.nets.pytorch_backend.conformer.convolution.ConvolutionModule\",\"t\":[\"class espnet.nets.pytorch_backend.conformer.convolution.ConvolutionModule(channels, kernel_size, activation=ReLU(), bias=True)\",\"Bases: Module\",\"ConvolutionModule in Conformer model.\",\"Parameters:\",\"channels (int) – The number of channels of conv layers.\",\"kernel_size (int) – Kernerl size of conv layers.\",\"Construct an ConvolutionModule object.\",\"forward(x)\",\"Compute convolution module.\",\"Parameters:x (torch.Tensor) – Input tensor (#batch, time, channels).\",\"Returns: Output tensor (#batch, time, channels).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"724\":{\"h\":\"espnet.nets.chainer_backend.transformer.training.CustomConverter\",\"t\":[\"class espnet.nets.chainer_backend.transformer.training.CustomConverter\",\"Bases: object\",\"Custom Converter.\",\"Parameters:subsampling_factor (int) – The subsampling factor.\",\"Initialize subsampling.\"]},\"725\":{\"h\":\"espnet.nets.pytorch_backend.transducer.custom_decoder.CustomDecoder\",\"t\":[\"class espnet.nets.pytorch_backend.transducer.custom_decoder.CustomDecoder(odim: int, dec_arch: List, input_layer: str = 'embed', repeat_block: int = 0, joint_activation_type: str = 'tanh', positional_encoding_type: str = 'abs_pos', positionwise_layer_type: str = 'linear', positionwise_activation_type: str = 'relu', input_layer_dropout_rate: float = 0.0, blank_id: int = 0)\",\"Bases: TransducerDecoderInterface, Module\",\"Custom decoder module for Transducer model.\",\"Parameters:\",\"odim – Output dimension.\",\"dec_arch – Decoder block architecture (type and parameters).\",\"input_layer – Input layer type.\",\"repeat_block – Number of times dec_arch is repeated.\",\"joint_activation_type – Type of activation for joint network.\",\"positional_encoding_type – Positional encoding type.\",\"positionwise_layer_type – Positionwise layer type.\",\"positionwise_activation_type – Positionwise activation type.\",\"input_layer_dropout_rate – Dropout rate for input layer.\",\"blank_id – Blank symbol ID.\",\"Construct a CustomDecoder object.\",\"batch_score(hyps: List[Hypothesis] | List[ExtendedHypothesis], dec_states: List[Tensor | None], cache: Dict[str, Any], use_lm: bool)\",\"One-step forward hypotheses.\",\"Parameters:\",\"hyps – Hypotheses.\",\"dec_states – Decoder hidden states. [N x (B, U, D_dec)]\",\"cache – Pairs of (h_dec, dec_states) for each label sequences. (keys)\",\"use_lm – Whether to compute label ID sequences for LM.\",\"Returns: Decoder output sequences. (B, D_dec) dec_states: Decoder hidden states. [N x (B, U, D_dec)] lm_labels: Label ID sequences for LM. (B,)\",\"Return type: dec_out\",\"create_batch_states(states: List[Tensor | None], new_states: List[Tensor | None], check_list: List[List[int]])\",\"Create decoder hidden states sequences.\",\"Parameters:\",\"states – Decoder hidden states. [N x (B, U, D_dec)]\",\"new_states – Decoder hidden states. [B x [N x (1, U, D_dec)]]\",\"check_list – Label ID sequences.\",\"Returns: New decoder hidden states. [N x (B, U, D_dec)]\",\"Return type: states\",\"forward(dec_input: Tensor, dec_mask: Tensor)\",\"Encode label ID sequences.\",\"Parameters:\",\"dec_input – Label ID sequences. (B, U)\",\"dec_mask – Label mask sequences. (B, U)\",\"Returns: Decoder output sequences. (B, U, D_dec) dec_output_mask: Mask of decoder output sequences. (B, U)\",\"Return type: dec_output\",\"init_state(batch_size: int | None = None)\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. [N x None]\",\"Return type: state\",\"score(hyp: Hypothesis, cache: Dict[str, Any])\",\"One-step forward hypothesis.\",\"Parameters:\",\"hyp – Hypothesis.\",\"cache – Pairs of (dec_out, dec_state) for each label sequence. (key)\",\"Returns: Decoder output sequence. (1, D_dec) dec_state: Decoder hidden states. [N x (1, U, D_dec)] lm_label: Label ID for LM. (1,)\",\"Return type: dec_out\",\"select_state(states: List[Tensor | None], idx: int)\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. [N x (B, U, D_dec)]\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. [N x (1, U, D_dec)]\",\"Return type: state_idx\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"training : bool\"]},\"726\":{\"h\":\"espnet.nets.pytorch_backend.transducer.custom_encoder.CustomEncoder\",\"t\":[\"class espnet.nets.pytorch_backend.transducer.custom_encoder.CustomEncoder(idim: int, enc_arch: List, input_layer: str = 'linear', repeat_block: int = 1, self_attn_type: str = 'selfattn', positional_encoding_type: str = 'abs_pos', positionwise_layer_type: str = 'linear', positionwise_activation_type: str = 'relu', conv_mod_activation_type: str = 'relu', aux_enc_output_layers: List = [], input_layer_dropout_rate: float = 0.0, input_layer_pos_enc_dropout_rate: float = 0.0, padding_idx: int = -1)\",\"Bases: Module\",\"Custom encoder module for transducer models.\",\"Parameters:\",\"idim – Input dimension.\",\"enc_arch – Encoder block architecture (type and parameters).\",\"input_layer – Input layer type.\",\"repeat_block – Number of times blocks_arch is repeated.\",\"self_attn_type – Self-attention type.\",\"positional_encoding_type – Positional encoding type.\",\"positionwise_layer_type – Positionwise layer type.\",\"positionwise_activation_type – Positionwise activation type.\",\"conv_mod_activation_type – Convolutional module activation type.\",\"aux_enc_output_layers – Layer IDs for auxiliary encoder output sequences.\",\"input_layer_dropout_rate – Dropout rate for input layer.\",\"input_layer_pos_enc_dropout_rate – Dropout rate for input layer pos. enc.\",\"padding_idx – Padding symbol ID for embedding layer.\",\"Construct an CustomEncoder object.\",\"forward(feats: Tensor, mask: Tensor)\",\"Encode feature sequences.\",\"Parameters:\",\"feats – Feature sequences. (B, F, D_feats)\",\"feats_mask – Feature mask sequences. (B, 1, F)\",\"Returns: Encoder output sequences. (B, T, D_enc) with/without : Auxiliary encoder output sequences. (B, T, D_enc_aux)\",\"enc_out_mask: Mask for encoder output sequences. (B, 1, T) with/without : Mask for auxiliary encoder output sequences. (B, T, D_enc_aux)\",\"Return type: enc_out\",\"training : bool\"]},\"727\":{\"h\":\"espnet.nets.chainer_backend.transformer.training.CustomParallelUpdater\",\"t\":[\"class espnet.nets.chainer_backend.transformer.training.CustomParallelUpdater(train_iters, optimizer, converter, devices, accum_grad=1)\",\"Bases: MultiprocessParallelUpdater\",\"Custom Parallel Updater for chainer.\",\"Defines the main update routine.\",\"Parameters:\",\"train_iter (iterator|dict *[*str,iterator]) – Dataset iterator for the training dataset. It can also be a dictionary that maps strings to iterators. If this is just an iterator, then the iterator is registered by the name 'main'.\",\"optimizer (optimizer|dict *[*str,optimizer]) – Optimizer to update parameters. It can also be a dictionary that maps strings to optimizers. If this is just an optimizer, then the optimizer is registered by the name 'main'.\",\"converter (espnet.asr.chainer_backend.asr.CustomConverter) – Converter function to build input arrays. Each batch extracted by the main iterator and the device option are passed to this function. chainer.dataset.concat_examples() is used by default.\",\"device (torch.device) – Device to which the training data is sent. Negative value indicates the host memory (CPU).\",\"accum_grad (int) – The number of gradient accumulation. if set to 2, the network parameters will be updated once in twice, i.e. actual batchsize will be doubled.\",\"Initialize custom parallel updater.\",\"update()\",\"Update step for Custom Parallel Updater.\",\"update_core()\",\"Process main update routine for Custom Parallel Updater.\"]},\"728\":{\"h\":\"espnet.nets.chainer_backend.transformer.training.CustomUpdater\",\"t\":[\"class espnet.nets.chainer_backend.transformer.training.CustomUpdater(train_iter, optimizer, converter, device, accum_grad=1)\",\"Bases: StandardUpdater\",\"Custom updater for chainer.\",\"Parameters:\",\"train_iter (iterator|dict *[*str,iterator]) – Dataset iterator for the training dataset. It can also be a dictionary that maps strings to iterators. If this is just an iterator, then the iterator is registered by the name 'main'.\",\"optimizer (optimizer|dict *[*str,optimizer]) – Optimizer to update parameters. It can also be a dictionary that maps strings to optimizers. If this is just an optimizer, then the optimizer is registered by the name 'main'.\",\"converter (espnet.asr.chainer_backend.asr.CustomConverter) – Converter function to build input arrays. Each batch extracted by the main iterator and the device option are passed to this function. chainer.dataset.concat_examples() is used by default.\",\"device (intordict) – The destination device info to send variables. In the case of cpu or single gpu, device=-1 or 0, respectively. In the case of multi-gpu, device={“main”:0, “sub_1”: 1, …}.\",\"accum_grad (int) – The number of gradient accumulation. if set to 2, the network parameters will be updated once in twice, i.e. actual batchsize will be doubled.\",\"Initialize Custom Updater.\",\"update()\",\"Update step for Custom Updater.\",\"update_core()\",\"Process main update routine for Custom Updater.\"]},\"729\":{\"h\":\"espnet.nets.pytorch_backend.frontends.dnn_beamformer.DNN_Beamformer\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.dnn_beamformer.DNN_Beamformer(bidim, btype='blstmp', blayers=3, bunits=300, bprojs=320, bnmask=2, dropout_rate=0.0, badim=320, ref_channel: int = -1, beamformer_type='mvdr')\",\"Bases: Module\",\"DNN mask based Beamformer\",\"Citation: : Multichannel End-to-end Speech Recognition; T. Ochiai et al., 2017; https://arxiv.org/abs/1703.04783\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(data: ComplexTensor, ilens: LongTensor)\",\"The forward function\",\"Notation: : B: Batch C: Channel T: Time or Sequence length F: Freq\",\"Parameters:\",\"data (ComplexTensor) – (B, T, C, F)\",\"ilens (torch.Tensor) – (B,)\",\"Returns: (B, T, F) ilens (torch.Tensor): (B,)\",\"Return type: enhanced (ComplexTensor)\",\"training : bool\"]},\"730\":{\"h\":\"espnet.nets.pytorch_backend.frontends.dnn_wpe.DNN_WPE\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.dnn_wpe.DNN_WPE(wtype: str = 'blstmp', widim: int = 257, wlayers: int = 3, wunits: int = 300, wprojs: int = 320, dropout_rate: float = 0.0, taps: int = 5, delay: int = 3, use_dnn_mask: bool = True, iterations: int = 1, normalization: bool = False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(data: ComplexTensor, ilens: LongTensor)\",\"The forward function\",\"Notation: : B: Batch C: Channel T: Time or Sequence length F: Freq or Some dimension of the feature vector\",\"Parameters:\",\"data – (B, C, T, F)\",\"ilens – (B,)\",\"Returns: (B, C, T, F) ilens: (B,)\",\"Return type: data\",\"training : bool\"]},\"731\":{\"h\":\"espnet.nets.chainer_backend.transformer.decoder.Decoder\",\"t\":[\"class espnet.nets.chainer_backend.transformer.decoder.Decoder(odim, args, initialW=None, initial_bias=None)\",\"Bases: Chain\",\"Decoder layer.\",\"Parameters:\",\"odim (int) – The output dimension.\",\"n_layers (int) – Number of ecoder layers.\",\"n_units (int) – Number of attention units.\",\"d_units (int) – Dimension of input vector of decoder.\",\"h (int) – Number of attention heads.\",\"dropout (float) – Dropout rate.\",\"initialW (Initializer) – Initializer to initialize the weight.\",\"initial_bias (Initializer) – Initializer to initialize the bias.\",\"Initialize Decoder.\",\"forward(ys_pad, source, x_mask)\",\"Forward decoder.\",\"Parameters:\",\"e (xp.array) – input token ids, int64 (batch, maxlen_out)\",\"yy_mask (xp.array) – input token mask, uint8 (batch, maxlen_out)\",\"source (xp.array) – encoded memory, float32 (batch, maxlen_in, feat)\",\"xy_mask (xp.array) – encoded memory mask, uint8 (batch, maxlen_in)\",\"Return e: decoded token score before softmax (batch, maxlen_out, token)\",\"Return type: chainer.Variable\",\"make_attention_mask(source_block, target_block)\",\"Prepare the attention mask.\",\"Parameters:\",\"source_block (ndarray) – Source block with dimensions: (B x S).\",\"target_block (ndarray) – Target block with dimensions: (B x T).\",\"Returns: Mask with dimensions (B, S, T).\",\"Return type: ndarray\",\"recognize(e, yy_mask, source)\",\"Process recognition function.\"]},\"732\":{\"h\":\"espnet.nets.chainer_backend.transformer.decoder_layer.DecoderLayer\",\"t\":[\"class espnet.nets.chainer_backend.transformer.decoder_layer.DecoderLayer(n_units, d_units=0, h=8, dropout=0.1, initialW=None, initial_bias=None)\",\"Bases: Chain\",\"Single decoder layer module.\",\"Parameters:\",\"n_units (int) – Number of input/output dimension of a FeedForward layer.\",\"d_units (int) – Number of units of hidden layer in a FeedForward layer.\",\"h (int) – Number of attention heads.\",\"dropout (float) – Dropout rate\",\"Initialize DecoderLayer.\",\"forward(e, s, xy_mask, yy_mask, batch)\",\"Compute Encoder layer.\",\"Parameters:\",\"e (chainer.Variable) – Batch of padded features. (B, Lmax)\",\"s (chainer.Variable) – Batch of padded character. (B, Tmax)\",\"Returns: Computed variable of decoder.\",\"Return type: chainer.Variable\"]},\"733\":{\"h\":\"espnet.nets.pytorch_backend.lm.default.DefaultRNNLM\",\"t\":[\"class espnet.nets.pytorch_backend.lm.default.DefaultRNNLM(n_vocab, args)\",\"Bases: BatchScorerInterface, LMInterface, Module\",\"Default RNNLM for LMInterface Implementation.\"]},\"734\":{\"h\":\"NOTE\",\"t\":[\"PyTorch seems to have memory leak when one GPU compute this after data parallel. If parallel GPUs compute this, it seems to be fine. See also https://github.com/espnet/espnet/issues/1075\",\"Initialize class.\",\"Parameters:\",\"n_vocab (int) – The size of the vocabulary\",\"args (argparse.Namespace) – configurations. see py:method:add_arguments\",\"static add_arguments(parser)\",\"Add arguments to command line argument parser.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"final_score(state)\",\"Score eos.\",\"Parameters:state – Scorer state for prefix tokens\",\"Returns: final score\",\"Return type: float\",\"forward(x, t)\",\"Compute LM loss value from buffer sequences.\",\"Parameters:\",\"x (torch.Tensor) – Input ids. (batch, len)\",\"t (torch.Tensor) – Target ids. (batch, len)\",\"Returns: Tuple of : loss to backward (scalar), negative log-likelihood of t: -log p(t) (scalar) and the number of elements in x (scalar)\",\"Return type: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\",\"Notes\",\"The last two return values are used in perplexity: p(t)^{-n} = exp(-log p(t) / n)\",\"load_state_dict(d)\",\"Load state dict.\",\"score(y, state, x)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – 2D encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"state_dict()\",\"Dump state dict.\",\"training : bool\"]},\"735\":{\"h\":\"espnet.nets.pytorch_backend.fastspeech.duration_calculator.DurationCalculator\",\"t\":[\"class espnet.nets.pytorch_backend.fastspeech.duration_calculator.DurationCalculator(teacher_model)\",\"Bases: Module\",\"Duration calculator module for FastSpeech.\",\"Initialize duration calculator module.\",\"Parameters:teacher_model (e2e_tts_transformer.Transformer) – Pretrained auto-regressive Transformer.\",\"forward(xs, ilens, ys, olens, spembs=None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of the padded sequences of character ids (B, Tmax).\",\"ilens (Tensor) – Batch of lengths of each input sequence (B,).\",\"ys (Tensor) – Batch of the padded sequence of target features (B, Lmax, odim).\",\"olens (Tensor) – Batch of lengths of each output sequence (B,).\",\"spembs (Tensor,optional) – Batch of speaker embedding vectors (B, spk_embed_dim).\",\"Returns: Batch of durations (B, Tmax).\",\"Return type: Tensor\",\"training : bool\"]},\"736\":{\"h\":\"espnet.nets.pytorch_backend.fastspeech.duration_predictor.DurationPredictor\",\"t\":[\"class espnet.nets.pytorch_backend.fastspeech.duration_predictor.DurationPredictor(idim, n_layers=2, n_chans=384, kernel_size=3, dropout_rate=0.1, offset=1.0)\",\"Bases: Module\",\"Duration predictor module.\",\"This is a module of duration predictor described in FastSpeech: Fast, Robust and Controllable Text to Speech. The duration predictor predicts a duration of each frame in log domain from the hidden embeddings of encoder.\"]},\"737\":{\"h\":\"NOTE\",\"t\":[\"The calculation domain of outputs is different between in forward and in inference. In forward, the outputs are calculated in log domain but in inference, those are calculated in linear domain.\",\"Initilize duration predictor module.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"n_layers (int,optional) – Number of convolutional layers.\",\"n_chans (int,optional) – Number of channels of convolutional layers.\",\"kernel_size (int,optional) – Kernel size of convolutional layers.\",\"dropout_rate (float,optional) – Dropout rate.\",\"offset (float,optional) – Offset value to avoid nan in log domain.\",\"forward(xs, x_masks=None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of input sequences (B, Tmax, idim).\",\"x_masks (ByteTensor,optional) – Batch of masks indicating padded part (B, Tmax).\",\"Returns: Batch of predicted durations in log domain (B, Tmax).\",\"Return type: Tensor\",\"inference(xs, x_masks=None)\",\"Inference duration.\",\"Parameters:\",\"xs (Tensor) – Batch of input sequences (B, Tmax, idim).\",\"x_masks (ByteTensor,optional) – Batch of masks indicating padded part (B, Tmax).\",\"Returns: Batch of predicted durations in linear domain (B, Tmax).\",\"Return type: LongTensor\",\"training : bool\"]},\"738\":{\"h\":\"espnet.nets.pytorch_backend.fastspeech.duration_predictor.DurationPredictorLoss\",\"t\":[\"class espnet.nets.pytorch_backend.fastspeech.duration_predictor.DurationPredictorLoss(offset=1.0, reduction='mean')\",\"Bases: Module\",\"Loss function module for duration predictor.\",\"The loss value is Calculated in log domain to make it Gaussian.\",\"Initilize duration predictor loss module.\",\"Parameters:\",\"offset (float,optional) – Offset value to avoid nan in log domain.\",\"reduction (str) – Reduction type in loss calculation.\",\"forward(outputs, targets)\",\"Calculate forward propagation.\",\"Parameters:\",\"outputs (Tensor) – Batch of prediction durations in log domain (B, T)\",\"targets (LongTensor) – Batch of groundtruth durations in linear domain (B, T)\",\"Returns: Mean squared error loss value.\",\"Return type: Tensor\"]},\"739\":{\"h\":\"NOTE\",\"t\":[\"outputs is in log domain but targets is in linear domain.\",\"training : bool\"]},\"740\":{\"h\":\"espnet.nets.pytorch_backend.transformer.dynamic_conv.DynamicConvolution\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.dynamic_conv.DynamicConvolution(wshare, n_feat, dropout_rate, kernel_size, use_kernel_mask=False, use_bias=False)\",\"Bases: Module\",\"Dynamic Convolution layer.\",\"This implementation is based on https://github.com/pytorch/fairseq/tree/master/fairseq\",\"Parameters:\",\"wshare (int) – the number of kernel of convolution\",\"n_feat (int) – the number of features\",\"dropout_rate (float) – dropout_rate\",\"kernel_size (int) – kernel size (length)\",\"use_kernel_mask (bool) – Use causal mask or not for convolution kernel\",\"use_bias (bool) – Use bias term or not.\",\"Construct Dynamic Convolution layer.\",\"forward(query, key, value, mask)\",\"Forward of ‘Dynamic Convolution’.\",\"This function takes query, key and value but uses only quert. This is just for compatibility with self-attention layer (attention.py)\",\"Parameters:\",\"query (torch.Tensor) – (batch, time1, d_model) input tensor\",\"key (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"value (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"mask (torch.Tensor) – (batch, time1, time2) mask\",\"Returns: (batch, time1, d_model) output\",\"Return type: x (torch.Tensor)\",\"training : bool\"]},\"741\":{\"h\":\"espnet.nets.pytorch_backend.transformer.dynamic_conv2d.DynamicConvolution2D\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.dynamic_conv2d.DynamicConvolution2D(wshare, n_feat, dropout_rate, kernel_size, use_kernel_mask=False, use_bias=False)\",\"Bases: Module\",\"Dynamic 2-Dimensional Convolution layer.\",\"This implementation is based on https://github.com/pytorch/fairseq/tree/master/fairseq\",\"Parameters:\",\"wshare (int) – the number of kernel of convolution\",\"n_feat (int) – the number of features\",\"dropout_rate (float) – dropout_rate\",\"kernel_size (int) – kernel size (length)\",\"use_kernel_mask (bool) – Use causal mask or not for convolution kernel\",\"use_bias (bool) – Use bias term or not.\",\"Construct Dynamic 2-Dimensional Convolution layer.\",\"forward(query, key, value, mask)\",\"Forward of ‘Dynamic 2-Dimensional Convolution’.\",\"This function takes query, key and value but uses only query. This is just for compatibility with self-attention layer (attention.py)\",\"Parameters:\",\"query (torch.Tensor) – (batch, time1, d_model) input tensor\",\"key (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"value (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"mask (torch.Tensor) – (batch, time1, time2) mask\",\"Returns: (batch, time1, d_model) output\",\"Return type: x (torch.Tensor)\",\"training : bool\"]},\"742\":{\"h\":\"espnet.nets.chainer_backend.e2e_asr.E2E\",\"t\":[\"<!-- _espnet.nets.chainer_backend.e2e_asr.E2E -->\",\"class espnet.nets.chainer_backend.e2e_asr.E2E(idim, odim, args, flag_return=True)\",\"Bases: ChainerASRInterface\",\"E2E module for chainer backend.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"args (parser.args) – Training config.\",\"flag_return (bool) – If True, train() would return additional metrics in addition to the training loss.\",\"Construct an E2E object.\",\"Parameters:\",\"idim (int) – dimension of inputs\",\"odim (int) – dimension of outputs\",\"args (Namespace) – argument Namespace containing options\",\"static add_arguments(parser)\",\"Add arguments.\",\"calculate_all_attentions(xs, ilens, ys)\",\"E2E attention calculation.\",\"Parameters:\",\"xs (List) – List of padded input sequences. [(T1, idim), (T2, idim), …]\",\"ilens (np.ndarray) – Batch of lengths of input sequences. (B)\",\"ys (List) – List of character id sequence tensor. [(L1), (L2), (L3), …]\",\"Returns: Attention weights. (B, Lmax, Tmax)\",\"Return type: float np.ndarray\",\"static custom_converter(subsampling_factor=0)\",\"Get customconverter of the model.\",\"static custom_parallel_updater(iters, optimizer, converter, devices, accum_grad=1)\",\"Get custom_parallel_updater of the model.\",\"static custom_updater(iters, optimizer, converter, device=-1, accum_grad=1)\",\"Get custom_updater of the model.\",\"forward(xs, ilens, ys)\",\"E2E forward propagation.\",\"Parameters:\",\"xs (chainer.Variable) – Batch of padded character ids. (B, Tmax)\",\"ilens (chainer.Variable) – Batch of length of each input batch. (B,)\",\"ys (chainer.Variable) – Batch of padded target features. (B, Lmax, odim)\",\"Returns: Loss that calculated by attention and ctc loss. float (optional): Ctc loss. float (optional): Attention loss. float (optional): Accuracy.\",\"Return type: float\",\"get_total_subsampling_factor()\",\"Get total subsampling factor.\",\"recognize(x, recog_args, char_list, rnnlm=None)\",\"E2E greedy/beam search.\",\"Parameters:\",\"x (chainer.Variable) – Input tensor for recognition.\",\"recog_args (parser.args) – Arguments of config file.\",\"char_list (List *[*str]) – List of Characters.\",\"rnnlm (Module) – RNNLM module defined at espnet.lm.chainer_backend.lm.\",\"Returns: Result of recognition.\",\"Return type: List[Dict[str, Any]]\"]},\"743\":{\"h\":\"espnet.nets.chainer_backend.deterministic_embed_id.EmbedID\",\"t\":[\"class espnet.nets.chainer_backend.deterministic_embed_id.EmbedID(in_size, out_size, initialW=None, ignore_label=None)\",\"Bases: Link\",\"Efficient linear layer for one-hot input.\",\"This is a link that wraps the embed_id() function. This link holds the ID (word) embedding matrix W as a parameter.\",\"Parameters:\",\"in_size (int) – Number of different identifiers (a.k.a. vocabulary size).\",\"out_size (int) – Output dimension.\",\"initialW (Initializer) – Initializer to initialize the weight.\",\"ignore_label (int) – If ignore_label is an int value, i-th column of return value is filled with 0.\",\"embed_id()\",\"W\",\"Embedding parameter matrix.\",\"Type:Variable\"]},\"744\":{\"h\":\"Examples\",\"t\":[\">>> W = np.array([[0, 0, 0], ... [1, 1, 1], ... [2, 2, 2]]).astype('f') >>> W array([[ 0., 0., 0.], [ 1., 1., 1.], [ 2., 2., 2.]], dtype=float32) >>> l = L.EmbedID(W.shape[0], W.shape[1], initialW=W) >>> x = np.array([2, 1]).astype('i') >>> x array([2, 1], dtype=int32) >>> y = l(x) >>> y.data array([[ 2., 2., 2.], [ 1., 1., 1.]], dtype=float32)\",\"ignore_label = None\"]},\"745\":{\"h\":\"espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction\",\"t\":[\"class espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction(ignore_label=None)\",\"Bases: FunctionNode\",\"backward(indexes, grad_outputs)\",\"Computes gradients w.r.t. specified inputs given output gradients.\",\"This method is used to compute one step of the backpropagation corresponding to the forward computation of this function node. Given the gradients w.r.t. output variables, this method computes the gradients w.r.t. specified input variables. Note that this method does not need to compute any input gradients not specified by target_input_indices.\",\"Unlike Function.backward(), gradients are given as Variable objects and this method itself has to return input gradients as Variable objects. It enables the function node to return the input gradients with the full computational history, in which case it supports differentiable backpropagation or higher-order differentiation.\",\"The default implementation returns None s, which means the function is not differentiable.\",\"Parameters:\",\"target_input_indexes (tupleofint) – Sorted indices of the input variables w.r.t. which the gradients are required. It is guaranteed that this tuple contains at least one element.\",\"grad_outputs (tuple of Variables) – Gradients w.r.t. the output variables. If the gradient w.r.t. an output variable is not given, the corresponding element is None.\",\"Returns: Tuple of variables that represent the gradients w.r.t. specified input variables. The length of the tuple can be same as either len(target_input_indexes) or the number of inputs. In the latter case, the elements not specified by target_input_indexes will be discarded.\",\"SEE ALSO\",\"backward_accumulate() provides an alternative interface that allows you to implement the backward computation fused with the gradient accumulation.\",\"check_type_forward(in_types)\",\"Checks types of input data before forward propagation.\",\"This method is called before forward() and validates the types of input variables using the type checking utilities.\",\"Parameters:in_types (TypeInfoTuple) – The type information of input variables for forward().\",\"forward(inputs)\",\"Computes the output arrays from the input arrays.\",\"It delegates the procedure to forward_cpu() or forward_gpu() by default. Which of them this method selects is determined by the type of input arrays. Implementations of FunctionNode must implement either CPU/GPU methods or this method.\",\"Parameters:inputs – Tuple of input array(s).\",\"Returns: Tuple of output array(s).\",\"WARNING\",\"Implementations of FunctionNode must take care that the return value must be a tuple even if it returns only one array.\"]},\"746\":{\"h\":\"espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDGrad\",\"t\":[\"class espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDGrad(w_shape, ignore_label=None)\",\"Bases: FunctionNode\",\"backward(indexes, grads)\",\"Computes gradients w.r.t. specified inputs given output gradients.\",\"This method is used to compute one step of the backpropagation corresponding to the forward computation of this function node. Given the gradients w.r.t. output variables, this method computes the gradients w.r.t. specified input variables. Note that this method does not need to compute any input gradients not specified by target_input_indices.\",\"Unlike Function.backward(), gradients are given as Variable objects and this method itself has to return input gradients as Variable objects. It enables the function node to return the input gradients with the full computational history, in which case it supports differentiable backpropagation or higher-order differentiation.\",\"The default implementation returns None s, which means the function is not differentiable.\",\"Parameters:\",\"target_input_indexes (tupleofint) – Sorted indices of the input variables w.r.t. which the gradients are required. It is guaranteed that this tuple contains at least one element.\",\"grad_outputs (tuple of Variables) – Gradients w.r.t. the output variables. If the gradient w.r.t. an output variable is not given, the corresponding element is None.\",\"Returns: Tuple of variables that represent the gradients w.r.t. specified input variables. The length of the tuple can be same as either len(target_input_indexes) or the number of inputs. In the latter case, the elements not specified by target_input_indexes will be discarded.\",\"SEE ALSO\",\"backward_accumulate() provides an alternative interface that allows you to implement the backward computation fused with the gradient accumulation.\",\"forward(inputs)\",\"Computes the output arrays from the input arrays.\",\"It delegates the procedure to forward_cpu() or forward_gpu() by default. Which of them this method selects is determined by the type of input arrays. Implementations of FunctionNode must implement either CPU/GPU methods or this method.\",\"Parameters:inputs – Tuple of input array(s).\",\"Returns: Tuple of output array(s).\",\"WARNING\",\"Implementations of FunctionNode must take care that the return value must be a tuple even if it returns only one array.\"]},\"747\":{\"h\":\"espnet.nets.chainer_backend.transformer.encoder.Encoder\",\"t\":[\"class espnet.nets.chainer_backend.transformer.encoder.Encoder(idim, attention_dim=256, attention_heads=4, linear_units=2048, num_blocks=6, dropout_rate=0.1, positional_dropout_rate=0.1, attention_dropout_rate=0.0, input_layer='conv2d', pos_enc_class=<class 'espnet.nets.chainer_backend.transformer.embedding.PositionalEncoding'>, initialW=None, initial_bias=None)\",\"Bases: Chain\",\"Encoder.\",\"Parameters:\",\"input_type (str) – Sampling type. input_type must be conv2d or ‘linear’ currently.\",\"idim (int) – Dimension of inputs.\",\"n_layers (int) – Number of encoder layers.\",\"n_units (int) – Number of input/output dimension of a FeedForward layer.\",\"d_units (int) – Number of units of hidden layer in a FeedForward layer.\",\"h (int) – Number of attention heads.\",\"dropout (float) – Dropout rate\",\"Initialize Encoder.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"args (Namespace) – Training config.\",\"initialW (int,optional) – Initializer to initialize the weight.\",\"initial_bias (bool,optional) – Initializer to initialize the bias.\",\"forward(e, ilens)\",\"Compute Encoder layer.\",\"Parameters:\",\"e (chainer.Variable) – Batch of padded character. (B, Tmax)\",\"ilens (chainer.Variable) – Batch of length of each input batch. (B,)\",\"Returns: Computed variable of encoder. numpy.array: Mask. chainer.Variable: Batch of lengths of each encoder outputs.\",\"Return type: chainer.Variable\"]},\"748\":{\"h\":\"espnet.nets.chainer_backend.transformer.encoder_layer.EncoderLayer\",\"t\":[\"class espnet.nets.chainer_backend.transformer.encoder_layer.EncoderLayer(n_units, d_units=0, h=8, dropout=0.1, initialW=None, initial_bias=None)\",\"Bases: Chain\",\"Single encoder layer module.\",\"Parameters:\",\"n_units (int) – Number of input/output dimension of a FeedForward layer.\",\"d_units (int) – Number of units of hidden layer in a FeedForward layer.\",\"h (int) – Number of attention heads.\",\"dropout (float) – Dropout rate\",\"Initialize EncoderLayer.\",\"forward(e, xx_mask, batch)\",\"Forward Positional Encoding.\"]},\"749\":{\"h\":\"espnet.nets.pytorch_backend.transformer.encoder_mix.EncoderMix\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.encoder_mix.EncoderMix(idim, attention_dim=256, attention_heads=4, linear_units=2048, num_blocks_sd=4, num_blocks_rec=8, dropout_rate=0.1, positional_dropout_rate=0.1, attention_dropout_rate=0.0, input_layer='conv2d', pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before=True, concat_after=False, positionwise_layer_type='linear', positionwise_conv_kernel_size=1, padding_idx=-1, num_spkrs=2)\",\"Bases: Encoder, Module\",\"Transformer encoder module.\",\"Parameters:\",\"idim (int) – input dim\",\"attention_dim (int) – dimension of attention\",\"attention_heads (int) – the number of heads of multi head attention\",\"linear_units (int) – the number of units of position-wise feed forward\",\"num_blocks (int) – the number of decoder blocks\",\"dropout_rate (float) – dropout rate\",\"attention_dropout_rate (float) – dropout rate in attention\",\"positional_dropout_rate (float) – dropout rate after adding positional encoding\",\"input_layer (strortorch.nn.Module) – input layer type\",\"pos_enc_class (class) – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before (bool) – whether to use layer_norm before the first block\",\"concat_after (bool) – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type (str) – linear of conv1d\",\"positionwise_conv_kernel_size (int) – kernel size of positionwise conv1d layer\",\"padding_idx (int) – padding_idx for input_layer=embed\",\"Construct an Encoder object.\",\"forward(xs, masks)\",\"Encode input sequence.\",\"Parameters:\",\"xs (torch.Tensor) – input tensor\",\"masks (torch.Tensor) – input mask\",\"Returns: position embedded tensor and mask\",\"Rtype Tuple[torch.Tensor, torch.Tensor]:\",\"forward_one_step(xs, masks, *, cache=None)\",\"Encode input frame.\",\"Parameters:\",\"xs (torch.Tensor) – input tensor\",\"masks (torch.Tensor) – input mask\",\"cache (List *[*torch.Tensor]) – cache tensors\",\"Returns: position embedded tensor, mask and new cache\",\"Rtype Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\",\"training : bool\"]},\"750\":{\"h\":\"espnet.nets.e2e_mt_common.ErrorCalculator\",\"t\":[\"<!-- _espnet.nets.e2e_mt_common.ErrorCalculator -->\",\"class espnet.nets.e2e_mt_common.ErrorCalculator(char_list, sym_space, sym_pad, report_bleu=False)\",\"Bases: object\",\"Calculate BLEU for ST and MT models during training.\",\"Parameters:\",\"y_hats – numpy array with predicted text\",\"y_pads – numpy array with true (target) text\",\"char_list – vocabulary list\",\"sym_space – space symbol\",\"sym_pad – pad symbol\",\"report_bleu – report BLUE score if True\",\"Construct an ErrorCalculator object.\",\"calculate_bleu_ctc(ys_hat, ys_pad)\",\"Calculate sentence-level BLEU score for CTC.\",\"Parameters:\",\"ys_hat (torch.Tensor) – prediction (batch, seqlen)\",\"ys_pad (torch.Tensor) – reference (batch, seqlen)\",\"Returns: corpus-level BLEU score\",\":rtype float\",\"calculate_corpus_bleu(ys_hat, ys_pad)\",\"Calculate corpus-level BLEU score in a mini-batch.\",\"Parameters:\",\"seqs_hat (torch.Tensor) – prediction (batch, seqlen)\",\"seqs_true (torch.Tensor) – reference (batch, seqlen)\",\"Returns: corpus-level BLEU score\",\":rtype float\"]},\"751\":{\"h\":\"espnet.nets.transducer_decoder_interface.ExtendedHypothesis\",\"t\":[\"class espnet.nets.transducer_decoder_interface.ExtendedHypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor, lm_state: Dict[str, Any] | List[Any] | None = None, dec_out: List[Tensor] | None = None, lm_scores: Tensor | None = None)\",\"Bases: Hypothesis\",\"Extended hypothesis definition for NSC beam search and mAES.\",\"dec_out : List[Tensor]= None\",\"lm_scores : Tensor= None\"]},\"752\":{\"h\":\"espnet.nets.pytorch_backend.frontends.feature_transform.FeatureTransform\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.feature_transform.FeatureTransform(fs: int = 16000, n_fft: int = 512, n_mels: int = 80, fmin: float = 0.0, fmax: float | None = None, stats_file: str | None = None, apply_uttmvn: bool = True, uttmvn_norm_means: bool = True, uttmvn_norm_vars: bool = False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: ComplexTensor, ilens: LongTensor | ndarray | List[int])\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"753\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"754\":{\"h\":\"espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer(idim, odim, args=None)\",\"Bases: TTSInterface, Module\",\"Feed Forward Transformer for TTS a.k.a. FastSpeech.\",\"This is a module of FastSpeech, feed-forward Transformer with duration predictor described in FastSpeech: Fast, Robust and Controllable Text to Speech, which does not require any auto-regressive processing during inference, resulting in fast decoding compared with auto-regressive Transformer.\",\"Initialize feed-forward Transformer module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"args (Namespace,optional) – \",\"elayers (int): Number of encoder layers.\",\"eunits (int): Number of encoder hidden units.\",\"adim (int): Number of attention transformation dimensions.\",\"aheads (int): Number of heads for multi head attention.\",\"dlayers (int): Number of decoder layers.\",\"dunits (int): Number of decoder hidden units.\",\"use_scaled_pos_enc (bool): : Whether to use trainable scaled positional encoding.\",\"encoder_normalize_before (bool): : Whether to perform layer normalization before encoder block.\",\"decoder_normalize_before (bool): : Whether to perform layer normalization before decoder block.\",\"encoder_concat_after (bool): Whether to concatenate attention : layer’s input and output in encoder.\",\"decoder_concat_after (bool): Whether to concatenate attention : layer’s input and output in decoder.\",\"duration_predictor_layers (int): Number of duration predictor layers.\",\"duration_predictor_chans (int): Number of duration predictor channels.\",\"duration_predictor_kernel_size (int): : Kernel size of duration predictor.\",\"spk_embed_dim (int): Number of speaker embedding dimensions.\",\"spk_embed_integration_type: How to integrate speaker embedding.\",\"teacher_model (str): Teacher auto-regressive transformer model path.\",\"reduction_factor (int): Reduction factor.\",\"transformer_init (float): How to initialize transformer parameters.\",\"transformer_lr (float): Initial value of learning rate.\",\"transformer_warmup_steps (int): Optimizer warmup steps.\",\"transformer_enc_dropout_rate (float): : Dropout rate in encoder except attention & positional encoding.\",\"transformer_enc_positional_dropout_rate (float): : Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float): : Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float): : Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float): : Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float): : Dropout rate in deocoder self-attention module.\",\"transformer_enc_dec_attn_dropout_rate (float): : Dropout rate in encoder-deocoder attention module.\",\"use_masking (bool): : Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool): : Whether to apply weighted masking in loss calculation.\",\"transfer_encoder_from_teacher: : Whether to transfer encoder using teacher encoder parameters.\",\"transferred_encoder_module: : Encoder module to be initialized using teacher parameters.\",\"static add_arguments(parser)\",\"Add model-specific arguments to the parser.\",\"property attention_plot_class\",\"Return plot class for attention weight plot.\",\"property base_plot_keys\",\"Return base key names to plot during training.\",\"keys should match what chainer.reporter reports. If you add the key loss, the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values.\",\"Returns: List of strings which are base keys to plot during training.\",\"Return type: list\",\"calculate_all_attentions(xs, ilens, ys, olens, spembs=None, extras=None, *args, **kwargs)\",\"Calculate all of the attention weights.\",\"Parameters:\",\"xs (Tensor) – Batch of padded character ids (B, Tmax).\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Tensor,optional) – Batch of speaker embedding vectors (B, spk_embed_dim).\",\"extras (Tensor,optional) – Batch of precalculated durations (B, Tmax, 1).\",\"Returns: Dict of attention weights and outputs.\",\"Return type: dict\",\"forward(xs, ilens, ys, olens, spembs=None, extras=None, *args, **kwargs)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of padded character ids (B, Tmax).\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Tensor,optional) – Batch of speaker embedding vectors (B, spk_embed_dim).\",\"extras (Tensor,optional) – Batch of precalculated durations (B, Tmax, 1).\",\"Returns: Loss value.\",\"Return type: Tensor\",\"inference(x, inference_args, spemb=None, *args, **kwargs)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"x (Tensor) – Input sequence of characters (T,).\",\"inference_args (Namespace) – Dummy for compatibility.\",\"spemb (Tensor,optional) – Speaker embedding vector (spk_embed_dim).\",\"Returns: Output sequence of features (L, odim). None: Dummy for compatibility. None: Dummy for compatibility.\",\"Return type: Tensor\",\"training : bool\"]},\"755\":{\"h\":\"espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformerLoss\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformerLoss(use_masking=True, use_weighted_masking=False)\",\"Bases: Module\",\"Loss function module for feed-forward Transformer.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs, before_outs, d_outs, ys, ds, ilens, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"d_outs (Tensor) – Batch of outputs of duration predictor (B, Tmax).\",\"ys (Tensor) – Batch of target features (B, Lmax, odim).\",\"ds (Tensor) – Batch of durations (B, Tmax).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Duration predictor loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"756\":{\"h\":\"espnet.nets.pytorch_backend.frontends.frontend.Frontend\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.frontend.Frontend(idim: int, use_wpe: bool = False, wtype: str = 'blstmp', wlayers: int = 3, wunits: int = 300, wprojs: int = 320, wdropout_rate: float = 0.0, taps: int = 5, delay: int = 3, use_dnn_mask_for_wpe: bool = True, use_beamformer: bool = False, btype: str = 'blstmp', blayers: int = 3, bunits: int = 300, bprojs: int = 320, bnmask: int = 2, badim: int = 320, ref_channel: int = -1, bdropout_rate=0.0)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: ComplexTensor, ilens: LongTensor | ndarray | List[int])\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"757\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"758\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.GDCAttLoc\",\"t\":[\"class espnet.nets.pytorch_backend.rnn.attentions.GDCAttLoc(eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Global duration control attention module. Reference: Singing-Tacotron: Global Duration Control Attention and Dynamic Filter for End-to-end Singing Voice Synthesis (https://arxiv.org/abs/2202.07907) :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int att_dim: attention dimension :param int aconv_chans: # channels of attention convolution :param int aconv_filts: filter size of attention convolution :param bool han_mode: flag to swith on mode of hierarchical attention\",\"and not store pre_compute_enc_h\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_hs_pad, enc_hs_len, trans_token, dec_z, att_prev, scaling=1.0, last_attended_idx=None, backward_window=1, forward_window=3)\",\"Calcualte AttLoc forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor trans_token: Global transition token\",\"for duration (B x T_max x 1)\",\"Parameters:\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – previous attention weight (B x T_max)\",\"scaling (float) – scaling parameter before applying softmax\",\"forward_window (int) – forward window size when constraining attention\",\"last_attended_idx (int) – index of the inputs of the last attended\",\"backward_window (int) – backward window size in attention constraint\",\"forward_window – forward window size in attetion constraint\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"reset states\",\"training : bool\"]},\"759\":{\"h\":\"espnet.nets.pytorch_backend.gtn_ctc.GTNCTCLossFunction\",\"t\":[\"class espnet.nets.pytorch_backend.gtn_ctc.GTNCTCLossFunction(*args, **kwargs)\",\"Bases: Function\",\"GTN CTC module.\",\"static backward(ctx, grad_output)\",\"Backward computation.\",\"Parameters:grad_output (torch.tensor) – backward passed gradient value\",\"Returns: cumulative gradient output\",\"Return type: (torch.Tensor, None, None, None)\",\"static create_ctc_graph(target, blank_idx)\",\"Build gtn graph.\",\"Parameters:\",\"target (list) – single target sequence\",\"blank_idx (int) – index of blank token\",\"Returns: gtn graph of target sequence\",\"Return type: gtn.Graph\",\"static forward(ctx, log_probs, targets, ilens, blank_idx=0, reduction='none')\",\"Forward computation.\",\"Parameters:\",\"log_probs (torch.tensor) – batched log softmax probabilities (B, Tmax, oDim)\",\"targets (list) – batched target sequences, list of lists\",\"blank_idx (int) – index of blank token\",\"Returns: ctc loss value\",\"Return type: torch.Tensor\"]},\"760\":{\"h\":\"espnet.nets.pytorch_backend.frontends.feature_transform.GlobalMVN\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.feature_transform.GlobalMVN(stats_file: str, norm_means: bool = True, norm_vars: bool = True, eps: float = 1e-20)\",\"Bases: Module\",\"Apply global mean and variance normalization\",\"Parameters:\",\"stats_file (str) – npy file of 1-dim array or text file. From the _first element to the {(len(array) - 1) / 2}th element are treated as the sum of features, and the rest excluding the last elements are treated as the sum of the square value of features, and the last elements eqauls to the number of samples.\",\"std_floor (float) –\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x: Tensor, ilens: LongTensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"761\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"762\":{\"h\":\"espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss(sigma=0.4, alpha=1.0, reset_always=True)\",\"Bases: Module\",\"Guided attention loss function module.\",\"This module calculates the guided attention loss described in Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention, which forces the attention to be diagonal.\",\"Initialize guided attention loss module.\",\"Parameters:\",\"sigma (float,optional) – Standard deviation to control how close attention to a diagonal.\",\"alpha (float,optional) – Scaling coefficient (lambda).\",\"reset_always (bool,optional) – Whether to always reset masks.\",\"forward(att_ws, ilens, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"att_ws (Tensor) – Batch of attention weights (B, T_max_out, T_max_in).\",\"ilens (LongTensor) – Batch of input lengths (B,).\",\"olens (LongTensor) – Batch of output lengths (B,).\",\"Returns: Guided attention loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"763\":{\"h\":\"espnet.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss(sigma=0.4, alpha=1.0, reset_always=True)\",\"Bases: GuidedAttentionLoss\",\"Guided attention loss function module for multi head attention.\",\"Parameters:\",\"sigma (float,optional) – Standard deviation to control\",\"diagonal. (how close attention to a) –\",\"alpha (float,optional) – Scaling coefficient (lambda).\",\"reset_always (bool,optional) – Whether to always reset masks.\",\"Initialize guided attention loss module.\",\"Parameters:\",\"sigma (float,optional) – Standard deviation to control how close attention to a diagonal.\",\"alpha (float,optional) – Scaling coefficient (lambda).\",\"reset_always (bool,optional) – Whether to always reset masks.\",\"forward(att_ws, ilens, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"att_ws (Tensor) – Batch of multi head attention weights (B, H, T_max_out, T_max_in).\",\"ilens (LongTensor) – Batch of input lengths (B,).\",\"olens (LongTensor) – Batch of output lengths (B,).\",\"Returns: Guided attention loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"764\":{\"h\":\"espnet.nets.pytorch_backend.tacotron2.cbhg.HighwayNet\",\"t\":[\"class espnet.nets.pytorch_backend.tacotron2.cbhg.HighwayNet(idim)\",\"Bases: Module\",\"Highway Network module.\",\"This is a module of Highway Network introduced in Highway Networks.\",\"Initialize Highway Network module.\",\"Parameters:idim (int) – Dimension of the inputs.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Batch of inputs (B, …, idim).\",\"Returns: Batch of outputs, which are the same shape as inputs (B, …, idim).\",\"Return type: Tensor\",\"training : bool\"]},\"765\":{\"h\":\"espnet.nets.beam_search_partially_AR.Hypothesis\",\"t\":[\"class espnet.nets.beam_search_partially_AR.Hypothesis(yseq: Tensor, score: float | Tensor = 0, scores: Dict[str, float | Tensor] = {}, states: Dict[str, Any] = {})\",\"Bases: tuple\",\"Hypothesis data type.\",\"Create new instance of Hypothesis(yseq, score, scores, states)\",\"asdict()\",\"Convert data to JSON-friendly dict.\",\"score : float | Tensor\",\"Alias for field number 1\",\"scores : Dict[str, float | Tensor]\",\"Alias for field number 2\",\"states : Dict[str, Any]\",\"Alias for field number 3\",\"yseq : Tensor\",\"Alias for field number 0\"]},\"766\":{\"h\":\"espnet.nets.pytorch_backend.transducer.joint_network.JointNetwork\",\"t\":[\"class espnet.nets.pytorch_backend.transducer.joint_network.JointNetwork(joint_output_size: int, encoder_output_size: int, decoder_output_size: int, joint_space_size: int, joint_activation_type: int)\",\"Bases: Module\",\"Transducer joint network module.\",\"Parameters:\",\"joint_output_size – Joint network output dimension\",\"encoder_output_size – Encoder output dimension.\",\"decoder_output_size – Decoder output dimension.\",\"joint_space_size – Dimension of joint space.\",\"joint_activation_type – Type of activation for joint network.\",\"Joint network initializer.\",\"forward(enc_out: Tensor, dec_out: Tensor, is_aux: bool = False, quantization: bool = False)\",\"Joint computation of encoder and decoder hidden state sequences.\",\"Parameters:\",\"enc_out – Expanded encoder output state sequences (B, T, 1, D_enc)\",\"dec_out – Expanded decoder output state sequences (B, 1, U, D_dec)\",\"is_aux – Whether auxiliary tasks in used.\",\"quantization – Whether dynamic quantization is used.\",\"Returns: Joint output state sequences. (B, T, U, D_out)\",\"Return type: joint_out\",\"training : bool\"]},\"767\":{\"h\":\"espnet.nets.lm_interface.LMInterface\",\"t\":[\"<!-- _espnet.nets.lm_interface.LMInterface -->\",\"class espnet.nets.lm_interface.LMInterface\",\"Bases: ScorerInterface\",\"LM Interface for ESPnet model implementation.\",\"static add_arguments(parser)\",\"Add arguments to command line argument parser.\",\"classmethod build(n_vocab: int, **kwargs)\",\"Initialize this class with python-level args.\",\"Parameters:idim (int) – The number of vocabulary.\",\"Returns: A new instance of LMInterface.\",\"Return type: LMinterface\",\"forward(x, t)\",\"Compute LM loss value from buffer sequences.\",\"Parameters:\",\"x (torch.Tensor) – Input ids. (batch, len)\",\"t (torch.Tensor) – Target ids. (batch, len)\",\"Returns: Tuple of : loss to backward (scalar), negative log-likelihood of t: -log p(t) (scalar) and the number of elements in x (scalar)\",\"Return type: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\",\"Notes\",\"The last two return values are used in perplexity: p(t)^{-n} = exp(-log p(t) / n)\"]},\"768\":{\"h\":\"espnet.nets.chainer_backend.transformer.label_smoothing_loss.LabelSmoothingLoss\",\"t\":[\"class espnet.nets.chainer_backend.transformer.label_smoothing_loss.LabelSmoothingLoss(smoothing, n_target_vocab, normalize_length=False, ignore_id=-1)\",\"Bases: Chain\",\"Label Smoothing Loss.\",\"Parameters:\",\"smoothing (float) – smoothing rate (0.0 means the conventional CE).\",\"n_target_vocab (int) – number of classes.\",\"normalize_length (bool) – normalize loss by sequence length if True.\",\"Initialize Loss.\",\"forward(ys_block, ys_pad)\",\"Forward Loss.\",\"Parameters:\",\"ys_block (chainer.Variable) – Predicted labels.\",\"ys_pad (chainer.Variable) – Target (true) labels.\",\"Returns: Training loss.\",\"Return type: float\"]},\"769\":{\"h\":\"espnet.nets.chainer_backend.transformer.layer_norm.LayerNorm\",\"t\":[\"class espnet.nets.chainer_backend.transformer.layer_norm.LayerNorm(dims, eps=1e-12)\",\"Bases: LayerNormalization\",\"Redirect to L.LayerNormalization.\",\"Initialize LayerNorm.\"]},\"770\":{\"h\":\"espnet.nets.pytorch_backend.transformer.embedding.LearnableFourierPosEnc\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.embedding.LearnableFourierPosEnc(d_model, dropout_rate=0.0, max_len=5000, gamma=1.0, apply_scaling=False, hidden_dim=None)\",\"Bases: Module\",\"Learnable Fourier Features for Positional Encoding.\",\"See https://arxiv.org/pdf/2106.02795.pdf\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"gamma (float) – init parameter for the positional kernel variance see https://arxiv.org/pdf/2106.02795.pdf.\",\"apply_scaling (bool) – Whether to scale the input before adding the pos encoding.\",\"hidden_dim (int) – if not None, we modulate the pos encodings with an MLP whose hidden layer has hidden_dim neurons.\",\"Initialize class.\",\"extend_pe(x)\",\"Reset the positional encodings.\",\"forward(x: Tensor)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"771\":{\"h\":\"espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention(n_head, n_feat, dropout_rate, zero_triu=False)\",\"Bases: MultiHeadedAttention\",\"Multi-Head Attention layer with relative position encoding (old version).\",\"Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"Paper: https://arxiv.org/abs/1901.02860\",\"Parameters:\",\"n_head (int) – The number of heads.\",\"n_feat (int) – The number of features.\",\"dropout_rate (float) – Dropout rate.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"Construct an RelPositionMultiHeadedAttention object.\",\"forward(query, key, value, pos_emb, mask)\",\"Compute ‘Scaled Dot Product Attention’ with rel. positional encoding.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"pos_emb (torch.Tensor) – Positional embedding tensor (#batch, time1, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"rel_shift(x)\",\"Compute relative positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, head, time1, time2).\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"772\":{\"h\":\"espnet.nets.pytorch_backend.transformer.embedding.LegacyRelPositionalEncoding\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.embedding.LegacyRelPositionalEncoding(d_model, dropout_rate, max_len=5000)\",\"Bases: PositionalEncoding\",\"Relative positional encoding module (old version).\",\"Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"See : Appendix B in https://arxiv.org/abs/1901.02860\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"Initialize class.\",\"forward(x)\",\"Compute positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *). torch.Tensor: Positional embedding tensor (1, time, *).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"773\":{\"h\":\"espnet.nets.scorers.length_bonus.LengthBonus\",\"t\":[\"class espnet.nets.scorers.length_bonus.LengthBonus(n_vocab: int)\",\"Bases: BatchScorerInterface\",\"Length bonus in beam search.\",\"Initialize class.\",\"Parameters:n_vocab (int) – The number of tokens in vocabulary for beam search\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"score(y, state, x)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – 2D encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (n_vocab) and None\",\"Return type: tuple[torch.Tensor, Any]\"]},\"774\":{\"h\":\"espnet.nets.pytorch_backend.fastspeech.length_regulator.LengthRegulator\",\"t\":[\"class espnet.nets.pytorch_backend.fastspeech.length_regulator.LengthRegulator(pad_value=0.0)\",\"Bases: Module\",\"Length regulator module for feed-forward Transformer.\",\"This is a module of length regulator described in FastSpeech: Fast, Robust and Controllable Text to Speech. The length regulator expands char or phoneme-level embedding features to frame-level by repeating each feature based on the corresponding predicted durations.\",\"Initilize length regulator module.\",\"Parameters:pad_value (float,optional) – Value used for padding.\",\"forward(xs, ds, alpha=1.0)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of sequences of char or phoneme embeddings (B, Tmax, D).\",\"ds (LongTensor) – Batch of durations of each frame (B, T).\",\"alpha (float,optional) – Alpha value to control speed of speech.\",\"Returns: replicated input tensor based on durations (B, T*, D).\",\"Return type: Tensor\",\"training : bool\"]},\"775\":{\"h\":\"espnet.nets.pytorch_backend.transformer.lightconv.LightweightConvolution\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.lightconv.LightweightConvolution(wshare, n_feat, dropout_rate, kernel_size, use_kernel_mask=False, use_bias=False)\",\"Bases: Module\",\"Lightweight Convolution layer.\",\"This implementation is based on https://github.com/pytorch/fairseq/tree/master/fairseq\",\"Parameters:\",\"wshare (int) – the number of kernel of convolution\",\"n_feat (int) – the number of features\",\"dropout_rate (float) – dropout_rate\",\"kernel_size (int) – kernel size (length)\",\"use_kernel_mask (bool) – Use causal mask or not for convolution kernel\",\"use_bias (bool) – Use bias term or not.\",\"Construct Lightweight Convolution layer.\",\"forward(query, key, value, mask)\",\"Forward of ‘Lightweight Convolution’.\",\"This function takes query, key and value but uses only query. This is just for compatibility with self-attention layer (attention.py)\",\"Parameters:\",\"query (torch.Tensor) – (batch, time1, d_model) input tensor\",\"key (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"value (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"mask (torch.Tensor) – (batch, time1, time2) mask\",\"Returns: (batch, time1, d_model) output\",\"Return type: x (torch.Tensor)\",\"training : bool\"]},\"776\":{\"h\":\"espnet.nets.pytorch_backend.transformer.lightconv2d.LightweightConvolution2D\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.lightconv2d.LightweightConvolution2D(wshare, n_feat, dropout_rate, kernel_size, use_kernel_mask=False, use_bias=False)\",\"Bases: Module\",\"Lightweight 2-Dimensional Convolution layer.\",\"This implementation is based on https://github.com/pytorch/fairseq/tree/master/fairseq\",\"Parameters:\",\"wshare (int) – the number of kernel of convolution\",\"n_feat (int) – the number of features\",\"dropout_rate (float) – dropout_rate\",\"kernel_size (int) – kernel size (length)\",\"use_kernel_mask (bool) – Use causal mask or not for convolution kernel\",\"use_bias (bool) – Use bias term or not.\",\"Construct Lightweight 2-Dimensional Convolution layer.\",\"forward(query, key, value, mask)\",\"Forward of ‘Lightweight 2-Dimensional Convolution’.\",\"This function takes query, key and value but uses only query. This is just for compatibility with self-attention layer (attention.py)\",\"Parameters:\",\"query (torch.Tensor) – (batch, time1, d_model) input tensor\",\"key (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"value (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"mask (torch.Tensor) – (batch, time1, time2) mask\",\"Returns: (batch, time1, d_model) output\",\"Return type: x (torch.Tensor)\",\"training : bool\"]},\"777\":{\"h\":\"espnet.nets.chainer_backend.transformer.subsampling.LinearSampling\",\"t\":[\"class espnet.nets.chainer_backend.transformer.subsampling.LinearSampling(idim, dims, dropout=0.1, initialW=None, initial_bias=None)\",\"Bases: Chain\",\"Linear 1D subsampling.\",\"Parameters:\",\"idim (int) – input dim\",\"odim (int) – output dim\",\"dropout_rate (flaot) – dropout rate\",\"Initialize LinearSampling.\",\"forward(xs, ilens)\",\"Subsample x.\",\"Parameters:x (chainer.Variable) – input tensor\",\"Returns: subsampled x and mask\"]},\"778\":{\"h\":\"espnet.nets.pytorch_backend.frontends.feature_transform.LogMel\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.feature_transform.LogMel(fs: int = 16000, n_fft: int = 512, n_mels: int = 80, fmin: float = 0.0, fmax: float | None = None, htk: bool = False, norm=1)\",\"Bases: Module\",\"Convert STFT to fbank feats\",\"The arguments is same as librosa.filters.mel\",\"Parameters:\",\"fs – number > 0 [scalar] sampling rate of the incoming signal\",\"n_fft – int > 0 [scalar] number of FFT components\",\"n_mels – int > 0 [scalar] number of Mel bands to generate\",\"fmin – float >= 0 [scalar] lowest frequency (in Hz)\",\"fmax – float >= 0 [scalar] highest frequency (in Hz). If None, use fmax = fs / 2.0\",\"htk – use HTK formula instead of Slaney\",\"norm – {None, 1, np.inf} [scalar] if 1, divide the triangular mel weights by the width of the mel band (area normalization). Otherwise, leave all the triangles aiming for a peak value of 1.0\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(feat: Tensor, ilens: LongTensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"779\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"780\":{\"h\":\"espnet.nets.pytorch_backend.transformer.longformer_attention.LongformerAttention\"},\"781\":{\"h\":\"espnet.nets.mt_interface.MTInterface\",\"t\":[\"<!-- _espnet.nets.mt_interface.MTInterface -->\",\"class espnet.nets.mt_interface.MTInterface\",\"Bases: object\",\"MT Interface for ESPnet model implementation.\",\"static add_arguments(parser)\",\"Add arguments to parser.\",\"property attention_plot_class\",\"Get attention plot class.\",\"classmethod build(idim: int, odim: int, **kwargs)\",\"Initialize this class with python-level args.\",\"Parameters:\",\"idim (int) – The number of an input feature dim.\",\"odim (int) – The number of output vocab.\",\"Returns: A new instance of ASRInterface.\",\"Return type: ASRinterface\",\"calculate_all_attentions(xs, ilens, ys)\",\"Calculate attention.\",\"Parameters:\",\"xs (list) – list of padded input sequences [(T1, idim), (T2, idim), …]\",\"ilens (ndarray) – batch of lengths of input sequences (B)\",\"ys (list) – list of character id sequence tensor [(L1), (L2), (L3), …]\",\"Returns: attention weights (B, Lmax, Tmax)\",\"Return type: float ndarray\",\"forward(xs, ilens, ys)\",\"Compute loss for training.\",\"Parameters:\",\"xs – For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable\",\"ilens – batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int\",\"ys – For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable\",\"Returns: loss value\",\"Return type: torch.Tensor for pytorch, chainer.Variable for chainer\",\"translate(x, trans_args, char_list=None, rnnlm=None)\",\"Translate x for evaluation.\",\"Parameters:\",\"x (ndarray) – input acouctic feature (B, T, D) or (T, D)\",\"trans_args (namespace) – argment namespace contraining options\",\"char_list (list) – list of characters\",\"rnnlm (torch.nn.Module) – language model module\",\"Returns: N-best decoding results\",\"Return type: list\",\"translate_batch(x, trans_args, char_list=None, rnnlm=None)\",\"Beam search implementation for batch.\",\"Parameters:\",\"x (torch.Tensor) – encoder hidden state sequences (B, Tmax, Henc)\",\"trans_args (namespace) – argument namespace containing options\",\"char_list (list) – list of characters\",\"rnnlm (torch.nn.Module) – language model module\",\"Returns: N-best decoding results\",\"Return type: list\"]},\"782\":{\"h\":\"espnet.nets.pytorch_backend.frontends.mask_estimator.MaskEstimator\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.mask_estimator.MaskEstimator(type, idim, layers, units, projs, dropout, nmask=1)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs: ComplexTensor, ilens: LongTensor)\",\"The forward function\",\"Parameters:\",\"xs – (B, F, C, T)\",\"ilens – (B,)\",\"Returns: The hidden vector (B, F, C, T) masks: A tuple of the masks. (B, F, C, T) ilens: (B,)\",\"Return type: hs (torch.Tensor)\",\"training : bool\"]},\"783\":{\"h\":\"espnet.nets.scorer_interface.MaskParallelScorerInterface\",\"t\":[\"class espnet.nets.scorer_interface.MaskParallelScorerInterface\",\"Bases: ScorerInterface\"]},\"784\":{\"h\":\"espnet.nets.chainer_backend.transformer.attention.MultiHeadAttention\",\"t\":[\"class espnet.nets.chainer_backend.transformer.attention.MultiHeadAttention(n_units, h=8, dropout=0.1, initialW=None, initial_bias=None)\",\"Bases: Chain\",\"Multi Head Attention Layer.\",\"Parameters:\",\"n_units (int) – Number of input units.\",\"h (int) – Number of attention heads.\",\"dropout (float) – Dropout rate.\",\"initialW – Initializer to initialize the weight.\",\"initial_bias – Initializer to initialize the bias.\",\"h – the number of heads\",\"n_units – the number of features\",\"dropout_rate (float) – dropout rate\",\"Initialize MultiHeadAttention.\",\"forward(e_var, s_var=None, mask=None, batch=1)\",\"Core function of the Multi-head attention layer.\",\"Parameters:\",\"e_var (chainer.Variable) – Variable of input array.\",\"s_var (chainer.Variable) – Variable of source array from encoder.\",\"mask (chainer.Variable) – Attention mask.\",\"batch (int) – Batch size.\",\"Returns: Outout of multi-head attention layer.\",\"Return type: chainer.Variable\"]},\"785\":{\"h\":\"espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention(n_head, n_feat, dropout_rate)\",\"Bases: Module\",\"Multi-Head Attention layer.\",\"Parameters:\",\"n_head (int) – The number of heads.\",\"n_feat (int) – The number of features.\",\"dropout_rate (float) – Dropout rate.\",\"Construct an MultiHeadedAttention object.\",\"forward(query, key, value, mask, expand_kv=False)\",\"Compute scaled dot product attention.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"expand_kv (bool) – Used only for partially autoregressive (PAR) decoding.\",\"When set to True, Linear layers are computed only for the first batch. This is useful to reduce the memory usage during decoding when the batch size is #beam_size x #mask_count, which can be very large. Typically, in single waveform inference of PAR, Linear layers should not be computed for all batches for source-attention.\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"forward_attention(value, scores, mask)\",\"Compute attention context vector.\",\"Parameters:\",\"value (torch.Tensor) – Transformed value (#batch, n_head, time2, d_k).\",\"scores (torch.Tensor) – Attention score (#batch, n_head, time1, time2).\",\"mask (torch.Tensor) – Mask (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Transformed value (#batch, time1, d_model) : weighted by the attention score (#batch, time1, time2).\",\"Return type: torch.Tensor\",\"forward_qkv(query, key, value, expand_kv=False)\",\"Transform query, key and value.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"expand_kv (bool) – Used only for partially autoregressive (PAR) decoding.\",\"Returns: Transformed query tensor (#batch, n_head, time1, d_k). torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k). torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"786\":{\"h\":\"espnet.nets.pytorch_backend.transformer.multi_layer_conv.MultiLayeredConv1d\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.multi_layer_conv.MultiLayeredConv1d(in_chans, hidden_chans, kernel_size, dropout_rate)\",\"Bases: Module\",\"Multi-layered conv1d for Transformer block.\",\"This is a module of multi-leyered conv1d designed to replace positionwise feed-forward network in Transforner block, which is introduced in FastSpeech: Fast, Robust and Controllable Text to Speech.\",\"Initialize MultiLayeredConv1d module.\",\"Parameters:\",\"in_chans (int) – Number of input channels.\",\"hidden_chans (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size of conv1d.\",\"dropout_rate (float) – Dropout rate.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Batch of input tensors (B, T, in_chans).\",\"Returns: Batch of output tensors (B, T, hidden_chans).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"787\":{\"h\":\"espnet.nets.pytorch_backend.transformer.repeat.MultiSequential\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.repeat.MultiSequential(*args, layer_drop_rate=0.0)\",\"Bases: Sequential\",\"Multi-input multi-output torch.nn.Sequential.\",\"Initialize MultiSequential with layer_drop.\",\"Parameters:layer_drop_rate (float) – Probability of dropping out each fn (layer).\",\"forward(*args)\",\"Repeat.\"]},\"788\":{\"h\":\"espnet.nets.scorers.ngram.NgramFullScorer\",\"t\":[\"<!-- _espnet.nets.scorers.ngram.NgramFullScorer -->\"]},\"789\":{\"h\":\"espnet.nets.scorers.ngram.NgramPartScorer\",\"t\":[\"<!-- _espnet.nets.scorers.ngram.NgramPartScorer -->\"]},\"790\":{\"h\":\"espnet.nets.scorers.ngram.Ngrambase\",\"t\":[\"<!-- _espnet.nets.scorers.ngram.Ngrambase -->\"]},\"791\":{\"h\":\"espnet.nets.chainer_backend.rnn.attentions.NoAtt\",\"t\":[\"class espnet.nets.chainer_backend.rnn.attentions.NoAtt\",\"Bases: Chain\",\"Compute non-attention layer.\",\"This layer is a dummy attention layer to be compatible with other attention-based models.\",\"reset()\",\"Reset states.\"]},\"792\":{\"h\":\"espnet.nets.pytorch_backend.transformer.optimizer.NoamOpt\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.optimizer.NoamOpt(model_size, factor, warmup, optimizer)\",\"Bases: object\",\"Optim wrapper that implements rate.\",\"Construct an NoamOpt object.\",\"load_state_dict(state_dict)\",\"Load state_dict.\",\"property param_groups\",\"Return param_groups.\",\"rate(step=None)\",\"Implement lrate above.\",\"state_dict()\",\"Return state_dict.\",\"step()\",\"Update parameters and rate.\",\"zero_grad()\",\"Reset gradient.\"]},\"793\":{\"h\":\"espnet.nets.pytorch_backend.wavenet.OneHot\",\"t\":[\"<!-- _espnet.nets.pytorch_backend.wavenet.OneHot -->\",\"class espnet.nets.pytorch_backend.wavenet.OneHot(depth)\",\"Bases: Module\",\"Convert to one-hot vector.\",\"Parameters:depth (int) – Dimension of one-hot vector.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (LongTensor) – long tensor variable with the shape (B, T)\",\"Returns: float tensor variable with the shape (B, depth, T)\",\"Return type: Tensor\",\"training : bool\"]},\"794\":{\"h\":\"espnet.nets.pytorch_backend.e2e_asr_mix.PIT\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_asr_mix.PIT(num_spkrs)\",\"Bases: object\",\"Permutation Invariant Training (PIT) module.\",\"Parameters:num_spkrs (int) – number of speakers for PIT process (2 or 3)\",\"Initialize PIT module.\",\"min_pit_sample(loss)\",\"Compute the PIT loss for each sample.\",\"Parameters:loss (1-D torch.Tensor) – list of losses for one sample, including [h1r1, h1r2, h2r1, h2r2] or [h1r1, h1r2, h1r3, h2r1, h2r2, h2r3, h3r1, h3r2, h3r3]\",\":return minimum loss of best permutation :rtype torch.Tensor (1) :return the best permutation :rtype List: len=2\",\"permutationDFS(source, start)\",\"Get permutations with DFS.\",\"The final result is all permutations of the ‘source’ sequence. e.g. [[1, 2], [2, 1]] or\",\"[[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 2, 1], [3, 1, 2]]\",\"Parameters:\",\"source (np.ndarray) – (num_spkrs, 1), e.g. [1, 2, …, N]\",\"start (int) – the start point to permute\",\"pit_process(losses)\",\"Compute the PIT loss for a batch.\",\"Parameters:losses (torch.Tensor) – losses (B, 1|4|9)\",\":return minimum losses of a batch with best permutation :rtype torch.Tensor (B) :return the best permutation :rtype torch.LongTensor (B, 1|2|3)\"]},\"795\":{\"h\":\"espnet.nets.scorer_interface.PartialScorerInterface\",\"t\":[\"class espnet.nets.scorer_interface.PartialScorerInterface\",\"Bases: ScorerInterface\",\"Partial scorer interface for beam search.\",\"The partial scorer performs scoring when non-partial scorer finished scoring, and receives pre-pruned next tokens to score because it is too heavy to score all the tokens.\"]},\"796\":{\"h\":\"Examples\",\"t\":[\"Prefix search for connectionist-temporal-classification models : * espnet.nets.scorers.ctc.CTCPrefixScorer\",\"score_partial(y: Tensor, next_tokens: Tensor, state: Any, x: Tensor)\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D prefix token\",\"next_tokens (torch.Tensor) – torch.int64 next token to score\",\"state – decoder state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys\",\"Returns: Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\"]},\"797\":{\"h\":\"espnet.nets.beam_search_partially_AR.PartiallyARBeamSearch\",\"t\":[\"class espnet.nets.beam_search_partially_AR.PartiallyARBeamSearch(*args, **kwargs)\",\"Bases: BatchBeamSearch\",\"Partially autoregressive beam search implementation. Partially autoregressive hypothesis is a set of BatchHypothesis.\",\"We need to use add_mask function to add a hypothesis for a mask. Before search and beam search method, each partially autoregressive hypothesis is extracted to BatchHypothesis, and applied the same process as the batched_beam_search.\",\"Initialize beam search.\",\"Parameters:\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"token_list (list *[*str]) – List of tokens for debug log\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"return_hs (bool) – Whether to return hidden intermediates\",\"normalize_length (bool) – If true, select the best ended hypotheses based on length-normalized scores rather than the accumulated scores\",\"add_mask(primer: List[int], eos: int)\",\"Add a mask to a batch of hypotheses.\",\"Parameters:primer (torch.Tensor) – Primer yseq.\",\"batch_beam(weighted_scores: Tensor)\",\"Batch-compute topk full token ids and partial token ids.\",\"Parameters:weighted_scores (torch.Tensor) – The weighted sum scores for each tokens. Its shape is (n_beam, self.vocab_size).\",\"Returns: The topk full (prev_hyp, new_token) ids and partial (prev_hyp, new_token) ids. Their shapes are all (self.beam_size,)\",\"Return type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\",\"forward(x: Tensor, max_seq_len: int | None = None)\",\"Perform beam search.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths If maxlenratio<0.0, its absolute value is interpreted as a constant max output length.\",\"minlenratio (float) – Input length ratio to obtain min output length.\",\"Returns: N-best decoding results\",\"Return type: list[Hypothesis]\",\"init_hyp(x: Tensor)\",\"Get an initial hypothesis data for each mask.\",\"Parameters:x (torch.Tensor) – The encoder output feature\",\"Returns: The initial hypothesis.\",\"Return type:PartiallyARHypothesis\",\"init_masks()\",\"post_process(i: int, maxlen: int, running_hyps: PartiallyARHypothesis, ended_hyps: List[List[Hypothesis]])\",\"Perform post-processing of beam search iterations. Extract BatchHypothesis for each mask, and perform post-process. Then merge BatchHypothesis.\",\"Parameters:\",\"i (int) – The length of hypothesis tokens.\",\"maxlen (int) – The maximum length of tokens in beam search.\",\"maxlenratio (int) – The maximum length ratio in beam search.\",\"running_hyps (BatchHypothesis) – The running hypotheses in beam search.\",\"ended_hyps (List[Hypothesis]) – The ended hypotheses in beam search.\",\"Returns: The new running hypotheses.\",\"Return type:BatchHypothesis\",\"score_full(hyp: PartiallyARHypothesis, x: Tensor, is_first: bool = False)\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (PartiallyARHypothesis) – Hypothesis with prefix tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"search(running_hyps: PartiallyARHypothesis, x: Tensor)\",\"Search new tokens for running hypotheses and encoded speech x.\",\"Parameters:\",\"running_hyps (BatchHypothesis) – Running hypotheses on beam\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"Returns: Best sorted hypotheses\",\"Return type:BatchHypothesis\",\"training : bool\"]},\"798\":{\"h\":\"espnet.nets.beam_search_partially_AR.PartiallyARHypothesis\",\"t\":[\"class espnet.nets.beam_search_partially_AR.PartiallyARHypothesis(yseq: Tensor, score: float | Tensor | None = None, states: Dict[str, Any] = {}, yseq_length: Tensor = tensor([]), eos: Tensor = tensor([]))\",\"Bases: tuple\",\"Hypothesis data type for partially autoregressive decoding.\",\"Create new instance of PartiallyARHypothesis(yseq, score, states, yseq_length, eos)\",\"asdict()\",\"Convert data to JSON-friendly dict.\",\"eos : Tensor\",\"Alias for field number 4\",\"score : float | Tensor\",\"Alias for field number 1\",\"states : Dict[str, Any]\",\"Alias for field number 2\",\"yseq : Tensor\",\"Alias for field number 0\",\"yseq_length : Tensor\",\"Alias for field number 3\"]},\"799\":{\"h\":\"espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport(att_vis_fn, data, outdir, converter, transform, device, reverse=False, ikey='input', iaxis=0, okey='output', oaxis=0, subsampling_factor=1)\",\"Bases: PlotAttentionReport\",\"get_attention_weights()\",\"Return attention weights.\",\"Returns: attention weights. float. Its shape would be : differ from backend. * pytorch-> 1) multi-head case => (B, H, Lmax, Tmax), 2) \",\"other case => (B, Lmax, Tmax).\",\"chainer-> (B, Lmax, Tmax)\",\"Return type: numpy.ndarray\",\"log_attentions(logger, step)\",\"Add image files of att_ws matrix to the tensorboard.\",\"plotfn(*args, **kwargs)\"]},\"800\":{\"h\":\"espnet.nets.chainer_backend.transformer.embedding.PositionalEncoding\",\"t\":[\"class espnet.nets.chainer_backend.transformer.embedding.PositionalEncoding(n_units, dropout=0.1, length=5000)\",\"Bases: Chain\",\"Positional encoding module.\",\"Parameters:\",\"n_units (int) – embedding dim\",\"dropout (float) – dropout rate\",\"length (int) – maximum input length\",\"Initialize Positional Encoding.\",\"forward(e)\",\"Forward Positional Encoding.\"]},\"801\":{\"h\":\"espnet.nets.chainer_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward\",\"t\":[\"class espnet.nets.chainer_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward(n_units, d_units=0, dropout=0.1, initialW=None, initial_bias=None)\",\"Bases: Chain\",\"Positionwise feed forward.\",\":param : param int idim: input dimenstion :param : param int hidden_units: number of hidden units :param : param float dropout_rate: dropout rate\",\"Initialize PositionwiseFeedForward.\",\"Parameters:\",\"n_units (int) – Input dimension.\",\"d_units (int,optional) – Output dimension of hidden layer.\",\"dropout (float,optional) – Dropout ratio.\",\"initialW (int,optional) – Initializer to initialize the weight.\",\"initial_bias (bool,optional) – Initializer to initialize the bias.\"]},\"802\":{\"h\":\"espnet.nets.pytorch_backend.tacotron2.decoder.Postnet\",\"t\":[\"class espnet.nets.pytorch_backend.tacotron2.decoder.Postnet(idim, odim, n_layers=5, n_chans=512, n_filts=5, dropout_rate=0.5, use_batch_norm=True)\",\"Bases: Module\",\"Postnet module for Spectrogram prediction network.\",\"This is a module of Postnet in Spectrogram prediction network, which described in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. The Postnet predicts refines the predicted Mel-filterbank of the decoder, which helps to compensate the detail structure of spectrogram.\",\"Initialize postnet module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"n_layers (int,optional) – The number of layers.\",\"n_filts (int,optional) – The number of filter size.\",\"n_units (int,optional) – The number of filter channels.\",\"use_batch_norm (bool,optional) – Whether to use batch normalization..\",\"dropout_rate (float,optional) – Dropout rate..\",\"forward(xs)\",\"Calculate forward propagation.\",\"Parameters:xs (Tensor) – Batch of the sequences of padded input tensors (B, idim, Tmax).\",\"Returns: Batch of padded output tensor. (B, odim, Tmax).\",\"Return type: Tensor\",\"training : bool\"]},\"803\":{\"h\":\"espnet.nets.pytorch_backend.tacotron2.decoder.Prenet\",\"t\":[\"class espnet.nets.pytorch_backend.tacotron2.decoder.Prenet(idim, n_layers=2, n_units=256, dropout_rate=0.5)\",\"Bases: Module\",\"Prenet module for decoder of Spectrogram prediction network.\",\"This is a module of Prenet in the decoder of Spectrogram prediction network, which described in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. The Prenet preforms nonlinear conversion of inputs before input to auto-regressive lstm, which helps to learn diagonal attentions.\"]},\"804\":{\"h\":\"NOTE\",\"t\":[\"This module alway applies dropout even in evaluation. See the detail in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.\",\"Initialize prenet module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"n_layers (int,optional) – The number of prenet layers.\",\"n_units (int,optional) – The number of prenet units.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Batch of input tensors (B, …, idim).\",\"Returns: Batch of output tensors (B, …, odim).\",\"Return type: Tensor\",\"training : bool\"]},\"805\":{\"h\":\"espnet.nets.chainer_backend.rnn.encoders.RNN\",\"t\":[\"class espnet.nets.chainer_backend.rnn.encoders.RNN(idim, elayers, cdim, hdim, dropout, typ='lstm')\",\"Bases: Chain\",\"RNN Module.\",\"Parameters:\",\"idim (int) – Dimension of the imput.\",\"elayers (int) – Number of encoder layers.\",\"cdim (int) – Number of rnn units.\",\"hdim (int) – Number of projection units.\",\"dropout (float) – Dropout rate.\",\"typ (str) – Rnn type.\"]},\"806\":{\"h\":\"espnet.nets.pytorch_backend.transducer.rnn_decoder.RNNDecoder\",\"t\":[\"class espnet.nets.pytorch_backend.transducer.rnn_decoder.RNNDecoder(odim: int, dtype: str, dlayers: int, dunits: int, embed_dim: int, dropout_rate: float = 0.0, dropout_rate_embed: float = 0.0, blank_id: int = 0)\",\"Bases: TransducerDecoderInterface, Module\",\"RNN decoder module for Transducer model.\",\"Parameters:\",\"odim – Output dimension.\",\"dtype – Decoder units type.\",\"dlayers – Number of decoder layers.\",\"dunits – Number of decoder units per layer..\",\"embed_dim – Embedding layer dimension.\",\"dropout_rate – Dropout rate for decoder layers.\",\"dropout_rate_embed – Dropout rate for embedding layer.\",\"blank_id – Blank symbol ID.\",\"Transducer initializer.\",\"batch_score(hyps: List[Hypothesis] | List[ExtendedHypothesis], dec_states: Tuple[Tensor, Tensor | None], cache: Dict[str, Any], use_lm: bool)\",\"One-step forward hypotheses.\",\"Parameters:\",\"hyps – Hypotheses.\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"cache – Pairs of (dec_out, dec_states) for each label sequences. (keys)\",\"use_lm – Whether to compute label ID sequences for LM.\",\"Returns: Decoder output sequences. (B, D_dec) dec_states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec)) lm_labels: Label ID sequences for LM. (B,)\",\"Return type: dec_out\",\"create_batch_states(states: Tuple[Tensor, Tensor | None], new_states: List[Tuple[Tensor, Tensor | None]], check_list: List | None = None)\",\"Create decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"new_states – Decoder hidden states. [N x ((1, D_dec), (1, D_dec))]\",\"Returns: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"Return type: states\",\"forward(labels: Tensor)\",\"Encode source label sequences.\",\"Parameters:labels – Label ID sequences. (B, L)\",\"Returns: Decoder output sequences. (B, T, U, D_dec)\",\"Return type: dec_out\",\"init_state(batch_size: int)\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"rnn_forward(sequence: Tensor, state: Tuple[Tensor, Tensor | None])\",\"Encode source label sequences.\",\"Parameters:\",\"sequence – RNN input sequences. (B, D_emb)\",\"state – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"Returns: RNN output sequences. (B, D_dec) (h_next, c_next): Decoder hidden states. (N, B, D_dec), (N, B, D_dec))\",\"Return type: sequence\",\"score(hyp: Hypothesis, cache: Dict[str, Any])\",\"One-step forward hypothesis.\",\"Parameters:\",\"hyp – Hypothesis.\",\"cache – Pairs of (dec_out, state) for each label sequence. (key)\",\"Returns: Decoder output sequence. (1, D_dec) new_state: Decoder hidden states. ((N, 1, D_dec), (N, 1, D_dec)) label: Label ID for LM. (1,)\",\"Return type: dec_out\",\"select_state(states: Tuple[Tensor, Tensor | None], idx: int)\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. : ((N, 1, D_dec), (N, 1, D_dec))\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"training : bool\"]},\"807\":{\"h\":\"espnet.nets.pytorch_backend.lm.default.RNNLM\",\"t\":[\"class espnet.nets.pytorch_backend.lm.default.RNNLM(n_vocab, n_layers, n_units, n_embed=None, typ='lstm', dropout_rate=0.5, emb_dropout_rate=0.0, tie_weights=False)\",\"Bases: Module\",\"A pytorch RNNLM.\",\"Initialize class.\",\"Parameters:\",\"n_vocab (int) – The size of the vocabulary\",\"n_layers (int) – The number of layers to create\",\"n_units (int) – The number of units per layer\",\"typ (str) – The RNN type\",\"forward(state, x)\",\"Forward neural networks.\",\"training : bool\",\"zero_state(batchsize)\",\"Initialize state.\"]},\"808\":{\"h\":\"espnet.nets.chainer_backend.rnn.encoders.RNNP\",\"t\":[\"class espnet.nets.chainer_backend.rnn.encoders.RNNP(idim, elayers, cdim, hdim, subsample, dropout, typ='blstm')\",\"Bases: Chain\",\"RNN with projection layer module.\",\"Parameters:\",\"idim (int) – Dimension of inputs.\",\"elayers (int) – Number of encoder layers.\",\"cdim (int) – Number of rnn units. (resulted in cdim * 2 if bidirectional)\",\"hdim (int) – Number of projection units.\",\"subsample (np.ndarray) – List to use sabsample the input array.\",\"dropout (float) – Dropout rate.\",\"typ (str) – The RNN type.\"]},\"809\":{\"h\":\"espnet.nets.pytorch_backend.transformer.attention.RelPositionMultiHeadedAttention\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.attention.RelPositionMultiHeadedAttention(n_head, n_feat, dropout_rate, zero_triu=False)\",\"Bases: MultiHeadedAttention\",\"Multi-Head Attention layer with relative position encoding (new implementation).\",\"Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"Paper: https://arxiv.org/abs/1901.02860\",\"Parameters:\",\"n_head (int) – The number of heads.\",\"n_feat (int) – The number of features.\",\"dropout_rate (float) – Dropout rate.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"Construct an RelPositionMultiHeadedAttention object.\",\"forward(query, key, value, pos_emb, mask)\",\"Compute ‘Scaled Dot Product Attention’ with rel. positional encoding.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"pos_emb (torch.Tensor) – Positional embedding tensor (#batch, 2*time1-1, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"rel_shift(x)\",\"Compute relative positional encoding.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (batch, head, time1, 2*time1-1).\",\"vector. (time1 means the lengthofquery) –\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"810\":{\"h\":\"espnet.nets.pytorch_backend.transformer.embedding.RelPositionalEncoding\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.embedding.RelPositionalEncoding(d_model, dropout_rate, max_len=5000)\",\"Bases: Module\",\"Relative positional encoding module (new implementation).\",\"Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"See : Appendix B in https://arxiv.org/abs/1901.02860\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"Construct an PositionalEncoding object.\",\"extend_pe(x)\",\"Reset the positional encodings.\",\"forward(x: Tensor)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"811\":{\"h\":\"espnet.nets.pytorch_backend.e2e_asr.Reporter\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_asr.Reporter(**links: Link)\",\"Bases: Chain\",\"A chainer reporter wrapper.\",\"report(loss_ctc, loss_att, acc, cer_ctc, cer, wer, mtl_loss)\",\"Report at every step.\"]},\"812\":{\"h\":\"espnet.nets.st_interface.STInterface\",\"t\":[\"<!-- _espnet.nets.st_interface.STInterface -->\",\"class espnet.nets.st_interface.STInterface\",\"Bases: ASRInterface\",\"ST Interface for ESPnet model implementation.\",\"NOTE: This class is inherited from ASRInterface to enable joint translation and recognition when performing multi-task learning with the ASR task.\",\"translate(x, trans_args, char_list=None, rnnlm=None, ensemble_models=[])\",\"Recognize x for evaluation.\",\"Parameters:\",\"x (ndarray) – input acouctic feature (B, T, D) or (T, D)\",\"trans_args (namespace) – argment namespace contraining options\",\"char_list (list) – list of characters\",\"rnnlm (torch.nn.Module) – language model module\",\"Returns: N-best decoding results\",\"Return type: list\",\"translate_batch(x, trans_args, char_list=None, rnnlm=None)\",\"Beam search implementation for batch.\",\"Parameters:\",\"x (torch.Tensor) – encoder hidden state sequences (B, Tmax, Henc)\",\"trans_args (namespace) – argument namespace containing options\",\"char_list (list) – list of characters\",\"rnnlm (torch.nn.Module) – language model module\",\"Returns: N-best decoding results\",\"Return type: list\"]},\"813\":{\"h\":\"espnet.nets.pytorch_backend.transformer.embedding.ScaledPositionalEncoding\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.embedding.ScaledPositionalEncoding(d_model, dropout_rate, max_len=5000)\",\"Bases: PositionalEncoding\",\"Scaled positional encoding module.\",\"See Sec. 3.2 https://arxiv.org/abs/1809.08895\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"Initialize class.\",\"forward(x)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\",\"reset_parameters()\",\"Reset parameters.\",\"training : bool\"]},\"814\":{\"h\":\"espnet.nets.scorer_interface.ScorerInterface\",\"t\":[\"class espnet.nets.scorer_interface.ScorerInterface\",\"Bases: object\",\"Scorer interface for beam search.\",\"The scorer performs scoring of the all tokens in vocabulary.\"]},\"815\":{\"h\":\"Examples\",\"t\":[\"Search heuristics : * espnet.nets.scorers.length_bonus.LengthBonus\",\"Decoder networks of the sequence-to-sequence models : * espnet.nets.pytorch_backend.nets.transformer.decoder.Decoder\",\"espnet.nets.pytorch_backend.nets.rnn.decoders.Decoder\",\"Neural language models : * espnet.nets.pytorch_backend.lm.transformer.TransformerLM\",\"espnet.nets.pytorch_backend.lm.default.DefaultRNNLM\",\"espnet.nets.pytorch_backend.lm.seq_rnn.SequentialRNNLM\",\"final_score(state: Any)\",\"Score eos (optional).\",\"Parameters:state – Scorer state for prefix tokens\",\"Returns: final score\",\"Return type: float\",\"init_state(x: Tensor)\",\"Get an initial state for decoding (optional).\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"score(y: Tensor, state: Any, x: Tensor)\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys.\",\"Returns: Tuple of : scores for next token that has a shape of (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"select_state(state: Any, i: int, new_id: int | None = None)\",\"Select state with relative ids in the main beam search.\",\"Parameters:\",\"state – Decoder state for prefix tokens\",\"i (int) – Index to select a state in the main beam search\",\"new_id (int) – New label index to select a state if necessary\",\"Returns: pruned state\",\"Return type: state\"]},\"816\":{\"h\":\"espnet.nets.pytorch_backend.streaming.segment.SegmentStreamingE2E\",\"t\":[\"class espnet.nets.pytorch_backend.streaming.segment.SegmentStreamingE2E(e2e, recog_args, rnnlm=None)\",\"Bases: object\",\"SegmentStreamingE2E constructor.\",\"Parameters:\",\"e2e (E2E) – E2E ASR object\",\"recog_args – arguments for “recognize” method of E2E\",\"accept_input(x)\",\"Call this method each time a new batch of input is available.\"]},\"817\":{\"h\":\"espnet.nets.pytorch_backend.lm.seq_rnn.SequentialRNNLM\",\"t\":[\"class espnet.nets.pytorch_backend.lm.seq_rnn.SequentialRNNLM(n_vocab, args)\",\"Bases: LMInterface, Module\",\"Sequential RNNLM.\",\"SEE ALSO\",\"https://github.com/pytorch/examples/blob/4581968193699de14b56527296262dd76ab43557/word_language_model/model.py\",\"Initialize class.\",\"Parameters:\",\"n_vocab (int) – The size of the vocabulary\",\"args (argparse.Namespace) – configurations. see py:method:add_arguments\",\"static add_arguments(parser)\",\"Add arguments to command line argument parser.\",\"forward(x, t)\",\"Compute LM loss value from buffer sequences.\",\"Parameters:\",\"x (torch.Tensor) – Input ids. (batch, len)\",\"t (torch.Tensor) – Target ids. (batch, len)\",\"Returns: Tuple of : loss to backward (scalar), negative log-likelihood of t: -log p(t) (scalar) and the number of elements in x (scalar)\",\"Return type: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\",\"Notes\",\"The last two return values are used in perplexity: p(t)^{-n} = exp(-log p(t) / n)\",\"init_state(x)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"score(y, state, x)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – 2D encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"training : bool\"]},\"818\":{\"h\":\"espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding(d_model, dropout_rate, max_len=5000)\",\"Bases: Module\",\"Streaming Positional encoding.\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"Construct an PositionalEncoding object.\",\"extend_pe(length, device, dtype)\",\"Reset the positional encodings.\",\"forward(x: Tensor, start_idx: int = 0)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"819\":{\"h\":\"espnet.nets.pytorch_backend.conformer.swish.Swish\",\"t\":[\"class espnet.nets.pytorch_backend.conformer.swish.Swish\",\"Bases: Module\",\"Construct an Swish object.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Return Swich activation function.\",\"training : bool\"]},\"820\":{\"h\":\"espnet.nets.tts_interface.TTSInterface\",\"t\":[\"<!-- _espnet.nets.tts_interface.TTSInterface -->\",\"class espnet.nets.tts_interface.TTSInterface\",\"Bases: object\",\"TTS Interface for ESPnet model implementation.\",\"Initilize TTS module.\",\"static add_arguments(parser)\",\"Add model specific argments to parser.\",\"property attention_plot_class\",\"Plot attention weights.\",\"property base_plot_keys\",\"Return base key names to plot during training.\",\"The keys should match what chainer.reporter reports. if you add the key loss, the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values.\",\"Returns: Base keys to plot during training.\",\"Return type: list[str]\",\"calculate_all_attentions(*args, **kwargs)\",\"Calculate TTS attention weights.\",\"Parameters:Tensor – Batch of attention weights (B, Lmax, Tmax).\",\"forward(*args, **kwargs)\",\"Calculate TTS forward propagation.\",\"Returns: Loss value.\",\"Return type: Tensor\",\"inference(*args, **kwargs)\",\"Generate the sequence of features given the sequences of characters.\",\"Returns: The sequence of generated features (L, odim). Tensor: The sequence of stop probabilities (L,). Tensor: The sequence of attention weights (L, T).\",\"Return type: Tensor\",\"load_pretrained_model(model_path)\",\"Load pretrained model parameters.\"]},\"821\":{\"h\":\"espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_vc_tacotron2.Tacotron2(idim, odim, args=None)\",\"Bases: TTSInterface, Module\",\"VC Tacotron2 module for VC.\",\"This is a module of Tacotron2-based VC model, which convert the sequence of acoustic features into the sequence of acoustic features.\",\"Initialize Tacotron2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"args (Namespace,optional) – \",\"spk_embed_dim (int): Dimension of the speaker embedding.\",\"elayers (int): The number of encoder blstm layers.\",\"eunits (int): The number of encoder blstm units.\",\"econv_layers (int): The number of encoder conv layers.\",\"econv_filts (int): The number of encoder conv filter size.\",\"econv_chans (int): The number of encoder conv filter channels.\",\"dlayers (int): The number of decoder lstm layers.\",\"dunits (int): The number of decoder lstm units.\",\"prenet_layers (int): The number of prenet layers.\",\"prenet_units (int): The number of prenet units.\",\"postnet_layers (int): The number of postnet layers.\",\"postnet_filts (int): The number of postnet filter size.\",\"postnet_chans (int): The number of postnet filter channels.\",\"output_activation (int): The name of activation function for outputs.\",\"adim (int): The number of dimension of mlp in attention.\",\"aconv_chans (int): The number of attention conv filter channels.\",\"aconv_filts (int): The number of attention conv filter size.\",\"cumulate_att_w (bool): Whether to cumulate previous attention weight.\",\"use_batch_norm (bool): Whether to use batch normalization.\",\"use_concate (int): : Whether to concatenate encoder embedding with decoder lstm outputs.\",\"dropout_rate (float): Dropout rate.\",\"zoneout_rate (float): Zoneout rate.\",\"reduction_factor (int): Reduction factor.\",\"spk_embed_dim (int): Number of speaker embedding dimenstions.\",\"spc_dim (int): Number of spectrogram embedding dimenstions : (only for use_cbhg=True).\",\"use_cbhg (bool): Whether to use CBHG module.\",\"cbhg_conv_bank_layers (int): : The number of convoluional banks in CBHG.\",\"cbhg_conv_bank_chans (int): : The number of channels of convolutional bank in CBHG.\",\"cbhg_proj_filts (int): : The number of filter size of projection layeri in CBHG.\",\"cbhg_proj_chans (int): : The number of channels of projection layer in CBHG.\",\"cbhg_highway_layers (int): : The number of layers of highway network in CBHG.\",\"cbhg_highway_units (int): : The number of units of highway network in CBHG.\",\"cbhg_gru_units (int): The number of units of GRU in CBHG.\",\"use_masking (bool): Whether to mask padded part in loss calculation.\",\"bce_pos_weight (float): Weight of positive sample of stop token : (only for use_masking=True).\",\"use-guided-attn-loss (bool): Whether to use guided attention loss.\",\"guided-attn-loss-sigma (float) Sigma in guided attention loss.\",\"guided-attn-loss-lamdba (float): Lambda in guided attention loss.\",\"static add_arguments(parser)\",\"Add model-specific arguments to the parser.\",\"property base_plot_keys\",\"Return base key names to plot during training.\",\"keys should match what chainer.reporter reports. If you add the key loss, the reporter will report main/loss\",\"and validation/main/loss values.\",\"also loss.png will be created as a figure visulizing main/loss : and validation/main/loss values.\",\"Returns: List of strings which are base keys to plot during training.\",\"Return type: list\",\"calculate_all_attentions(xs, ilens, ys, spembs=None, *args, **kwargs)\",\"Calculate all of the attention weights.\",\"Parameters:\",\"xs (Tensor) – Batch of padded acoustic features (B, Tmax, idim).\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Tensor,optional) – Batch of speaker embedding vectors (B, spk_embed_dim).\",\"Returns: Batch of attention weights (B, Lmax, Tmax).\",\"Return type: numpy.ndarray\",\"forward(xs, ilens, ys, labels, olens, spembs=None, spcs=None, *args, **kwargs)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of padded acoustic features (B, Tmax, idim).\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Tensor,optional) – Batch of speaker embedding vectors (B, spk_embed_dim).\",\"spcs (Tensor,optional) – Batch of groundtruth spectrograms (B, Lmax, spc_dim).\",\"Returns: Loss value.\",\"Return type: Tensor\",\"inference(x, inference_args, spemb=None, *args, **kwargs)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"x (Tensor) – Input sequence of acoustic features (T, idim).\",\"inference_args (Namespace) – \",\"threshold (float): Threshold in inference.\",\"minlenratio (float): Minimum length ratio in inference.\",\"maxlenratio (float): Maximum length ratio in inference.\",\"spemb (Tensor,optional) – Speaker embedding vector (spk_embed_dim).\",\"Returns: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T).\",\"Return type: Tensor\",\"training : bool\"]},\"822\":{\"h\":\"espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss(use_masking=True, use_weighted_masking=False, bce_pos_weight=20.0)\",\"Bases: Module\",\"Loss function module for Tacotron2.\",\"Initialize Tactoron2 loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float) – Weight of positive sample of stop token.\",\"forward(after_outs, before_outs, logits, ys, labels, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"logits (Tensor) – Batch of stop logits (B, Lmax).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"labels (LongTensor) – Batch of the sequences of stop token labels (B, Lmax).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Mean square error loss value. Tensor: Binary cross entropy loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"823\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.TooShortUttError\",\"t\":[\"class espnet.nets.pytorch_backend.transformer.subsampling.TooShortUttError(message, actual_size, limit)\",\"Bases: Exception\",\"Raised when the utt is too short for subsampling.\",\"Parameters:\",\"message (str) – Message for error catch\",\"actual_size (int) – the short size that cannot pass the subsampling\",\"limit (int) – the limit size for subsampling\",\"Construct a TooShortUttError for error handler.\"]},\"824\":{\"h\":\"espnet.nets.transducer_decoder_interface.TransducerDecoderInterface\",\"t\":[\"class espnet.nets.transducer_decoder_interface.TransducerDecoderInterface\",\"Bases: object\",\"Decoder interface for Transducer models.\",\"batch_score(hyps: List[Hypothesis] | List[ExtendedHypothesis], dec_states: Tuple[Tensor, Tensor | None] | List[Tensor | None], cache: Dict[str, Any], use_lm: bool)\",\"One-step forward hypotheses.\",\"Parameters:\",\"hyps – Hypotheses.\",\"dec_states – Decoder hidden states.\",\"cache – Pairs of (dec_out, dec_states) for each label sequence. (key)\",\"use_lm – Whether to compute label ID sequences for LM.\",\"Returns: Decoder output sequences. dec_states: Decoder hidden states. lm_labels: Label ID sequences for LM.\",\"Return type: dec_out\",\"create_batch_states(states: Tuple[Tensor, Tensor | None] | List[Tensor | None], new_states: List[Tuple[Tensor, Tensor | None] | List[Tensor | None]], l_tokens: List[List[int]])\",\"Create decoder hidden states.\",\"Parameters:\",\"batch_states – Batch of decoder states\",\"l_states – List of decoder states\",\"l_tokens – List of token sequences for input batch\",\"Returns: Batch of decoder states\",\"Return type: batch_states\",\"init_state(batch_size: int)\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states.\",\"Return type: state\",\"score(hyp: Hypothesis, cache: Dict[str, Any])\",\"One-step forward hypothesis.\",\"Parameters:\",\"hyp – Hypothesis.\",\"cache – Pairs of (dec_out, dec_state) for each token sequence. (key)\",\"Returns: Decoder output sequence. new_state: Decoder hidden states. lm_tokens: Label ID for LM.\",\"Return type: dec_out\",\"select_state(batch_states: Tuple[Tensor, Tensor | None] | List[Tensor], idx: int)\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"batch_states – Decoder hidden states.\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID.\",\"Return type: state_idx\"]},\"825\":{\"h\":\"espnet.nets.pytorch_backend.transducer.transducer_tasks.TransducerTasks\",\"t\":[\"class espnet.nets.pytorch_backend.transducer.transducer_tasks.TransducerTasks(encoder_dim: int, decoder_dim: int, joint_dim: int, output_dim: int, joint_activation_type: str = 'tanh', transducer_loss_weight: float = 1.0, ctc_loss: bool = False, ctc_loss_weight: float = 0.5, ctc_loss_dropout_rate: float = 0.0, lm_loss: bool = False, lm_loss_weight: float = 0.5, lm_loss_smoothing_rate: float = 0.0, aux_transducer_loss: bool = False, aux_transducer_loss_weight: float = 0.2, aux_transducer_loss_mlp_dim: int = 320, aux_trans_loss_mlp_dropout_rate: float = 0.0, symm_kl_div_loss: bool = False, symm_kl_div_loss_weight: float = 0.2, fastemit_lambda: float = 0.0, blank_id: int = 0, ignore_id: int = -1, training: bool = False)\",\"Bases: Module\",\"Transducer tasks module.\",\"Initialize module for Transducer tasks.\",\"Parameters:\",\"encoder_dim – Encoder outputs dimension.\",\"decoder_dim – Decoder outputs dimension.\",\"joint_dim – Joint space dimension.\",\"output_dim – Output dimension.\",\"joint_activation_type – Type of activation for joint network.\",\"transducer_loss_weight – Weight for main transducer loss.\",\"ctc_loss – Compute CTC loss.\",\"ctc_loss_weight – Weight of CTC loss.\",\"ctc_loss_dropout_rate – Dropout rate for CTC loss inputs.\",\"lm_loss – Compute LM loss.\",\"lm_loss_weight – Weight of LM loss.\",\"lm_loss_smoothing_rate – Smoothing rate for LM loss’ label smoothing.\",\"aux_transducer_loss – Compute auxiliary transducer loss.\",\"aux_transducer_loss_weight – Weight of auxiliary transducer loss.\",\"aux_transducer_loss_mlp_dim – Hidden dimension for aux. transducer MLP.\",\"aux_trans_loss_mlp_dropout_rate – Dropout rate for aux. transducer MLP.\",\"symm_kl_div_loss – Compute KL divergence loss.\",\"symm_kl_div_loss_weight – Weight of KL divergence loss.\",\"fastemit_lambda – Regularization parameter for FastEmit.\",\"blank_id – Blank symbol ID.\",\"ignore_id – Padding symbol ID.\",\"training – Whether the model was initializated in training or inference mode.\",\"compute_aux_transducer_and_symm_kl_div_losses(aux_enc_out: Tensor, dec_out: Tensor, joint_out: Tensor, target: Tensor, aux_t_len: Tensor, u_len: Tensor)\",\"Compute auxiliary Transducer loss and Jensen-Shannon divergence loss.\",\"Parameters:\",\"aux_enc_out – Encoder auxiliary output sequences. [N x (B, T_aux, D_enc_aux)]\",\"dec_out – Decoder output sequences. (B, U, D_dec)\",\"joint_out – Joint output sequences. (B, T, U, D_joint)\",\"target – Target character ID sequences. (B, L)\",\"aux_t_len – Auxiliary time lengths. [N x (B,)]\",\"u_len – True U lengths. (B,)\",\"Returns: Auxiliary Transducer loss and KL divergence loss values.\",\"compute_ctc_loss(enc_out: Tensor, target: Tensor, t_len: Tensor, u_len: Tensor)\",\"Compute CTC loss.\",\"Parameters:\",\"enc_out – Encoder output sequences. (B, T, D_enc)\",\"target – Target character ID sequences. (B, U)\",\"t_len – Time lengths. (B,)\",\"u_len – Label lengths. (B,)\",\"Returns: CTC loss value.\",\"compute_lm_loss(dec_out: Tensor, target: Tensor)\",\"Forward LM loss.\",\"Parameters:\",\"dec_out – Decoder output sequences. (B, U, D_dec)\",\"target – Target label ID sequences. (B, U)\",\"Returns: LM loss value.\",\"compute_transducer_loss(enc_out: Tensor, dec_out: tensor, target: Tensor, t_len: Tensor, u_len: Tensor)\",\"Compute Transducer loss.\",\"Parameters:\",\"enc_out – Encoder output sequences. (B, T, D_enc)\",\"dec_out – Decoder output sequences. (B, U, D_dec)\",\"target – Target label ID sequences. (B, L)\",\"t_len – Time lengths. (B,)\",\"u_len – Label lengths. (B,)\",\"Returns: Joint output sequences. (B, T, U, D_joint), Transducer loss value.\",\"Return type: (joint_out, loss_trans)\",\"forward(enc_out: Tensor, aux_enc_out: List[Tensor], dec_out: Tensor, labels: Tensor, enc_out_len: Tensor, aux_enc_out_len: Tensor)\",\"Forward main and auxiliary task.\",\"Parameters:\",\"enc_out – Encoder output sequences. (B, T, D_enc)\",\"aux_enc_out – Encoder intermediate output sequences. (B, T_aux, D_enc_aux)\",\"dec_out – Decoder output sequences. (B, U, D_dec)\",\"target – Target label ID sequences. (B, L)\",\"t_len – Time lengths. (B,)\",\"aux_t_len – Auxiliary time lengths. (B,)\",\"u_len – Label lengths. (B,)\",\"Returns: Weighted losses. : (transducer loss, ctc loss, aux Transducer loss, KL div loss, LM loss)\",\"cer: Sentence-level CER score. wer: Sentence-level WER score.\",\"get_target()\",\"Set target label ID sequences.\",\"Args:\",\"Returns: Target label ID sequences. (B, L)\",\"Return type: target\",\"get_transducer_tasks_io(labels: Tensor, enc_out_len: Tensor, aux_enc_out_len: List | None)\",\"Get Transducer tasks inputs and outputs.\",\"Parameters:\",\"labels – Label ID sequences. (B, U)\",\"enc_out_len – Time lengths. (B,)\",\"aux_enc_out_len – Auxiliary time lengths. [N X (B,)]\",\"Returns: Target label ID sequences. (B, L) lm_loss_target: LM loss target label ID sequences. (B, U) t_len: Time lengths. (B,) aux_t_len: Auxiliary time lengths. [N x (B,)] u_len: Label lengths. (B,)\",\"Return type: target\",\"set_target(target: Tensor)\",\"Set target label ID sequences.\",\"Parameters:target – Target label ID sequences. (B, L)\",\"training : bool\"]},\"826\":{\"h\":\"espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer\",\"t\":[\"class espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer(idim, odim, args=None)\",\"Bases: TTSInterface, Module\",\"Text-to-Speech Transformer module.\",\"This is a module of text-to-speech Transformer described in Neural Speech Synthesis with Transformer Network, which convert the sequence of characters or phonemes into the sequence of Mel-filterbanks.\",\"Initialize TTS-Transformer module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"args (Namespace,optional) – \",\"embed_dim (int): Dimension of character embedding.\",\"eprenet_conv_layers (int): : Number of encoder prenet convolution layers.\",\"eprenet_conv_chans (int): : Number of encoder prenet convolution channels.\",\"eprenet_conv_filts (int): Filter size of encoder prenet convolution.\",\"dprenet_layers (int): Number of decoder prenet layers.\",\"dprenet_units (int): Number of decoder prenet hidden units.\",\"elayers (int): Number of encoder layers.\",\"eunits (int): Number of encoder hidden units.\",\"adim (int): Number of attention transformation dimensions.\",\"aheads (int): Number of heads for multi head attention.\",\"dlayers (int): Number of decoder layers.\",\"dunits (int): Number of decoder hidden units.\",\"postnet_layers (int): Number of postnet layers.\",\"postnet_chans (int): Number of postnet channels.\",\"postnet_filts (int): Filter size of postnet.\",\"use_scaled_pos_enc (bool): : Whether to use trainable scaled positional encoding.\",\"use_batch_norm (bool): : Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool): : Whether to perform layer normalization before encoder block.\",\"decoder_normalize_before (bool): : Whether to perform layer normalization before decoder block.\",\"encoder_concat_after (bool): Whether to concatenate attention : layer’s input and output in encoder.\",\"decoder_concat_after (bool): Whether to concatenate attention : layer’s input and output in decoder.\",\"reduction_factor (int): Reduction factor.\",\"spk_embed_dim (int): Number of speaker embedding dimenstions.\",\"spk_embed_integration_type: How to integrate speaker embedding.\",\"transformer_init (float): How to initialize transformer parameters.\",\"transformer_lr (float): Initial value of learning rate.\",\"transformer_warmup_steps (int): Optimizer warmup steps.\",\"transformer_enc_dropout_rate (float): : Dropout rate in encoder except attention & positional encoding.\",\"transformer_enc_positional_dropout_rate (float): : Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float): : Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float): : Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float): : Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float): : Dropout rate in deocoder self-attention module.\",\"transformer_enc_dec_attn_dropout_rate (float): : Dropout rate in encoder-deocoder attention module.\",\"eprenet_dropout_rate (float): Dropout rate in encoder prenet.\",\"dprenet_dropout_rate (float): Dropout rate in decoder prenet.\",\"postnet_dropout_rate (float): Dropout rate in postnet.\",\"use_masking (bool): : Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool): : Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float): Positive sample weight in bce calculation : (only for use_masking=true).\",\"loss_type (str): How to calculate loss.\",\"use_guided_attn_loss (bool): Whether to use guided attention loss.\",\"num_heads_applied_guided_attn (int): : Number of heads in each layer to apply guided attention loss.\",\"num_layers_applied_guided_attn (int): : Number of layers to apply guided attention loss.\",\"modules_applied_guided_attn (list): : List of module names to apply guided attention loss.\",\"guided-attn-loss-sigma (float) Sigma in guided attention loss.\",\"guided-attn-loss-lambda (float): Lambda in guided attention loss.\",\"static add_arguments(parser)\",\"Add model-specific arguments to the parser.\",\"property attention_plot_class\",\"Return plot class for attention weight plot.\",\"property base_plot_keys\",\"Return base key names to plot during training.\",\"keys should match what chainer.reporter reports. If you add the key loss, the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values.\",\"Returns: List of strings which are base keys to plot during training.\",\"Return type: list\",\"calculate_all_attentions(xs, ilens, ys, olens, spembs=None, skip_output=False, keep_tensor=False, *args, **kwargs)\",\"Calculate all of the attention weights.\",\"Parameters:\",\"xs (Tensor) – Batch of padded character ids (B, Tmax).\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Tensor,optional) – Batch of speaker embedding vectors (B, spk_embed_dim).\",\"skip_output (bool,optional) – Whether to skip calculate the final output.\",\"keep_tensor (bool,optional) – Whether to keep original tensor.\",\"Returns: Dict of attention weights and outputs.\",\"Return type: dict\",\"forward(xs, ilens, ys, labels, olens, spembs=None, *args, **kwargs)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of padded character ids (B, Tmax).\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Tensor,optional) – Batch of speaker embedding vectors (B, spk_embed_dim).\",\"Returns: Loss value.\",\"Return type: Tensor\",\"inference(x, inference_args, spemb=None, *args, **kwargs)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"x (Tensor) – Input sequence of characters (T,).\",\"inference_args (Namespace) – \",\"threshold (float): Threshold in inference.\",\"minlenratio (float): Minimum length ratio in inference.\",\"maxlenratio (float): Maximum length ratio in inference.\",\"spemb (Tensor,optional) – Speaker embedding vector (spk_embed_dim).\",\"Returns: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Encoder-decoder (source) attention weights (#layers, #heads, L, T).\",\"Return type: Tensor\",\"training : bool\"]},\"827\":{\"h\":\"espnet.nets.pytorch_backend.transducer.transformer_decoder_layer.TransformerDecoderLayer\",\"t\":[\"class espnet.nets.pytorch_backend.transducer.transformer_decoder_layer.TransformerDecoderLayer(hdim: int, self_attention: MultiHeadedAttention, feed_forward: PositionwiseFeedForward, dropout_rate: float)\",\"Bases: Module\",\"Transformer decoder layer module for custom Transducer model.\",\"Parameters:\",\"hdim – Hidden dimension.\",\"self_attention – Self-attention module.\",\"feed_forward – Feed forward module.\",\"dropout_rate – Dropout rate.\",\"Construct an DecoderLayer object.\",\"forward(sequence: Tensor, mask: Tensor, cache: Tensor | None = None)\",\"Compute previous decoder output sequences.\",\"Parameters:\",\"sequence – Transformer input sequences. (B, U, D_dec)\",\"mask – Transformer intput mask sequences. (B, U)\",\"cache – Cached decoder output sequences. (B, (U - 1), D_dec)\",\"Returns: Transformer output sequences. (B, U, D_dec) mask: Transformer output mask sequences. (B, U)\",\"Return type: sequence\",\"training : bool\"]},\"828\":{\"h\":\"espnet.nets.pytorch_backend.lm.transformer.TransformerLM\",\"t\":[\"class espnet.nets.pytorch_backend.lm.transformer.TransformerLM(n_vocab, args)\",\"Bases: Module, LMInterface, BatchScorerInterface\",\"Transformer language model.\",\"Initialize class.\",\"Parameters:\",\"n_vocab (int) – The size of the vocabulary\",\"args (argparse.Namespace) – configurations. see py:method:add_arguments\",\"static add_arguments(parser)\",\"Add arguments to command line argument parser.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch (required).\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(x: Tensor, t: Tensor)\",\"Compute LM loss value from buffer sequences.\",\"Parameters:\",\"x (torch.Tensor) – Input ids. (batch, len)\",\"t (torch.Tensor) – Target ids. (batch, len)\",\"Returns: Tuple of : loss to backward (scalar), negative log-likelihood of t: -log p(t) (scalar) and the number of elements in x (scalar)\",\"Return type: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\",\"Notes\",\"The last two return values are used in perplexity: p(t)^{-n} = exp(-log p(t) / n)\",\"score(y: Tensor, state: Any, x: Tensor)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"training : bool\"]},\"829\":{\"h\":\"espnet.nets.scorers.uasr.UASRPrefixScorer\",\"t\":[\"<!-- _espnet.nets.scorers.uasr.UASRPrefixScorer -->\",\"class espnet.nets.scorers.uasr.UASRPrefixScorer(eos: int)\",\"Bases: CTCPrefixScorer\",\"Decoder interface wrapper for CTCPrefixScore.\",\"Initialize class.\",\"batch_init_state(x: Tensor)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"init_state(x: Tensor)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\"]},\"830\":{\"h\":\"espnet.nets.pytorch_backend.wavenet.UpSampling\",\"t\":[\"class espnet.nets.pytorch_backend.wavenet.UpSampling(upsampling_factor, bias=True)\",\"Bases: Module\",\"Upsampling layer with deconvolution.\",\"Parameters:upsampling_factor (int) – Upsampling factor.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor with the shape (B, C, T)\",\"Returns: Tensor with the shape (B, C, T’) where T’ = T * upsampling_factor.\",\"Return type: Tensor\",\"training : bool\"]},\"831\":{\"h\":\"espnet.nets.pytorch_backend.frontends.feature_transform.UtteranceMVN\",\"t\":[\"class espnet.nets.pytorch_backend.frontends.feature_transform.UtteranceMVN(norm_means: bool = True, norm_vars: bool = False, eps: float = 1e-20)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x: Tensor, ilens: LongTensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"832\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"833\":{\"h\":\"espnet.nets.chainer_backend.rnn.encoders.VGG2L\",\"t\":[\"class espnet.nets.chainer_backend.rnn.encoders.VGG2L(in_channel=1)\",\"Bases: Chain\",\"VGG motibated cnn layers.\",\"Parameters:in_channel (int) – Number of channels.\"]},\"834\":{\"h\":\"espnet.nets.chainer_backend.transformer.training.VaswaniRule\",\"t\":[\"class espnet.nets.chainer_backend.transformer.training.VaswaniRule(attr, d, warmup_steps=4000, init=None, target=None, optimizer=None, scale=1.0)\",\"Bases: Extension\",\"Trainer extension to shift an optimizer attribute magically by Vaswani.\",\"Parameters:\",\"attr (str) – Name of the attribute to shift.\",\"rate (float) – Rate of the exponential shift. This value is multiplied to the attribute at each call.\",\"init (float) – Initial value of the attribute. If it is None, the extension extracts the attribute at the first call and uses it as the initial value.\",\"target (float) – Target value of the attribute. If the attribute reaches this value, the shift stops.\",\"optimizer (Optimizer) – Target optimizer to adjust the attribute. If it is None, the main optimizer of the updater is used.\",\"Initialize Vaswani rule extension.\",\"initialize(trainer)\",\"Initialize Optimizer values.\",\"serialize(serializer)\",\"Serialize extension.\"]},\"835\":{\"h\":\"espnet.nets.pytorch_backend.wavenet.WaveNet\",\"t\":[\"class espnet.nets.pytorch_backend.wavenet.WaveNet(n_quantize=256, n_aux=28, n_resch=512, n_skipch=256, dilation_depth=10, dilation_repeat=3, kernel_size=2, upsampling_factor=0)\",\"Bases: Module\",\"Conditional wavenet.\",\"Parameters:\",\"n_quantize (int) – Number of quantization.\",\"n_aux (int) – Number of aux feature dimension.\",\"n_resch (int) – Number of filter channels for residual block.\",\"n_skipch (int) – Number of filter channels for skip connection.\",\"dilation_depth (int) – Number of dilation depth (e.g. if set 10, max dilation = 2^(10-1)).\",\"dilation_repeat (int) – Number of dilation repeat.\",\"kernel_size (int) – Filter size of dilated causal convolution.\",\"upsampling_factor (int) – Upsampling factor.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, h)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (LongTensor) – Quantized input waveform tensor with the shape (B, T).\",\"h (Tensor) – Auxiliary feature tensor with the shape (B, n_aux, T).\",\"Returns: Logits with the shape (B, T, n_quantize).\",\"Return type: Tensor\",\"generate(x, h, n_samples, interval=None, mode='sampling')\",\"Generate a waveform with fast genration algorithm.\",\"This generation based on Fast WaveNet Generation Algorithm.\",\"Parameters:\",\"x (LongTensor) – Initial waveform tensor with the shape (T,).\",\"h (Tensor) – Auxiliary feature tensor with the shape (n_samples + T, n_aux).\",\"n_samples (int) – Number of samples to be generated.\",\"interval (int,optional) – Log interval.\",\"mode (str,optional) – “sampling” or “argmax”.\",\"Returns: Generated quantized waveform (n_samples).\",\"Return type: ndarray\",\"training : bool\"]},\"836\":{\"h\":\"espnet.nets.pytorch_backend.streaming.window.WindowStreamingE2E\",\"t\":[\"class espnet.nets.pytorch_backend.streaming.window.WindowStreamingE2E(e2e, recog_args, rnnlm=None)\",\"Bases: object\",\"WindowStreamingE2E constructor.\",\"Parameters:\",\"e2e (E2E) – E2E ASR object\",\"recog_args – arguments for “recognize” method of E2E\",\"accept_input(x)\",\"Call this method each time a new batch of input is available.\",\"decode_with_attention_offline()\",\"Run the attention decoder offline.\",\"Works even if the previous layers (encoder and CTC decoder) were being run in the online mode. This method should be run after all the audio has been consumed. This is used mostly to compare the results between offline and online implementation of the previous layers.\"]},\"837\":{\"h\":\"espnet.nets.pytorch_backend.tacotron2.decoder.ZoneOutCell\",\"t\":[\"class espnet.nets.pytorch_backend.tacotron2.decoder.ZoneOutCell(cell, zoneout_rate=0.1)\",\"Bases: Module\",\"ZoneOut Cell module.\",\"This is a module of zoneout described in Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations. This code is modified from eladhoffer/seq2seq.pytorch.\"]},\"838\":{\"h\":\"Examples\",\"t\":[\">>> lstm = torch.nn.LSTMCell(16, 32) >>> lstm = ZoneOutCell(lstm, 0.5)\",\"Initialize zone out cell module.\",\"Parameters:\",\"cell (torch.nn.Module) – Pytorch recurrent cell module e.g. torch.nn.Module.LSTMCell.\",\"zoneout_rate (float,optional) – Probability of zoneout from 0.0 to 1.0.\",\"forward(inputs, hidden)\",\"Calculate forward propagation.\",\"Parameters:\",\"inputs (Tensor) – Batch of input tensor (B, input_size).\",\"hidden (tuple) – \",\"Tensor: Batch of initial hidden states (B, hidden_size).\",\"Tensor: Batch of initial cell states (B, hidden_size).\",\"Returns:\",\"Tensor: Batch of next hidden states (B, hidden_size).\",\"Tensor: Batch of next cell states (B, hidden_size).\",\"Return type: tuple\",\"training : bool\"]},\"839\":{\"h\":\"espnet.nets.pytorch_backend.conformer.argument.add_arguments_conformer_common\",\"t\":[\"espnet.nets.pytorch_backend.conformer.argument.add_arguments_conformer_common(group)\",\"Add Transformer common arguments.\"]},\"840\":{\"h\":\"espnet.nets.pytorch_backend.rnn.argument.add_arguments_rnn_attention_common\",\"t\":[\"espnet.nets.pytorch_backend.rnn.argument.add_arguments_rnn_attention_common(group)\",\"Define common arguments for RNN attention.\"]},\"841\":{\"h\":\"espnet.nets.pytorch_backend.rnn.argument.add_arguments_rnn_decoder_common\",\"t\":[\"espnet.nets.pytorch_backend.rnn.argument.add_arguments_rnn_decoder_common(group)\",\"Define common arguments for RNN decoder.\"]},\"842\":{\"h\":\"espnet.nets.pytorch_backend.rnn.argument.add_arguments_rnn_encoder_common\",\"t\":[\"espnet.nets.pytorch_backend.rnn.argument.add_arguments_rnn_encoder_common(group)\",\"Define common arguments for RNN encoder.\"]},\"843\":{\"h\":\"espnet.nets.pytorch_backend.transformer.argument.add_arguments_transformer_common\",\"t\":[\"espnet.nets.pytorch_backend.transformer.argument.add_arguments_transformer_common(group)\",\"Add Transformer common arguments.\"]},\"844\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_auxiliary_task_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_auxiliary_task_arguments(group: _ArgumentGroup)\",\"Add arguments for auxiliary task.\"]},\"845\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_custom_decoder_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_custom_decoder_arguments(group: _ArgumentGroup)\",\"Define arguments for Custom decoder.\"]},\"846\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_custom_encoder_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_custom_encoder_arguments(group: _ArgumentGroup)\",\"Define arguments for Custom encoder.\"]},\"847\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_custom_training_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_custom_training_arguments(group: _ArgumentGroup)\",\"Define arguments for training with Custom architecture.\"]},\"848\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_decoder_general_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_decoder_general_arguments(group: _ArgumentGroup)\",\"Define general arguments for encoder.\"]},\"849\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_encoder_general_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_encoder_general_arguments(group: _ArgumentGroup)\",\"Define general arguments for encoder.\"]},\"850\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_rnn_decoder_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_rnn_decoder_arguments(group: _ArgumentGroup)\",\"Define arguments for RNN decoder.\"]},\"851\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_rnn_encoder_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_rnn_encoder_arguments(group: _ArgumentGroup)\",\"Define arguments for RNN encoder.\"]},\"852\":{\"h\":\"espnet.nets.pytorch_backend.transformer.add_sos_eos.add_sos_eos\",\"t\":[\"espnet.nets.pytorch_backend.transformer.add_sos_eos.add_sos_eos(ys_pad, sos, eos, ignore_id)\",\"Add <sos> and <eos> labels.\",\"Parameters:\",\"ys_pad (torch.Tensor) – batch of padded target sequences (B, Lmax)\",\"sos (int) – index of <sos>\",\"eos (int) – index of <eos>\",\"ignore_id (int) – index of padding\",\"Returns: padded tensor (B, Lmax)\",\"Return type: torch.Tensor\",\"Returns: padded tensor (B, Lmax)\",\"Return type: torch.Tensor\"]},\"853\":{\"h\":\"espnet.nets.pytorch_backend.transducer.arguments.add_transducer_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.arguments.add_transducer_arguments(group: _ArgumentGroup)\",\"Define general arguments for Transducer model.\"]},\"854\":{\"h\":\"espnet.nets.pytorch_backend.frontends.beamformer.apply_beamforming_vector\",\"t\":[\"espnet.nets.pytorch_backend.frontends.beamformer.apply_beamforming_vector(beamform_vector: ComplexTensor, mix: ComplexTensor)\"]},\"855\":{\"h\":\"espnet.nets.chainer_backend.rnn.attentions.att_for\",\"t\":[\"espnet.nets.chainer_backend.rnn.attentions.att_for(args)\",\"Returns an attention layer given the program arguments.\",\"Parameters:args (Namespace) – The arguments.\",\"Returns: The corresponding attention module.\",\"Return type: chainer.Chain\"]},\"856\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.att_to_numpy\",\"t\":[\"espnet.nets.pytorch_backend.rnn.attentions.att_to_numpy(att_ws, att)\",\"Converts attention weights to a numpy array given the attention\",\"Parameters:\",\"att_ws (list) – The attention weights\",\"att (torch.nn.Module) – The attention\",\"Return type: np.ndarray\",\"Returns: The numpy array of the attention weights\"]},\"857\":{\"h\":\"espnet.nets.beam_search.beam_search\",\"t\":[\"<!-- _espnet.nets.beam_search.beam_search -->\",\"espnet.nets.beam_search.beam_search(x: Tensor, sos: int, eos: int, beam_size: int, vocab_size: int, scorers: Dict[str, ScorerInterface], weights: Dict[str, float], token_list: List[str] | None = None, maxlenratio: float = 0.0, minlenratio: float = 0.0, pre_beam_ratio: float = 1.5, pre_beam_score_key: str = 'full')\",\"Perform beam search with scorers.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"token_list (list *[*str]) – List of tokens for debug log\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\",\"minlenratio (float) – Input length ratio to obtain min output length.\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"Returns: N-best decoding results\",\"Return type: list\"]},\"858\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.build_blocks\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.build_blocks(net_part: str, idim: int, input_layer_type: str, blocks: List[Dict[str, Any]], repeat_block: int = 0, self_attn_type: str = 'self_attn', positional_encoding_type: str = 'abs_pos', positionwise_layer_type: str = 'linear', positionwise_activation_type: str = 'relu', conv_mod_activation_type: str = 'relu', input_layer_dropout_rate: float = 0.0, input_layer_pos_enc_dropout_rate: float = 0.0, padding_idx: int = -1)\",\"Build custom model blocks.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"idim – Input dimension.\",\"input_layer – Input layer type.\",\"blocks – Blocks parameters for network part.\",\"repeat_block – Number of times provided blocks are repeated.\",\"positional_encoding_type – Positional encoding layer type.\",\"positionwise_layer_type – Positionwise layer type.\",\"positionwise_activation_type – Positionwise activation type.\",\"conv_mod_activation_type – Convolutional module activation type.\",\"input_layer_dropout_rate – Dropout rate for input layer.\",\"input_layer_pos_enc_dropout_rate – Dropout rate for input layer pos. enc.\",\"padding_idx – Padding symbol ID for embedding layer.\",\"Returns: Input layer all_blocks: Encoder/Decoder network. out_dim: Network output dimension. conv_subsampling_factor: Subsampling factor in frontend CNN.\",\"Return type: in_layer\"]},\"859\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.build_conformer_block\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.build_conformer_block(block: Dict[str, Any], self_attn_class: str, pw_layer_type: str, pw_activation_type: str, conv_mod_activation_type: str)\",\"Build function for conformer block.\",\"Parameters:\",\"block – Conformer block parameters.\",\"self_attn_type – Self-attention module type.\",\"pw_layer_type – Positionwise layer type.\",\"pw_activation_type – Positionwise activation type.\",\"conv_mod_activation_type – Convolutional module activation type.\",\"Returns: Function to create conformer (encoder) block.\"]},\"860\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.build_conv1d_block\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.build_conv1d_block(block: Dict[str, Any], block_type: str)\",\"Build function for causal conv1d block.\",\"Parameters:block – CausalConv1d or Conv1D block parameters.\",\"Returns: Function to create conv1d (encoder) or causal conv1d (decoder) block.\"]},\"861\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.build_input_layer\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.build_input_layer(block: Dict[str, Any], pos_enc_class: Module, padding_idx: int)\",\"Build input layer.\",\"Parameters:\",\"block – Architecture definition of input layer.\",\"pos_enc_class – Positional encoding class.\",\"padding_idx – Padding symbol ID for embedding layer (if provided).\",\"Returns: Input layer module. subsampling_factor: Subsampling factor.\"]},\"862\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.build_transformer_block\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.build_transformer_block(net_part: str, block: Dict[str, Any], pw_layer_type: str, pw_activation_type: str)\",\"Build function for transformer block.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"block – Transformer block parameters.\",\"pw_layer_type – Positionwise layer type.\",\"pw_activation_type – Positionwise activation type.\",\"Returns: Function to create transformer (encoder or decoder) block.\"]},\"863\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.check_batch_states\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.check_batch_states(states, max_len, pad_id)\",\"Check decoder hidden states and left pad or trim if necessary.\",\"Parameters:\",\"state – Decoder hidden states. [N x (B, ?, D_dec)]\",\"max_len – maximum sequence length.\",\"pad_id – Padding symbol ID.\",\"Returns: Decoder hidden states. [N x (B, max_len, dec_dim)]\",\"Return type: final\"]},\"864\":{\"h\":\"espnet.nets.pytorch_backend.transformer.subsampling.check_short_utt\",\"t\":[\"espnet.nets.pytorch_backend.transformer.subsampling.check_short_utt(ins, size)\",\"Check if the utterance is too short for subsampling.\"]},\"865\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.check_state\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.check_state(state: List[Tensor | None], max_len: int, pad_id: int)\",\"Check decoder hidden states and left pad or trim if necessary.\",\"Parameters:\",\"state – Decoder hidden states. [N x (?, D_dec)]\",\"max_len – maximum sequence length.\",\"pad_id – Padding symbol ID.\",\"Returns: Decoder hidden states. [N x (1, max_len, D_dec)]\",\"Return type: final\"]},\"866\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.create_lm_batch_states\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.create_lm_batch_states(lm_states: List[Any] | Dict[str, Any], lm_layers, is_wordlm: bool)\",\"Create LM hidden states.\",\"Parameters:\",\"lm_states – LM hidden states.\",\"lm_layers – Number of LM layers.\",\"is_wordlm – Whether provided LM is a word-level LM.\",\"Returns: LM hidden states.\",\"Return type: new_states\"]},\"867\":{\"h\":\"espnet.nets.chainer_backend.ctc.ctc_for\",\"t\":[\"<!-- _espnet.nets.chainer_backend.ctc.ctc_for -->\",\"espnet.nets.chainer_backend.ctc.ctc_for(args, odim)\",\"Return the CTC layer corresponding to the args.\",\"Parameters:\",\"args (Namespace) – The program arguments.\",\"odim (int) – The output dimension.\",\"Returns: The CTC module.\"]},\"868\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.custom_torch_load\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.custom_torch_load(model_path: str, model: Module, training: bool = True)\",\"Load Transducer model with training-only modules and parameters removed.\",\"Parameters:\",\"model_path – Model path.\",\"model – Transducer model.\"]},\"869\":{\"h\":\"espnet.nets.pytorch_backend.wavenet.decode_mu_law\",\"t\":[\"espnet.nets.pytorch_backend.wavenet.decode_mu_law(y, mu=256)\",\"Perform mu-law decoding.\",\"Parameters:\",\"x (ndarray) – Quantized audio signal with the range from 0 to mu - 1.\",\"mu (int) – Quantized level.\",\"Returns: Audio signal with the range from -1 to 1.\",\"Return type: ndarray\"]},\"870\":{\"h\":\"espnet.nets.chainer_backend.rnn.decoders.decoder_for\",\"t\":[\"espnet.nets.chainer_backend.rnn.decoders.decoder_for(args, odim, sos, eos, att, labeldist)\",\"Return the decoding layer corresponding to the args.\",\"Parameters:\",\"args (Namespace) – The program arguments.\",\"odim (int) – The output dimension.\",\"sos (int) – Number to indicate the start of sequences.\",\"eos (int) –\",\"att (Module) – Attention module defined at espnet.nets.chainer_backend.attentions.\",\"labeldist (numpy.array) – Distributed array of length od transcript.\",\"Returns: The decoder module.\",\"Return type: chainer.Chain\"]},\"871\":{\"h\":\"espnet.nets.pytorch_backend.tacotron2.decoder.decoder_init\",\"t\":[\"espnet.nets.pytorch_backend.tacotron2.decoder.decoder_init(m)\",\"Initialize decoder parameters.\"]},\"872\":{\"h\":\"espnet.nets.asr_interface.dynamic_import_asr\",\"t\":[\"espnet.nets.asr_interface.dynamic_import_asr(module, backend)\",\"Import ASR models dynamically.\",\"Parameters:\",\"module (str) – module_name:class_name or alias in predefined_asr\",\"backend (str) – NN backend. e.g., pytorch, chainer\",\"Returns: ASR class\",\"Return type: type\"]},\"873\":{\"h\":\"espnet.nets.lm_interface.dynamic_import_lm\",\"t\":[\"<!-- _espnet.nets.lm_interface.dynamic_import_lm -->\",\"espnet.nets.lm_interface.dynamic_import_lm(module, backend)\",\"Import LM class dynamically.\",\"Parameters:\",\"module (str) – module_name:class_name or alias in predefined_lms\",\"backend (str) – NN backend. e.g., pytorch, chainer\",\"Returns: LM class\",\"Return type: type\"]},\"874\":{\"h\":\"espnet.nets.st_interface.dynamic_import_st\",\"t\":[\"<!-- _espnet.nets.st_interface.dynamic_import_st -->\",\"espnet.nets.st_interface.dynamic_import_st(module, backend)\",\"Import ST models dynamically.\",\"Parameters:\",\"module (str) – module_name:class_name or alias in predefined_st\",\"backend (str) – NN backend. e.g., pytorch, chainer\",\"Returns: ST class\",\"Return type: type\"]},\"875\":{\"h\":\"espnet.nets.chainer_backend.deterministic_embed_id.embed_id\",\"t\":[\"espnet.nets.chainer_backend.deterministic_embed_id.embed_id(x, W, ignore_label=None)\",\"Efficient linear function for one-hot input.\",\"This function implements so called word embeddings. It takes two arguments: a set of IDs (words) x in $B$ dimensional integer vector, and a set of all ID (word) embeddings W in $V \\\\times d$ float32 matrix. It outputs $B \\\\times d$ matrix whose i-th column is the x[i]-th column of W. This function is only differentiable on the input W.\",\"Parameters:\",\"x (chainer.Variable|np.ndarray) – Batch vectors of IDs. Each element must be signed integer.\",\"W (chainer.Variable|np.ndarray) – Distributed representation of each ID (a.k.a. word embeddings).\",\"ignore_label (int) – If ignore_label is an int value, i-th column of return value is filled with 0.\",\"Returns: Embedded variable.\",\"Return type: chainer.Variable\",\"EmbedID\"]},\"876\":{\"h\":\"Examples\",\"t\":[\">>> x = np.array([2, 1]).astype('i') >>> x array([2, 1], dtype=int32) >>> W = np.array([[0, 0, 0], ... [1, 1, 1], ... [2, 2, 2]]).astype('f') >>> W array([[ 0., 0., 0.], [ 1., 1., 1.], [ 2., 2., 2.]], dtype=float32) >>> F.embed_id(x, W).data array([[ 2., 2., 2.], [ 1., 1., 1.]], dtype=float32) >>> F.embed_id(x, W, ignore_label=1).data array([[ 2., 2., 2.], [ 0., 0., 0.]], dtype=float32)\"]},\"877\":{\"h\":\"espnet.nets.pytorch_backend.wavenet.encode_mu_law\",\"t\":[\"espnet.nets.pytorch_backend.wavenet.encode_mu_law(x, mu=256)\",\"Perform mu-law encoding.\",\"Parameters:\",\"x (ndarray) – Audio signal with the range from -1 to 1.\",\"mu (int) – Quantized level.\",\"Returns: Quantized audio signal with the range from 0 to mu - 1.\",\"Return type: ndarray\"]},\"878\":{\"h\":\"espnet.nets.chainer_backend.rnn.encoders.encoder_for\",\"t\":[\"espnet.nets.chainer_backend.rnn.encoders.encoder_for(args, idim, subsample)\",\"Return the Encoder module.\",\"Parameters:\",\"idim (int) – Dimension of input array.\",\"subsample (numpy.array) – Subsample number. egs).1_2_2_2_1\",\"Return : chainer.nn.Module: Encoder module.\"]},\"879\":{\"h\":\"espnet.nets.pytorch_backend.tacotron2.encoder.encoder_init\",\"t\":[\"espnet.nets.pytorch_backend.tacotron2.encoder.encoder_init(m)\",\"Initialize encoder parameters.\"]},\"880\":{\"h\":\"espnet.nets.e2e_asr_common.end_detect\",\"t\":[\"<!-- _espnet.nets.e2e_asr_common.end_detect -->\",\"espnet.nets.e2e_asr_common.end_detect(ended_hyps, i, M=3, D_end=-10.0)\",\"End detection.\",\"described in Eq. (50) of S. Watanabe et al “Hybrid CTC/Attention Architecture for End-to-End Speech Recognition”\",\"Parameters:\",\"ended_hyps –\",\"i –\",\"M –\",\"D_end –\",\"Returns:\"]},\"881\":{\"h\":\"espnet.nets.pytorch_backend.frontends.feature_transform.feature_transform_for\",\"t\":[\"espnet.nets.pytorch_backend.frontends.feature_transform.feature_transform_for(args, n_fft)\"]},\"882\":{\"h\":\"espnet.nets.pytorch_backend.frontends.frontend.frontend_for\",\"t\":[\"espnet.nets.pytorch_backend.frontends.frontend.frontend_for(args, idim)\"]},\"883\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.get_activation\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.get_activation(act)\",\"Return activation function.\"]},\"884\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.get_decoder_input\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.get_decoder_input(labels: Tensor, blank_id: int, ignore_id: int)\",\"Prepare decoder input.\",\"Parameters:labels – Label ID sequences. (B, L)\",\"Returns: Label ID sequences with blank prefix. (B, U)\",\"Return type: decoder_input\"]},\"885\":{\"h\":\"espnet.nets.pytorch_backend.frontends.beamformer.get_mvdr_vector\",\"t\":[\"espnet.nets.pytorch_backend.frontends.beamformer.get_mvdr_vector(psd_s: ComplexTensor, psd_n: ComplexTensor, reference_vector: Tensor, eps: float = 1e-15)\",\"Return the MVDR(Minimum Variance Distortionless Response) vector:\",\"h = (Npsd^-1 @ Spsd) / (Tr(Npsd^-1 @ Spsd)) @ u\",\"Reference: : On optimal frequency-domain multichannel linear filtering for noise reduction; M. Souden et al., 2010; https://ieeexplore.ieee.org/document/5089420\",\"Parameters:\",\"psd_s (ComplexTensor) – (…, F, C, C)\",\"psd_n (ComplexTensor) – (…, F, C, C)\",\"reference_vector (torch.Tensor) – (…, C)\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (ComplexTensor)r\"]},\"886\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.get_pos_enc_and_att_class\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.get_pos_enc_and_att_class(net_part: str, pos_enc_type: str, self_attn_type: str)\",\"Get positional encoding and self attention module class.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"pos_enc_type – Positional encoding type.\",\"self_attn_type – Self-attention type.\",\"Returns: Positional encoding class. self_attn_class: Self-attention class.\",\"Return type: pos_enc_class\"]},\"887\":{\"h\":\"espnet.nets.pytorch_backend.frontends.beamformer.get_power_spectral_density_matrix\",\"t\":[\"espnet.nets.pytorch_backend.frontends.beamformer.get_power_spectral_density_matrix(xs: ComplexTensor, mask: Tensor, normalization=True, eps: float = 1e-15)\",\"Return cross-channel power spectral density (PSD) matrix\",\"Parameters:\",\"xs (ComplexTensor) – (…, F, C, T)\",\"mask (torch.Tensor) – (…, F, C, T)\",\"normalization (bool) –\",\"eps (float) –\",\"Returns : psd (ComplexTensor): (…, F, C, C)\"]},\"888\":{\"h\":\"espnet.nets.pytorch_backend.transformer.optimizer.get_std_opt\",\"t\":[\"espnet.nets.pytorch_backend.transformer.optimizer.get_std_opt(model_params, d_model, warmup, factor)\",\"Get standard NoamOpt.\"]},\"889\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.get_subsample\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.get_subsample(train_args, mode, arch)\",\"Parse the subsampling factors from the args for the specified mode and arch.\",\"Parameters:\",\"train_args – argument Namespace containing options.\",\"mode – one of (‘asr’, ‘mt’, ‘st’)\",\"arch – one of (‘rnn’, ‘rnn-t’, ‘rnn_mix’, ‘rnn_mulenc’, ‘transformer’)\",\"Returns: subsampling factors.\",\"Return type: np.ndarray / List[np.ndarray]\"]},\"890\":{\"h\":\"espnet.nets.e2e_asr_common.get_vgg2l_odim\",\"t\":[\"<!-- _espnet.nets.e2e_asr_common.get_vgg2l_odim -->\",\"espnet.nets.e2e_asr_common.get_vgg2l_odim(idim, in_channel=3, out_channel=128)\",\"Return the output size of the VGG frontend.\",\"Parameters:\",\"in_channel – input channel size\",\"out_channel – output channel size\",\"Returns: output size\",\":rtype int\"]},\"891\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.init_lm_state\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.init_lm_state(lm_model: Module)\",\"Initialize LM hidden states.\",\"Parameters:lm_model – LM module.\",\"Returns: Initial LM hidden states.\",\"Return type: lm_state\"]},\"892\":{\"h\":\"espnet.nets.pytorch_backend.rnn.attentions.initial_att\",\"t\":[\"espnet.nets.pytorch_backend.rnn.attentions.initial_att(atype, eprojs, dunits, aheads, adim, awin, aconv_chans, aconv_filts, han_mode=False)\",\"Instantiates a single attention module\",\"Parameters:\",\"atype (str) – attention type\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"adim (int) – attention dimension\",\"awin (int) – attention window size\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention\",\"Returns: The attention module\"]},\"893\":{\"h\":\"espnet.nets.pytorch_backend.wavenet.initialize\",\"t\":[\"espnet.nets.pytorch_backend.wavenet.initialize(m)\",\"Initilize conv layers with xavier.\",\"Parameters:m (torch.nn.Module) – Torch module.\"]},\"894\":{\"h\":\"espnet.nets.pytorch_backend.transducer.initializer.initializer\",\"t\":[\"espnet.nets.pytorch_backend.transducer.initializer.initializer(model: Module, args: Namespace)\",\"Initialize Transducer model.\",\"Parameters:\",\"model – Transducer model.\",\"args – Namespace containing model options.\"]},\"895\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.is_prefix\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.is_prefix(x: List[int], pref: List[int])\",\"Check if pref is a prefix of x.\",\"Parameters:\",\"x – Label ID sequence.\",\"pref – Prefix label ID sequence.\",\"Returns: Whether pref is a prefix of x.\"]},\"896\":{\"h\":\"espnet.nets.e2e_asr_common.label_smoothing_dist\",\"t\":[\"espnet.nets.e2e_asr_common.label_smoothing_dist(odim, lsm_type, transcript=None, blank=0)\",\"Obtain label distribution for loss smoothing.\",\"Parameters:\",\"odim –\",\"lsm_type –\",\"blank –\",\"transcript –\",\"Returns:\"]},\"897\":{\"h\":\"espnet.nets.pytorch_backend.initialization.lecun_normal_init_parameters\",\"t\":[\"espnet.nets.pytorch_backend.initialization.lecun_normal_init_parameters(module)\",\"Initialize parameters in the LeCun’s manner.\"]},\"898\":{\"h\":\"espnet.nets.chainer_backend.transformer.mask.make_history_mask\",\"t\":[\"espnet.nets.chainer_backend.transformer.mask.make_history_mask(xp, block)\",\"Prepare the history mask.\",\"Parameters:block (ndarray) – Block with dimensions: (B x S).\",\"Returns: History mask with dimensions (B, S, S).\",\"Return type: ndarray, np.ndarray\"]},\"899\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.make_non_pad_mask\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.make_non_pad_mask(lengths, xs=None, length_dim=-1)\",\"Make mask tensor containing indices of non-padded part.\",\"Parameters:\",\"lengths (LongTensororList) – Batch of lengths (B,).\",\"xs (Tensor,optional) – The reference tensor. If set, masks will be the same shape as this tensor.\",\"length_dim (int,optional) – Dimension indicator of the above tensor. See the example.\",\"Returns: mask tensor containing indices of padded part. : dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2)\",\"Return type: ByteTensor\"]},\"900\":{\"h\":\"Examples\",\"t\":[\"With only lengths.\",\">>> lengths = [5, 3, 2] >>> make_non_pad_mask(lengths) masks = [[1, 1, 1, 1 ,1], [1, 1, 1, 0, 0], [1, 1, 0, 0, 0]]\",\"With the reference tensor.\",\">>> xs = torch.zeros((3, 2, 4)) >>> make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 0], [1, 1, 1, 0]], [[1, 1, 0, 0], [1, 1, 0, 0]]], dtype=torch.uint8) >>> xs = torch.zeros((3, 2, 6)) >>> make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\",\"With the reference tensor and dimension indicator.\",\">>> xs = torch.zeros((3, 6, 6)) >>> make_non_pad_mask(lengths, xs, 1) tensor([[[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8) >>> make_non_pad_mask(lengths, xs, 2) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\"]},\"901\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.make_pad_mask\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.make_pad_mask(lengths, xs=None, length_dim=-1, maxlen=None)\",\"Make mask tensor containing indices of padded part.\",\"Parameters:\",\"lengths (LongTensororList) – Batch of lengths (B,).\",\"xs (Tensor,optional) – The reference tensor. If set, masks will be the same shape as this tensor.\",\"length_dim (int,optional) – Dimension indicator of the above tensor. See the example.\",\"Returns: Mask tensor containing indices of padded part. : dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2)\",\"Return type: Tensor\"]},\"902\":{\"h\":\"Examples\",\"t\":[\"With only lengths.\",\">>> lengths = [5, 3, 2] >>> make_pad_mask(lengths) masks = [[0, 0, 0, 0 ,0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 1]]\",\"With the reference tensor.\",\">>> xs = torch.zeros((3, 2, 4)) >>> make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 1], [0, 0, 0, 1]], [[0, 0, 1, 1], [0, 0, 1, 1]]], dtype=torch.uint8) >>> xs = torch.zeros((3, 2, 6)) >>> make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\",\"With the reference tensor and dimension indicator.\",\">>> xs = torch.zeros((3, 6, 6)) >>> make_pad_mask(lengths, xs, 1) tensor([[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8) >>> make_pad_mask(lengths, xs, 2) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\"]},\"903\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.mask_by_length\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.mask_by_length(xs, lengths, fill=0)\",\"Mask tensor according to length.\",\"Parameters:\",\"xs (Tensor) – Batch of input tensor (B, *).\",\"lengths (LongTensororList) – Batch of lengths (B,).\",\"fill (intorfloat) – Value to fill masked part.\",\"Returns: Batch of masked input tensor (B, *).\",\"Return type: Tensor\"]},\"904\":{\"h\":\"Examples\",\"t\":[\">>> x = torch.arange(5).repeat(3, 1) + 1 >>> x tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]) >>> lengths = [5, 3, 2] >>> mask_by_length(x, lengths) tensor([[1, 2, 3, 4, 5], [1, 2, 3, 0, 0], [1, 2, 0, 0, 0]])\"]},\"905\":{\"h\":\"espnet.nets.pytorch_backend.maskctc.add_mask_token.mask_uniform\",\"t\":[\"espnet.nets.pytorch_backend.maskctc.add_mask_token.mask_uniform(ys_pad, mask_token, eos, ignore_id)\",\"Replace random tokens with <mask> label and add <eos> label.\",\"The number of <mask> is chosen from a uniform distribution between one and the target sequence’s length. :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :param int mask_token: index of <mask> :param int eos: index of <eos> :param int ignore_id: index of padding :return: padded tensor (B, Lmax) :rtype: torch.Tensor :return: padded tensor (B, Lmax) :rtype: torch.Tensor\"]},\"906\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.pad_list\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.pad_list(xs, pad_value)\",\"Perform padding for the list of tensors.\",\"Parameters:\",\"xs (List) – List of Tensors [(T_1, *), (T_2, *), …, (T_B, *)].\",\"pad_value (float) – Value for padding.\",\"Returns: Padded tensor (B, Tmax, *).\",\"Return type: Tensor\"]},\"907\":{\"h\":\"Examples\",\"t\":[\">>> x = [torch.ones(4), torch.ones(2), torch.ones(1)] >>> x [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])] >>> pad_list(x, 0) tensor([[1., 1., 1., 1.], [1., 1., 0., 0.], [1., 0., 0., 0.]])\"]},\"908\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.pad_sequence\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.pad_sequence(labels: List[int], pad_id: int)\",\"Left pad label ID sequences.\",\"Parameters:\",\"labels – Label ID sequence.\",\"pad_id – Padding symbol ID.\",\"Returns: Padded label ID sequences.\",\"Return type: final\"]},\"909\":{\"h\":\"espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention\",\"t\":[\"espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention(data, uttid_list, attn_dict, outdir, suffix='png', savefn=<function savefig>, ikey='input', iaxis=0, okey='output', oaxis=0, subsampling_factor=4)\",\"Plot multi head attentions.\",\"Parameters:\",\"data (dict) – utts info from json file\",\"uttid_list (List) – utterance IDs\",\"attn_dict (dict *[*str,torch.Tensor]) – multi head attention dict. values should be torch.Tensor (head, input_length, output_length)\",\"outdir (str) – dir to save fig\",\"suffix (str) – filename suffix including image type (e.g., png)\",\"savefn – function to save\",\"ikey (str) – key to access input\",\"iaxis (int) – dimension to access input\",\"okey (str) – key to access output\",\"oaxis (int) – dimension to access output\",\"subsampling_factor – subsampling factor in encoder\"]},\"910\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.prepare_body_model\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.prepare_body_model(net_part: str, blocks: List[Dict[str, Any]])\",\"Prepare model body blocks.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"blocks – Blocks parameters for network part.\",\"Returns: Network output dimension.\"]},\"911\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.prepare_input_layer\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.prepare_input_layer(input_layer_type: str, feats_dim: int, blocks: List[Dict[str, Any]], dropout_rate: float, pos_enc_dropout_rate: float)\",\"Prepare input layer arguments.\",\"Parameters:\",\"input_layer_type – Input layer type.\",\"feats_dim – Dimension of input features.\",\"blocks – Blocks parameters for network part.\",\"dropout_rate – Dropout rate for input layer.\",\"pos_enc_dropout_rate – Dropout rate for input layer pos. enc.\",\"Returns: Input block parameters.\",\"Return type: input_block\"]},\"912\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.recombine_hyps\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.recombine_hyps(hyps: List[Hypothesis])\",\"Recombine hypotheses with same label ID sequence.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Recombined hypotheses.\",\"Return type: final\"]},\"913\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.rename_state_dict\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.rename_state_dict(old_prefix: str, new_prefix: str, state_dict: Dict[str, Tensor])\",\"Replace keys of old prefix with new prefix in state dict.\"]},\"914\":{\"h\":\"espnet.nets.pytorch_backend.transformer.repeat.repeat\",\"t\":[\"espnet.nets.pytorch_backend.transformer.repeat.repeat(N, fn, layer_drop_rate=0.0)\",\"Repeat module N times.\",\"Parameters:\",\"N (int) – Number of repeat time.\",\"fn (Callable) – Function to generate module.\",\"layer_drop_rate (float) – Probability of dropping out each fn (layer).\",\"Returns: Repeated model instance.\",\"Return type:MultiSequential\"]},\"915\":{\"h\":\"espnet.nets.pytorch_backend.transducer.rnn_encoder.reset_backward_rnn_state\",\"t\":[\"espnet.nets.pytorch_backend.transducer.rnn_encoder.reset_backward_rnn_state(states: Tensor | List[Tensor | None])\",\"Set backward BRNN states to zeroes.\",\"Parameters:states – Encoder hidden states.\",\"Returns: Encoder hidden states with backward set to zero.\",\"Return type: states\"]},\"916\":{\"h\":\"espnet.nets.pytorch_backend.transformer.plot.savefig\",\"t\":[\"espnet.nets.pytorch_backend.transformer.plot.savefig(plot, filename)\"]},\"917\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.select_k_expansions\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.select_k_expansions(hyps: List[ExtendedHypothesis], topk_idxs: Tensor, topk_logps: Tensor, gamma: float)\",\"Return K hypotheses candidates for expansion from a list of hypothesis.\",\"K candidates are selected according to the extended hypotheses probabilities and a prune-by-value method. Where K is equal to beam_size + beta.\",\"Parameters:\",\"hyps – Hypotheses.\",\"topk_idxs – Indices of candidates hypothesis.\",\"topk_logps – Log-probabilities for hypotheses expansions.\",\"gamma – Allowed logp difference for prune-by-value method.\",\"Returns: Best K expansion hypotheses candidates.\",\"Return type: k_expansions\"]},\"918\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.select_lm_state\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.select_lm_state(lm_states: List[Any] | Dict[str, Any], idx: int, lm_layers: int, is_wordlm: bool)\",\"Get ID state from LM hidden states.\",\"Parameters:\",\"lm_states – LM hidden states.\",\"idx – LM state ID to extract.\",\"lm_layers – Number of LM layers.\",\"is_wordlm – Whether provided LM is a word-level LM.\",\"Returns: LM hidden state for given ID.\",\"Return type: idx_state\"]},\"919\":{\"h\":\"espnet.nets.pytorch_backend.initialization.set_forget_bias_to_one\",\"t\":[\"espnet.nets.pytorch_backend.initialization.set_forget_bias_to_one(bias)\",\"Initialize a bias vector in the forget gate with one.\"]},\"920\":{\"h\":\"espnet.nets.pytorch_backend.maskctc.mask.square_mask\",\"t\":[\"espnet.nets.pytorch_backend.maskctc.mask.square_mask(ys_in_pad, ignore_id)\",\"Create attention mask to avoid attending on padding tokens.\",\"Parameters:\",\"ys_pad (torch.Tensor) – batch of padded target sequences (B, Lmax)\",\"ignore_id (int) – index of padding\",\"dtype (torch.dtype) – result dtype\",\"Return type: torch.Tensor (B, Lmax, Lmax)\"]},\"921\":{\"h\":\"espnet.nets.pytorch_backend.transformer.mask.subsequent_mask\",\"t\":[\"espnet.nets.pytorch_backend.transformer.mask.subsequent_mask(size, device='cpu', dtype=torch.bool)\",\"Create mask for subsequent steps (size, size).\",\"Parameters:\",\"size (int) – size of mask\",\"device (str) – “cpu” or “cuda” or torch.Tensor.device\",\"dtype (torch.dtype) – result dtype\",\"Return type: torch.Tensor\",\">>> subsequent_mask(3) [[1, 0, 0], [1, 1, 0], [1, 1, 1]]\"]},\"922\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.subtract\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.subtract(x: List[ExtendedHypothesis], subset: List[ExtendedHypothesis])\",\"Remove elements of subset if corresponding label ID sequence already exist in x.\",\"Parameters:\",\"x – Set of hypotheses.\",\"subset – Subset of x.\",\"Returns: New set of hypotheses.\",\"Return type: final\"]},\"923\":{\"h\":\"espnet.nets.chainer_backend.transformer.training.sum_sqnorm\",\"t\":[\"espnet.nets.chainer_backend.transformer.training.sum_sqnorm(arr)\",\"Calculate the norm of the array.\",\"Parameters:arr (numpy.ndarray) –\",\"Returns: Sum of the norm calculated from the given array.\",\"Return type: Float\"]},\"924\":{\"h\":\"espnet.nets.pytorch_backend.transformer.mask.target_mask\",\"t\":[\"espnet.nets.pytorch_backend.transformer.mask.target_mask(ys_in_pad, ignore_id)\",\"Create mask for decoder self-attention.\",\"Parameters:\",\"ys_pad (torch.Tensor) – batch of padded target sequences (B, Lmax)\",\"ignore_id (int) – index of padding\",\"dtype (torch.dtype) – result dtype\",\"Return type: torch.Tensor (B, Lmax, Lmax)\"]},\"925\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.th_accuracy\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.th_accuracy(pad_outputs, pad_targets, ignore_label)\",\"Calculate accuracy.\",\"Parameters:\",\"pad_outputs (Tensor) – Prediction tensors (B * Lmax, D).\",\"pad_targets (LongTensor) – Target label tensors (B, Lmax, D).\",\"ignore_label (int) – Ignore label id.\",\"Returns: Accuracy value (0.0 - 1.0).\",\"Return type: float\"]},\"926\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.to_device\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.to_device(m, x)\",\"Send tensor into the device of the module.\",\"Parameters:\",\"m (torch.nn.Module) – Torch module.\",\"x (Tensor) – Torch tensor.\",\"Returns: Torch tensor located in the same place as torch module.\",\"Return type: Tensor\"]},\"927\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.to_torch_tensor\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.to_torch_tensor(x)\",\"Change to torch.Tensor or ComplexTensor from numpy.ndarray.\",\"Parameters:x – Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict.\",\"Returns: Type converted inputs.\",\"Return type: Tensor or ComplexTensor\"]},\"928\":{\"h\":\"Examples\",\"t\":[\">>> xs = np.ones(3, dtype=np.float32) >>> xs = to_torch_tensor(xs) tensor([1., 1., 1.]) >>> xs = torch.ones(3, 4, 5) >>> assert to_torch_tensor(xs) is xs >>> xs = {'real': xs, 'imag': xs} >>> to_torch_tensor(xs) ComplexTensor( Real: tensor([1., 1., 1.]) Imag; tensor([1., 1., 1.]) )\"]},\"929\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.trim_by_ctc_posterior\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.trim_by_ctc_posterior(h: Tensor, ctc_probs: Tensor, masks: Tensor, pos_emb: Tensor | None = None)\",\"Trim the encoder hidden output using CTC posterior. The continuous frames in the tail that confidently represent blank symbols are trimmed.\"]},\"930\":{\"h\":\"espnet.nets.pytorch_backend.nets_utils.triu_onnx\",\"t\":[\"espnet.nets.pytorch_backend.nets_utils.triu_onnx(x)\"]},\"931\":{\"h\":\"espnet.nets.pytorch_backend.initialization.uniform_init_parameters\",\"t\":[\"espnet.nets.pytorch_backend.initialization.uniform_init_parameters(module)\",\"Initialize parameters with an uniform distribution.\"]},\"932\":{\"h\":\"espnet.nets.pytorch_backend.frontends.feature_transform.utterance_mvn\",\"t\":[\"espnet.nets.pytorch_backend.frontends.feature_transform.utterance_mvn(x: Tensor, ilens: LongTensor, norm_means: bool = True, norm_vars: bool = False, eps: float = 1e-20)\",\"Apply utterance mean and variance normalization\",\"Parameters:\",\"x – (B, T, D), assumed zero padded\",\"ilens – (B, T, D)\",\"norm_means –\",\"norm_vars –\",\"eps –\"]},\"933\":{\"h\":\"espnet.nets.pytorch_backend.transducer.utils.valid_aux_encoder_output_layers\",\"t\":[\"espnet.nets.pytorch_backend.transducer.utils.valid_aux_encoder_output_layers(aux_layer_id: List[int], enc_num_layers: int, use_symm_kl_div_loss: bool, subsample: List[int])\",\"Check whether provided auxiliary encoder layer IDs are valid.\",\"Return the valid list sorted with duplicates removed.\",\"Parameters:\",\"aux_layer_id – Auxiliary encoder layer IDs.\",\"enc_num_layers – Number of encoder layers.\",\"use_symm_kl_div_loss – Whether symmetric KL divergence loss is used.\",\"subsample – Subsampling rate per layer.\",\"Returns: Valid list of auxiliary encoder layers.\",\"Return type: valid\"]},\"934\":{\"h\":\"espnet.nets.pytorch_backend.transducer.blocks.verify_block_arguments\",\"t\":[\"espnet.nets.pytorch_backend.transducer.blocks.verify_block_arguments(net_part: str, block: Dict[str, Any], num_block: int)\",\"Verify block arguments are valid.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"block – Block parameters.\",\"num_block – Block ID.\",\"Returns: Input and output dimension of the block.\",\"Return type: block_io\"]},\"935\":{\"h\":\"espnet.nets.pytorch_backend.conformer.argument.verify_rel_pos_type\",\"t\":[\"espnet.nets.pytorch_backend.conformer.argument.verify_rel_pos_type(args)\",\"Verify the relative positional encoding type for compatibility.\",\"Parameters:args (Namespace) – original arguments\",\"Returns: modified arguments\",\"Return type: args (Namespace)\"]},\"936\":{\"h\":\"espnet.st.pytorch_backend.st.CustomConverter\",\"t\":[\"class espnet.st.pytorch_backend.st.CustomConverter(subsampling_factor=1, dtype=torch.float32, use_source_text=False)\",\"Bases: CustomConverter\",\"Custom batch converter for Pytorch.\",\"Parameters:\",\"subsampling_factor (int) – The subsampling factor.\",\"dtype (torch.dtype) – Data type to convert.\",\"use_source_text (bool) – use source transcription.\",\"Construct a CustomConverter object.\"]},\"937\":{\"h\":\"espnet.st.pytorch_backend.st.train\",\"t\":[\"<!-- _espnet.st.pytorch_backend.st.train -->\",\"espnet.st.pytorch_backend.st.train(args)\",\"Train with the given args.\",\"Parameters:args (namespace) – The program arguments.\"]},\"938\":{\"h\":\"espnet.st.pytorch_backend.st.trans\",\"t\":[\"<!-- _espnet.st.pytorch_backend.st.trans -->\",\"espnet.st.pytorch_backend.st.trans(args)\",\"Decode with the given args.\",\"Parameters:args (namespace) – The program arguments.\"]},\"939\":{\"h\":\"espnet.transform.add_deltas.AddDeltas\",\"t\":[\"<!-- _espnet.transform.add_deltas.AddDeltas -->\",\"class espnet.transform.add_deltas.AddDeltas(window=2, order=2)\",\"Bases: object\"]},\"940\":{\"h\":\"espnet.transform.perturb.BandpassPerturbation\",\"t\":[\"class espnet.transform.perturb.BandpassPerturbation(lower=0.0, upper=0.75, seed=None, axes=(-1,))\",\"Bases: object\",\"Randomly dropout along the frequency axis.\",\"The original idea comes from the following: : “randomly-selected frequency band was cut off under the constraint of : leaving at least 1,000 Hz band within the range of less than 4,000Hz.” <br/> (The Hitachi/JHU CHiME-5 system: Advances in speech recognition for : everyday home environments using multiple microphone arrays; http://spandh.dcs.shef.ac.uk/chime_workshop/papers/CHiME_2018_paper_kanda.pdf)\"]},\"941\":{\"h\":\"espnet.transform.cmvn.CMVN\",\"t\":[\"<!-- _espnet.transform.cmvn.CMVN -->\",\"class espnet.transform.cmvn.CMVN(stats, norm_means=True, norm_vars=False, filetype='mat', utt2spk=None, spk2utt=None, reverse=False, std_floor=1e-20)\",\"Bases: object\"]},\"942\":{\"h\":\"espnet.transform.channel_selector.ChannelSelector\",\"t\":[\"class espnet.transform.channel_selector.ChannelSelector(train_channel='random', eval_channel=0, axis=1)\",\"Bases: object\",\"Select 1ch from multi-channel signal\"]},\"943\":{\"h\":\"espnet.transform.spec_augment.FreqMask\",\"t\":[\"<!-- _espnet.transform.spec_augment.FreqMask -->\",\"class espnet.transform.spec_augment.FreqMask(**kwargs)\",\"Bases: FuncTrans\",\"freq mask for spec agument\",\"Parameters:\",\"x (numpy.ndarray) – (time, freq)\",\"n_mask (int) – the number of masks\",\"inplace (bool) – overwrite\",\"replace_with_zero (bool) – pad zero on mask if true else use mean\"]},\"944\":{\"h\":\"espnet.transform.functional.FuncTrans\",\"t\":[\"<!-- _espnet.transform.functional.FuncTrans -->\",\"class espnet.transform.functional.FuncTrans(**kwargs)\",\"Bases: TransformInterface\",\"Functional Transformation\",\"WARNING\",\"Builtin or C/C++ functions may not work properly because this class heavily depends on the inspect module.\",\"Usage:\",\">>> def foo_bar(x, a=1, b=2): ... '''Foo bar ... :param x: input ... :param int a: default 1 ... :param int b: default 2 ... ''' ... return x + a - b\",\">>> class FooBar(FuncTrans): ... _func = foo_bar ... __doc__ = foo_bar.__doc__\",\"classmethod add_arguments(parser)\",\"classmethod default_params()\",\"property func\"]},\"945\":{\"h\":\"espnet.transform.spectrogram.IStft\",\"t\":[\"<!-- _espnet.transform.spectrogram.IStft -->\",\"class espnet.transform.spectrogram.IStft(n_shift, win_length=None, window='hann', center=True)\",\"Bases: object\"]},\"946\":{\"h\":\"espnet.transform.transform_interface.Identity\",\"t\":[\"class espnet.transform.transform_interface.Identity\",\"Bases: TransformInterface\",\"Identity Function\"]},\"947\":{\"h\":\"espnet.transform.spectrogram.LogMelSpectrogram\",\"t\":[\"class espnet.transform.spectrogram.LogMelSpectrogram(fs, n_mels, n_fft, n_shift, win_length=None, window='hann', fmin=None, fmax=None, eps=1e-10)\",\"Bases: object\"]},\"948\":{\"h\":\"espnet.transform.perturb.NoiseInjection\",\"t\":[\"<!-- _espnet.transform.perturb.NoiseInjection -->\",\"class espnet.transform.perturb.NoiseInjection(utt2noise=None, lower=-20, upper=-5, utt2ratio=None, filetype='list', dbunit=True, seed=None)\",\"Bases: object\",\"Add isotropic noise\"]},\"949\":{\"h\":\"espnet.transform.perturb.RIRConvolve\",\"t\":[\"<!-- _espnet.transform.perturb.RIRConvolve -->\",\"class espnet.transform.perturb.RIRConvolve(utt2rir, filetype='list')\",\"Bases: object\"]},\"950\":{\"h\":\"espnet.transform.spec_augment.SpecAugment\",\"t\":[\"<!-- _espnet.transform.spec_augment.SpecAugment -->\",\"class espnet.transform.spec_augment.SpecAugment(**kwargs)\",\"Bases: FuncTrans\",\"spec agument\",\"apply random time warping and time/freq masking default setting is based on LD (Librispeech double) in Table 2\",\"https://arxiv.org/pdf/1904.08779.pdf\",\"Parameters:\",\"x (numpy.ndarray) – (time, freq)\",\"resize_mode (str) – “PIL” (fast, nondifferentiable) or “sparse_image_warp” (slow, differentiable)\",\"max_time_warp (int) – maximum frames to warp the center frame in spectrogram (W)\",\"freq_mask_width (int) – maximum width of the random freq mask (F)\",\"n_freq_mask (int) – the number of the random freq mask (m_F)\",\"time_mask_width (int) – maximum width of the random time mask (T)\",\"n_time_mask (int) – the number of the random time mask (m_T)\",\"inplace (bool) – overwrite intermediate array\",\"replace_with_zero (bool) – pad zero on mask if true else use mean\"]},\"951\":{\"h\":\"espnet.transform.spectrogram.Spectrogram\",\"t\":[\"<!-- _espnet.transform.spectrogram.Spectrogram -->\",\"class espnet.transform.spectrogram.Spectrogram(n_fft, n_shift, win_length=None, window='hann')\",\"Bases: object\"]},\"952\":{\"h\":\"espnet.transform.perturb.SpeedPerturbation\",\"t\":[\"<!-- _espnet.transform.perturb.SpeedPerturbation -->\",\"class espnet.transform.perturb.SpeedPerturbation(lower=0.9, upper=1.1, utt2ratio=None, keep_length=True, res_type='kaiser_best', seed=None)\",\"Bases: object\",\"The speed perturbation in kaldi uses sox-speed instead of sox-tempo, and sox-speed just to resample the input, i.e pitch and tempo are changed both.\",\"“Why use speed option instead of tempo -s in SoX for speed perturbation” https://groups.google.com/forum/#!topic/kaldi-help/8OOG7eE4sZ8\",\"WARNING\",\"This function is very slow because of resampling. I recommmend to apply speed-perturb outside the training using sox.\"]},\"953\":{\"h\":\"espnet.transform.spectrogram.Stft\",\"t\":[\"<!-- _espnet.transform.spectrogram.Stft -->\",\"class espnet.transform.spectrogram.Stft(n_fft, n_shift, win_length=None, window='hann', center=True, pad_mode='reflect')\",\"Bases: object\"]},\"954\":{\"h\":\"espnet.transform.spectrogram.Stft2LogMelSpectrogram\",\"t\":[\"class espnet.transform.spectrogram.Stft2LogMelSpectrogram(fs, n_mels, n_fft, fmin=None, fmax=None, eps=1e-10)\",\"Bases: object\"]},\"955\":{\"h\":\"espnet.transform.spec_augment.TimeMask\",\"t\":[\"<!-- _espnet.transform.spec_augment.TimeMask -->\",\"class espnet.transform.spec_augment.TimeMask(**kwargs)\",\"Bases: FuncTrans\",\"freq mask for spec agument\",\"Parameters:\",\"spec (numpy.ndarray) – (time, freq)\",\"n_mask (int) – the number of masks\",\"inplace (bool) – overwrite\",\"replace_with_zero (bool) – pad zero on mask if true else use mean\"]},\"956\":{\"h\":\"espnet.transform.spec_augment.TimeWarp\",\"t\":[\"<!-- _espnet.transform.spec_augment.TimeWarp -->\",\"class espnet.transform.spec_augment.TimeWarp(**kwargs)\",\"Bases: FuncTrans\",\"time warp for spec augment\",\"move random center frame by the random width ~ uniform(-window, window) :param numpy.ndarray x: spectrogram (time, freq) :param int max_time_warp: maximum time frames to warp :param bool inplace: overwrite x with the result :param str mode: “PIL” (default, fast, not differentiable) or “sparse_image_warp”\",\"(slow, differentiable)\",\"Returns numpy.ndarray: time warped spectrogram (time, freq)\"]},\"957\":{\"h\":\"espnet.transform.transform_interface.TransformInterface\",\"t\":[\"class espnet.transform.transform_interface.TransformInterface\",\"Bases: object\",\"Transform Interface\",\"classmethod add_arguments(parser)\"]},\"958\":{\"h\":\"espnet.transform.transformation.Transformation\",\"t\":[\"class espnet.transform.transformation.Transformation(conffile=None)\",\"Bases: object\",\"Apply some functions to the mini-batch\"]},\"959\":{\"h\":\"Examples\",\"t\":[\">>> kwargs = {\\\"process\\\": [{\\\"type\\\": \\\"fbank\\\", ... \\\"n_mels\\\": 80, ... \\\"fs\\\": 16000}, ... {\\\"type\\\": \\\"cmvn\\\", ... \\\"stats\\\": \\\"data/train/cmvn.ark\\\", ... \\\"norm_vars\\\": True}, ... {\\\"type\\\": \\\"delta\\\", \\\"window\\\": 2, \\\"order\\\": 2}]} >>> transform = Transformation(kwargs) >>> bs = 10 >>> xs = [np.random.randn(100, 80).astype(np.float32) ... for _ in range(bs)] >>> xs = transform(xs)\"]},\"960\":{\"h\":\"espnet.transform.cmvn.UtteranceCMVN\",\"t\":[\"<!-- _espnet.transform.cmvn.UtteranceCMVN -->\",\"class espnet.transform.cmvn.UtteranceCMVN(norm_means=True, norm_vars=False, std_floor=1e-20)\",\"Bases: object\"]},\"961\":{\"h\":\"espnet.transform.perturb.VolumePerturbation\",\"t\":[\"class espnet.transform.perturb.VolumePerturbation(lower=-1.6, upper=1.6, utt2ratio=None, dbunit=True, seed=None)\",\"Bases: object\"]},\"962\":{\"h\":\"espnet.transform.wpe.WPE\",\"t\":[\"<!-- _espnet.transform.wpe.WPE -->\",\"class espnet.transform.wpe.WPE(taps=10, delay=3, iterations=3, psd_context=0, statistics_mode='full')\",\"Bases: object\"]},\"963\":{\"h\":\"espnet.transform.add_deltas.add_deltas\",\"t\":[\"<!-- _espnet.transform.add_deltas.add_deltas -->\",\"espnet.transform.add_deltas.add_deltas(x, window=2, order=2)\"]},\"964\":{\"h\":\"espnet.transform.add_deltas.delta\",\"t\":[\"<!-- _espnet.transform.add_deltas.delta -->\",\"espnet.transform.add_deltas.delta(feat, window)\"]},\"965\":{\"h\":\"espnet.transform.spec_augment.freq_mask\",\"t\":[\"<!-- _espnet.transform.spec_augment.freq_mask -->\",\"espnet.transform.spec_augment.freq_mask(x, F=30, n_mask=2, replace_with_zero=True, inplace=False)\",\"freq mask for spec agument\",\"Parameters:\",\"x (numpy.ndarray) – (time, freq)\",\"n_mask (int) – the number of masks\",\"inplace (bool) – overwrite\",\"replace_with_zero (bool) – pad zero on mask if true else use mean\"]},\"966\":{\"h\":\"espnet.transform.spectrogram.istft\",\"t\":[\"<!-- _espnet.transform.spectrogram.istft -->\",\"espnet.transform.spectrogram.istft(x, n_shift, win_length=None, window='hann', center=True)\"]},\"967\":{\"h\":\"espnet.transform.spectrogram.logmelspectrogram\",\"t\":[\"espnet.transform.spectrogram.logmelspectrogram(x, fs, n_mels, n_fft, n_shift, win_length=None, window='hann', fmin=None, fmax=None, eps=1e-10, pad_mode='reflect')\"]},\"968\":{\"h\":\"espnet.transform.spec_augment.spec_augment\",\"t\":[\"<!-- _espnet.transform.spec_augment.spec_augment -->\",\"espnet.transform.spec_augment.spec_augment(x, resize_mode='PIL', max_time_warp=80, max_freq_width=27, n_freq_mask=2, max_time_width=100, n_time_mask=2, inplace=True, replace_with_zero=True)\",\"spec agument\",\"apply random time warping and time/freq masking default setting is based on LD (Librispeech double) in Table 2\",\"https://arxiv.org/pdf/1904.08779.pdf\",\"Parameters:\",\"x (numpy.ndarray) – (time, freq)\",\"resize_mode (str) – “PIL” (fast, nondifferentiable) or “sparse_image_warp” (slow, differentiable)\",\"max_time_warp (int) – maximum frames to warp the center frame in spectrogram (W)\",\"freq_mask_width (int) – maximum width of the random freq mask (F)\",\"n_freq_mask (int) – the number of the random freq mask (m_F)\",\"time_mask_width (int) – maximum width of the random time mask (T)\",\"n_time_mask (int) – the number of the random time mask (m_T)\",\"inplace (bool) – overwrite intermediate array\",\"replace_with_zero (bool) – pad zero on mask if true else use mean\"]},\"969\":{\"h\":\"espnet.transform.spectrogram.spectrogram\",\"t\":[\"<!-- _espnet.transform.spectrogram.spectrogram -->\",\"espnet.transform.spectrogram.spectrogram(x, n_fft, n_shift, win_length=None, window='hann')\"]},\"970\":{\"h\":\"espnet.transform.spectrogram.stft\",\"t\":[\"<!-- _espnet.transform.spectrogram.stft -->\",\"espnet.transform.spectrogram.stft(x, n_fft, n_shift, win_length=None, window='hann', center=True, pad_mode='reflect')\"]},\"971\":{\"h\":\"espnet.transform.spectrogram.stft2logmelspectrogram\",\"t\":[\"espnet.transform.spectrogram.stft2logmelspectrogram(x_stft, fs, n_mels, n_fft, fmin=None, fmax=None, eps=1e-10)\"]},\"972\":{\"h\":\"espnet.transform.spec_augment.time_mask\",\"t\":[\"<!-- _espnet.transform.spec_augment.time_mask -->\",\"espnet.transform.spec_augment.time_mask(spec, T=40, n_mask=2, replace_with_zero=True, inplace=False)\",\"freq mask for spec agument\",\"Parameters:\",\"spec (numpy.ndarray) – (time, freq)\",\"n_mask (int) – the number of masks\",\"inplace (bool) – overwrite\",\"replace_with_zero (bool) – pad zero on mask if true else use mean\"]},\"973\":{\"h\":\"espnet.transform.spec_augment.time_warp\",\"t\":[\"<!-- _espnet.transform.spec_augment.time_warp -->\",\"espnet.transform.spec_augment.time_warp(x, max_time_warp=80, inplace=False, mode='PIL')\",\"time warp for spec augment\",\"move random center frame by the random width ~ uniform(-window, window) :param numpy.ndarray x: spectrogram (time, freq) :param int max_time_warp: maximum time frames to warp :param bool inplace: overwrite x with the result :param str mode: “PIL” (default, fast, not differentiable) or “sparse_image_warp”\",\"(slow, differentiable)\",\"Returns numpy.ndarray: time warped spectrogram (time, freq)\"]},\"974\":{\"h\":\"espnet.tts.pytorch_backend.tts.CustomConverter\",\"t\":[\"class espnet.tts.pytorch_backend.tts.CustomConverter\",\"Bases: object\",\"Custom converter.\",\"Initilize module.\"]},\"975\":{\"h\":\"espnet.tts.pytorch_backend.tts.CustomEvaluator\",\"t\":[\"class espnet.tts.pytorch_backend.tts.CustomEvaluator(model, iterator, target, device)\",\"Bases: BaseEvaluator\",\"Custom evaluator.\",\"Initilize module.\",\"Parameters:\",\"model (torch.nn.Module) – Pytorch model instance.\",\"iterator (chainer.dataset.Iterator) – Iterator for validation.\",\"target (chainer.Chain) – Dummy chain instance.\",\"device (torch.device) – The device to be used in evaluation.\",\"evaluate()\",\"Evaluate over validation iterator.\"]},\"976\":{\"h\":\"espnet.tts.pytorch_backend.tts.CustomUpdater\",\"t\":[\"class espnet.tts.pytorch_backend.tts.CustomUpdater(model, grad_clip, iterator, optimizer, device, accum_grad=1)\",\"Bases: StandardUpdater\",\"Custom updater.\",\"Initilize module.\",\"Parameters:\",\"model (torch.nn.Module) – Pytorch model instance.\",\"grad_clip (float) – The gradient clipping value.\",\"iterator (chainer.dataset.Iterator) – Iterator for training.\",\"optimizer (torch.optim.Optimizer) – Pytorch optimizer instance.\",\"device (torch.device) – The device to be used in training.\",\"update()\",\"Run update function.\",\"update_core()\",\"Update model one step.\"]},\"977\":{\"h\":\"espnet.tts.pytorch_backend.tts.decode\",\"t\":[\"<!-- _espnet.tts.pytorch_backend.tts.decode -->\",\"espnet.tts.pytorch_backend.tts.decode(args)\",\"Decode with E2E-TTS model.\"]},\"978\":{\"h\":\"espnet.tts.pytorch_backend.tts.train\",\"t\":[\"<!-- _espnet.tts.pytorch_backend.tts.train -->\",\"espnet.tts.pytorch_backend.tts.train(args)\",\"Train E2E-TTS model.\"]},\"979\":{\"h\":\"espnet.utils.training.evaluator.BaseEvaluator\",\"t\":[\"class espnet.utils.training.evaluator.BaseEvaluator(iterator, target, converter=<function concat_examples>, device=None, eval_hook=None, eval_func=None)\",\"Bases: Evaluator\",\"Base Evaluator in ESPnet\"]},\"980\":{\"h\":\"espnet.utils.cli_writers.BaseWriter\",\"t\":[\"<!-- _espnet.utils.cli_writers.BaseWriter -->\",\"class espnet.utils.cli_writers.BaseWriter\",\"Bases: object\",\"close()\"]},\"981\":{\"h\":\"espnet.utils.dataset.ChainerDataLoader\",\"t\":[\"<!-- _espnet.utils.dataset.ChainerDataLoader -->\",\"class espnet.utils.dataset.ChainerDataLoader(**kwargs)\",\"Bases: object\",\"Pytorch dataloader in chainer style.\",\"Parameters:torch.utils.data.dataloader.Dataloader (all args for) –\",\"Init function.\",\"property epoch_detail\",\"Epoch_detail required by chainer.\",\"finalize()\",\"Implement finalize function.\",\"static get_first_element(x)\",\"Get first element of a given array-like object.\",\"next()\",\"Implement next function.\",\"serialize(serializer)\",\"Serialize and deserialize function.\",\"start_shuffle()\",\"Shuffle function for sortagrad.\"]},\"982\":{\"h\":\"espnet.utils.cli_readers.HDF5Reader\",\"t\":[\"<!-- _espnet.utils.cli_readers.HDF5Reader -->\",\"class espnet.utils.cli_readers.HDF5Reader(rspecifier, return_shape=False)\",\"Bases: object\"]},\"983\":{\"h\":\"espnet.utils.cli_writers.HDF5Writer\",\"t\":[\"<!-- _espnet.utils.cli_writers.HDF5Writer -->\",\"class espnet.utils.cli_writers.HDF5Writer(wspecifier, write_num_frames=None, compress=False)\",\"Bases: BaseWriter\"]},\"984\":{\"h\":\"Examples\",\"t\":[\">>> with HDF5Writer('ark:out.h5', compress=True) as f: ... f['key'] = array\"]},\"985\":{\"h\":\"espnet.utils.cli_readers.KaldiReader\",\"t\":[\"<!-- _espnet.utils.cli_readers.KaldiReader -->\",\"class espnet.utils.cli_readers.KaldiReader(rspecifier, return_shape=False, segments=None)\",\"Bases: object\"]},\"986\":{\"h\":\"espnet.utils.cli_writers.KaldiWriter\",\"t\":[\"<!-- _espnet.utils.cli_writers.KaldiWriter -->\",\"class espnet.utils.cli_writers.KaldiWriter(wspecifier, write_num_frames=None, compress=False, compression_method=2)\",\"Bases: BaseWriter\"]},\"987\":{\"h\":\"espnet.utils.io_utils.LoadInputsAndTargets\",\"t\":[\"<!-- _espnet.utils.io_utils.LoadInputsAndTargets -->\",\"class espnet.utils.io_utils.LoadInputsAndTargets(mode='asr', preprocess_conf=None, load_input=True, load_output=True, sort_in_input_length=True, use_speaker_embedding=False, use_second_target=False, preprocess_args=None, keep_all_data_on_mem=False)\",\"Bases: object\",\"Create a mini-batch from a list of dicts\",\">>> batch = [('utt1', ... dict(input=[dict(feat='some.ark:123', ... filetype='mat', ... name='input1', ... shape=[100, 80])], ... output=[dict(tokenid='1 2 3 4', ... name='target1', ... shape=[4, 31])]])) >>> l = LoadInputsAndTargets() >>> feat, target = l(batch)\",\"Param: str mode: Specify the task mode, “asr” or “tts”\",\"Param: str preprocess_conf: The path of a json file for pre-processing\",\"Param: bool load_input: If False, not to load the input data\",\"Param: bool load_output: If False, not to load the output data\",\"Param: bool sort_in_input_length: Sort the mini-batch in descending order of the input length\",\"Param: bool use_speaker_embedding: Used for tts mode only\",\"Param: bool use_second_target: Used for tts mode only\",\"Param: dict preprocess_args: Set some optional arguments for preprocessing\",\"Param: Optional[dict] preprocess_args: Used for tts mode only\"]},\"988\":{\"h\":\"espnet.utils.training.iterators.ShufflingEnabler\",\"t\":[\"class espnet.utils.training.iterators.ShufflingEnabler(iterators)\",\"Bases: Extension\",\"An extension enabling shuffling on an Iterator\",\"Inits the ShufflingEnabler\",\"Parameters:iterators (list *[*Iterator]) – The iterators to enable shuffling on\"]},\"989\":{\"h\":\"espnet.utils.io_utils.SoundHDF5File\",\"t\":[\"<!-- _espnet.utils.io_utils.SoundHDF5File -->\",\"class espnet.utils.io_utils.SoundHDF5File(filepath, mode='r+', format=None, dtype='int16', **kwargs)\",\"Bases: object\",\"Collecting sound files to a HDF5 file\",\">>> f = SoundHDF5File('a.flac.h5', mode='a') >>> array = np.random.randint(0, 100, 100, dtype=np.int16) >>> f['id'] = (array, 16000) >>> array, rate = f['id']\",\"Param: str filepath:\",\"Param: str mode:\",\"Param: str format: The type used when saving wav. flac, nist, htk, etc.\",\"Param: str dtype:\",\"close()\",\"create_dataset(name, shape=None, data=None, **kwds)\",\"items()\",\"keys()\",\"values()\"]},\"990\":{\"h\":\"espnet.utils.cli_readers.SoundHDF5Reader\",\"t\":[\"<!-- _espnet.utils.cli_readers.SoundHDF5Reader -->\",\"class espnet.utils.cli_readers.SoundHDF5Reader(rspecifier, return_shape=False)\",\"Bases: object\"]},\"991\":{\"h\":\"espnet.utils.cli_writers.SoundHDF5Writer\",\"t\":[\"<!-- _espnet.utils.cli_writers.SoundHDF5Writer -->\",\"class espnet.utils.cli_writers.SoundHDF5Writer(wspecifier, write_num_frames=None, pcm_format='wav')\",\"Bases: BaseWriter\"]},\"992\":{\"h\":\"Examples\",\"t\":[\">>> fs = 16000 >>> with SoundHDF5Writer('ark:out.h5') as f: ... f['key'] = fs, array\"]},\"993\":{\"h\":\"espnet.utils.cli_readers.SoundReader\",\"t\":[\"<!-- _espnet.utils.cli_readers.SoundReader -->\",\"class espnet.utils.cli_readers.SoundReader(rspecifier, return_shape=False)\",\"Bases: object\"]},\"994\":{\"h\":\"espnet.utils.cli_writers.SoundWriter\",\"t\":[\"<!-- _espnet.utils.cli_writers.SoundWriter -->\",\"class espnet.utils.cli_writers.SoundWriter(wspecifier, write_num_frames=None, pcm_format='wav')\",\"Bases: BaseWriter\"]},\"995\":{\"h\":\"Examples\",\"t\":[\">>> fs = 16000 >>> with SoundWriter('ark,scp:outdir,out.scp') as f: ... f['key'] = fs, array\"]},\"996\":{\"h\":\"espnet.utils.training.tensorboard_logger.TensorboardLogger\",\"t\":[\"class espnet.utils.training.tensorboard_logger.TensorboardLogger(logger, att_reporter=None, ctc_reporter=None, entries=None, epoch=0)\",\"Bases: Extension\",\"A tensorboard logger extension\",\"Init the extension\",\"Parameters:\",\"logger (SummaryWriter) – The logger to use\",\"att_reporter (PlotAttentionReporter) – The (optional) PlotAttentionReporter\",\"entries – The entries to watch\",\"epoch (int) – The starting epoch\",\"default_name = 'espnet_tensorboard_logger'\"]},\"997\":{\"h\":\"espnet.utils.training.iterators.ToggleableShufflingMultiprocessIterator\",\"t\":[\"class espnet.utils.training.iterators.ToggleableShufflingMultiprocessIterator(dataset, batch_size, repeat=True, shuffle=True, n_processes=None, n_prefetch=1, shared_mem=None, maxtasksperchild=20)\",\"Bases: MultiprocessIterator\",\"A MultiprocessIterator having its shuffling property activated during training\",\"Init the iterator\",\"Parameters:\",\"dataset (torch.nn.Tensor) – The dataset to take batches from\",\"batch_size (int) – The batch size\",\"repeat (bool) – Whether to repeat batches or not (enables multiple epochs)\",\"shuffle (bool) – Whether to shuffle the order of the batches\",\"n_processes (int) – How many processes to use\",\"n_prefetch (int) – The number of prefetch to use\",\"shared_mem (int) – How many memory to share between processes\",\"maxtasksperchild (int) – Maximum number of tasks per child\",\"start_shuffle()\",\"Starts shuffling (or reshuffles) the batches\"]},\"998\":{\"h\":\"espnet.utils.training.iterators.ToggleableShufflingSerialIterator\",\"t\":[\"class espnet.utils.training.iterators.ToggleableShufflingSerialIterator(dataset, batch_size, repeat=True, shuffle=True)\",\"Bases: SerialIterator\",\"A SerialIterator having its shuffling property activated during training\",\"Init the Iterator\",\"Parameters:\",\"dataset (torch.nn.Tensor) – The dataset to take batches from\",\"batch_size (int) – The batch size\",\"repeat (bool) – Whether to repeat data (allow multiple epochs)\",\"shuffle (bool) – Whether to shuffle the batches\",\"start_shuffle()\",\"Starts shuffling (or reshuffles) the batches\"]},\"999\":{\"h\":\"espnet.utils.dataset.Transform\",\"t\":[\"<!-- _espnet.utils.dataset.Transform -->\",\"class espnet.utils.dataset.Transform(converter, load)\",\"Bases: object\",\"Transform function container.\",\"lambda can’t work well when using DDP because lambda is not pickable in the case of multi process. This class is required for DDP use case.\",\"Parameters:\",\"converter – batch converter\",\"load – function object to load data and create minibatch\",\"Initialize.\"]},\"1000\":{\"h\":\"espnet.utils.dataset.TransformDataset\",\"t\":[\"<!-- _espnet.utils.dataset.TransformDataset -->\",\"class espnet.utils.dataset.TransformDataset(data, transform)\",\"Bases: Dataset\",\"Transform Dataset for pytorch backend.\",\"Parameters:\",\"data – list object from make_batchset\",\"transform – transform function\",\"Init function.\"]},\"1001\":{\"h\":\"espnet.utils.spec_augment.apply_interpolation\",\"t\":[\"espnet.utils.spec_augment.apply_interpolation(query_points, train_points, w, v, order)\",\"Apply polyharmonic interpolation model to data.\",\"Notes\",\"Given coefficients w and v for the interpolation model, we evaluate interpolated function values at query_points.\",\"Parameters:\",\"query_points – [b, m, d] x values to evaluate the interpolation at\",\"train_points – [b, n, d] x values that act as the interpolation centers ( the c variables in the wikipedia article) w: [b, n, k] weights on each interpolation center v: [b, d, k] weights on each input dimension\",\"order – order of the interpolation\",\"Returns: Polyharmonic interpolation evaluated at points defined in query_points.\"]},\"1002\":{\"h\":\"espnet.utils.cli_utils.assert_scipy_wav_style\",\"t\":[\"espnet.utils.cli_utils.assert_scipy_wav_style(value)\"]},\"1003\":{\"h\":\"espnet.utils.training.batchfy.batchfy_by_bin\",\"t\":[\"espnet.utils.training.batchfy.batchfy_by_bin(sorted_data, batch_bins, num_batches=0, min_batch_size=1, shortest_first=False, ikey='input', okey='output')\",\"Make variably sized batch set, which maximizes\",\"the number of bins up to batch_bins.\",\"Parameters:\",\"sorted_data (Dict *[*str,Dict *[*str,Any]]) – dictionary loaded from data.json\",\"batch_bins (int) – Maximum frames of a batch\",\"num_batches (int) – # number of batches to use (for debug)\",\"min_batch_size (int) – minimum batch size (for multi-gpu)\",\"test (int) – Return only every test batches\",\"shortest_first (bool) – Sort from batch with shortest samples to longest if true, otherwise reverse\",\"ikey (str) – key to access input (for ASR ikey=”input”, for TTS ikey=”output”.)\",\"okey (str) – key to access output (for ASR okey=”output”. for TTS okey=”input”.)\",\"Returns: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches\"]},\"1004\":{\"h\":\"espnet.utils.training.batchfy.batchfy_by_frame\",\"t\":[\"espnet.utils.training.batchfy.batchfy_by_frame(sorted_data, max_frames_in, max_frames_out, max_frames_inout, num_batches=0, min_batch_size=1, shortest_first=False, ikey='input', okey='output')\",\"Make variable batch set, which maximizes the number of frames to max_batch_frame.\",\"Parameters:\",\"sorteddata (Dict *[*str,Dict *[*str,Any]]) – dictionary loaded from data.json\",\"max_frames_in (int) – Maximum input frames of a batch\",\"max_frames_out (int) – Maximum output frames of a batch\",\"max_frames_inout (int) – Maximum input+output frames of a batch\",\"num_batches (int) – # number of batches to use (for debug)\",\"min_batch_size (int) – minimum batch size (for multi-gpu)\",\"test (int) – Return only every test batches\",\"shortest_first (bool) – Sort from batch with shortest samples to longest if true, otherwise reverse\",\"ikey (str) – key to access input (for ASR ikey=”input”, for TTS ikey=”output”.)\",\"okey (str) – key to access output (for ASR okey=”output”. for TTS okey=”input”.)\",\"Returns: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches\"]},\"1005\":{\"h\":\"espnet.utils.training.batchfy.batchfy_by_seq\",\"t\":[\"espnet.utils.training.batchfy.batchfy_by_seq(sorted_data, batch_size, max_length_in, max_length_out, min_batch_size=1, shortest_first=False, ikey='input', iaxis=0, okey='output', oaxis=0)\",\"Make batch set from json dictionary\",\"Parameters:\",\"sorted_data (Dict *[*str,Dict *[*str,Any]]) – dictionary loaded from data.json\",\"batch_size (int) – batch size\",\"max_length_in (int) – maximum length of input to decide adaptive batch size\",\"max_length_out (int) – maximum length of output to decide adaptive batch size\",\"min_batch_size (int) – mininum batch size (for multi-gpu)\",\"shortest_first (bool) – Sort from batch with shortest samples to longest if true, otherwise reverse\",\"ikey (str) – key to access input (for ASR ikey=”input”, for TTS, MT ikey=”output”.)\",\"iaxis (int) – dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=”1”.)\",\"okey (str) – key to access output (for ASR, MT okey=”output”. for TTS okey=”input”.)\",\"oaxis (int) – dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.)\",\"Returns: List[List[Tuple[str, dict]]] list of batches\"]},\"1006\":{\"h\":\"espnet.utils.training.batchfy.batchfy_shuffle\",\"t\":[\"espnet.utils.training.batchfy.batchfy_shuffle(data, batch_size, min_batch_size, num_batches, shortest_first)\"]},\"1007\":{\"h\":\"espnet.utils.training.train_utils.check_early_stop\",\"t\":[\"espnet.utils.training.train_utils.check_early_stop(trainer, epochs)\",\"Checks an early stopping trigger and warns the user if it’s the case\",\"Parameters:\",\"trainer – The trainer used for training\",\"epochs – The maximum number of epochs\"]},\"1008\":{\"h\":\"espnet.utils.check_kwargs.check_kwargs\",\"t\":[\"<!-- _espnet.utils.check_kwargs.check_kwargs -->\",\"espnet.utils.check_kwargs.check_kwargs(func, kwargs, name=None)\",\"check kwargs are valid for func\",\"If kwargs are invalid, raise TypeError as same as python default :param function func: function to be validated :param dict kwargs: keyword arguments for func :param str name: name used in TypeError (default is func name)\"]},\"1009\":{\"h\":\"espnet.utils.spec_augment.create_dense_flows\",\"t\":[\"espnet.utils.spec_augment.create_dense_flows(flattened_flows, batch_size, image_height, image_width)\"]},\"1010\":{\"h\":\"espnet.utils.spec_augment.cross_squared_distance_matrix\",\"t\":[\"espnet.utils.spec_augment.cross_squared_distance_matrix(x, y)\",\"Pairwise squared distance between two (batch) matrices’ rows (2nd dim).\",\"Computes the pairwise distances between rows of x and rows of y Args: x: [batch_size, n, d] float Tensor y: [batch_size, m, d] float Tensor Returns: squared_dists: [batch_size, n, m] float Tensor, where squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2\"]},\"1011\":{\"h\":\"espnet.utils.spec_augment.dense_image_warp\",\"t\":[\"<!-- _espnet.utils.spec_augment.dense_image_warp -->\",\"espnet.utils.spec_augment.dense_image_warp(image, flow)\",\"Image warping using per-pixel flow vectors.\",\"Apply a non-linear warp to the image, where the warp is specified by a dense flow field of offset vectors that define the correspondences of pixel values in the output image back to locations in the source image. Specifically, the pixel value at output[b, j, i, c] is images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c]. The locations specified by this formula do not necessarily map to an int index. Therefore, the pixel value is obtained by bilinear interpolation of the 4 nearest pixels around (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside of the image, we use the nearest pixel values at the image boundary. Args: image: 4-D float Tensor with shape [batch, height, width, channels]. flow: A 4-D float Tensor with shape [batch, height, width, 2]. name: A name for the operation (optional). Note that image and flow can be of type tf.half, tf.float32, or tf.float64, and do not necessarily have to be the same type. Returns: A 4-D float Tensor with shape`[batch, height, width, channels]` and same type as input image. Raises: ValueError: if height < 2 or width < 2 or the inputs have the wrong number of dimensions.\"]},\"1012\":{\"h\":\"espnet.utils.dynamic_import.dynamic_import\",\"t\":[\"<!-- _espnet.utils.dynamic_import.dynamic_import -->\",\"espnet.utils.dynamic_import.dynamic_import(import_path, alias={})\",\"dynamic import module and class\",\"Parameters:\",\"import_path (str) – syntax ‘module_name:class_name’ e.g., ‘espnet.transform.add_deltas:AddDeltas’\",\"alias (dict) – shortcut for registered class\",\"Returns: imported class\"]},\"1013\":{\"h\":\"espnet.utils.cli_readers.file_reader_helper\",\"t\":[\"espnet.utils.cli_readers.file_reader_helper(rspecifier: str, filetype: str = 'mat', return_shape: bool = False, segments: str | None = None)\",\"Read uttid and array in kaldi style\",\"This function might be a bit confusing as “ark” is used for HDF5 to imitate “kaldi-rspecifier”.\",\"Parameters:\",\"rspecifier – Give as “ark:feats.ark” or “scp:feats.scp”\",\"filetype – “mat” is kaldi-martix, “hdf5”: HDF5\",\"return_shape – Return the shape of the matrix, instead of the matrix. This can reduce IO cost for HDF5.\",\"Return type: Generator[Tuple[str, np.ndarray], None, None]\"]},\"1014\":{\"h\":\"Examples\",\"t\":[\"Read from kaldi-matrix ark file:\",\">>> for u, array in file_reader_helper('ark:feats.ark', 'mat'): ... array\",\"Read from HDF5 file:\",\">>> for u, array in file_reader_helper('ark:feats.h5', 'hdf5'): ... array\"]},\"1015\":{\"h\":\"espnet.utils.cli_writers.file_writer_helper\",\"t\":[\"espnet.utils.cli_writers.file_writer_helper(wspecifier: str, filetype: str = 'mat', write_num_frames: str | None = None, compress: bool = False, compression_method: int = 2, pcm_format: str = 'wav')\",\"Write matrices in kaldi style\",\"Parameters:\",\"wspecifier – e.g. ark,scp:out.ark,out.scp\",\"filetype – “mat” is kaldi-martix, “hdf5”: HDF5\",\"write_num_frames – e.g. ‘ark,t:num_frames.txt’\",\"compress – Compress or not\",\"compression_method – Specify compression level\",\"Write in kaldi-matrix-ark with “kaldi-scp” file:\",\">>> with file_writer_helper('ark,scp:out.ark,out.scp') as f: >>> f['uttid'] = array\",\"This “scp” has the following format:\",\"uttidA out.ark:1234 uttidB out.ark:2222\",\"where, 1234 and 2222 points the strating byte address of the matrix. (For detail, see official documentation of Kaldi)\",\"Write in HDF5 with “scp” file:\",\">>> with file_writer_helper('ark,scp:out.h5,out.scp', 'hdf5') as f: >>> f['uttid'] = array\",\"This “scp” file is created as:\",\"uttidA out.h5:uttidA uttidB out.h5:uttidB\",\"HDF5 can be, unlike “kaldi-ark”, accessed to any keys, so originally “scp” is not required for random-reading. Nevertheless we create “scp” for HDF5 because it is useful for some use-case. e.g. Concatenation, Splitting.\"]},\"1016\":{\"h\":\"espnet.utils.fill_missing_args.fill_missing_args\",\"t\":[\"espnet.utils.fill_missing_args.fill_missing_args(args, add_arguments)\",\"Fill missing arguments in args.\",\"Parameters:\",\"args (NamespaceorNone) – Namesapce containing hyperparameters.\",\"add_arguments (function) – Function to add arguments.\",\"Returns: Arguments whose missing ones are filled with default value.\",\"Return type: Namespace\"]},\"1017\":{\"h\":\"Examples\",\"t\":[\">>> from argparse import Namespace >>> from espnet.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2 >>> args = Namespace() >>> fill_missing_args(args, Tacotron2.add_arguments_fn) Namespace(aconv_chans=32, aconv_filts=15, adim=512, atype='location', ...)\"]},\"1018\":{\"h\":\"espnet.utils.spec_augment.flatten_grid_locations\",\"t\":[\"espnet.utils.spec_augment.flatten_grid_locations(grid_locations, image_height, image_width)\"]},\"1019\":{\"h\":\"espnet.utils.spec_augment.freq_mask\",\"t\":[\"<!-- _espnet.utils.spec_augment.freq_mask -->\",\"espnet.utils.spec_augment.freq_mask(spec, F=30, num_masks=1, replace_with_zero=False)\",\"Frequency masking\",\"Parameters:\",\"spec (torch.Tensor) – input tensor with shape (T, dim)\",\"F (int) – maximum width of each mask\",\"num_masks (int) – number of masks\",\"replace_with_zero (bool) – if True, masked parts will be filled with 0, if False, filled with mean\"]},\"1020\":{\"h\":\"espnet.utils.cli_utils.get_commandline_args\",\"t\":[\"espnet.utils.cli_utils.get_commandline_args()\"]},\"1021\":{\"h\":\"espnet.utils.spec_augment.get_flat_grid_locations\",\"t\":[\"espnet.utils.spec_augment.get_flat_grid_locations(image_height, image_width, device)\"]},\"1022\":{\"h\":\"espnet.utils.spec_augment.get_grid_locations\",\"t\":[\"espnet.utils.spec_augment.get_grid_locations(image_height, image_width, device)\"]},\"1023\":{\"h\":\"espnet.utils.cli_writers.get_num_frames_writer\",\"t\":[\"espnet.utils.cli_writers.get_num_frames_writer(write_num_frames: str)\"]},\"1024\":{\"h\":\"Examples\",\"t\":[\">>> get_num_frames_writer('ark,t:num_frames.txt')\"]},\"1025\":{\"h\":\"espnet.utils.spec_augment.interpolate_bilinear\",\"t\":[\"espnet.utils.spec_augment.interpolate_bilinear(grid, query_points, name='interpolate_bilinear', indexing='ij')\",\"Similar to Matlab’s interp2 function.\",\"Notes\",\"Finds values for query points on a grid using bilinear interpolation.\",\"Parameters:\",\"grid – a 4-D float Tensor of shape [batch, height, width, channels].\",\"query_points – a 3-D float Tensor of N points with shape [batch, N, 2].\",\"name – a name for the operation (optional).\",\"indexing – whether the query points are specified as row and column (ij), or Cartesian coordinates (xy).\",\"Returns: a 3-D Tensor with shape [batch, N, channels]\",\"Return type: values\",\"Raises:\",\"ValueError – if the indexing mode is invalid, or if the shape of the inputs\",\"invalid. –\"]},\"1026\":{\"h\":\"espnet.utils.spec_augment.interpolate_spline\",\"t\":[\"espnet.utils.spec_augment.interpolate_spline(train_points, train_values, query_points, order, regularization_weight=0.0)\"]},\"1027\":{\"h\":\"espnet.utils.cli_utils.is_scipy_wav_style\",\"t\":[\"<!-- _espnet.utils.cli_utils.is_scipy_wav_style -->\",\"espnet.utils.cli_utils.is_scipy_wav_style(value)\"]},\"1028\":{\"h\":\"espnet.utils.training.batchfy.make_batchset\",\"t\":[\"espnet.utils.training.batchfy.make_batchset(data, batch_size=0, max_length_in=inf, max_length_out=inf, num_batches=0, min_batch_size=1, shortest_first=False, batch_sort_key='input', swap_io=False, mt=False, count='auto', batch_bins=0, batch_frames_in=0, batch_frames_out=0, batch_frames_inout=0, iaxis=0, oaxis=0)\",\"Make batch set from json dictionary\",\"if utts have “category” value,\",\">>> data = {'utt1': {'category': 'A', 'input': ...}, ... 'utt2': {'category': 'B', 'input': ...}, ... 'utt3': {'category': 'B', 'input': ...}, ... 'utt4': {'category': 'A', 'input': ...}} >>> make_batchset(data, batchsize=2, ...) [[('utt1', ...), ('utt4', ...)], [('utt2', ...), ('utt3': ...)]]\",\"Note that if any utts doesn’t have “category”, perform as same as batchfy_by_\",\"Parameters:\",\"data (Dict *[*str,Dict *[*str,Any]]) – dictionary loaded from data.json\",\"batch_size (int) – maximum number of sequences in a minibatch.\",\"batch_bins (int) – maximum number of bins (frames x dim) in a minibatch.\",\"batch_frames_in (int) – maximum number of input frames in a minibatch.\",\"batch_frames_out (int) – maximum number of output frames in a minibatch.\",\"batch_frames_out – maximum number of input+output frames in a minibatch.\",\"count (str) – strategy to count maximum size of batch. For choices, see espnet.asr.batchfy.BATCH_COUNT_CHOICES\",\"max_length_in (int) – maximum length of input to decide adaptive batch size\",\"max_length_out (int) – maximum length of output to decide adaptive batch size\",\"num_batches (int) – # number of batches to use (for debug)\",\"min_batch_size (int) – minimum batch size (for multi-gpu)\",\"shortest_first (bool) – Sort from batch with shortest samples to longest if true, otherwise reverse\",\"batch_sort_key (str) – how to sort data before creating minibatches [“input”, “output”, “shuffle”]\",\"swap_io (bool) – if True, use “input” as output and “output” as input in data dict\",\"mt (bool) – if True, use 0-axis of “output” as output and 1-axis of “output” as input in data dict\",\"iaxis (int) – dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=”1”.)\",\"oaxis (int) – dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.)\",\"Returns: List[List[Tuple[str, dict]]] list of batches\"]},\"1029\":{\"h\":\"espnet.utils.cli_writers.parse_wspecifier\",\"t\":[\"<!-- _espnet.utils.cli_writers.parse_wspecifier -->\",\"espnet.utils.cli_writers.parse_wspecifier(wspecifier: str)\",\"Parse wspecifier to dict\"]},\"1030\":{\"h\":\"Examples\",\"t\":[\">>> parse_wspecifier('ark,scp:out.ark,out.scp') {'ark': 'out.ark', 'scp': 'out.scp'}\"]},\"1031\":{\"h\":\"espnet.utils.spec_augment.phi\",\"t\":[\"<!-- _espnet.utils.spec_augment.phi -->\",\"espnet.utils.spec_augment.phi(r, order)\",\"Coordinate-wise nonlinearity used to define the order of the interpolation.\",\"See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition. Args: r: input op order: interpolation order Returns: phi_k evaluated coordinate-wise on r, for k = r\"]},\"1032\":{\"h\":\"espnet.utils.deterministic_utils.set_deterministic_chainer\",\"t\":[\"espnet.utils.deterministic_utils.set_deterministic_chainer(args)\",\"Ensures chainer produces deterministic results depending on the program arguments\",\"Parameters:args (Namespace) – The program arguments\"]},\"1033\":{\"h\":\"espnet.utils.deterministic_utils.set_deterministic_pytorch\",\"t\":[\"espnet.utils.deterministic_utils.set_deterministic_pytorch(args)\",\"Ensures pytorch produces deterministic results depending on the program arguments\",\"Parameters:args (Namespace) – The program arguments\"]},\"1034\":{\"h\":\"espnet.utils.training.train_utils.set_early_stop\",\"t\":[\"espnet.utils.training.train_utils.set_early_stop(trainer, args, is_lm=False)\",\"Sets the early stop trigger given the program arguments\",\"Parameters:\",\"trainer – The trainer used for training\",\"args – The program arguments\",\"is_lm – If the trainer is for a LM (epoch instead of epochs)\"]},\"1035\":{\"h\":\"espnet.utils.spec_augment.solve_interpolation\",\"t\":[\"espnet.utils.spec_augment.solve_interpolation(train_points, train_values, order, regularization_weight)\"]},\"1036\":{\"h\":\"espnet.utils.spec_augment.sparse_image_warp\",\"t\":[\"espnet.utils.spec_augment.sparse_image_warp(img_tensor, source_control_point_locations, dest_control_point_locations, interpolation_order=2, regularization_weight=0.0, num_boundaries_points=0)\"]},\"1037\":{\"h\":\"espnet.utils.spec_augment.specaug\",\"t\":[\"<!-- _espnet.utils.spec_augment.specaug -->\",\"espnet.utils.spec_augment.specaug(spec, W=5, F=30, T=40, num_freq_masks=2, num_time_masks=2, replace_with_zero=False)\",\"SpecAugment\",\"Reference: : SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition (https://arxiv.org/pdf/1904.08779.pdf)\",\"This implementation modified from https://github.com/zcaceres/spec_augment\",\"Parameters:\",\"spec (torch.Tensor) – input tensor with the shape (T, dim)\",\"W (int) – time warp parameter\",\"F (int) – maximum width of each freq mask\",\"T (int) – maximum width of each time mask\",\"num_freq_masks (int) – number of frequency masks\",\"num_time_masks (int) – number of time masks\",\"replace_with_zero (bool) – if True, masked parts will be filled with 0, if False, filled with mean\"]},\"1038\":{\"h\":\"espnet.utils.cli_utils.strtobool\",\"t\":[\"<!-- _espnet.utils.cli_utils.strtobool -->\",\"espnet.utils.cli_utils.strtobool(x)\"]},\"1039\":{\"h\":\"espnet.utils.spec_augment.time_mask\",\"t\":[\"<!-- _espnet.utils.spec_augment.time_mask -->\",\"espnet.utils.spec_augment.time_mask(spec, T=40, num_masks=1, replace_with_zero=False)\",\"Time masking\",\"Parameters:\",\"spec (torch.Tensor) – input tensor with shape (T, dim)\",\"T (int) – maximum width of each mask\",\"num_masks (int) – number of masks\",\"replace_with_zero (bool) – if True, masked parts will be filled with 0, if False, filled with mean\"]},\"1040\":{\"h\":\"espnet.utils.spec_augment.time_warp\",\"t\":[\"<!-- _espnet.utils.spec_augment.time_warp -->\",\"espnet.utils.spec_augment.time_warp(spec, W=5)\",\"Time warping\",\"Parameters:\",\"spec (torch.Tensor) – input tensor with shape (T, dim)\",\"W (int) – time warp parameter\"]},\"1041\":{\"h\":\"espnet.vc.pytorch_backend.vc.CustomConverter\",\"t\":[\"class espnet.vc.pytorch_backend.vc.CustomConverter\",\"Bases: object\",\"Custom converter.\",\"Initilize module.\"]},\"1042\":{\"h\":\"espnet.vc.pytorch_backend.vc.CustomEvaluator\",\"t\":[\"class espnet.vc.pytorch_backend.vc.CustomEvaluator(model, iterator, target, device)\",\"Bases: BaseEvaluator\",\"Custom evaluator.\",\"Initilize module.\",\"Parameters:\",\"model (torch.nn.Module) – Pytorch model instance.\",\"iterator (chainer.dataset.Iterator) – Iterator for validation.\",\"target (chainer.Chain) – Dummy chain instance.\",\"device (torch.device) – The device to be used in evaluation.\",\"evaluate()\",\"Evaluate over validation iterator.\"]},\"1043\":{\"h\":\"espnet.vc.pytorch_backend.vc.CustomUpdater\",\"t\":[\"<!-- _espnet.vc.pytorch_backend.vc.CustomUpdater -->\",\"class espnet.vc.pytorch_backend.vc.CustomUpdater(model, grad_clip, iterator, optimizer, device, accum_grad=1)\",\"Bases: StandardUpdater\",\"Custom updater.\",\"Initilize module.\",\"Parameters:\",\"model (torch.nn.Module) – Pytorch model instance.\",\"grad_clip (float) – The gradient clipping value.\",\"iterator (chainer.dataset.Iterator) – Iterator for training.\",\"optimizer (torch.optim.Optimizer) – Pytorch optimizer instance.\",\"device (torch.device) – The device to be used in training.\",\"update()\",\"Run update function.\",\"update_core()\",\"Update model one step.\"]},\"1044\":{\"h\":\"espnet.vc.pytorch_backend.vc.decode\",\"t\":[\"<!-- _espnet.vc.pytorch_backend.vc.decode -->\",\"espnet.vc.pytorch_backend.vc.decode(args)\",\"Decode with E2E VC model.\"]},\"1045\":{\"h\":\"espnet.vc.pytorch_backend.vc.train\",\"t\":[\"<!-- _espnet.vc.pytorch_backend.vc.train -->\",\"espnet.vc.pytorch_backend.vc.train(args)\",\"Train E2E VC model.\"]},\"1046\":{\"h\":\"espnet2.asr_transducer.decoder.abs_decoder.AbsDecoder\",\"t\":[\"class espnet2.asr_transducer.decoder.abs_decoder.AbsDecoder\",\"Bases: Module, ABC\",\"Abstract decoder module.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract batch_score(hyps: List[Any])\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Decoder output sequences. states: Decoder hidden states.\",\"Return type: out\",\"abstract create_batch_states(new_states: List[List[Dict[str, Tensor | None]] | List[List[Tensor]] | Tuple[Tensor, Tensor | None]])\",\"Create batch of decoder hidden states given a list of new states.\",\"Parameters:new_states – Decoder hidden states.\",\"Returns: Decoder hidden states.\",\"abstract forward(labels: Tensor)\",\"Encode source label sequences.\",\"Parameters:labels – Label ID sequences.\",\"Returns: Decoder output sequences.\",\"abstract init_state(batch_size: int)\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Decoder hidden states.\",\"abstract score(label_sequence: List[int], states: List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, Tensor | None])\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"state – Decoder hidden states.\",\"Returns: Decoder output sequence. state: Decoder hidden states.\",\"Return type: out\",\"abstract select_state(states: List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, Tensor | None], idx: int = 0)\",\"Get specified ID state from batch of states, if provided.\",\"Parameters:\",\"states – Decoder hidden states.\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID.\",\"abstract set_device(device: Tensor)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"training : bool\"]},\"1047\":{\"h\":\"espnet2.asr_transducer.normalization.BasicNorm\",\"t\":[\"class espnet2.asr_transducer.normalization.BasicNorm(normalized_shape: int, eps: float = 0.25)\",\"Bases: Module\",\"BasicNorm module definition.\",\"Reference: https://github.com/k2-fsa/icefall/pull/288\",\"Parameters:\",\"normalized_shape – Expected size.\",\"eps – Value added to the denominator for numerical stability.\",\"Construct a BasicNorm object.\",\"forward(x: Tensor)\",\"Compute basic normalization.\",\"Parameters:x – Input sequences. (B, T, D_hidden)\",\"Returns: Output sequences. (B, T, D_hidden)\",\"training : bool\"]},\"1048\":{\"h\":\"espnet2.asr_transducer.beam_search_transducer.BeamSearchTransducer\",\"t\":[\"class espnet2.asr_transducer.beam_search_transducer.BeamSearchTransducer(decoder: AbsDecoder, joint_network: JointNetwork, beam_size: int, lm: Module | None = None, lm_weight: float = 0.1, search_type: str = 'default', max_sym_exp: int = 3, u_max: int = 50, nstep: int = 2, expansion_gamma: float = 2.3, expansion_beta: int = 2, score_norm: bool = False, nbest: int = 1, streaming: bool = False)\",\"Bases: object\",\"Beam search implementation for Transducer.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint network module.\",\"beam_size – Size of the beam.\",\"lm – LM module.\",\"lm_weight – LM weight for soft fusion.\",\"search_type – Search algorithm to use during inference.\",\"max_sym_exp – Number of maximum symbol expansions at each time step. (TSD)\",\"u_max – Maximum expected target sequence length. (ALSD)\",\"nstep – Number of maximum expansion steps at each time step. (mAES)\",\"expansion_gamma – Allowed logp difference for prune-by-value method. (mAES)\",\"expansion_beta – Number of additional candidates for expanded hypotheses selection. (mAES)\",\"score_norm – Normalize final scores by length.\",\"nbest – Number of final hypothesis.\",\"streaming – Whether to perform chunk-by-chunk beam search.\",\"Construct a BeamSearchTransducer object.\",\"align_length_sync_decoding(enc_out: Tensor)\",\"Alignment-length synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:h – Encoder output sequences. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"create_lm_batch_inputs(hyps_seq: List[List[int]])\",\"Make batch of inputs with left padding for LM scoring.\",\"Parameters:hyps_seq – Hypothesis sequences.\",\"Returns: Padded batch of sequences.\",\"default_beam_search(enc_out: Tensor)\",\"Beam search implementation without prefix search.\",\"Modified from https://arxiv.org/pdf/1211.3711.pdf\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"modified_adaptive_expansion_search(enc_out: Tensor)\",\"Modified version of Adaptive Expansion Search (mAES).\",\"Based on AES (https://ieeexplore.ieee.org/document/9250505) and : NSC (https://arxiv.org/abs/2201.05420).\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"recombine_hyps(hyps: List[Hypothesis])\",\"Recombine hypotheses with same label ID sequence.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Recombined hypotheses.\",\"Return type: final\",\"reset_cache()\",\"Reset cache for streaming decoding.\",\"select_k_expansions(hyps: List[ExtendedHypothesis], topk_idx: Tensor, topk_logp: Tensor)\",\"Return K hypotheses candidates for expansion from a list of hypothesis.\",\"K candidates are selected according to the extended hypotheses probabilities and a prune-by-value method. Where K is equal to beam_size + beta.\",\"Parameters:\",\"hyps – Hypotheses.\",\"topk_idx – Indices of candidates hypothesis.\",\"topk_logp – Log-probabilities of candidates hypothesis.\",\"Returns: Best K expansion hypotheses candidates.\",\"Return type: k_expansions\",\"sort_nbest(hyps: List[Hypothesis])\",\"Sort in-place hypotheses by score or score given sequence length.\",\"Parameters:hyps – Hypothesis.\",\"Returns: Sorted hypothesis.\",\"Return type: hyps\",\"time_sync_decoding(enc_out: Tensor)\",\"Time synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\"]},\"1049\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.branchformer.Branchformer\",\"t\":[\"class espnet2.asr_transducer.encoder.blocks.branchformer.Branchformer(block_size: int, linear_size: int, self_att: ~torch.nn.modules.module.Module, conv_mod: ~torch.nn.modules.module.Module, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0)\",\"Bases: Module\",\"Branchformer module definition.\",\"Reference: https://arxiv.org/pdf/2207.02971.pdf\",\"Parameters:\",\"block_size – Input/output size.\",\"linear_size – Linear layers’ hidden size.\",\"self_att – Self-attention module instance.\",\"conv_mod – Convolution module instance.\",\"norm_class – Normalization class.\",\"norm_args – Normalization module arguments.\",\"dropout_rate – Dropout rate.\",\"Construct a Branchformer object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0)\",\"Encode chunk of input sequence.\",\"Parameters:\",\"x – Branchformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T_2)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"Returns: Branchformer output sequences. (B, T, D_block) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None)\",\"Encode input sequences.\",\"Parameters:\",\"x – Branchformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: Branchformer output sequences. (B, T, D_block) mask: Source mask. (B, T) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device)\",\"Initialize/Reset self-attention and convolution modules cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"device – Device to use for cache tensor.\",\"training : bool\"]},\"1050\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.conformer.Conformer\",\"t\":[\"class espnet2.asr_transducer.encoder.blocks.conformer.Conformer(block_size: int, self_att: ~torch.nn.modules.module.Module, feed_forward: ~torch.nn.modules.module.Module, feed_forward_macaron: ~torch.nn.modules.module.Module, conv_mod: ~torch.nn.modules.module.Module, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0)\",\"Bases: Module\",\"Conformer module definition.\",\"Parameters:\",\"block_size – Input/output size.\",\"self_att – Self-attention module instance.\",\"feed_forward – Feed-forward module instance.\",\"feed_forward_macaron – Feed-forward module instance for macaron network.\",\"conv_mod – Convolution module instance.\",\"norm_class – Normalization module class.\",\"norm_args – Normalization module arguments.\",\"dropout_rate – Dropout rate.\",\"Construct a Conformer object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0)\",\"Encode chunk of input sequence.\",\"Parameters:\",\"x – Conformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T_2)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"Returns: Conformer output sequences. (B, T, D_block) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None)\",\"Encode input sequences.\",\"Parameters:\",\"x – Conformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: Conformer output sequences. (B, T, D_block) mask: Source mask. (B, T) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device)\",\"Initialize/Reset self-attention and convolution modules cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"device – Device to use for cache tensor.\",\"training : bool\"]},\"1051\":{\"h\":\"espnet2.asr_transducer.encoder.modules.convolution.ConformerConvolution\",\"t\":[\"class espnet2.asr_transducer.encoder.modules.convolution.ConformerConvolution(channels: int, kernel_size: int, activation: Module = ReLU(), norm_args: Dict = {}, causal: bool = False)\",\"Bases: Module\",\"ConformerConvolution module definition.\",\"Parameters:\",\"channels – The number of channels.\",\"kernel_size – Size of the convolving kernel.\",\"activation – Activation function.\",\"norm_args – Normalization module arguments.\",\"causal – Whether to use causal convolution (set to True if streaming).\",\"Construct an ConformerConvolution object.\",\"forward(x: Tensor, mask: Tensor | None = None, cache: Tensor | None = None)\",\"Compute convolution module.\",\"Parameters:\",\"x – ConformerConvolution input sequences. (B, T, D_hidden)\",\"mask – Source mask. (B, T_2)\",\"cache – ConformerConvolution input cache. (1, D_hidden, conv_kernel)\",\"Returns: ConformerConvolution output sequences. (B, ?, D_hidden) cache: ConformerConvolution output cache. (1, D_hidden, conv_kernel)\",\"Return type: x\",\"training : bool\"]},\"1052\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.conv1d.Conv1d\",\"t\":[\"class espnet2.asr_transducer.encoder.blocks.conv1d.Conv1d(input_size: int, output_size: int, kernel_size: int | Tuple, stride: int | Tuple = 1, dilation: int | Tuple = 1, groups: int | Tuple = 1, bias: bool = True, batch_norm: bool = False, relu: bool = True, causal: bool = False, dropout_rate: float = 0.0)\",\"Bases: Module\",\"Conv1d module definition.\",\"Parameters:\",\"input_size – Input dimension.\",\"output_size – Output dimension.\",\"kernel_size – Size of the convolving kernel.\",\"stride – Stride of the convolution.\",\"dilation – Spacing between the kernel points.\",\"groups – Number of blocked connections from input channels to output channels.\",\"bias – Whether to add a learnable bias to the output.\",\"batch_norm – Whether to use batch normalization after convolution.\",\"relu – Whether to use a ReLU activation after convolution.\",\"causal – Whether to use causal convolution (set to True if streaming).\",\"dropout_rate – Dropout rate.\",\"Construct a Conv1d object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0)\",\"Encode chunk of input sequence.\",\"Parameters:\",\"x – Conv1d input sequences. (B, T, D_in)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_in)\",\"mask – Source mask. (B, T)\",\"left_context – Number of previous frames the attention module can see in current chunk (not used here).\",\"Returns: Conv1d output sequences. (B, T, D_out) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_out)\",\"Return type: x\",\"create_new_mask(mask: Tensor)\",\"Create new mask for output sequences.\",\"Parameters:mask – Mask of input sequences. (B, T)\",\"Returns: Mask of output sequences. (B, sub(T))\",\"Return type: mask\",\"create_new_pos_enc(pos_enc: Tensor)\",\"Create new positional embedding vector.\",\"Parameters:pos_enc – Input sequences positional embedding. (B, 2 * (T - 1), D_in)\",\"Returns: Output sequences positional embedding. : (B, 2 * (sub(T) - 1), D_in)\",\"Return type: pos_enc\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor | None = None, chunk_mask: Tensor | None = None)\",\"Encode input sequences.\",\"Parameters:\",\"x – Conv1d input sequences. (B, T, D_in)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_in)\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: Conv1d output sequences. (B, sub(T), D_out) mask: Source mask. (B, T) or (B, sub(T)) pos_enc: Positional embedding sequences. \",\"(B, 2 * (T - 1), D_att) or (B, 2 * (sub(T) - 1), D_out)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device)\",\"Initialize/Reset Conv1d cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk (not used here).\",\"device – Device to use for cache tensor.\",\"training : bool\"]},\"1053\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.conv_input.ConvInput\",\"t\":[\"class espnet2.asr_transducer.encoder.blocks.conv_input.ConvInput(input_size: int, conv_size: int | Tuple, subsampling_factor: int = 4, vgg_like: bool = True, output_size: int | None = None)\",\"Bases: Module\",\"ConvInput module definition.\",\"Parameters:\",\"input_size – Input size.\",\"conv_size – Convolution size.\",\"subsampling_factor – Subsampling factor.\",\"vgg_like – Whether to use a VGG-like network.\",\"output_size – Block output dimension.\",\"Construct a ConvInput object.\",\"forward(x: Tensor, mask: Tensor | None = None)\",\"Encode input sequences.\",\"Parameters:\",\"x – ConvInput input sequences. (B, T, D_feats)\",\"mask – Mask of input sequences. (B, 1, T)\",\"Returns: ConvInput output sequences. (B, sub(T), D_out) mask: Mask of output sequences. (B, 1, sub(T))\",\"Return type: x\",\"training : bool\"]},\"1054\":{\"h\":\"espnet2.asr_transducer.encoder.modules.convolution.ConvolutionalSpatialGatingUnit\",\"t\":[\"class espnet2.asr_transducer.encoder.modules.convolution.ConvolutionalSpatialGatingUnit(size: int, kernel_size: int, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0, causal: bool = False)\",\"Bases: Module\",\"Convolutional Spatial Gating Unit module definition.\",\"Parameters:\",\"size – Initial size to determine the number of channels.\",\"kernel_size – Size of the convolving kernel.\",\"norm_class – Normalization module class.\",\"norm_args – Normalization module arguments.\",\"dropout_rate – Dropout rate.\",\"causal – Whether to use causal convolution (set to True if streaming).\",\"Construct a ConvolutionalSpatialGatingUnit object.\",\"forward(x: Tensor, mask: Tensor | None = None, cache: Tensor | None = None)\",\"Compute convolution module.\",\"Parameters:\",\"x – ConvolutionalSpatialGatingUnit input sequences. (B, T, D_hidden)\",\"mask – Source mask. (B, T_2)\",\"cache – ConvolutionalSpationGatingUnit input cache. (1, D_hidden, conv_kernel)\",\"Returns: ConvolutionalSpatialGatingUnit output sequences. (B, ?, D_hidden)\",\"Return type: x\",\"training : bool\"]},\"1055\":{\"h\":\"espnet2.asr_transducer.encoder.modules.convolution.DepthwiseConvolution\",\"t\":[\"class espnet2.asr_transducer.encoder.modules.convolution.DepthwiseConvolution(size: int, kernel_size: int, causal: bool = False)\",\"Bases: Module\",\"Depth-wise Convolution module definition.\",\"Parameters:\",\"size – Initial size to determine the number of channels.\",\"kernel_size – Size of the convolving kernel.\",\"causal – Whether to use causal convolution (set to True if streaming).\",\"Construct a DepthwiseConvolution object.\",\"forward(x: Tensor, mask: Tensor | None = None, cache: Tensor | None = None)\",\"Compute convolution module.\",\"Parameters:\",\"x – DepthwiseConvolution input sequences. (B, T, D_hidden)\",\"mask – Source mask. (B, T_2)\",\"cache – DepthwiseConvolution input cache. (1, conv_kernel, D_hidden)\",\"Returns: DepthwiseConvolution output sequences. (B, ?, D_hidden)\",\"Return type: x\",\"training : bool\"]},\"1056\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer\",\"t\":[\"class espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer(block_size: int, linear_size: int, self_att: ~torch.nn.modules.module.Module, feed_forward: ~torch.nn.modules.module.Module, feed_forward_macaron: ~torch.nn.modules.module.Module, conv_mod: ~torch.nn.modules.module.Module, depthwise_conv_mod: ~torch.nn.modules.module.Module, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0)\",\"Bases: Module\",\"E-Branchformer module definition.\",\"Reference: https://arxiv.org/pdf/2210.00077.pdf\",\"Parameters:\",\"block_size – Input/output size.\",\"linear_size – Linear layers’ hidden size.\",\"self_att – Self-attention module instance.\",\"feed_forward – Feed-forward module instance.\",\"feed_forward_macaron – Feed-forward module instance for macaron network.\",\"conv_mod – ConvolutionalSpatialGatingUnit module instance.\",\"depthwise_conv_mod – DepthwiseConvolution module instance.\",\"norm_class – Normalization class.\",\"norm_args – Normalization module arguments.\",\"dropout_rate – Dropout rate.\",\"Construct a E-Branchformer object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0)\",\"Encode chunk of input sequence.\",\"Parameters:\",\"x – E-Branchformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T_2)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"Returns: E-Branchformer output sequences. (B, T, D_block) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None)\",\"Encode input sequences.\",\"Parameters:\",\"x – E-Branchformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: E-Branchformer output sequences. (B, T, D_block) mask: Source mask. (B, T) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device)\",\"Initialize/Reset self-attention and convolution modules cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"device – Device to use for cache tensor.\",\"training : bool\"]},\"1057\":{\"h\":\"espnet2.asr_transducer.espnet_transducer_model.ESPnetASRTransducerModel\",\"t\":[\"class espnet2.asr_transducer.espnet_transducer_model.ESPnetASRTransducerModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, encoder: Encoder, decoder: AbsDecoder, joint_network: JointNetwork, transducer_weight: float = 1.0, use_k2_pruned_loss: bool = False, k2_pruned_loss_args: Dict = {}, warmup_steps: int = 25000, validation_nstep: int = 2, fastemit_lambda: float = 0.0, auxiliary_ctc_weight: float = 0.0, auxiliary_ctc_dropout_rate: float = 0.0, auxiliary_lm_loss_weight: float = 0.0, auxiliary_lm_loss_smoothing: float = 0.05, ignore_id: int = -1, sym_space: str = '<space>', sym_blank: str = '<blank>', report_cer: bool = False, report_wer: bool = False, extract_feats_in_collect_stats: bool = True)\",\"Bases: AbsESPnetModel\",\"ESPnet2ASRTransducerModel module definition.\",\"Parameters:\",\"vocab_size – Size of complete vocabulary (w/ SOS/EOS and blank included).\",\"token_list – List of tokens in vocabulary (minus reserved tokens).\",\"frontend – Frontend module.\",\"specaug – SpecAugment module.\",\"normalize – Normalization module.\",\"encoder – Encoder module.\",\"decoder – Decoder module.\",\"joint_network – Joint Network module.\",\"transducer_weight – Weight of the Transducer loss.\",\"use_k2_pruned_loss – Whether to use k2 pruned Transducer loss.\",\"k2_pruned_loss_args – Arguments of the k2 loss pruned Transducer loss.\",\"warmup_steps – Number of steps in warmup, used for pruned loss scaling.\",\"validation_nstep – Maximum number of symbol expansions at each time step when reporting CER or/and WER using mAES.\",\"fastemit_lambda – FastEmit lambda value.\",\"auxiliary_ctc_weight – Weight of auxiliary CTC loss.\",\"auxiliary_ctc_dropout_rate – Dropout rate for auxiliary CTC loss inputs.\",\"auxiliary_lm_loss_weight – Weight of auxiliary LM loss.\",\"auxiliary_lm_loss_smoothing – Smoothing rate for LM loss’ label smoothing.\",\"ignore_id – Initial padding ID.\",\"sym_space – Space symbol.\",\"sym_blank – Blank Symbol.\",\"report_cer – Whether to report Character Error Rate during validation.\",\"report_wer – Whether to report Word Error Rate during validation.\",\"extract_feats_in_collect_stats – Whether to use extract_feats stats collection.\",\"Construct an ESPnetASRTransducerModel object.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"Collect features sequences and features lengths sequences.\",\"Parameters:\",\"speech – Speech sequences. (B, S)\",\"speech_lengths – Speech sequences lengths. (B,)\",\"text – Label ID sequences. (B, L)\",\"text_lengths – Label ID sequences lengths. (B,)\",\"kwargs – Contains “utts_id”.\",\"Returns: “feats”: Features sequences. (B, T, D_feats), : ”feats_lengths”: Features sequences lengths. (B,)\",\"Return type: {}\",\"encode(speech: Tensor, speech_lengths: Tensor)\",\"Encoder speech sequences.\",\"Parameters:\",\"speech – Speech sequences. (B, S)\",\"speech_lengths – Speech sequences lengths. (B,)\",\"Returns: Encoder outputs. (B, T, D_enc) encoder_out_lens: Encoder outputs lengths. (B,)\",\"Return type: encoder_out\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"Forward architecture and compute loss(es).\",\"Parameters:\",\"speech – Speech sequences. (B, S)\",\"speech_lengths – Speech sequences lengths. (B,)\",\"text – Label ID sequences. (B, L)\",\"text_lengths – Label ID sequences lengths. (B,)\",\"kwargs – Contains “utts_id”.\",\"Returns: Main loss value. stats: Task statistics. weight: Task weights.\",\"Return type: loss\",\"training : bool\"]},\"1058\":{\"h\":\"espnet2.asr_transducer.encoder.encoder.Encoder\",\"t\":[\"class espnet2.asr_transducer.encoder.encoder.Encoder(input_size: int, body_conf: List[Dict[str, Any]], input_conf: Dict[str, Any] = {}, main_conf: Dict[str, Any] = {})\",\"Bases: Module\",\"Encoder module definition.\",\"Parameters:\",\"input_size – Input size.\",\"body_conf – Encoder body configuration.\",\"input_conf – Encoder input configuration.\",\"main_conf – Encoder main configuration.\",\"Construct an Encoder object.\",\"chunk_forward(x: Tensor, x_len: Tensor, processed_frames: tensor, left_context: int = 32)\",\"Encode input sequences as chunks.\",\"Parameters:\",\"x – Encoder input features. (1, T_in, F)\",\"x_len – Encoder input features lengths. (1,)\",\"processed_frames – Number of frames already seen.\",\"left_context – Number of previous frames (AFTER subsampling) the attention module can see in current chunk.\",\"Returns: Encoder outputs. (B, T_out, D_enc)\",\"Return type: x\",\"forward(x: Tensor, x_len: Tensor)\",\"Encode input sequences.\",\"Parameters:\",\"x – Encoder input features. (B, T_in, F)\",\"x_len – Encoder input features lengths. (B,)\",\"Returns: Encoder outputs. (B, T_out, D_enc) x_len: Encoder outputs lenghts. (B,)\",\"Return type: x\",\"reset_cache(left_context: int, device: device)\",\"Initialize/Reset encoder cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames (AFTER subsampling) the attention module can see in current chunk.\",\"device – Device ID.\",\"training : bool\"]},\"1059\":{\"h\":\"espnet2.asr_transducer.error_calculator.ErrorCalculator\",\"t\":[\"class espnet2.asr_transducer.error_calculator.ErrorCalculator(decoder: AbsDecoder, joint_network: JointNetwork, token_list: List[int], sym_space: str, sym_blank: str, nstep: int = 2, report_cer: bool = False, report_wer: bool = False)\",\"Bases: object\",\"Calculate CER and WER for transducer models.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint Network module.\",\"token_list – List of token units.\",\"sym_space – Space symbol.\",\"sym_blank – Blank symbol.\",\"nstep – Maximum number of symbol expansions at each time step w/ mAES.\",\"report_cer – Whether to compute CER.\",\"report_wer – Whether to compute WER.\",\"Construct an ErrorCalculatorTransducer object.\",\"calculate_cer(char_pred: Tensor, char_target: Tensor)\",\"Calculate sentence-level CER score.\",\"Parameters:\",\"char_pred – Prediction character sequences. (B, ?)\",\"char_target – Target character sequences. (B, ?)\",\"Returns: Average sentence-level CER score.\",\"calculate_wer(char_pred: Tensor, char_target: Tensor)\",\"Calculate sentence-level WER score.\",\"Parameters:\",\"char_pred – Prediction character sequences. (B, ?)\",\"char_target – Target character sequences. (B, ?)\",\"Returns: Average sentence-level WER score\",\"convert_to_char(pred: Tensor, target: Tensor)\",\"Convert label ID sequences to character sequences.\",\"Parameters:\",\"pred – Prediction label ID sequences. (B, U)\",\"target – Target label ID sequences. (B, L)\",\"Returns: Prediction character sequences. (B, ?) char_target: Target character sequences. (B, ?)\",\"Return type: char_pred\"]},\"1060\":{\"h\":\"espnet2.asr_transducer.beam_search_transducer.ExtendedHypothesis\",\"t\":[\"class espnet2.asr_transducer.beam_search_transducer.ExtendedHypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | None = None, lm_state: Dict[str, Any] | List[Any] | None = None, dec_out: Tensor | None = None, lm_score: Tensor | None = None)\",\"Bases: Hypothesis\",\"Extended hypothesis definition for NSC beam search and mAES.\",\":param : Hypothesis dataclass arguments. :param dec_out: Decoder output sequence. (B, D_dec) :param lm_score: Log-probabilities of the LM for given label. (vocab_size)\",\"dec_out : Tensor= None\",\"lm_score : Tensor= None\"]},\"1061\":{\"h\":\"espnet2.asr_transducer.activation.FTSwish\",\"t\":[\"<!-- _espnet2.asr_transducer.activation.FTSwish -->\",\"class espnet2.asr_transducer.activation.FTSwish(threshold: float = -0.2, mean_shift: float = 0)\",\"Bases: Module\",\"Flatten-T Swish activation definition.\",\"FTSwish(x) = x * sigmoid(x) + threshold : where FTSwish(x) < 0 = threshold\",\"Reference: https://arxiv.org/abs/1812.06247\",\"Parameters:\",\"threshold – Threshold value for FTSwish activation formulation. (threshold < 0)\",\"mean_shift – Mean shifting value for FTSwish activation formulation. (applied only if != 0, disabled by default)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Forward computation.\",\"training : bool\"]},\"1062\":{\"h\":\"espnet2.asr_transducer.decoder.modules.rwkv.feed_forward.FeedForward\",\"t\":[\"class espnet2.asr_transducer.decoder.modules.rwkv.feed_forward.FeedForward(size: int, hidden_size: int, block_id: int, num_blocks: int)\",\"Bases: Module\",\"FeedForward module definition.\",\"Parameters:\",\"size – Input/Output size.\",\"hidden_size – Hidden size.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"Construct a FeedForward object.\",\"forward(x: Tensor, state: List[Tensor] | None = None)\",\"Compute channel mixing.\",\"Parameters:\",\"x – FeedForward input sequences. (B, U, size)\",\"state – Decoder hidden state. [5 x (B, 1, size, N)]\",\"Returns: FeedForward output sequences. (B, U, size) state: Decoder hidden state. [5 x (B, 1, size, N)]\",\"Return type: x\",\"reset_parameters(size: int, block_id: int, num_blocks: int)\",\"Reset module parameters.\",\"Parameters:\",\"size – Block size.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"training : bool\"]},\"1063\":{\"h\":\"espnet2.asr_transducer.beam_search_transducer.Hypothesis\",\"t\":[\"class espnet2.asr_transducer.beam_search_transducer.Hypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | None = None, lm_state: Dict[str, Any] | List[Any] | None = None)\",\"Bases: object\",\"Default hypothesis definition for Transducer search algorithms.\",\"Parameters:\",\"score – Total log-probability.\",\"yseq – Label sequence as integer ID sequence.\",\"dec_state – RNN/MEGA Decoder state (None if Stateless).\",\"lm_state – RNNLM state. ((N, D_lm), (N, D_lm)) or None\",\"dec_state : Tuple[Tensor, Tensor | None] | None= None\",\"lm_state : Dict[str, Any] | List[Any] | None= None\",\"score : float\",\"yseq : List[int]\"]},\"1064\":{\"h\":\"espnet2.asr_transducer.joint_network.JointNetwork\",\"t\":[\"class espnet2.asr_transducer.joint_network.JointNetwork(output_size: int, encoder_size: int, decoder_size: int, joint_space_size: int = 256, joint_activation_type: str = 'tanh', lin_dec_bias: bool = True, **activation_parameters)\",\"Bases: Module\",\"Transducer joint network module.\",\"Parameters:\",\"output_size – Output size.\",\"encoder_size – Encoder output size.\",\"decoder_size – Decoder output size.\",\"joint_space_size – Joint space size.\",\"joint_act_type – Type of activation for joint network.\",\"**activation_parameters – Parameters for the activation function.\",\"Construct a JointNetwork object.\",\"forward(enc_out: Tensor, dec_out: Tensor, no_projection: bool = False)\",\"Joint computation of encoder and decoder hidden state sequences.\",\"Parameters:\",\"enc_out – Expanded encoder output state sequences. (B, T, s_range, D_enc) or (B, T, 1, D_enc)\",\"dec_out – Expanded decoder output state sequences. (B, T, s_range, D_dec) or (B, 1, U, D_dec)\",\"Returns: Joint output state sequences. : (B, T, U, D_out) or (B, T, s_range, D_out)\",\"Return type: joint_out\",\"training : bool\"]},\"1065\":{\"h\":\"espnet2.asr_transducer.decoder.blocks.mega.MEGA\",\"t\":[\"class espnet2.asr_transducer.decoder.blocks.mega.MEGA(size: int = 512, num_heads: int = 4, qk_size: int = 128, v_size: int = 1024, activation: ~torch.nn.modules.module.Module = ReLU(), normalization: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, rel_pos_bias_type: str = 'simple', max_positions: int = 2048, truncation_length: int | None = None, chunk_size: int = -1, dropout_rate: float = 0.0, att_dropout_rate: float = 0.0, ema_dropout_rate: float = 0.0)\",\"Bases: Module\",\"MEGA module.\",\"Parameters:\",\"size – Input/Output size.\",\"num_heads – Number of EMA heads.\",\"qk_size – Shared query and key size for attention module.\",\"v_size – Value size for attention module.\",\"qk_v_size – (QK, V) sizes for attention module.\",\"activation – Activation function type.\",\"normalization – Normalization module.\",\"rel_pos_bias_type – Type of relative position bias in attention module.\",\"max_positions – Maximum number of position for RelativePositionBias.\",\"truncation_length – Maximum length for truncation in EMA module.\",\"chunk_size – Chunk size for attention computation (-1 = full context).\",\"dropout_rate – Dropout rate for inner modules.\",\"att_dropout_rate – Dropout rate for the attention module.\",\"ema_dropout_rate – Dropout rate for the EMA module.\",\"Construct a MEGA object.\",\"forward(x: Tensor, mask: Tensor | None = None, attn_mask: Tensor | None = None, state: Dict[str, Tensor | None] | None = None)\",\"Compute moving average equiped gated attention.\",\"Parameters:\",\"x – MEGA input sequences. (L, B, size)\",\"mask – MEGA input sequence masks. (B, 1, L)\",\"attn_mask – MEGA attention mask. (1, L, L)\",\"state – Decoder hidden states.\",\"Returns: MEGA output sequences. (B, L, size) state: Decoder hidden states.\",\"Return type: x\",\"reset_parameters(val: int = 0.0, std: int = 0.02)\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std – Standard deviation.\",\"softmax_attention(query: Tensor, key: Tensor, mask: Tensor | None = None, attn_mask: Tensor | None = None)\",\"Compute attention weights with softmax.\",\"Parameters:\",\"query – Query tensor. (B, 1, L, D)\",\"key – Key tensor. (B, 1, L, D)\",\"mask – Sequence mask. (B, 1, L)\",\"attn_mask – Attention mask. (1, L, L)\",\"Returns: Attention weights. (B, 1, L, L)\",\"Return type: attn_weights\",\"training : bool\"]},\"1066\":{\"h\":\"espnet2.asr_transducer.decoder.mega_decoder.MEGADecoder\",\"t\":[\"class espnet2.asr_transducer.decoder.mega_decoder.MEGADecoder(vocab_size: int, block_size: int = 512, linear_size: int = 1024, qk_size: int = 128, v_size: int = 1024, num_heads: int = 4, rel_pos_bias_type: str = 'simple', max_positions: int = 2048, truncation_length: int | None = None, normalization_type: str = 'layer_norm', normalization_args: Dict = {}, activation_type: str = 'swish', activation_args: Dict = {}, chunk_size: int = -1, num_blocks: int = 4, dropout_rate: float = 0.0, embed_dropout_rate: float = 0.0, att_dropout_rate: float = 0.0, ema_dropout_rate: float = 0.0, ffn_dropout_rate: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"MEGA decoder module.\",\"Based on https://arxiv.org/pdf/2209.10655.pdf.\",\"Parameters:\",\"vocab_size – Vocabulary size.\",\"block_size – Input/Output size.\",\"linear_size – NormalizedPositionwiseFeedForward hidden size.\",\"qk_size – Shared query and key size for attention module.\",\"v_size – Value size for attention module.\",\"num_heads – Number of EMA heads.\",\"rel_pos_bias – Type of relative position bias in attention module.\",\"max_positions – Maximum number of position for RelativePositionBias.\",\"truncation_length – Maximum length for truncation in EMA module.\",\"normalization_type – Normalization layer type.\",\"normalization_args – Normalization layer arguments.\",\"activation_type – Activation function type.\",\"activation_args – Activation function arguments.\",\"chunk_size – Chunk size for attention computation (-1 = full context).\",\"num_blocks – Number of MEGA blocks.\",\"dropout_rate – Dropout rate for MEGA internal modules.\",\"embed_dropout_rate – Dropout rate for embedding layer.\",\"att_dropout_rate – Dropout rate for the attention module.\",\"ema_dropout_rate – Dropout rate for the EMA module.\",\"ffn_dropout_rate – Dropout rate for the feed-forward module.\",\"embed_pad – Embedding padding symbol ID.\",\"Construct a MEGADecoder object.\",\"batch_score(hyps: List[Hypothesis])\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: states:\",\"Return type: out\",\"create_batch_states(new_states: List[List[Dict[str, Tensor]]])\",\"Create batch of decoder hidden states given a list of new states.\",\"Parameters:new_states – Decoder hidden states. [B x [N x Dict]]\",\"Returns: Decoder hidden states. [N x Dict]\",\"forward(labels: Tensor)\",\"Encode source label sequences.\",\"Parameters:labels – Decoder input sequences. (B, L)\",\"Returns: Decoder output sequences. (B, U, D_dec)\",\"Return type: out\",\"inference(labels: Tensor, states: List[Dict[str, Tensor]])\",\"Encode source label sequences.\",\"Parameters:\",\"labels – Decoder input sequences. (B, L)\",\"states – Decoder hidden states. [B x Dict]\",\"Returns: Decoder output sequences. (B, U, D_dec) new_states: Decoder hidden states. [B x Dict]\",\"Return type: out\",\"init_state(batch_size: int = 0)\",\"Initialize MEGADecoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Decoder hidden states. [N x Dict]\",\"Return type: states\",\"score(label_sequence: List[int], states: List[Dict[str, Tensor]])\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"states – Decoder hidden states. (??)\",\"Returns: Decoder output sequence. (D_dec) states: Decoder hidden states. (??)\",\"select_state(states: List[Dict[str, Tensor]], idx: int)\",\"Select ID state from batch of decoder hidden states.\",\"Parameters:states – Decoder hidden states. [N x Dict]\",\"Returns: Decoder hidden states for given ID. [N x Dict]\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"stack_qk_states(state_list: List[Tensor], dim: int)\",\"Stack query or key states with different lengths.\",\"Parameters:state_list – List of query or key states.\",\"Returns: Query/Key state.\",\"Return type: new_state\",\"training : bool\"]},\"1067\":{\"h\":\"espnet2.asr_transducer.activation.Mish\",\"t\":[\"<!-- _espnet2.asr_transducer.activation.Mish -->\",\"class espnet2.asr_transducer.activation.Mish(softplus_beta: float = 1.0, softplus_threshold: int = 20, use_builtin: bool = False)\",\"Bases: Module\",\"Mish activation definition.\",\"Mish(x) = x * tanh(softplus(x))\",\"Reference: https://arxiv.org/abs/1908.08681.\",\"Parameters:\",\"softplus_beta – Beta value for softplus activation formulation. (Usually 0 > softplus_beta >= 2)\",\"softplus_threshold – Values above this revert to a linear function. (Usually 10 > softplus_threshold >= 20)\",\"use_builtin – Whether to use PyTorch activation function if available.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Forward computation.\",\"training : bool\"]},\"1068\":{\"h\":\"espnet2.asr_transducer.encoder.modules.multi_blocks.MultiBlocks\",\"t\":[\"class espnet2.asr_transducer.encoder.modules.multi_blocks.MultiBlocks(block_list: ~typing.List[~torch.nn.modules.module.Module], output_size: int, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict | None = None, blockdrop_rate: int = 0.0)\",\"Bases: Module\",\"MultiBlocks definition.\",\"Parameters:\",\"block_list – Individual blocks of the encoder architecture.\",\"output_size – Architecture output size.\",\"norm_class – Normalization module class.\",\"norm_args – Normalization module arguments.\",\"blockdrop_rate – Probability threshold of dropping out each block.\",\"Construct a MultiBlocks object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0)\",\"Forward each block of the encoder architecture.\",\"Parameters:\",\"x – MultiBlocks input sequences. (B, T, D_block_1)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_att)\",\"mask – Source mask. (B, T_2)\",\"left_context – Number of previous frames the attention module can see in current chunk (used by Conformer and Branchformer block).\",\"Returns: MultiBlocks output sequences. (B, T, D_block_N)\",\"Return type: x\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None)\",\"Forward each block of the encoder architecture.\",\"Parameters:\",\"x – MultiBlocks input sequences. (B, T, D_block_1)\",\"pos_enc – Positional embedding sequences.\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: Output sequences. (B, T, D_block_N)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device)\",\"Initialize/Reset encoder streaming cache.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk (used by Conformer and Branchformer block).\",\"device – Device to use for cache tensor.\",\"training : bool\"]},\"1069\":{\"h\":\"espnet2.asr_transducer.decoder.modules.mega.multi_head_damped_ema.MultiHeadDampedEMA\",\"t\":[\"class espnet2.asr_transducer.decoder.modules.mega.multi_head_damped_ema.MultiHeadDampedEMA(size: int, num_heads: int = 4, activation: Module = ReLU(), truncation_length: int | None = None)\",\"Bases: Module\",\"MultiHeadDampedEMA module definition.\",\"Parameters:\",\"size – Module size.\",\"num_heads – Number of attention heads.\",\"activation – Activation function type.\",\"truncation_length – Maximum length for truncation.\",\"Construct an MultiHeadDampedEMA object.\",\"compute_ema_coefficients()\",\"Compute EMA coefficients.\",\"Parameters:None –\",\"Returns: Damping factor / P-th order coefficient. : (size, num_heads, 1)\",\"prev_timestep_weight: Previous timestep weight / Q-th order coefficient. : (size, num_heads, 1)\",\"Return type: damping_factor\",\"compute_ema_kernel(length: int)\",\"Compute EMA kernel / vandermonde product.\",\"Parameters:length – Sequence length.\",\"Returns: EMA kernel / Vandermonde product. (size, L)\",\"ema_one_step(x: Tensor, state: Tensor | None = None)\",\"Perform exponential moving average for a single step.\",\"Parameters:\",\"x – MultiHeadDampedEMA input sequences. (B, D, 1)\",\"state – MultiHeadDampedEMA state. (B, D, num_heads)\",\"Returns: MultiHeadDamped output sequences. (B, 1, D) new_state: MultiHeadDampedEMA state. (B, D, num_heads)\",\"Return type: out\",\"forward(x: Tensor, mask: Tensor | None = None, state: Dict[str, Tensor] | None = None)\",\"Compute multi-dimensional damped EMA.\",\"Parameters:\",\"x – MultiHeadDampedEMA input sequence. (L, B, D)\",\"mask – Sequence mask. (B, 1, L)\",\"state – MultiHeadDampedEMA state. (B, D, num_heads)\",\"Returns: MultiHeadDampedEMA output sequence. (B, L, D) new_state: MultiHeadDampedEMA state. (B, D, num_heads)\",\"Return type: x\",\"get_ema_coefficients()\",\"Get EMA coefficients.\",\"Parameters:None –\",\"Returns: Damping factor / P-th order coefficient. (size, num_heads, 1) : Previous timestep weight / Q-th order coefficient. (size, num_heads, 1)\",\"reset_parameters(val: float = 0.0, std1: float = 0.2, std2: float = 1.0)\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std1 – Main standard deviation.\",\"std2 – Secondary standard deviation.\",\"training : bool\"]},\"1070\":{\"h\":\"espnet2.asr_transducer.decoder.modules.mega.feed_forward.NormalizedPositionwiseFeedForward\",\"t\":[\"class espnet2.asr_transducer.decoder.modules.mega.feed_forward.NormalizedPositionwiseFeedForward(size: int, hidden_size: int, normalization: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, activation: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.activation.ReLU'>, dropout_rate: float = 0.0)\",\"Bases: Module\",\"NormalizedPositionFeedForward module definition.\",\"Parameters:\",\"size – Input/Output size.\",\"hidden_size – Hidden size.\",\"normalization – Normalization module.\",\"activation – Activation function.\",\"dropout_rate – Dropout rate.\",\"Construct an NormalizedPositionwiseFeedForward object.\",\"forward(x: Tensor)\",\"Compute feed-forward module.\",\"Parameters:x – NormalizedPositionwiseFeedForward input sequences. (B, L, size)\",\"Returns: NormalizedPositionwiseFeedForward output sequences. (B, L, size)\",\"Return type: x\",\"reset_parameters(val: float = 0.0, std: float = 0.02)\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std – Standard deviation.\",\"training : bool\"]},\"1071\":{\"h\":\"espnet2.asr_transducer.frontend.online_audio_processor.OnlineAudioProcessor\",\"t\":[\"class espnet2.asr_transducer.frontend.online_audio_processor.OnlineAudioProcessor(feature_extractor: Module, normalization_module: Module, decoding_window: int, encoder_sub_factor: int, frontend_conf: Dict, device: device, audio_sampling_rate: int = 16000)\",\"Bases: object\",\"OnlineProcessor module definition.\",\"Parameters:\",\"feature_extractor – Feature extractor module.\",\"normalization_module – Normalization module.\",\"decoding_window – Size of the decoding window (in ms).\",\"encoder_sub_factor – Encoder subsampling factor.\",\"frontend_conf – Frontend configuration.\",\"device – Device to pin module tensors on.\",\"audio_sampling_rate – Input sampling rate.\",\"Construct an OnlineAudioProcessor.\",\"compute_features(samples: Tensor, is_final: bool)\",\"Compute features from input samples.\",\"Parameters:\",\"samples – Speech data. (S)\",\"is_final – Whether speech corresponds to the final chunk of data.\",\"Returns: Features sequence. (1, chunk_sz_bs, D_feats) feats_length: Features length sequence. (1,)\",\"Return type: feats\",\"get_current_feats(feats: Tensor, feats_length: Tensor, is_final: bool)\",\"Get features for current decoding window.\",\"Parameters:\",\"feats – Computed features sequence. (1, F, D_feats)\",\"feats_length – Computed features sequence length. (1,)\",\"is_final – Whether feats corresponds to the final chunk of data.\",\"Returns: Decoding window features sequence. (1, chunk_sz_bs, D_feats) feats_length: Decoding window features length sequence. (1,)\",\"Return type: feats\",\"get_current_samples(samples: Tensor, is_final: bool)\",\"Get samples for feature computation.\",\"Parameters:\",\"samples – Speech data. (S)\",\"is_final – Whether speech corresponds to the final chunk of data.\",\"Returns: New speech data. (1, decoding_samples)\",\"Return type: samples\",\"reset_cache()\",\"Reset cache parameters.\",\"Parameters:None –\",\"Returns: None\"]},\"1072\":{\"h\":\"espnet2.asr_transducer.normalization.RMSNorm\",\"t\":[\"class espnet2.asr_transducer.normalization.RMSNorm(normalized_shape: int, eps: float = 1e-05, partial: float = 0.0)\",\"Bases: Module\",\"RMSNorm module definition.\",\"Reference: https://arxiv.org/pdf/1910.07467.pdf\",\"Parameters:\",\"normalized_shape – Expected size.\",\"eps – Value added to the denominator for numerical stability.\",\"partial – Value defining the part of the input used for RMS stats.\",\"Construct a RMSNorm object.\",\"forward(x: Tensor)\",\"Compute RMS normalization.\",\"Parameters:x – Input sequences. (B, T, D_hidden)\",\"Returns: Output sequences. (B, T, D_hidden)\",\"Return type: x\",\"training : bool\"]},\"1073\":{\"h\":\"espnet2.asr_transducer.decoder.rnn_decoder.RNNDecoder\",\"t\":[\"class espnet2.asr_transducer.decoder.rnn_decoder.RNNDecoder(vocab_size: int, embed_size: int = 256, hidden_size: int = 256, rnn_type: str = 'lstm', num_layers: int = 1, dropout_rate: float = 0.0, embed_dropout_rate: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"RNN decoder module.\",\"Parameters:\",\"vocab_size – Vocabulary size.\",\"embed_size – Embedding size.\",\"hidden_size – Hidden size..\",\"rnn_type – Decoder layers type.\",\"num_layers – Number of decoder layers.\",\"dropout_rate – Dropout rate for decoder layers.\",\"embed_dropout_rate – Dropout rate for embedding layer.\",\"embed_pad – Embedding padding symbol ID.\",\"Construct a RNNDecoder object.\",\"batch_score(hyps: List[Hypothesis])\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Decoder output sequences. (B, D_dec) states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"Return type: out\",\"create_batch_states(new_states: List[Tuple[Tensor, Tensor | None]])\",\"Create decoder hidden states.\",\"Parameters:new_states – Decoder hidden states. [B x ((N, 1, D_dec), (N, 1, D_dec) or None)]\",\"Returns: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"Return type: states\",\"forward(labels: Tensor)\",\"Encode source label sequences.\",\"Parameters:labels – Label ID sequences. (B, L)\",\"Returns: Decoder output sequences. (B, U, D_dec)\",\"Return type: out\",\"init_state(batch_size: int)\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"rnn_forward(x: Tensor, state: Tuple[Tensor, Tensor | None])\",\"Encode source label sequences.\",\"Parameters:\",\"x – RNN input sequences. (B, D_emb)\",\"state – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"Returns: RNN output sequences. (B, D_dec) (h_next, c_next): Decoder hidden states. \",\"(N, B, D_dec), (N, B, D_dec) or None)\",\"Return type: x\",\"score(label_sequence: List[int], states: Tuple[Tensor, Tensor | None])\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"states – Decoder hidden states. ((N, 1, D_dec), (N, 1, D_dec) or None)\",\"Returns: Decoder output sequence. (1, D_dec) states: Decoder hidden states. \",\"((N, 1, D_dec), (N, 1, D_dec) or None)\",\"Return type: out\",\"select_state(states: Tuple[Tensor, Tensor | None], idx: int)\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. ((N, 1, D_dec), (N, 1, D_dec) or None)\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"training : bool\"]},\"1074\":{\"h\":\"espnet2.asr_transducer.decoder.blocks.rwkv.RWKV\",\"t\":[\"class espnet2.asr_transducer.decoder.blocks.rwkv.RWKV(size: int, linear_size: int, attention_size: int, context_size: int, block_id: int, num_blocks: int, normalization_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, normalization_args: ~typing.Dict = {}, att_dropout_rate: float = 0.0, ffn_dropout_rate: float = 0.0)\",\"Bases: Module\",\"RWKV module.\",\"Parameters:\",\"size – Input/Output size.\",\"linear_size – Feed-forward hidden size.\",\"attention_size – SelfAttention hidden size.\",\"context_size – Context size for WKV computation.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"normalization_class – Normalization layer class.\",\"normalization_args – Normalization layer arguments.\",\"att_dropout_rate – Dropout rate for the attention module.\",\"ffn_dropout_rate – Dropout rate for the feed-forward module.\",\"Construct a RWKV object.\",\"forward(x: Tensor, state: Tensor | None = None)\",\"Compute receptance weighted key value.\",\"Parameters:\",\"x – RWKV input sequences. (B, L, size)\",\"state – Decoder hidden states. [5 x (B, D_att/size, N)]\",\"Returns: RWKV output sequences. (B, L, size) x: Decoder hidden states. [5 x (B, D_att/size, N)]\",\"Return type: x\",\"training : bool\"]},\"1075\":{\"h\":\"espnet2.asr_transducer.decoder.rwkv_decoder.RWKVDecoder\",\"t\":[\"class espnet2.asr_transducer.decoder.rwkv_decoder.RWKVDecoder(vocab_size: int, block_size: int = 512, context_size: int = 1024, linear_size: int | None = None, attention_size: int | None = None, normalization_type: str = 'layer_norm', normalization_args: Dict = {}, num_blocks: int = 4, rescale_every: int = 0, embed_dropout_rate: float = 0.0, att_dropout_rate: float = 0.0, ffn_dropout_rate: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"RWKV decoder module.\",\"Based on https://arxiv.org/pdf/2305.13048.pdf.\",\"Parameters:\",\"vocab_size – Vocabulary size.\",\"block_size – Input/Output size.\",\"context_size – Context size for WKV computation.\",\"linear_size – FeedForward hidden size.\",\"attention_size – SelfAttention hidden size.\",\"normalization_type – Normalization layer type.\",\"normalization_args – Normalization layer arguments.\",\"num_blocks – Number of RWKV blocks.\",\"rescale_every – Whether to rescale input every N blocks (inference only).\",\"embed_dropout_rate – Dropout rate for embedding layer.\",\"att_dropout_rate – Dropout rate for the attention module.\",\"ffn_dropout_rate – Dropout rate for the feed-forward module.\",\"embed_pad – Embedding padding symbol ID.\",\"Construct a RWKVDecoder object.\",\"batch_score(hyps: List[Hypothesis])\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Decoder output sequence. (B, D_dec) states: Decoder hidden states. [5 x (B, 1, D_att/D_dec, N)]\",\"Return type: out\",\"create_batch_states(new_states: List[List[Dict[str, Tensor]]])\",\"Create batch of decoder hidden states given a list of new states.\",\"Parameters:new_states – Decoder hidden states. [B x [5 x (1, 1, D_att/D_dec, N)]\",\"Returns: Decoder hidden states. [5 x (B, 1, D_att/D_dec, N)]\",\"forward(labels: Tensor)\",\"Encode source label sequences.\",\"Parameters:labels – Decoder input sequences. (B, L)\",\"Returns: Decoder output sequences. (B, U, D_dec)\",\"Return type: out\",\"inference(labels: Tensor, states: Tensor)\",\"Encode source label sequences.\",\"Parameters:\",\"labels – Decoder input sequences. (B, L)\",\"states – Decoder hidden states. [5 x (B, D_att/D_dec, N)]\",\"Returns: Decoder output sequences. (B, U, D_dec) states: Decoder hidden states. [5 x (B, D_att/D_dec, N)]\",\"Return type: out\",\"init_state(batch_size: int = 1)\",\"Initialize RWKVDecoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Decoder hidden states. [5 x (B, 1, D_att/D_dec, N)]\",\"Return type: states\",\"score(label_sequence: List[int], states: List[Tensor])\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"states – Decoder hidden states. [5 x (1, 1, D_att/D_dec, N)]\",\"Returns: Decoder output sequence. (D_dec) states: Decoder hidden states. [5 x (1, 1, D_att/D_dec, N)]\",\"select_state(states: List[Tensor], idx: int)\",\"Select ID state from batch of decoder hidden states.\",\"Parameters:states – Decoder hidden states. [5 x (B, 1, D_att/D_dec, N)]\",\"Returns: Decoder hidden states for given ID. [5 x (1, 1, D_att/D_dec, N)]\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"training : bool\"]},\"1076\":{\"h\":\"espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention\",\"t\":[\"class espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention(num_heads: int, embed_size: int, dropout_rate: float = 0.0, simplified_attention_score: bool = False)\",\"Bases: Module\",\"RelPositionMultiHeadedAttention definition.\",\"Parameters:\",\"num_heads – Number of attention heads.\",\"embed_size – Embedding size.\",\"dropout_rate – Dropout rate.\",\"Construct an MultiHeadedAttention object.\",\"compute_attention_score(query: Tensor, key: Tensor, pos_enc: Tensor, left_context: int = 0)\",\"Attention score computation.\",\"Parameters:\",\"query – Transformed query tensor. (B, H, T_1, d_k)\",\"key – Transformed key tensor. (B, H, T_2, d_k)\",\"pos_enc – Positional embedding tensor. (B, 2 * T_1 - 1, size)\",\"left_context – Number of previous frames to use for current chunk attention computation.\",\"Returns: Attention score. (B, H, T_1, T_2)\",\"compute_simplified_attention_score(query: Tensor, key: Tensor, pos_enc: Tensor, left_context: int = 0)\",\"Simplified attention score computation.\",\"Reference: https://github.com/k2-fsa/icefall/pull/458\",\"Parameters:\",\"query – Transformed query tensor. (B, H, T_1, d_k)\",\"key – Transformed key tensor. (B, H, T_2, d_k)\",\"pos_enc – Positional embedding tensor. (B, 2 * T_1 - 1, size)\",\"left_context – Number of previous frames to use for current chunk attention computation.\",\"Returns: Attention score. (B, H, T_1, T_2)\",\"forward(query: Tensor, key: Tensor, value: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None, left_context: int = 0)\",\"Compute scaled dot product attention with rel. positional encoding.\",\"Parameters:\",\"query – Query tensor. (B, T_1, size)\",\"key – Key tensor. (B, T_2, size)\",\"value – Value tensor. (B, T_2, size)\",\"pos_enc – Positional embedding tensor. (B, 2 * T_1 - 1, size)\",\"mask – Source mask. (B, T_2)\",\"chunk_mask – Chunk mask. (T_1, T_1)\",\"left_context – Number of previous frames to use for current chunk attention computation.\",\"Returns: Output tensor. (B, T_1, H * d_k)\",\"forward_attention(value: Tensor, scores: Tensor, mask: Tensor, chunk_mask: Tensor | None = None)\",\"Compute attention context vector.\",\"Parameters:\",\"value – Transformed value. (B, H, T_2, d_k)\",\"scores – Attention score. (B, H, T_1, T_2)\",\"mask – Source mask. (B, T_2)\",\"chunk_mask – Chunk mask. (T_1, T_1)\",\"Returns: Transformed value weighted by attention score. (B, T_1, H * d_k)\",\"Return type: attn_output\",\"forward_qkv(query: Tensor, key: Tensor, value: Tensor)\",\"Transform query, key and value.\",\"Parameters:\",\"query – Query tensor. (B, T_1, size)\",\"key – Key tensor. (B, T_2, size)\",\"v – Value tensor. (B, T_2, size)\",\"Returns: Transformed query tensor. (B, H, T_1, d_k) k: Transformed key tensor. (B, H, T_2, d_k) v: Transformed value tensor. (B, H, T_2, d_k)\",\"Return type: q\",\"rel_shift(x: Tensor, left_context: int = 0)\",\"Compute relative positional encoding.\",\"Parameters:\",\"x – Input sequence. (B, H, T_1, 2 * T_1 - 1)\",\"left_context – Number of previous frames to use for current chunk attention computation.\",\"Returns: Output sequence. (B, H, T_1, T_2)\",\"Return type: x\",\"training : bool\"]},\"1077\":{\"h\":\"espnet2.asr_transducer.encoder.modules.positional_encoding.RelPositionalEncoding\",\"t\":[\"class espnet2.asr_transducer.encoder.modules.positional_encoding.RelPositionalEncoding(size: int, dropout_rate: float = 0.0, max_len: int = 5000)\",\"Bases: Module\",\"Relative positional encoding.\",\"Parameters:\",\"size – Module size.\",\"max_len – Maximum input length.\",\"dropout_rate – Dropout rate.\",\"Construct a RelativePositionalEncoding object.\",\"extend_pe(x: Tensor, left_context: int = 0)\",\"Reset positional encoding.\",\"Parameters:\",\"x – Input sequences. (B, T, ?)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"forward(x: Tensor, left_context: int = 0)\",\"Compute positional encoding.\",\"Parameters:\",\"x – Input sequences. (B, T, ?)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"Returns: Positional embedding sequences. (B, 2 * (T - 1), ?)\",\"Return type: pos_enc\",\"training : bool\"]},\"1078\":{\"h\":\"espnet2.asr_transducer.decoder.modules.mega.positional_bias.RelativePositionBias\",\"t\":[\"class espnet2.asr_transducer.decoder.modules.mega.positional_bias.RelativePositionBias(max_positions: int)\",\"Bases: Module\",\"RelativePositionBias module definition.\",\"Parameters:max_positions – Maximum number of relative positions.\",\"Construct a RelativePositionBias object.\",\"forward(length: int)\",\"Compute relative position bias.\",\"Parameters:length – Sequence length.\",\"Returns: Relative position bias. (L, L)\",\"Return type: tile\",\"reset_parameters(val: float = 0.0, std: float = 0.02)\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std – Standard deviation.\",\"training : bool\"]},\"1079\":{\"h\":\"espnet2.asr_transducer.decoder.modules.mega.positional_bias.RotaryRelativePositionBias\",\"t\":[\"class espnet2.asr_transducer.decoder.modules.mega.positional_bias.RotaryRelativePositionBias(size: int, max_positions: int = 2048)\",\"Bases: Module\",\"RotaryRelativePositionBias module definition.\",\"Parameters:\",\"size – Module embedding size.\",\"max_positions – Maximum number of relative positions.\",\"Construct a RotaryRelativePositionBias object.\",\"forward(length: int)\",\"Compute rotary relative position bias.\",\"Parameters:length – Sequence length.\",\"Returns: Rotary relative position bias. (L, L)\",\"Return type: bias\",\"static get_sinusoid_embeddings(max_positions: int, size: int)\",\"Compute sinusoidal positional embeddings.\",\"Parameters:\",\"max_positions – Maximum number of positions.\",\"size – Input size.\",\"Returns: Sine elements. (max_positions, size // 2) : Cos elements. (max_positions, size // 2)\",\"reset_parameters(val: float = 0.0, std: float = 0.02)\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std – Standard deviation.\",\"rotary(x: Tensor)\",\"Compute rotary positional embeddings.\",\"Parameters:x – Input sequence. (L, size)\",\"Returns: Rotary positional embeddings. (L, size)\",\"Return type: x\",\"training : bool\"]},\"1080\":{\"h\":\"espnet2.asr_transducer.normalization.ScaleNorm\",\"t\":[\"class espnet2.asr_transducer.normalization.ScaleNorm(normalized_shape: int, eps: float = 1e-05)\",\"Bases: Module\",\"ScaleNorm module definition.\",\"Reference: https://arxiv.org/pdf/1910.05895.pdf\",\"Parameters:\",\"normalized_shape – Expected size.\",\"eps – Value added to the denominator for numerical stability.\",\"Construct a ScaleNorm object.\",\"forward(x: Tensor)\",\"Compute scale normalization.\",\"Parameters:x – Input sequences. (B, T, D_hidden)\",\"Returns: Output sequences. (B, T, D_hidden)\",\"training : bool\"]},\"1081\":{\"h\":\"espnet2.asr_transducer.decoder.modules.rwkv.attention.SelfAttention\",\"t\":[\"class espnet2.asr_transducer.decoder.modules.rwkv.attention.SelfAttention(size: int, attention_size: int, context_size: int, block_id: int, num_blocks: int)\",\"Bases: Module\",\"SelfAttention module definition.\",\"Parameters:\",\"size – Input/Output size.\",\"attention_size – Attention hidden size.\",\"context_size – Context size for WKV kernel.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"Construct a SelfAttention object.\",\"forward(x: Tensor, state: List[Tensor] | None = None)\",\"Compute time mixing.\",\"Parameters:\",\"x – SelfAttention input sequences. (B, U, size)\",\"state – Decoder hidden states. [5 x (B, 1, D_att, N)]\",\"Returns: SelfAttention output sequences. (B, U, size)\",\"Return type: x\",\"reset_parameters(size: int, attention_size: int, block_id: int, num_blocks: int)\",\"Reset module parameters.\",\"Parameters:\",\"size – Block size.\",\"attention_size – Attention hidden size.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"training : bool\",\"wkv_linear_attention(time_decay: Tensor, time_first: Tensor, key: Tensor, value: Tensor, state: Tuple[Tensor, Tensor, Tensor])\",\"Compute WKV with state (i.e.: for inference).\",\"Parameters:\",\"time_decay – Channel-wise time decay vector. (D_att)\",\"time_first – Channel-wise time first vector. (D_att)\",\"key – Key tensor. (B, 1, D_att)\",\"value – Value tensor. (B, 1, D_att)\",\"state – Decoder hidden states. [3 x (B, D_att)]\",\"Returns: Weighted Key-Value. (B, 1, D_att) state: Decoder hidden states. [3 x (B, 1, D_att)]\",\"Return type: output\"]},\"1082\":{\"h\":\"espnet2.asr_transducer.activation.Smish\",\"t\":[\"<!-- _espnet2.asr_transducer.activation.Smish -->\",\"class espnet2.asr_transducer.activation.Smish(alpha: float = 1.0, beta: float = 1.0)\",\"Bases: Module\",\"Smish activation definition.\",\"Smish(x) = (alpha * x) * tanh(log(1 + sigmoid(beta * x))) : where alpha > 0 and beta > 0\",\"Reference: https://www.mdpi.com/2079-9292/11/4/540/htm.\",\"Parameters:\",\"alpha – Alpha value for Smish activation fomulation. (Usually, alpha = 1. If alpha <= 0, set value to 1).\",\"beta – Beta value for Smish activation formulation. (Usually, beta = 1. If beta <= 0, set value to 1).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Forward computation.\",\"training : bool\"]},\"1083\":{\"h\":\"espnet2.asr_transducer.decoder.stateless_decoder.StatelessDecoder\",\"t\":[\"class espnet2.asr_transducer.decoder.stateless_decoder.StatelessDecoder(vocab_size: int, embed_size: int = 256, embed_dropout_rate: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"Stateless Transducer decoder module.\",\"Parameters:\",\"vocab_size – Output size.\",\"embed_size – Embedding size.\",\"embed_dropout_rate – Dropout rate for embedding layer.\",\"embed_pad – Embed/Blank symbol ID.\",\"Construct a StatelessDecoder object.\",\"batch_score(hyps: List[Hypothesis])\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Decoder output sequences. (B, D_dec) states: Decoder hidden states. None\",\"Return type: out\",\"create_batch_states(new_states: List[Tensor | None])\",\"Create decoder hidden states.\",\"Parameters:new_states – Decoder hidden states. [N x None]\",\"Returns: Decoder hidden states. None\",\"Return type: states\",\"forward(labels: Tensor, states: Any | None = None)\",\"Encode source label sequences.\",\"Parameters:\",\"labels – Label ID sequences. (B, L)\",\"states – Decoder hidden states. None\",\"Returns: Decoder output sequences. (B, U, D_emb)\",\"Return type: embed\",\"init_state(batch_size: int)\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. None\",\"score(label_sequence: List[int], states: Any | None = None)\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"states – Decoder hidden states. None\",\"Returns: Decoder output sequence. (1, D_emb) state: Decoder hidden states. None\",\"select_state(states: Tensor | None, idx: int)\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. None\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. None\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"training : bool\"]},\"1084\":{\"h\":\"espnet2.asr_transducer.activation.Swish\",\"t\":[\"<!-- _espnet2.asr_transducer.activation.Swish -->\",\"class espnet2.asr_transducer.activation.Swish(beta: float = 1.0, use_builtin: bool = False)\",\"Bases: Module\",\"Swish activation definition.\",\"Swish(x) = (beta * x) * sigmoid(x) : where beta = 1 defines standard Swish activation.\",\"References\",\"https://arxiv.org/abs/2108.12943 / https://arxiv.org/abs/1710.05941v1. E-swish variant: https://arxiv.org/abs/1801.07145.\",\"Parameters:\",\"beta – Beta parameter for E-Swish. (beta >= 1. If beta < 1, use standard Swish).\",\"use_builtin – Whether to use PyTorch function if available.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Forward computation.\",\"training : bool\"]},\"1085\":{\"h\":\"espnet2.asr_transducer.utils.TooShortUttError\",\"t\":[\"class espnet2.asr_transducer.utils.TooShortUttError(message: str, actual_size: int, limit: int)\",\"Bases: Exception\",\"Raised when the utt is too short for subsampling.\",\"Parameters:\",\"message – Error message to display.\",\"actual_size – The size that cannot pass the subsampling.\",\"limit – The size limit for subsampling.\",\"Construct a TooShortUttError module.\"]},\"1086\":{\"h\":\"espnet2.asr_transducer.decoder.modules.rwkv.attention.WKVLinearAttention\",\"t\":[\"class espnet2.asr_transducer.decoder.modules.rwkv.attention.WKVLinearAttention(*args, **kwargs)\",\"Bases: Function\",\"WKVLinearAttention function definition.\",\"static backward(ctx, grad_output: Tensor)\",\"WKVLinearAttention function backward pass.\",\"Parameters:grad_output – Output gradient. (B, U, D_att)\",\"Returns: Gradient for channel-wise time decay vector. (D_att) grad_time_first: Gradient for channel-wise time first vector. (D_att) grad_key: Gradient for key tensor. (B, U, D_att) grad_value: Gradient for value tensor. (B, U, D_att)\",\"Return type: grad_time_decay\",\"static forward(ctx, time_decay: Tensor, time_first: Tensor, key: Tensor, value: tensor)\",\"WKVLinearAttention function forward pass.\",\"Parameters:\",\"time_decay – Channel-wise time decay vector. (D_att)\",\"time_first – Channel-wise time first vector. (D_att)\",\"key – Key tensor. (B, U, D_att)\",\"value – Value tensor. (B, U, D_att)\",\"Returns: Weighted Key-Value tensor. (B, U, D_att)\",\"Return type: out\"]},\"1087\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_body_blocks\",\"t\":[\"espnet2.asr_transducer.encoder.building.build_body_blocks(configuration: List[Dict[str, Any]], main_params: Dict[str, Any], output_size: int)\",\"Build encoder body blocks.\",\"Parameters:\",\"configuration – Body blocks configuration.\",\"main_params – Encoder main parameters.\",\"output_size – Architecture output size.\",\"Returns: MultiBlocks function encapsulation all encoder blocks.\"]},\"1088\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_branchformer_block\",\"t\":[\"espnet2.asr_transducer.encoder.building.build_branchformer_block(configuration: List[Dict[str, Any]], main_params: Dict[str, Any])\",\"Build Branchformer block.\",\"Parameters:\",\"configuration – Branchformer block configuration.\",\"main_params – Encoder main parameters.\",\"Returns: Branchformer block function.\"]},\"1089\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_conformer_block\",\"t\":[\"espnet2.asr_transducer.encoder.building.build_conformer_block(configuration: List[Dict[str, Any]], main_params: Dict[str, Any])\",\"Build Conformer block.\",\"Parameters:\",\"configuration – Conformer block configuration.\",\"main_params – Encoder main parameters.\",\"Returns: Conformer block function.\"]},\"1090\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_conv1d_block\",\"t\":[\"espnet2.asr_transducer.encoder.building.build_conv1d_block(configuration: List[Dict[str, Any]], causal: bool)\",\"Build Conv1d block.\",\"Parameters:configuration – Conv1d block configuration.\",\"Returns: Conv1d block function.\"]},\"1091\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_ebranchformer_block\",\"t\":[\"espnet2.asr_transducer.encoder.building.build_ebranchformer_block(configuration: List[Dict[str, Any]], main_params: Dict[str, Any])\",\"Build E-Branchformer block.\",\"Parameters:\",\"configuration – E-Branchformer block configuration.\",\"main_params – Encoder main parameters.\",\"Returns: E-Branchformer block function.\"]},\"1092\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_input_block\",\"t\":[\"espnet2.asr_transducer.encoder.building.build_input_block(input_size: int, configuration: Dict[str, str | int])\",\"Build encoder input block.\",\"Parameters:\",\"input_size – Input size.\",\"configuration – Input block configuration.\",\"Returns: ConvInput block function.\"]},\"1093\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_main_parameters\",\"t\":[\"espnet2.asr_transducer.encoder.building.build_main_parameters(pos_wise_act_type: str = 'swish', conv_mod_act_type: str = 'swish', pos_enc_dropout_rate: float = 0.0, pos_enc_max_len: int = 5000, simplified_att_score: bool = False, norm_type: str = 'layer_norm', conv_mod_norm_type: str = 'layer_norm', after_norm_eps: float | None = None, after_norm_partial: float | None = None, blockdrop_rate: float = 0.0, dynamic_chunk_training: bool = False, short_chunk_threshold: float = 0.75, short_chunk_size: int = 25, num_left_chunks: int = 0, **activation_parameters)\",\"Build encoder main parameters.\",\"Parameters:\",\"pos_wise_act_type – X-former position-wise feed-forward activation type.\",\"conv_mod_act_type – X-former convolution module activation type.\",\"pos_enc_dropout_rate – Positional encoding dropout rate.\",\"pos_enc_max_len – Positional encoding maximum length.\",\"simplified_att_score – Whether to use simplified attention score computation.\",\"norm_type – X-former normalization module type.\",\"conv_mod_norm_type – Conformer convolution module normalization type.\",\"after_norm_eps – Epsilon value for the final normalization.\",\"after_norm_partial – Value for the final normalization with RMSNorm.\",\"blockdrop_rate – Probability threshold of dropping out each encoder block.\",\"dynamic_chunk_training – Whether to use dynamic chunk training.\",\"short_chunk_threshold – Threshold for dynamic chunk selection.\",\"short_chunk_size – Minimum number of frames during dynamic chunk training.\",\"num_left_chunks – Number of left chunks the attention module can see. (null or negative value means full context)\",\"**activation_parameters – Parameters of the activation functions. (See espnet2/asr_transducer/activation.py)\",\"Returns: Main encoder parameters\"]},\"1094\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_positional_encoding\",\"t\":[\"espnet2.asr_transducer.encoder.building.build_positional_encoding(block_size: int, configuration: Dict[str, Any])\",\"Build positional encoding block.\",\"Parameters:\",\"block_size – Input/output size.\",\"configuration – Positional encoding configuration.\",\"Returns: Positional encoding module.\"]},\"1095\":{\"h\":\"espnet2.asr_transducer.utils.check_short_utt\",\"t\":[\"espnet2.asr_transducer.utils.check_short_utt(sub_factor: int, size: int)\",\"Check if the input is too short for subsampling.\",\"Parameters:\",\"sub_factor – Subsampling factor for Conv2DSubsampling.\",\"size – Input size.\",\"Returns: Whether an error should be sent. : Size limit for specified subsampling factor.\"]},\"1096\":{\"h\":\"espnet2.asr_transducer.activation.get_activation\",\"t\":[\"espnet2.asr_transducer.activation.get_activation(activation_type: str, ftswish_threshold: float = -0.2, ftswish_mean_shift: float = 0.0, hardtanh_min_val: int = -1.0, hardtanh_max_val: int = 1.0, leakyrelu_neg_slope: float = 0.01, smish_alpha: float = 1.0, smish_beta: float = 1.0, softplus_beta: float = 1.0, softplus_threshold: int = 20, swish_beta: float = 1.0)\",\"Return activation function.\",\"Parameters:\",\"activation_type – Activation function type.\",\"ftswish_threshold – Threshold value for FTSwish activation formulation.\",\"ftswish_mean_shift – Mean shifting value for FTSwish activation formulation.\",\"hardtanh_min_val – Minimum value of the linear region range for HardTanh.\",\"hardtanh_max_val – Maximum value of the linear region range for HardTanh.\",\"leakyrelu_neg_slope – Negative slope value for LeakyReLU activation formulation.\",\"smish_alpha – Alpha value for Smish activation fomulation.\",\"smish_beta – Beta value for Smish activation formulation.\",\"softplus_beta – Beta value for softplus activation formulation in Mish.\",\"softplus_threshold – Values above this revert to a linear function in Mish.\",\"swish_beta – Beta value for Swish variant formulation.\",\"Returns: Activation function.\"]},\"1097\":{\"h\":\"espnet2.asr_transducer.utils.get_convinput_module_parameters\",\"t\":[\"espnet2.asr_transducer.utils.get_convinput_module_parameters(input_size: int, last_conv_size, subsampling_factor: int, is_vgg: bool = True)\",\"Return the convolution module parameters.\",\"Parameters:\",\"input_size – Module input size.\",\"last_conv_size – Last convolution size for module output size computation.\",\"subsampling_factor – Total subsampling factor.\",\"is_vgg – Whether the module type is VGG-like.\",\"Returns: First MaxPool2D kernel size or second Conv2d kernel size and stride. output_size: Convolution module output size.\"]},\"1098\":{\"h\":\"espnet2.asr_transducer.normalization.get_normalization\",\"t\":[\"espnet2.asr_transducer.normalization.get_normalization(normalization_type: str, eps: float | None = None, partial: float | None = None)\",\"Get normalization module and arguments given parameters.\",\"Parameters:\",\"normalization_type – Normalization module type.\",\"eps – Value added to the denominator.\",\"partial – Value defining the part of the input used for RMS stats (RMSNorm).\",\"Returns: Normalization module class : Normalization module arguments\"]},\"1099\":{\"h\":\"espnet2.asr_transducer.utils.get_transducer_task_io\",\"t\":[\"espnet2.asr_transducer.utils.get_transducer_task_io(labels: Tensor, encoder_out_lens: Tensor, ignore_id: int = -1, blank_id: int = 0)\",\"Get Transducer loss I/O.\",\"Parameters:\",\"labels – Label ID sequences. (B, L)\",\"encoder_out_lens – Encoder output lengths. (B,)\",\"ignore_id – Padding symbol ID.\",\"blank_id – Blank symbol ID.\",\"Returns: Decoder inputs. (B, U) target: Target label ID sequences. (B, U) t_len: Time lengths. (B,) u_len: Label lengths. (B,)\",\"Return type: decoder_in\"]},\"1100\":{\"h\":\"espnet2.asr_transducer.decoder.modules.rwkv.attention.load_wkv_kernel\",\"t\":[\"espnet2.asr_transducer.decoder.modules.rwkv.attention.load_wkv_kernel(context_size: int)\",\"Load WKV CUDA kernel.\",\"Parameters:context_size – Context size.\"]},\"1101\":{\"h\":\"espnet2.asr_transducer.utils.make_chunk_mask\",\"t\":[\"espnet2.asr_transducer.utils.make_chunk_mask(size: int, chunk_size: int, num_left_chunks: int = 0, device: device | None = None)\",\"Create chunk mask for the subsequent steps (size, size).\",\"Reference: https://github.com/k2-fsa/icefall/blob/master/icefall/utils.py\",\"Parameters:\",\"size – Size of the source mask.\",\"chunk_size – Number of frames in chunk.\",\"num_left_chunks – Number of left chunks the attention module can see. (null or negative value means full context)\",\"device – Device for the mask tensor.\",\"Returns: Chunk mask. (size, size)\",\"Return type: mask\"]},\"1102\":{\"h\":\"espnet2.asr_transducer.utils.make_source_mask\",\"t\":[\"espnet2.asr_transducer.utils.make_source_mask(lengths: Tensor)\",\"Create source mask for given lengths.\",\"Reference: https://github.com/k2-fsa/icefall/blob/master/icefall/utils.py\",\"Parameters:lengths – Sequence lengths. (B,)\",\"Returns: Mask for the sequence lengths. (B, max_len)\"]},\"1103\":{\"h\":\"espnet2.asr_transducer.encoder.validation.validate_architecture\",\"t\":[\"espnet2.asr_transducer.encoder.validation.validate_architecture(input_conf: Dict[str, Any], body_conf: List[Dict[str, Any]], input_size: int)\",\"Validate specified architecture is valid.\",\"Parameters:\",\"input_conf – Encoder input block configuration.\",\"body_conf – Encoder body blocks configuration.\",\"input_size – Encoder input size.\",\"Returns: Encoder input block output size. : Encoder body block output size.\",\"Return type: input_block_osize\"]},\"1104\":{\"h\":\"espnet2.asr_transducer.encoder.validation.validate_block_arguments\",\"t\":[\"espnet2.asr_transducer.encoder.validation.validate_block_arguments(configuration: Dict[str, Any], block_id: int, previous_block_output: int)\",\"Validate block arguments.\",\"Parameters:\",\"configuration – Architecture configuration.\",\"block_id – Block ID.\",\"previous_block_output – Previous block output size.\",\"Returns: Block input size. output_size: Block output size.\",\"Return type: input_size\"]},\"1105\":{\"h\":\"espnet2.asr_transducer.encoder.validation.validate_input_block\",\"t\":[\"espnet2.asr_transducer.encoder.validation.validate_input_block(configuration: Dict[str, Any], body_first_conf: Dict[str, Any], input_size: int)\",\"Validate input block.\",\"Parameters:\",\"configuration – Encoder input block configuration.\",\"body_first_conf – Encoder first body block configuration.\",\"input_size – Encoder input block input size.\",\"Returns: Encoder input block output size.\",\"Return type: output_size\"]},\"1106\":{\"h\":\"espnet2.asvspoof.loss.am_softmax_loss.ASVSpoofAMSoftmaxLoss\",\"t\":[\"class espnet2.asvspoof.loss.am_softmax_loss.ASVSpoofAMSoftmaxLoss(weight: float = 1.0, enc_dim: int = 128, s: float = 20, m: float = 0.5)\",\"Bases: AbsASVSpoofLoss\",\"Binary loss for ASV Spoofing.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(label: Tensor, emb: Tensor, **kwargs)\",\"Forward.\",\"Parameters:\",\"label (torch.Tensor) – ground truth label [Batch, 1]\",\"emb (torch.Tensor) – encoder embedding output [Batch, T, enc_dim]\",\"score(emb: Tensor)\",\"Prediction.\",\"Parameters:emb (torch.Tensor) – encoder embedding output [Batch, T, enc_dim]\",\"training : bool\"]},\"1107\":{\"h\":\"espnet2.asvspoof.loss.binary_loss.ASVSpoofBinaryLoss\",\"t\":[\"class espnet2.asvspoof.loss.binary_loss.ASVSpoofBinaryLoss(weight: float = 1.0)\",\"Bases: AbsASVSpoofLoss\",\"Binary loss for ASV Spoofing.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(pred: Tensor, label: Tensor, **kwargs)\",\"Forward.\",\"Parameters:\",\"pred (torch.Tensor) – prediction probability [Batch, 2]\",\"label (torch.Tensor) – ground truth label [Batch, 2]\",\"score(pred: Tensor)\",\"training : bool\"]},\"1108\":{\"h\":\"espnet2.asvspoof.loss.oc_softmax_loss.ASVSpoofOCSoftmaxLoss\",\"t\":[\"class espnet2.asvspoof.loss.oc_softmax_loss.ASVSpoofOCSoftmaxLoss(weight: float = 1.0, enc_dim: int = 128, m_real: float = 0.5, m_fake: float = 0.2, alpha: float = 20.0)\",\"Bases: AbsASVSpoofLoss\",\"Binary loss for ASV Spoofing.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(label: Tensor, emb: Tensor, **kwargs)\",\"Forward.\",\"Parameters:\",\"label (torch.Tensor) – ground truth label [Batch, 1]\",\"emb (torch.Tensor) – encoder embedding output [Batch, T, enc_dim]\",\"score(emb: Tensor)\",\"Prediction.\",\"Parameters:emb (torch.Tensor) – encoder embedding output [Batch, T, enc_dim]\",\"training : bool\"]},\"1109\":{\"h\":\"espnet2.asvspoof.loss.abs_loss.AbsASVSpoofLoss\",\"t\":[\"class espnet2.asvspoof.loss.abs_loss.AbsASVSpoofLoss\",\"Bases: Module, ABC\",\"Base class for all ASV Spoofing loss modules.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(ref, inf)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1110\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\",\"abstract score(pred)\",\"training : bool\"]},\"1111\":{\"h\":\"espnet2.asvspoof.decoder.abs_decoder.AbsDecoder\",\"t\":[\"class espnet2.asvspoof.decoder.abs_decoder.AbsDecoder\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1112\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1113\":{\"h\":\"espnet2.asvspoof.espnet_model.ESPnetASVSpoofModel\",\"t\":[\"class espnet2.asvspoof.espnet_model.ESPnetASVSpoofModel(frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, encoder: AbsEncoder, preencoder: AbsPreEncoder | None, decoder: AbsDecoder, losses: Dict[str, AbsASVSpoofLoss])\",\"Bases: AbsESPnetModel\",\"ASV Spoofing model\",\"A simple ASV Spoofing model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor)\",\"Frontend + Encoder\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch,)\",\"bottleneck_feats – (Batch, Length, …): used for enh + diar\",\"forward(speech: Tensor, speech_lengths: Tensor | None = None, label: Tensor | None = None, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, samples)\",\"spk_labels – (Batch, )\",\"kwargs – “utt_id” is among the input.\",\"training : bool\"]},\"1114\":{\"h\":\"espnet2.asvspoof.decoder.linear_decoder.LinearDecoder\",\"t\":[\"class espnet2.asvspoof.decoder.linear_decoder.LinearDecoder(encoder_output_size: int)\",\"Bases: AbsDecoder\",\"Linear decoder for speaker diarization\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor | None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – hidden_space [Batch, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"training : bool\"]},\"1115\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.AVHubertConfig\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.AVHubertConfig(sample_rate: int = 16000, label_rate: int = -1, encoder_layers: int = 12, encoder_embed_dim: int = 768, encoder_ffn_embed_dim: int = 3072, encoder_attention_heads: int = 12, activation_fn: str = 'gelu', dropout: float = 0.1, attention_dropout: float = 0.1, activation_dropout: float = 0.0, encoder_layerdrop: float = 0.0, dropout_input: float = 0.0, dropout_features: float = 0.0, final_dim: int = 0, untie_final_proj: bool = False, layer_norm_first: bool = False, conv_feature_layers: str = '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', conv_bias: bool = False, logit_temp: float = 0.1, target_glu: bool = False, feature_grad_mult: float = 1.0, mask_length_audio: int = 10, mask_prob_audio: float = 0.65, mask_length_image: int = 10, mask_prob_image: float = 0.65, mask_selection: str = 'static', mask_other: float = 0, no_mask_overlap: bool = False, mask_min_space: int = 1, mask_channel_length: int = 10, mask_channel_prob: float = 0.0, mask_channel_selection: str = 'static', mask_channel_other: float = 0, no_mask_channel_overlap: bool = False, mask_channel_min_space: int = 1, conv_pos: int = 128, conv_pos_groups: int = 16, latent_temp: Tuple[float, float, float] = (2, 0.5, 0.999995), skip_masked: bool = False, skip_nomask: bool = False, resnet_relu_type: str = 'prelu', resnet_weights: str | None = None, sim_type: str = 'cosine', sub_encoder_layers: int = 0, audio_feat_dim: int = -1, modality_dropout: float = 0, audio_dropout: float = 0, modality_fuse: str = 'concat', selection_type: str = 'same_other_seq', masking_type: str = 'input', decoder_embed_dim: int = 768, decoder_ffn_embed_dim: int = 3072, decoder_layers: int = 6, decoder_layerdrop: float = 0.0, decoder_attention_heads: int = 4, decoder_learned_pos: bool = False, decoder_normalize_before: bool = False, no_token_positional_embeddings: bool = False, decoder_dropout: float = 0.1, decoder_attention_dropout: float = 0.1, decoder_activation_dropout: float = 0.0, max_target_positions: int = 2048, share_decoder_input_output_embed: bool = False, audio_only: bool = False, no_scale_embedding: bool = True)\",\"Bases: object\",\"Configuration from original AVHubert Github\",\"activation_dropout : float= 0.0\",\"activation_fn : str= 'gelu'\",\"attention_dropout : float= 0.1\",\"audio_dropout : float= 0\",\"audio_feat_dim : int= -1\",\"audio_only : bool= False\",\"conv_bias : bool= False\",\"conv_feature_layers : str= '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2'\",\"conv_pos : int= 128\",\"conv_pos_groups : int= 16\",\"decoder_activation_dropout : float= 0.0\",\"decoder_attention_dropout : float= 0.1\",\"decoder_attention_heads : int= 4\",\"decoder_dropout : float= 0.1\",\"decoder_embed_dim : int= 768\",\"decoder_ffn_embed_dim : int= 3072\",\"decoder_layerdrop : float= 0.0\",\"decoder_layers : int= 6\",\"decoder_learned_pos : bool= False\",\"decoder_normalize_before : bool= False\",\"dropout : float= 0.1\",\"dropout_features : float= 0.0\",\"dropout_input : float= 0.0\",\"encoder_attention_heads : int= 12\",\"encoder_embed_dim : int= 768\",\"encoder_ffn_embed_dim : int= 3072\",\"encoder_layerdrop : float= 0.0\",\"encoder_layers : int= 12\",\"feature_grad_mult : float= 1.0\",\"final_dim : int= 0\",\"label_rate : int= -1\",\"latent_temp : Tuple[float, float, float]= (2, 0.5, 0.999995)\",\"layer_norm_first : bool= False\",\"logit_temp : float= 0.1\",\"mask_channel_length : int= 10\",\"mask_channel_min_space : int= 1\",\"mask_channel_other : float= 0\",\"mask_channel_prob : float= 0.0\",\"mask_channel_selection : str= 'static'\",\"mask_length_audio : int= 10\",\"mask_length_image : int= 10\",\"mask_min_space : int= 1\",\"mask_other : float= 0\",\"mask_prob_audio : float= 0.65\",\"mask_prob_image : float= 0.65\",\"mask_selection : str= 'static'\",\"masking_type : str= 'input'\",\"max_target_positions : int= 2048\",\"modality_dropout : float= 0\",\"modality_fuse : str= 'concat'\",\"no_mask_channel_overlap : bool= False\",\"no_mask_overlap : bool= False\",\"no_scale_embedding : bool= True\",\"no_token_positional_embeddings : bool= False\",\"resnet_relu_type : str= 'prelu'\",\"resnet_weights : str | None= None\",\"sample_rate : int= 16000\",\"selection_type : str= 'same_other_seq'\",\"share_decoder_input_output_embed : bool= False\",\"sim_type : str= 'cosine'\",\"skip_masked : bool= False\",\"skip_nomask : bool= False\",\"sub_encoder_layers : int= 0\",\"target_glu : bool= False\",\"untie_final_proj : bool= False\"]},\"1116\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.AVHubertModel\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.AVHubertModel(cfg: AVHubertConfig, **kwargs)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"classmethod build_model(cfg: AVHubertConfig)\",\"Build a new model instance.\",\"extract_finetune(source, padding_mask=None, mask=False, ret_conv=False, output_layer=None)\",\"Forward AVHubert Pretrain Encoder.\",\"Parameters:\",\"source**['video']** – input tensor (B, 1, L, H, W)\",\"source**['audio']** – input tensor (B, F, L)\",\"padding_mask – input tensor (B, L)\",\"Returns: encoded tensor and mask\",\"forward_audio(source_audio)\",\"forward_features(source: Tensor, modality: str)\",\"forward_padding_mask(features: Tensor, padding_mask: Tensor)\",\"forward_transformer(source, padding_mask=None, output_layer=None)\",\"Forward AVHubert Pretrain Encoder (without frontend).\",\"Assume the source is already fused feature. :param source: input tensor (B, L, D*2) :param padding_mask: input tensor (B, L)\",\"Returns: encoded tensor and mask\",\"forward_video(source_video)\",\"modality_fusion(features_audio, features_video)\",\"training : bool\"]},\"1117\":{\"h\":\"espnet2.asr.decoder.abs_decoder.AbsDecoder\",\"t\":[\"<!-- _espnet2.asr.decoder.abs_decoder.AbsDecoder -->\",\"class espnet2.asr.decoder.abs_decoder.AbsDecoder\",\"Bases: Module, ScorerInterface, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1118\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1119\":{\"h\":\"espnet2.asr.encoder.abs_encoder.AbsEncoder\",\"t\":[\"<!-- _espnet2.asr.encoder.abs_encoder.AbsEncoder -->\",\"class espnet2.asr.encoder.abs_encoder.AbsEncoder\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1120\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size()\",\"training : bool\"]},\"1121\":{\"h\":\"espnet2.asr.frontend.abs_frontend.AbsFrontend\",\"t\":[\"class espnet2.asr.frontend.abs_frontend.AbsFrontend\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1122\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size()\",\"training : bool\"]},\"1123\":{\"h\":\"espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder\",\"t\":[\"class espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1124\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size()\",\"training : bool\"]},\"1125\":{\"h\":\"espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder\",\"t\":[\"class espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1126\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size()\",\"training : bool\"]},\"1127\":{\"h\":\"espnet2.asr.specaug.abs_specaug.AbsSpecAug\",\"t\":[\"<!-- _espnet2.asr.specaug.abs_specaug.AbsSpecAug -->\",\"class espnet2.asr.specaug.abs_specaug.AbsSpecAug\",\"Bases: Module\",\"Abstract class for the augmentation of spectrogram\",\"The process-flow:\",\"Frontend -> SpecAug -> Normalization -> Encoder -> Decoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, x_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1128\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1129\":{\"h\":\"espnet2.asr.state_spaces.components.Activation\",\"t\":[\"espnet2.asr.state_spaces.components.Activation(activation=None, size=None, dim=-1)\"]},\"1130\":{\"h\":\"espnet2.asr.state_spaces.residual.Affine\",\"t\":[\"<!-- _espnet2.asr.state_spaces.residual.Affine -->\",\"class espnet2.asr.state_spaces.residual.Affine(*args, scalar=True, gamma=0.0, **kwargs)\",\"Bases: Residual\",\"Residual connection with learnable scalar multipliers on the main branch.\",\"scalar: Single scalar multiplier, or one per dimension scale, power: Initialize to scale * layer_num**(-power)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y, transposed)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1131\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1132\":{\"h\":\"espnet2.asr.frontend.asteroid_frontend.AsteroidFrontend\",\"t\":[\"class espnet2.asr.frontend.asteroid_frontend.AsteroidFrontend(sinc_filters: int = 256, sinc_kernel_size: int = 251, sinc_stride: int = 16, preemph_coef: float = 0.97, log_term: float = 1e-06)\",\"Bases: AbsFrontend\",\"Asteroid Filterbank Frontend.\",\"Provides a Sinc-convolutional-based audio feature extractor. The same function can be achieved by using sliding_winodw frontend + sinc preencoder.\",\"NOTE(jiatong): this function is used in sentence-level classification tasks (e.g., spk). Other usages are not fully investigated.\",\"NOTE(jeeweon): this function implements the parameterized analytic filterbank layer in M. Pariente, S. Cornell, A. Deleforge and E. Vincent, “Filterbank design for end-to-end speech separation,” in Proc. ICASSP, 2020\",\"Initialize.\",\"Parameters:\",\"sinc_filters – the filter numbers for sinc.\",\"sinc_kernel_size – the kernel size for sinc.\",\"sinc_stride – the sincstride size of the first sinc-conv layer where it decides the compression rate (Hz).\",\"preemph_coef – the coeifficient for preempahsis.\",\"log_term – the log term to prevent infinity.\",\"forward(input: Tensor, input_length: Tensor)\",\"Apply the Asteroid filterbank frontend to the input.\",\"Parameters:\",\"input – Input (B, T).\",\"input_length – Input length (B,).\",\"Returns: Frame-wise output (B, T’, D).\",\"Return type: Tensor\",\"output_size()\",\"Return output length of feature dimension D.\",\"training : bool\"]},\"1133\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder\",\"t\":[\"class espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder(vocab_size: int, encoder_output_size: int, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True)\",\"Bases: AbsDecoder, BatchScorerInterface, MaskParallelScorerInterface\",\"Base class of Transfomer decoder module.\",\"Parameters:\",\"vocab_size – output dim\",\"encoder_output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"self_attention_dropout_rate – dropout rate for attention\",\"input_layer – input layer type\",\"use_output_layer – whether to use output layer\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor, return_hs: bool = False)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"batch_score_partially_AR(ys: Tensor, states: List[Any], xs: Tensor, yseq_lengths: Tensor)\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor, return_hs: bool = False, return_all_hs: bool = False)\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"return_hs – (bool) whether to return the last hidden output before output layer\",\"return_all_hs – (bool) whether to return all the hidden intermediates\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"forward_one_step(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, memory_mask: Tensor | None = None, *, cache: List[Tensor] | None = None, return_hs: bool = False)\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (batch, maxlen_out)\",\"tgt_mask – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"memory_mask – encoded memory mask (batch, 1, maxlen_in)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"return_hs – dec hidden state corresponding to ys, used for searchable hidden ints\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"forward_partially_AR(tgt: Tensor, tgt_mask: Tensor, tgt_lengths: Tensor, memory: Tensor, cache: List[Tensor] | None = None)\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (n_mask * n_beam, maxlen_out)\",\"tgt_mask – input token mask, (n_mask * n_beam, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"tgt_lengths – (n_mask * n_beam, )\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"score(ys, state, x, return_hs=False)\",\"Score.\",\"training : bool\"]},\"1134\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.BasicBlock\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.BasicBlock(inplanes, planes, stride=1, downsample=None, relu_type='relu')\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"expansion = 1\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1135\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1136\":{\"h\":\"espnet2.asr.bayes_risk_ctc.BayesRiskCTC\",\"t\":[\"<!-- _espnet2.asr.bayes_risk_ctc.BayesRiskCTC -->\",\"class espnet2.asr.bayes_risk_ctc.BayesRiskCTC(risk_strategy='exp', group_strategy='end', risk_factor=0.0)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"find_all_index(ragged_lat, ctc_graph, dense_fsa_vec, arc_map_a, arc_map_b)\",\"find_minimum_hlens(ys_pad, ylens)\",\"forward(nnet_output, ys_pad, hlens, ylens)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1137\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_core(nnet_output, ys, hlens, ylens)\",\"get_risk_scores(loss_state, hlens, risk_factor)\",\"Add the bayes risk in multiple ways\",\"training : bool\"]},\"1138\":{\"h\":\"espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer\",\"t\":[\"class espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer(decoder: AbsDecoder, joint_network: JointNetwork, beam_size: int, lm: Module | None = None, lm_weight: float = 0.1, search_type: str = 'default', max_sym_exp: int = 2, u_max: int = 50, nstep: int = 1, prefix_alpha: int = 1, expansion_gamma: int = 2.3, expansion_beta: int = 2, multi_blank_durations: List[int] = [], multi_blank_indices: List[int] = [], score_norm: bool = True, score_norm_during: bool = False, nbest: int = 1, token_list: List[str] | None = None)\",\"Bases: object\",\"Beam search implementation for Transducer.\",\"Initialize Transducer search module.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint network module.\",\"beam_size – Beam size.\",\"lm – LM class.\",\"lm_weight – LM weight for soft fusion.\",\"search_type – Search algorithm to use during inference.\",\"max_sym_exp – Number of maximum symbol expansions at each time step. (TSD)\",\"u_max – Maximum output sequence length. (ALSD)\",\"nstep – Number of maximum expansion steps at each time step. (NSC/mAES)\",\"prefix_alpha – Maximum prefix length in prefix search. (NSC/mAES)\",\"expansion_beta – Number of additional candidates for expanded hypotheses selection. (mAES)\",\"expansion_gamma – Allowed logp difference for prune-by-value method. (mAES)\",\"multi_blank_durations – The duration of each blank token. (MBG)\",\"multi_blank_indices – The index of each blank token in token_list. (MBG)\",\"score_norm – Normalize final scores by length. (“default”)\",\"score_norm_during – Normalize scores by length during search. (default, TSD, ALSD)\",\"nbest – Number of final hypothesis.\",\"align_length_sync_decoding(enc_out: Tensor)\",\"Alignment-length synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:h – Encoder output sequences. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"default_beam_search(enc_out: Tensor)\",\"Beam search implementation.\",\"Modified from https://arxiv.org/pdf/1211.3711.pdf\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"greedy_search(enc_out: Tensor)\",\"Greedy search implementation.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: 1-best hypotheses.\",\"Return type: hyp\",\"modified_adaptive_expansion_search(enc_out: Tensor)\",\"It’s the modified Adaptive Expansion Search (mAES) implementation.\",\"Based on/modified from https://ieeexplore.ieee.org/document/9250505 and NSC.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"multi_blank_greedy_search(enc_out: Tensor)\",\"Greedy Search for Multi-Blank Transducer (Multi-Blank Greedy, MBG).\",\"In this implementation, we assume:\",\"the index of standard blank is the last entry of self.multi_blank_indices\",\"rather than self.blank_id (to avoid too much change on original transducer)\",\"other entries in self.multi_blank_indices are big blanks that account for multiple frames.\",\"Based on https://arxiv.org/abs/2211.03541\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: 1-best hypothesis.\",\"Return type: hyp\",\"nsc_beam_search(enc_out: Tensor)\",\"N-step constrained beam search implementation.\",\"Based on/Modified from https://arxiv.org/pdf/2002.03577.pdf. Please reference ESPnet (b-flo, PR #2444) for any usage outside ESPnet until further modifications.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"prefix_search(hyps: List[ExtendedHypothesis], enc_out_t: Tensor)\",\"Prefix search for NSC and mAES strategies.\",\"Based on https://arxiv.org/pdf/1211.3711.pdf\",\"sort_nbest(hyps: List[Hypothesis] | List[ExtendedHypothesis])\",\"Sort hypotheses by score or score given sequence length.\",\"Parameters:hyps – Hypothesis.\",\"Returns: Sorted hypothesis.\",\"Return type: hyps\",\"time_sync_decoding(enc_out: Tensor)\",\"Time synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\"]},\"1139\":{\"h\":\"espnet2.asr.transducer.beam_search_transducer_streaming.BeamSearchTransducerStreaming\",\"t\":[\"class espnet2.asr.transducer.beam_search_transducer_streaming.BeamSearchTransducerStreaming(decoder: AbsDecoder, joint_network: JointNetwork, beam_size: int, lm: Module | None = None, lm_weight: float = 0.1, search_type: str = 'default', max_sym_exp: int = 2, u_max: int = 50, nstep: int = 1, prefix_alpha: int = 1, expansion_gamma: int = 2.3, expansion_beta: int = 2, score_norm: bool = True, score_norm_during: bool = False, nbest: int = 1, penalty: float = 0.0, token_list: List[str] | None = None, hold_n: int = 0)\",\"Bases: object\",\"Beam search implementation for Transducer.\",\"Initialize Transducer search module.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint network module.\",\"beam_size – Beam size.\",\"lm – LM class.\",\"lm_weight – LM weight for soft fusion.\",\"search_type – Search algorithm to use during inference.\",\"max_sym_exp – Number of maximum symbol expansions at each time step. (TSD)\",\"u_max – Maximum output sequence length. (ALSD)\",\"nstep – Number of maximum expansion steps at each time step. (NSC/mAES)\",\"prefix_alpha – Maximum prefix length in prefix search. (NSC/mAES)\",\"expansion_beta – Number of additional candidates for expanded hypotheses selection. (mAES)\",\"expansion_gamma – Allowed logp difference for prune-by-value method. (mAES)\",\"score_norm – Normalize final scores by length. (“default”)\",\"score_norm_during – Normalize scores by length during search. (default, TSD, ALSD)\",\"nbest – Number of final hypothesis.\",\"align_length_sync_decoding(enc_out: Tensor)\",\"Alignment-length synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:h – Encoder output sequences. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"default_beam_search(enc_out: Tensor)\",\"Beam search implementation.\",\"Modified from https://arxiv.org/pdf/1211.3711.pdf\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"greedy_search(enc_out: Tensor)\",\"Greedy search implementation.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: 1-best hypotheses.\",\"Return type: hyp\",\"modified_adaptive_expansion_search(enc_out: Tensor)\",\"It’s the modified Adaptive Expansion Search (mAES) implementation.\",\"Based on/modified from https://ieeexplore.ieee.org/document/9250505 and NSC.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"nsc_beam_search(enc_out: Tensor)\",\"N-step constrained beam search implementation.\",\"Based on/Modified from https://arxiv.org/pdf/2002.03577.pdf. Please reference ESPnet (b-flo, PR #2444) for any usage outside ESPnet until further modifications.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"prefix_search(hyps: List[ExtendedHypothesis], enc_out_t: Tensor)\",\"Prefix search for NSC and mAES strategies.\",\"Based on https://arxiv.org/pdf/1211.3711.pdf\",\"reset()\",\"sort_nbest(hyps: List[Hypothesis] | List[ExtendedHypothesis])\",\"Sort hypotheses by score or score given sequence length.\",\"Parameters:hyps – Hypothesis.\",\"Returns: Sorted hypothesis.\",\"Return type: hyps\",\"time_sync_decoding(enc_out: Tensor)\",\"Time synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\"]},\"1140\":{\"h\":\"espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder\",\"t\":[\"class espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder(input_size: int, output_size: int = 256, use_attn: bool = True, attention_heads: int = 4, attention_layer_type: str = 'rel_selfattn', pos_enc_layer_type: str = 'rel_pos', rel_pos_type: str = 'latest', use_cgmlp: bool = True, cgmlp_linear_units: int = 2048, cgmlp_conv_kernel: int = 31, use_linear_after_conv: bool = False, gate_activation: str = 'identity', merge_method: str = 'concat', cgmlp_weight: float | List[float] = 0.5, attn_branch_drop_rate: float | List[float] = 0.0, num_blocks: int = 12, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', zero_triu: bool = False, padding_idx: int = -1, stochastic_depth_rate: float | List[float] = 0.0)\",\"Bases: AbsEncoder\",\"Branchformer encoder module.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size()\",\"training : bool\"]},\"1141\":{\"h\":\"espnet2.asr.encoder.branchformer_encoder.BranchformerEncoderLayer\",\"t\":[\"class espnet2.asr.encoder.branchformer_encoder.BranchformerEncoderLayer(size: int, attn: Module | None, cgmlp: Module | None, dropout_rate: float, merge_method: str, cgmlp_weight: float = 0.5, attn_branch_drop_rate: float = 0.0, stochastic_depth_rate: float = 0.0)\",\"Bases: Module\",\"Branchformer encoder layer module.\",\"Parameters:\",\"size (int) – model dimension\",\"attn – standard self-attention or efficient attention, optional\",\"cgmlp – ConvolutionalGatingMLP, optional\",\"dropout_rate (float) – dropout probability\",\"merge_method (str) – concat, learned_ave, fixed_ave\",\"cgmlp_weight (float) – weight of the cgmlp branch, between 0 and 1, used if merge_method is fixed_ave\",\"attn_branch_drop_rate (float) – probability of dropping the attn branch, used if merge_method is learned_ave\",\"stochastic_depth_rate (float) – stochastic depth probability\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x_input, mask, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (Union *[*Tuple,torch.Tensor]) – Input tensor w/ or w/o pos emb. \",\"w/ pos emb: Tuple of tensors [(#batch, time, size), (1, time, size)].\",\"w/o pos emb: Tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, 1, time).\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, time).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"1142\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CPURNNT\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CPURNNT(minibatch: int, maxT: int, maxU: int, alphabet_size: int, workspace: Tensor, blank: int, fastemit_lambda: float, clamp: float, num_threads: int, batch_first: bool)\",\"Bases: object\",\"Helper class to compute the Transducer Loss on CPU.\",\"Parameters:\",\"minibatch – Size of the minibatch b.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"workspace – An allocated chunk of memory that will be sliced off and reshaped into required blocks used as working memory.\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of OMP threads to launch.\",\"batch_first – Bool that decides if batch dimension is first or third.\",\"compute_alphas(log_probs: Tensor, T: int, U: int, alphas: Tensor)\",\"Compute the probability of the forward variable alpha.\",\"Parameters:\",\"log_probs – Flattened tensor [B, T, U, V+1]\",\"T – Length of the acoustic sequence T (not padded).\",\"U – Length of the target sequence U (not padded).\",\"alphas – Working space memory for alpha of shape [B, T, U].\",\"Returns: Loglikelihood of the forward variable alpha.\",\"compute_betas_and_grads(grad: Tensor, log_probs: Tensor, T: int, U: int, alphas: Tensor, betas: Tensor, labels: Tensor, logll: Tensor)\",\"Compute backward variable beta as well as gradients of the activation\",\"matrix wrt loglikelihood of forward variable.\",\"Parameters:\",\"grad – Working space memory of flattened shape [B, T, U, V+1]\",\"log_probs – Activatio tensor of flattented shape [B, T, U, V+1]\",\"T – Length of the acoustic sequence T (not padded).\",\"U – Length of the target sequence U (not padded).\",\"alphas – Working space memory for alpha of shape [B, T, U].\",\"betas – Working space memory for alpha of shape [B, T, U].\",\"labels – Ground truth label of shape [B, U]\",\"logll – Loglikelihood of the forward variable.\",\"Returns: Loglikelihood of the forward variable and inplace updates the grad tensor.\",\"cost_and_grad(log_probs: Tensor, grads: Tensor, costs: Tensor, flat_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\",\"cost_and_grad_kernel(log_probs: Tensor, grad: Tensor, labels: Tensor, mb: int, T: int, U: int, bytes_used: int)\",\"score_forward(log_probs: Tensor, costs: Tensor, flat_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\"]},\"1143\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.CTAReduce\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.CTAReduce(tid: int, x, storage, count: int, R_opid: int)\",\"CUDA Warp reduction kernel.\",\"It is a device kernel to be called by other kernels.\",\"The data will be read from the right segement recursively, and reduced (ROP) onto the left half. Operation continues while warp size is larger than a given offset. Beyond this offset, warp reduction is performed via shfl_down_sync, which halves the reduction space and sums the two halves at each call.\"]},\"1144\":{\"h\":\"NOTE\",\"t\":[\"Efficient warp occurs at input shapes of 2 ^ K.\",\"References\",\"Warp Primitives [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/]\",\"Parameters:\",\"tid – CUDA thread index\",\"x – activation. Single float.\",\"storage – shared memory of size CTA_REDUCE_SIZE used for reduction in parallel threads.\",\"count – equivalent to num_rows, which is equivalent to alphabet_size (V+1)\",\"R_opid – Operator ID for reduction. See R_Op for more information.\"]},\"1145\":{\"h\":\"espnet2.asr.ctc.CTC\",\"t\":[\"<!-- _espnet2.asr.ctc.CTC -->\",\"class espnet2.asr.ctc.CTC(odim: int, encoder_output_size: int, dropout_rate: float = 0.0, ctc_type: str = 'builtin', reduce: bool = True, ignore_nan_grad: bool | None = None, zero_infinity: bool = True, brctc_risk_strategy: str = 'exp', brctc_group_strategy: str = 'end', brctc_risk_factor: float = 0.0)\",\"Bases: Module\",\"CTC module.\",\"Parameters:\",\"odim – dimension of outputs\",\"encoder_output_size – number of encoder projection units\",\"dropout_rate – dropout rate (0.0 ~ 1.0)\",\"ctc_type – builtin or gtnctc\",\"reduce – reduce the CTC loss into a scalar\",\"ignore_nan_grad – Same as zero_infinity (keeping for backward compatiblity)\",\"zero_infinity – Whether to zero infinite losses and the associated gradients.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"argmax(hs_pad)\",\"argmax of frame activations\",\"Parameters:hs_pad (torch.Tensor) – 3d tensor (B, Tmax, eprojs)\",\"Returns: argmax applied 2d tensor (B, Tmax)\",\"Return type: torch.Tensor\",\"forward(hs_pad, hlens, ys_pad, ys_lens)\",\"Calculate CTC loss.\",\"Parameters:\",\"hs_pad – batch of padded hidden state sequences (B, Tmax, D)\",\"hlens – batch of lengths of hidden state sequences (B)\",\"ys_pad – batch of padded character id sequence tensor (B, Lmax)\",\"ys_lens – batch of lengths of character sequence (B)\",\"log_softmax(hs_pad)\",\"log_softmax of frame activations\",\"Parameters:hs_pad (Tensor) – 3d tensor (B, Tmax, eprojs)\",\"Returns: log softmax applied 3d tensor (B, Tmax, odim)\",\"Return type: torch.Tensor\",\"loss_fn(th_pred, th_target, th_ilen, th_olen)\",\"softmax(hs_pad)\",\"softmax of frame activations\",\"Parameters:hs_pad (Tensor) – 3d tensor (B, Tmax, eprojs)\",\"Returns: softmax applied 3d tensor (B, Tmax, odim)\",\"Return type: torch.Tensor\",\"training : bool\"]},\"1146\":{\"h\":\"espnet2.asr.state_spaces.cauchy.CauchyMultiply\"},\"1147\":{\"h\":\"espnet2.asr.state_spaces.cauchy.CauchyMultiplySymmetric\"},\"1148\":{\"h\":\"espnet2.asr.encoder.conformer_encoder.ConformerEncoder\",\"t\":[\"class espnet2.asr.encoder.conformer_encoder.ConformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, ctc_trim: bool = False, stochastic_depth_rate: float | List[float] = 0.0, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000)\",\"Bases: AbsEncoder\",\"Conformer encoder module.\",\"Parameters:\",\"input_size (int) – Input dimension.\",\"output_size (int) – Dimension of attention.\",\"attention_heads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"num_blocks (int) – The number of decoder blocks.\",\"dropout_rate (float) – Dropout rate.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. If True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) If False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"rel_pos_type (str) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"encoder_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"encoder_attn_layer_type (str) – Encoder attention layer type.\",\"activation_type (str) – Encoder activation function type.\",\"macaron_style (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_module (bool) – Whether to use convolution module.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"cnn_module_kernel (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, ctc: CTC | None = None, return_all_hs: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"ctc (CTC) – ctc module for intermediate CTC loss\",\"return_all_hs (bool) – whether to return all hidden states\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size()\",\"training : bool\"]},\"1149\":{\"h\":\"espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder\",\"t\":[\"class espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'>, selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, cnn_module_kernel: int = 31, padding_idx: int = -1, block_size: int = 40, hop_size: int = 16, look_ahead: int = 16, init_average: bool = True, ctx_pos_enc: bool = True)\",\"Bases: AbsEncoder\",\"Contextual Block Conformer encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"input_layer – input layer type\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type – linear of conv1d\",\"positionwise_conv_kernel_size – kernel size of positionwise conv1d layer\",\"padding_idx – padding_idx for input_layer=embed\",\"block_size – block size for contextual block processing\",\"hop_Size – hop size for block processing\",\"look_ahead – look-ahead size for block_processing\",\"init_average – whether to use average as initial context (otherwise max values)\",\"ctx_pos_enc – whether to use positional encoding to the context vectors\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final=True, infer_mode=False)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"infer_mode – whether to be used for inference. This is used to distinguish between forward_train (train and validate) and forward_infer (decode).\",\"Returns: position embedded tensor and mask\",\"forward_infer(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final: bool = True)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"forward_train(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"training : bool\"]},\"1150\":{\"h\":\"espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder\",\"t\":[\"class espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, padding_idx: int = -1, block_size: int = 40, hop_size: int = 16, look_ahead: int = 16, init_average: bool = True, ctx_pos_enc: bool = True)\",\"Bases: AbsEncoder\",\"Contextual Block Transformer encoder module.\",\"Details in Tsunoo et al. “Transformer ASR with contextual block processing” (https://arxiv.org/abs/1910.07204)\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of encoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"input_layer – input layer type\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type – linear of conv1d\",\"positionwise_conv_kernel_size – kernel size of positionwise conv1d layer\",\"padding_idx – padding_idx for input_layer=embed\",\"block_size – block size for contextual block processing\",\"hop_Size – hop size for block processing\",\"look_ahead – look-ahead size for block_processing\",\"init_average – whether to use average as initial context (otherwise max values)\",\"ctx_pos_enc – whether to use positional encoding to the context vectors\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final=True, infer_mode=False)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"infer_mode – whether to be used for inference. This is used to distinguish between forward_train (train and validate) and forward_infer (decode).\",\"Returns: position embedded tensor and mask\",\"forward_infer(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final: bool = True)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"forward_train(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"training : bool\"]},\"1151\":{\"h\":\"espnet2.asr.layers.cgmlp.ConvolutionalGatingMLP\",\"t\":[\"class espnet2.asr.layers.cgmlp.ConvolutionalGatingMLP(size: int, linear_units: int, kernel_size: int, dropout_rate: float, use_linear_after_conv: bool, gate_activation: str)\",\"Bases: Module\",\"Convolutional Gating MLP (cgMLP).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, mask)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1152\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1153\":{\"h\":\"espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit\",\"t\":[\"class espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit(size: int, kernel_size: int, dropout_rate: float, use_linear_after_conv: bool, gate_activation: str)\",\"Bases: Module\",\"Convolutional Spatial Gating Unit (CSGU).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"espnet_initialization_fn()\",\"forward(x, gate_add=None)\",\"Forward method\",\"Parameters:\",\"x (torch.Tensor) – (N, T, D)\",\"gate_add (torch.Tensor) – (N, T, D/2)\",\"Returns: (N, T, D/2)\",\"Return type: out (torch.Tensor)\",\"training : bool\"]},\"1154\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CpuRNNT_index\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CpuRNNT_index(U: int, maxU: int, minibatch: int, alphabet_size: int, batch_first: bool)\",\"Bases: object\",\"A placeholder Index computation class that emits the resolved index in a\",\"flattened tensor, mimicing pointer indexing in CUDA kernels on the CPU.\",\"Parameters:\",\"U – Length of the current target sample (without padding).\",\"maxU – Max Length of the padded target samples.\",\"minibatch – Minibatch index\",\"alphabet_size – Size of the vocabulary including RNNT blank - V+1.\",\"batch_first – Bool flag determining if batch index is first or third.\"]},\"1155\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CpuRNNT_metadata\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CpuRNNT_metadata(T: int, U: int, workspace: Tensor, bytes_used: int, blank: int, labels: Tensor, log_probs: Tensor, idx: CpuRNNT_index)\",\"Bases: object\",\"Metadata for CPU based RNNT loss calculation. Holds the working space memory.\",\"Parameters:\",\"T – Length of the acoustic sequence (without padding).\",\"U – Length of the target sequence (without padding).\",\"workspace – Working space memory for the CPU.\",\"bytes_used – Number of bytes currently used for indexing the working space memory. Generally 0.\",\"blank – Index of the blank token in the vocabulary.\",\"labels – Ground truth padded labels matrix of shape [B, U]\",\"log_probs – Log probs / activation matrix of flattented shape [B, T, U, V+1]\",\"idx –\",\"setup_probs(T: int, U: int, labels: Tensor, blank: int, log_probs: Tensor, idx: CpuRNNT_index)\"]},\"1156\":{\"h\":\"espnet2.asr.state_spaces.residual.DecayResidual\",\"t\":[\"class espnet2.asr.state_spaces.residual.DecayResidual(*args, power=0.5, l2=True)\",\"Bases: Residual\",\"Residual connection that can decay the linear combination depending on depth.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y, transposed)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1157\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1158\":{\"h\":\"espnet2.asr.frontend.default.DefaultFrontend\",\"t\":[\"class espnet2.asr.frontend.default.DefaultFrontend(fs: int | str = 16000, n_fft: int = 512, win_length: int | None = None, hop_length: int = 128, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True, n_mels: int = 80, fmin: int | None = None, fmax: int | None = None, htk: bool = False, frontend_conf: dict | None = {'badim': 320, 'bdropout_rate': 0.0, 'blayers': 3, 'bnmask': 2, 'bprojs': 320, 'btype': 'blstmp', 'bunits': 300, 'delay': 3, 'ref_channel': -1, 'taps': 5, 'use_beamformer': False, 'use_dnn_mask_for_wpe': True, 'use_wpe': False, 'wdropout_rate': 0.0, 'wlayers': 3, 'wprojs': 320, 'wtype': 'blstmp', 'wunits': 300}, apply_stft: bool = True)\",\"Bases: AbsFrontend\",\"Conventional frontend structure for ASR.\",\"Stft -> WPE -> MVDR-Beamformer -> Power-spec -> Log-Mel-Fbank\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1159\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"1160\":{\"h\":\"espnet2.asr.state_spaces.pool.DownAvgPool\",\"t\":[\"<!-- _espnet2.asr.state_spaces.pool.DownAvgPool -->\",\"class espnet2.asr.state_spaces.pool.DownAvgPool(d_input, stride=1, expand=1, transposed=True)\",\"Bases: SequenceModule\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1161\":{\"h\":\"espnet2.asr.state_spaces.pool.DownLinearPool\",\"t\":[\"class espnet2.asr.state_spaces.pool.DownLinearPool(d_input, stride=1, expand=1, transposed=True)\",\"Bases: SequenceModule\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1162\":{\"h\":\"espnet2.asr.state_spaces.pool.DownPool\",\"t\":[\"<!-- _espnet2.asr.state_spaces.pool.DownPool -->\",\"class espnet2.asr.state_spaces.pool.DownPool(d_input, d_output=None, expand=None, stride=1, transposed=True, weight_norm=True, initializer=None, activation=None)\",\"Bases: SequenceModule\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"default_state(*batch_shape, device=None)\",\"Create initial state for a batch of inputs.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step one time step as a recurrent model.\",\"x: (…, H)\",\"training : bool\"]},\"1163\":{\"h\":\"espnet2.asr.state_spaces.pool.DownPool2d\",\"t\":[\"<!-- _espnet2.asr.state_spaces.pool.DownPool2d -->\",\"class espnet2.asr.state_spaces.pool.DownPool2d(d_input, d_output, stride=1, transposed=True, weight_norm=True)\",\"Bases: SequenceModule\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"training : bool\"]},\"1164\":{\"h\":\"espnet2.asr.state_spaces.pool.DownSample\",\"t\":[\"<!-- _espnet2.asr.state_spaces.pool.DownSample -->\",\"class espnet2.asr.state_spaces.pool.DownSample(d_input, stride=1, expand=1, transposed=True)\",\"Bases: SequenceModule\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1165\":{\"h\":\"espnet2.asr.state_spaces.pool.DownSpectralPool\",\"t\":[\"class espnet2.asr.state_spaces.pool.DownSpectralPool(d_input, stride=1, expand=1, transposed=True)\",\"Bases: SequenceModule\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"forward(x)\",\"Forward pass.\",\"x: (B, L…, D)\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1166\":{\"h\":\"espnet2.asr.state_spaces.components.DropoutNd\",\"t\":[\"class espnet2.asr.state_spaces.components.DropoutNd(p: float = 0.5, tie=True, transposed=True)\",\"Bases: Module\",\"Initialize dropout module.\",\"tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\",\"forward(X)\",\"Forward pass.\",\"X: (batch, dim, lengths…)\",\"training : bool\"]},\"1167\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder\",\"t\":[\"class espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11), conv_usebias: int = False)\",\"Bases: BaseTransformerDecoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"training : bool\"]},\"1168\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder\",\"t\":[\"class espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11), conv_usebias: int = False)\",\"Bases: BaseTransformerDecoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"training : bool\"]},\"1169\":{\"h\":\"espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder\",\"t\":[\"class espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, attention_layer_type: str = 'rel_selfattn', pos_enc_layer_type: str = 'rel_pos', rel_pos_type: str = 'latest', cgmlp_linear_units: int = 2048, cgmlp_conv_kernel: int = 31, use_linear_after_conv: bool = False, gate_activation: str = 'identity', num_blocks: int = 12, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', zero_triu: bool = False, padding_idx: int = -1, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000, use_ffn: bool = False, macaron_ffn: bool = False, ffn_activation_type: str = 'swish', linear_units: int = 2048, positionwise_layer_type: str = 'linear', merge_conv_kernel: int = 3, interctc_layer_idx=None, interctc_use_conditioning: bool = False)\",\"Bases: AbsEncoder\",\"E-Branchformer encoder module.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, ctc: CTC | None = None, max_layer: int | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"ctc (CTC) – Intermediate CTC module.\",\"max_layer (int) – Layer depth below which InterCTC is applied.\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size()\",\"training : bool\"]},\"1170\":{\"h\":\"espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoderLayer\",\"t\":[\"class espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoderLayer(size: int, attn: Module, cgmlp: Module, feed_forward: Module | None, feed_forward_macaron: Module | None, dropout_rate: float, merge_conv_kernel: int = 3)\",\"Bases: Module\",\"E-Branchformer encoder layer module.\",\"Parameters:\",\"size (int) – model dimension\",\"attn – standard self-attention or efficient attention\",\"cgmlp – ConvolutionalGatingMLP\",\"feed_forward – feed-forward module, optional\",\"feed_forward – macaron-style feed-forward module, optional\",\"dropout_rate (float) – dropout probability\",\"merge_conv_kernel (int) – kernel size of the depth-wise conv in merge module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x_input, mask, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (Union *[*Tuple,torch.Tensor]) – Input tensor w/ or w/o pos emb. \",\"w/ pos emb: Tuple of tensors [(#batch, time, size), (1, time, size)].\",\"w/o pos emb: Tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, 1, time).\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, time).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"1171\":{\"h\":\"espnet2.asr.espnet_model.ESPnetASRModel\",\"t\":[\"<!-- _espnet2.asr.espnet_model.ESPnetASRModel -->\",\"class espnet2.asr.espnet_model.ESPnetASRModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder | None, ctc: CTC, joint_network: Module | None, aux_ctc: dict | None = None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', transducer_multi_blank_durations: List = [], transducer_multi_blank_sigma: float = 0.05, sym_sos: str = '<sos/eos>', sym_eos: str = '<sos/eos>', extract_feats_in_collect_stats: bool = True, lang_token_id: int = -1)\",\"Bases: AbsESPnetModel\",\"CTC-attention hybrid Encoder-Decoder model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batchify_nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor, batch_size: int = 100)\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param encoder_out: (Batch, Length, Dim) :param encoder_out_lens: (Batch,) :param ys_pad: (Batch, Length) :param ys_pad_lens: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase GPU memory usage\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor)\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\",\"nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor)\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"Normally, this function is called in batchify_nll.\",\"Parameters:\",\"encoder_out – (Batch, Length, Dim)\",\"encoder_out_lens – (Batch,)\",\"ys_pad – (Batch, Length)\",\"ys_pad_lens – (Batch,)\",\"training : bool\"]},\"1172\":{\"h\":\"espnet2.asr.discrete_asr_espnet_model.ESPnetDiscreteASRModel\",\"t\":[\"class espnet2.asr.discrete_asr_espnet_model.ESPnetDiscreteASRModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder, ctc: CTC | None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, src_vocab_size: int = 0, src_token_list: Tuple[str, ...] | List[str] = [], ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_bleu: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', extract_feats_in_collect_stats: bool = True, share_decoder_input_output_embed: bool = False, share_encoder_decoder_input_embed: bool = False)\",\"Bases: ESPnetMTModel\",\"Encoder-Decoder model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"encode(src_text: Tensor, src_text_lengths: Tensor)\",\"Frontend + Encoder. Note that this method is used by mt_inference.py\",\"Parameters:\",\"src_text – (Batch, Length, …)\",\"src_text_lengths – (Batch, )\",\"forward(text: Tensor, text_lengths: Tensor, src_text: Tensor, src_text_lengths: Tensor, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"src_text – (Batch, length)\",\"src_text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\",\"training : bool\"]},\"1173\":{\"h\":\"espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer\",\"t\":[\"class espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer(decoder: AbsDecoder, joint_network: Module, token_list: List[int], sym_space: str, sym_blank: str, report_cer: bool = False, report_wer: bool = False)\",\"Bases: object\",\"Calculate CER and WER for transducer models.\",\"Parameters:\",\"decoder – Decoder module.\",\"token_list – List of tokens.\",\"sym_space – Space symbol.\",\"sym_blank – Blank symbol.\",\"report_cer – Whether to compute CER.\",\"report_wer – Whether to compute WER.\",\"Construct an ErrorCalculatorTransducer.\",\"calculate_cer(char_pred: Tensor, char_target: Tensor)\",\"Calculate sentence-level CER score.\",\"Parameters:\",\"char_pred – Prediction character sequences. (B, ?)\",\"char_target – Target character sequences. (B, ?)\",\"Returns: Average sentence-level CER score.\",\"calculate_wer(char_pred: Tensor, char_target: Tensor)\",\"Calculate sentence-level WER score.\",\"Parameters:\",\"char_pred – Prediction character sequences. (B, ?)\",\"char_target – Target character sequences. (B, ?)\",\"Returns: Average sentence-level WER score\",\"convert_to_char(pred: Tensor, target: Tensor)\",\"Convert label ID sequences to character sequences.\",\"Parameters:\",\"pred – Prediction label ID sequences. (B, U)\",\"target – Target label ID sequences. (B, L)\",\"Returns: Prediction character sequences. (B, ?) char_target: Target character sequences. (B, ?)\",\"Return type: char_pred\"]},\"1174\":{\"h\":\"espnet2.asr.decoder.whisper_decoder.ExpandedTokenEmbedding\",\"t\":[\"class espnet2.asr.decoder.whisper_decoder.ExpandedTokenEmbedding(ori_emebedding, additional_size)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1175\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\",\"property weight\"]},\"1176\":{\"h\":\"espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis\",\"t\":[\"class espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor, lm_state: Dict[str, Any] | List[Any] | None = None, dec_out: List[Tensor] | None = None, lm_scores: Tensor | None = None)\",\"Bases: Hypothesis\",\"Extended hypothesis definition for NSC beam search and mAES.\",\"dec_out : List[Tensor]= None\",\"lm_scores : Tensor= None\"]},\"1177\":{\"h\":\"espnet2.asr.state_spaces.ff.FF\",\"t\":[\"<!-- _espnet2.asr.state_spaces.ff.FF -->\",\"class espnet2.asr.state_spaces.ff.FF(d_input, expand=2, d_output=None, transposed=False, activation='gelu', initializer=None, dropout=0.0, tie_dropout=False)\",\"Bases: SequenceModule\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, *args, **kwargs)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1178\":{\"h\":\"espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder\",\"t\":[\"class espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder(input_size: int, w2v_url: str, w2v_dir_path: str = './', output_size: int = 256, normalize_before: bool = False, freeze_finetune_updates: int = 0)\",\"Bases: AbsEncoder\",\"FairSeq Wav2Vec2 encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"w2v_url – url to Wav2Vec2.0 pretrained model\",\"w2v_dir_path – directory to download the Wav2Vec2.0 pretrained model.\",\"normalize_before – whether to use layer_norm before the first block\",\"finetune_last_n_layers – last n layers to be finetuned in Wav2Vec2.0 0 means to finetune every layer if freeze_w2v=False.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Forward FairSeqWav2Vec2 Encoder.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1179\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.FairseqAVHubertEncoder\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.FairseqAVHubertEncoder(input_size: int = 1, avhubert_url: str = './', avhubert_dir_path: str = './', freeze_finetune_updates: int = 0, encoder_embed_dim: int = 1024, encoder_layerdrop: float = 0.05, dropout_input: float = 0.1, dropout_features: float = 0.1, dropout: float = 0.1, attention_dropout: float = 0.1, feature_grad_mult: float = 0.1, activation_dropout: float = 0.0, wav_input: bool = False, layer_norm_first: bool = True, audio_feat_dim: int = 104, encoder_layers: int = 24, encoder_ffn_embed_dim: int = 4096, encoder_attention_heads: int = 16, extracted: bool = False, pretrain: bool = True, modality_dropout: float = 0.0, audio_dropout: float = 0.0, noise_augmentation: bool = False, noise_path: str = './data/babble_noise.pt', max_noise_weight: float = 0.5, audio_only: bool = False)\",\"Bases: AbsEncoder\",\"FairSeq AVHubert pretrained encoder module\",\"Parameters:\",\"input_size – input dim\",\"avhubert_url – download link for pre-trained avhubert model\",\"avhubert_dir_path – dir_path for downloading pre-trained avhubert model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Dict[str, Tensor], ilens: Tensor, prev_states: Tensor | None = None)\",\"Forward AVHubert Encoder.\",\"Parameters:\",\"xs_pad**[video]** – input tensor (B, 1, L, H, W)\",\"xs_pad**[audio]** – input tensor (B, D, L)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"forward_fusion(xs_pad: Dict[str, Tensor])\",\"output_size()\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1180\":{\"h\":\"espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder\",\"t\":[\"class espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder(input_size: int, hubert_url: str = './', hubert_dir_path: str = './', output_size: int = 256, normalize_before: bool = False, freeze_finetune_updates: int = 0, dropout_rate: float = 0.0, activation_dropout: float = 0.1, attention_dropout: float = 0.0, mask_length: int = 10, mask_prob: float = 0.75, mask_selection: str = 'static', mask_other: int = 0, apply_mask: bool = True, mask_channel_length: int = 64, mask_channel_prob: float = 0.5, mask_channel_other: int = 0, mask_channel_selection: str = 'static', layerdrop: float = 0.1, feature_grad_mult: float = 0.0)\",\"Bases: AbsEncoder\",\"FairSeq Hubert encoder module, used for loading pretrained weight and finetuning\",\"Parameters:\",\"input_size – input dim\",\"hubert_url – url to Hubert pretrained model\",\"hubert_dir_path – directory to download the Wav2Vec2.0 pretrained model.\",\"output_size – dimension of attention\",\"normalize_before – whether to use layer_norm before the first block\",\"freeze_finetune_updates – steps that freeze all layers except output layer before tuning the whole model (nessasary to prevent overfit).\",\"dropout_rate – dropout rate\",\"activation_dropout – dropout rate in activation function\",\"attention_dropout – dropout rate in attention\",\"Hubert specific Args: : Please refer to: https://github.com/pytorch/fairseq/blob/master/fairseq/models/hubert/hubert.py\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Forward Hubert ASR Encoder.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1181\":{\"h\":\"espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder\",\"t\":[\"class espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder(input_size: int = 1, output_size: int = 1024, linear_units: int = 1024, attention_heads: int = 12, num_blocks: int = 12, dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0, activation_dropout_rate: float = 0.0, hubert_dict: str = './dict.txt', label_rate: int = 100, checkpoint_activations: bool = False, sample_rate: int = 16000, use_amp: bool = False, **kwargs)\",\"Bases: AbsEncoder\",\"FairSeq Hubert pretrain encoder module, only used for pretraining stage\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"linear_units – dimension of feedforward layers\",\"attention_heads – the number of heads of multi head attention\",\"num_blocks – the number of encoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"hubert_dict – target dictionary for Hubert pretraining\",\"label_rate – label frame rate. -1 for sequence label\",\"sample_rate – target sample rate.\",\"use_amp – whether to use automatic mixed precision\",\"normalize_before – whether to use layer_norm before the first block\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"cast_mask_emb()\",\"forward(xs_pad: Tensor, ilens: Tensor, ys_pad: Tensor, ys_pad_length: Tensor, prev_states: Tensor | None = None)\",\"Forward Hubert Pretrain Encoder.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1182\":{\"h\":\"espnet2.asr.layers.fastformer.FastSelfAttention\",\"t\":[\"class espnet2.asr.layers.fastformer.FastSelfAttention(size, attention_heads, dropout_rate)\",\"Bases: Module\",\"Fast self-attention used in Fastformer.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"espnet_initialization_fn()\",\"forward(xs_pad, mask)\",\"Forward method.\",\"Parameters:\",\"xs_pad – (batch, time, size = n_heads * attn_dim)\",\"mask – (batch, 1, time), nonpadding is 1, padding is 0\",\"Returns: (batch, time, size)\",\"Return type: torch.Tensor\",\"init_weights(module)\",\"training : bool\",\"transpose_for_scores(x)\",\"Reshape and transpose to compute scores.\",\"Parameters:x – (batch, time, size = n_heads * attn_dim)\",\"Returns: (batch, n_heads, time, attn_dim)\"]},\"1183\":{\"h\":\"espnet2.asr.state_spaces.residual.Feedforward\",\"t\":[\"class espnet2.asr.state_spaces.residual.Feedforward(*args)\",\"Bases: Residual\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"training : bool\"]},\"1184\":{\"h\":\"espnet2.asr.frontend.fused.FusedFrontends\",\"t\":[\"<!-- _espnet2.asr.frontend.fused.FusedFrontends -->\",\"class espnet2.asr.frontend.fused.FusedFrontends(frontends=None, align_method='linear_projection', proj_dim=100, fs=16000)\",\"Bases: AbsFrontend\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1185\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"1186\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt.GPURNNT\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt.GPURNNT(minibatch: int, maxT: int, maxU: int, alphabet_size: int, workspace, blank: int, fastemit_lambda: float, clamp: float, num_threads: int, stream)\",\"Bases: object\",\"Helper class to launch the CUDA Kernels to compute the Transducer Loss.\",\"Parameters:\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"workspace – An allocated chunk of memory that will be sliced off and reshaped into required blocks used as working memory.\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of OMP threads to launch.\",\"stream – Numba Cuda Stream.\",\"compute_cost_and_score(acts: Tensor, grads: Tensor | None, costs: Tensor, labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\",\"Compute both the loss and the gradients.\",\"Parameters:\",\"acts – A flattened tensor of shape [B, T, U, V+1] representing the activation matrix.\",\"grad – A flattented zero tensor of same shape as acts.\",\"costs – A zero vector of length B which will be updated inplace with the log probability costs.\",\"flat_labels – A flattened matrix of labels of shape [B, U]\",\"label_lengths – A vector of length B that contains the original lengths of the acoustic sequence.\",\"input_lengths – A vector of length B that contains the original lengths of the target sequence.\",\"Updates: : This will launch kernels that will update inline the following variables:\",\"grads: Gradients of the activation matrix wrt the costs vector.\",\"costs: Negative log likelihood of the forward variable.\",\"Returns: An enum that either represents a successful RNNT operation or failure.\",\"cost_and_grad(acts: Tensor, grads: Tensor, costs: Tensor, pad_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\",\"log_softmax(acts: Tensor, denom: Tensor)\",\"Computes the log softmax denominator of the input activation tensor\",\"and stores the result in denom.\",\"Parameters:\",\"acts – Activation tensor of shape [B, T, U, V+1]. The input must be represented as a flat tensor of shape [B * T * U * (V+1)] to allow pointer indexing.\",\"denom – A zero tensor of same shape as acts.\",\"Updates: : This kernel inplace updates the denom tensor\",\"score_forward(acts: Tensor, costs: Tensor, pad_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\"]},\"1187\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.GradMultiply\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.GradMultiply(*args, **kwargs)\",\"Bases: Function\",\"static backward(ctx, grad)\",\"Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).\",\"This function is to be overridden by all subclasses.\",\"It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.\",\"The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output.\",\"static forward(ctx, x, scale)\",\"Performs the operation.\",\"This function is to be overridden by all subclasses.\",\"It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).\",\"The context can be used to store arbitrary data that can be then retrieved during the backward pass. Tensors should not be stored directly on ctx (though this is not currently enforced for backward compatibility). Instead, tensors should be saved either with ctx.save_for_backward() if they are intended to be used in backward (equivalently, vjp) or ctx.save_for_forward() if they are intended to be used for in jvp.\"]},\"1188\":{\"h\":\"espnet2.asr.state_spaces.residual.Highway\",\"t\":[\"<!-- _espnet2.asr.state_spaces.residual.Highway -->\",\"class espnet2.asr.state_spaces.residual.Highway(*args, scaling_correction=False, elemwise=False)\",\"Bases: Residual\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y, transposed=False)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1189\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1190\":{\"h\":\"espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder\",\"t\":[\"class espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder(vocab_size: int, encoder_output_size: int, model_name_or_path: str, causal_lm: bool = False, prefix: str = '', postfix: str = '')\",\"Bases: AbsDecoder, BatchScorerInterface\",\"Hugging Face Transformers Decoder.\",\"Parameters:\",\"encoder_output_size – dimension of encoder attention\",\"model_name_or_path – Hugging Face Transformers model name\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"add_prefix_postfix(enc_out, hlens, ys_in_pad, ys_in_lens)\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor, speech: Tensor | None = None)\",\"Score new token batch (required).\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor)\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input tensor (batch, maxlen_out, #mels)\",\"ys_in_lens – (batch)\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"reload_pretrained_parameters()\",\"score(ys, state, x, speech=None)\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys.\",\"Returns: Tuple of : scores for next token that has a shape of (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"training : bool\"]},\"1191\":{\"h\":\"espnet2.asr.encoder.hugging_face_transformers_encoder.HuggingFaceTransformersEncoder\",\"t\":[\"class espnet2.asr.encoder.hugging_face_transformers_encoder.HuggingFaceTransformersEncoder(input_size: int, model_name_or_path: str, lang_token_id: int = -1)\",\"Bases: AbsEncoder\",\"Hugging Face Transformers PostEncoder.\",\"Initialize the module.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Forward.\",\"output_size()\",\"Get the output size.\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1192\":{\"h\":\"espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder\",\"t\":[\"class espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder(input_size: int, model_name_or_path: str, length_adaptor_n_layers: int = 0, lang_token_id: int = -1)\",\"Bases: AbsPostEncoder\",\"Hugging Face Transformers PostEncoder.\",\"Initialize the module.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Forward.\",\"output_size()\",\"Get the output size.\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1193\":{\"h\":\"espnet2.asr.transducer.beam_search_transducer.Hypothesis\",\"t\":[\"class espnet2.asr.transducer.beam_search_transducer.Hypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor, lm_state: Dict[str, Any] | List[Any] | None = None)\",\"Bases: object\",\"Default hypothesis definition for Transducer search algorithms.\",\"dec_state : Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor\",\"lm_state : Dict[str, Any] | List[Any]= None\",\"score : float\",\"yseq : List[int]\"]},\"1194\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.I_Op\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.I_Op(value)\",\"Bases: Enum\",\"Represents an operation that is performed on the input tensor\",\"EXPONENTIAL = 0\",\"IDENTITY = 1\"]},\"1195\":{\"h\":\"espnet2.asr.postencoder.length_adaptor_postencoder.LengthAdaptorPostEncoder\",\"t\":[\"class espnet2.asr.postencoder.length_adaptor_postencoder.LengthAdaptorPostEncoder(input_size: int, length_adaptor_n_layers: int = 0, input_layer: str | None = None, output_size: int | None = None, dropout_rate: float = 0.1, return_int_enc: bool = False)\",\"Bases: AbsPostEncoder\",\"Length Adaptor PostEncoder.\",\"Initialize the module.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Forward.\",\"output_size()\",\"Get the output size.\",\"training : bool\"]},\"1196\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder\",\"t\":[\"class espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11), conv_usebias: int = False)\",\"Bases: BaseTransformerDecoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"training : bool\"]},\"1197\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder\",\"t\":[\"class espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11), conv_usebias: int = False)\",\"Bases: BaseTransformerDecoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"training : bool\"]},\"1198\":{\"h\":\"espnet2.asr.preencoder.sinc.LightweightSincConvs\",\"t\":[\"class espnet2.asr.preencoder.sinc.LightweightSincConvs(fs: int | str | float = 16000, in_channels: int = 1, out_channels: int = 256, activation_type: str = 'leakyrelu', dropout_type: str = 'dropout', windowing_type: str = 'hamming', scale_type: str = 'mel')\",\"Bases: AbsPreEncoder\",\"Lightweight Sinc Convolutions.\",\"Instead of using precomputed features, end-to-end speech recognition can also be done directly from raw audio using sinc convolutions, as described in “Lightweight End-to-End Speech Recognition from Raw Audio Data Using Sinc-Convolutions” by Kürzinger et al. https://arxiv.org/abs/2010.07597\",\"To use Sinc convolutions in your model instead of the default f-bank frontend, set this module as your pre-encoder with preencoder: sinc and use the input of the sliding window frontend with frontend: sliding_window in your yaml configuration file. So that the process flow is:\",\"Frontend (SlidingWindow) -> SpecAug -> Normalization -> Pre-encoder (LightweightSincConvs) -> Encoder -> Decoder\",\"Note that this method also performs data augmentation in time domain (vs. in spectral domain in the default frontend). Use plot_sinc_filters.py to visualize the learned Sinc filters.\",\"Initialize the module.\",\"Parameters:\",\"fs – Sample rate.\",\"in_channels – Number of input channels.\",\"out_channels – Number of output channels (for each input channel).\",\"activation_type – Choice of activation function.\",\"dropout_type – Choice of dropout function.\",\"windowing_type – Choice of windowing function.\",\"scale_type – Choice of filter-bank initialization scale.\",\"espnet_initialization_fn()\",\"Initialize sinc filters with filterbank values.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Apply Lightweight Sinc Convolutions.\",\"The input shall be formatted as (B, T, C_in, D_in) with B as batch size, T as time dimension, C_in as channels, and D_in as feature dimension.\",\"The output will then be (B, T, C_out*D_out) with C_out and D_out as output dimensions.\",\"The current module structure only handles D_in=400, so that D_out=1. Remark for the multichannel case: C_out is the number of out_channels given at initialization multiplied with C_in.\",\"gen_lsc_block(in_channels: int, out_channels: int, depthwise_kernel_size: int = 9, depthwise_stride: int = 1, depthwise_groups=None, pointwise_groups=0, dropout_probability: float = 0.15, avgpool=False)\",\"Generate a convolutional block for Lightweight Sinc convolutions.\",\"Each block consists of either a depthwise or a depthwise-separable convolutions together with dropout, (batch-)normalization layer, and an optional average-pooling layer.\",\"Parameters:\",\"in_channels – Number of input channels.\",\"out_channels – Number of output channels.\",\"depthwise_kernel_size – Kernel size of the depthwise convolution.\",\"depthwise_stride – Stride of the depthwise convolution.\",\"depthwise_groups – Number of groups of the depthwise convolution.\",\"pointwise_groups – Number of groups of the pointwise convolution.\",\"dropout_probability – Dropout probability in the block.\",\"avgpool – If True, an AvgPool layer is inserted.\",\"Returns: Neural network building block.\",\"Return type: torch.nn.Sequential\",\"output_size()\",\"Get the output size.\",\"training : bool\"]},\"1199\":{\"h\":\"espnet2.asr.state_spaces.components.LinearActivation\",\"t\":[\"espnet2.asr.state_spaces.components.LinearActivation(d_input, d_output, bias=True, zero_bias_init=False, transposed=False, initializer=None, activation=None, activate=False, weight_norm=False, **kwargs)\",\"Return a linear module, initialization, and activation.\"]},\"1200\":{\"h\":\"espnet2.asr.encoder.linear_encoder.LinearEncoder\",\"t\":[\"class espnet2.asr.encoder.linear_encoder.LinearEncoder(input_size: int, output_size: int = 256, dropout_rate: float = 0.1, input_layer: str | None = 'conv2d', normalize_before: bool = True, padding_idx: int = -1)\",\"Bases: AbsEncoder\",\"Linear encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"linear_units – the number of units of position-wise feed forward\",\"dropout_rate – dropout rate\",\"input_layer – input layer type\",\"normalize_before – whether to use layer_norm before the first block\",\"padding_idx – padding_idx for input_layer=embed\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"training : bool\"]},\"1201\":{\"h\":\"espnet2.asr.preencoder.linear.LinearProjection\",\"t\":[\"class espnet2.asr.preencoder.linear.LinearProjection(input_size: int, output_size: int, dropout: float = 0.0)\",\"Bases: AbsPreEncoder\",\"Linear Projection Preencoder.\",\"Initialize the module.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Forward.\",\"output_size()\",\"Get the output size.\",\"training : bool\"]},\"1202\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.LogSoftmaxGradModification\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.LogSoftmaxGradModification(*args, **kwargs)\",\"Bases: Function\",\"static backward(ctx, grad_output)\",\"Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).\",\"This function is to be overridden by all subclasses.\",\"It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.\",\"The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output.\",\"static forward(ctx, acts, clamp)\",\"Performs the operation.\",\"This function is to be overridden by all subclasses.\",\"It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).\",\"The context can be used to store arbitrary data that can be then retrieved during the backward pass. Tensors should not be stored directly on ctx (though this is not currently enforced for backward compatibility). Instead, tensors should be saved either with ctx.save_for_backward() if they are intended to be used in backward (equivalently, vjp) or ctx.save_for_forward() if they are intended to be used for in jvp.\"]},\"1203\":{\"h\":\"espnet2.asr.encoder.longformer_encoder.LongformerEncoder\",\"t\":[\"class espnet2.asr.encoder.longformer_encoder.LongformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'abs_pos', selfattention_layer_type: str = 'lf_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, attention_windows: list = [100, 100, 100, 100, 100, 100], attention_dilation: list = [1, 1, 1, 1, 1, 1], attention_mode: str = 'sliding_chunks')\",\"Bases: ConformerEncoder\",\"Longformer SA Conformer encoder module.\",\"Parameters:\",\"input_size (int) – Input dimension.\",\"output_size (int) – Dimension of attention.\",\"attention_heads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"num_blocks (int) – The number of decoder blocks.\",\"dropout_rate (float) – Dropout rate.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. If True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) If False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"rel_pos_type (str) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"encoder_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"encoder_attn_layer_type (str) – Encoder attention layer type.\",\"activation_type (str) – Encoder activation function type.\",\"macaron_style (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_module (bool) – Whether to use convolution module.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"cnn_module_kernel (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"attention_windows (list) – Layer-wise attention window sizes for longformer self-attn\",\"attention_dilation (list) – Layer-wise attention dilation sizes for longformer self-attn\",\"attention_mode (str) – Implementation for longformer self-attn. Default=”sliding_chunks” Choose ‘n2’, ‘tvm’ or ‘sliding_chunks’. More details in https://github.com/allenai/longformer\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, ctc: CTC | None = None, return_all_hs: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"ctc (CTC) – ctc module for intermediate CTC loss\",\"return_all_hs (bool) – whether to return all hidden states\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size()\",\"training : bool\"]},\"1204\":{\"h\":\"espnet2.asr.decoder.mlm_decoder.MLMDecoder\",\"t\":[\"<!-- _espnet2.asr.decoder.mlm_decoder.MLMDecoder -->\",\"class espnet2.asr.decoder.mlm_decoder.MLMDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False)\",\"Bases: AbsDecoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor)\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"Returns: tuple containing: x: decoded token score before softmax (batch, maxlen_out, token)\",\"if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"training : bool\"]},\"1205\":{\"h\":\"espnet2.asr.maskctc_model.MaskCTCInference\",\"t\":[\"<!-- _espnet2.asr.maskctc_model.MaskCTCInference -->\",\"class espnet2.asr.maskctc_model.MaskCTCInference(asr_model: MaskCTCModel, n_iterations: int, threshold_probability: float)\",\"Bases: Module\",\"Mask-CTC-based non-autoregressive inference\",\"Initialize Mask-CTC inference\",\"forward(enc_out: Tensor)\",\"Perform Mask-CTC inference\",\"ids2text(ids: List[int])\",\"training : bool\"]},\"1206\":{\"h\":\"espnet2.asr.maskctc_model.MaskCTCModel\",\"t\":[\"<!-- _espnet2.asr.maskctc_model.MaskCTCModel -->\",\"class espnet2.asr.maskctc_model.MaskCTCModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: MLMDecoder, ctc: CTC, joint_network: Module | None = None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', sym_mask: str = '<mask>', extract_feats_in_collect_stats: bool = True)\",\"Bases: ESPnetASRModel\",\"Hybrid CTC/Masked LM Encoder-Decoder model (Mask-CTC)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batchify_nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor, batch_size: int = 100)\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param encoder_out: (Batch, Length, Dim) :param encoder_out_lens: (Batch,) :param ys_pad: (Batch, Length) :param ys_pad_lens: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase GPU memory usage\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor)\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"Normally, this function is called in batchify_nll.\",\"Parameters:\",\"encoder_out – (Batch, Length, Dim)\",\"encoder_out_lens – (Batch,)\",\"ys_pad – (Batch, Length)\",\"ys_pad_lens – (Batch,)\",\"training : bool\"]},\"1207\":{\"h\":\"espnet2.asr.frontend.melspec_torch.MelSpectrogramTorch\",\"t\":[\"class espnet2.asr.frontend.melspec_torch.MelSpectrogramTorch(preemp: bool = True, n_fft: int = 512, log: bool = False, win_length: int = 400, hop_length: int = 160, f_min: int = 20, f_max: int = 7600, n_mels: int = 80, window_fn: str = 'hamming', mel_scale: str = 'htk', normalize: str | None = None)\",\"Bases: AbsFrontend\",\"Mel-Spectrogram using Torchaudio Implementation.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_length: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1208\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"Return output length of feature dimension D.\",\"training : bool\"]},\"1209\":{\"h\":\"espnet2.asr.state_spaces.attention.MultiHeadedAttention\",\"t\":[\"class espnet2.asr.state_spaces.attention.MultiHeadedAttention(n_feat, n_head, dropout=0.0, transposed=False, **kwargs)\",\"Bases: SequenceModule\",\"Multi-Head Attention layer inheriting SequenceModule.\",\"Comparing default MHA module in ESPnet, this module returns additional dummy state and has step function for autoregressive inference.\",\"Parameters:\",\"n_head (int) – The number of heads.\",\"n_feat (int) – The number of features.\",\"dropout_rate (float) – Dropout rate.\",\"Construct an MultiHeadedAttention object.\",\"forward(query, memory=None, mask=None, *args, **kwargs)\",\"Compute scaled dot product attention.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"forward_attention(value, scores, mask)\",\"Compute attention context vector.\",\"Parameters:\",\"value (torch.Tensor) – Transformed value (#batch, n_head, time2, d_k).\",\"scores (torch.Tensor) – Attention score (#batch, n_head, time1, time2).\",\"mask (torch.Tensor) – Mask (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Transformed value (#batch, time1, d_model) : weighted by the attention score (#batch, time1, time2).\",\"Return type: torch.Tensor\",\"forward_qkv(query, key, value)\",\"Transform query, key and value.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"Returns: Transformed query tensor (#batch, n_head, time1, d_k). torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k). torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\",\"Return type: torch.Tensor\",\"step(query, state, memory=None, mask=None, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1210\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt.MultiblankGPURNNT\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt.MultiblankGPURNNT(sigma: float, num_big_blanks: int, minibatch: int, maxT: int, maxU: int, alphabet_size: int, workspace, big_blank_workspace, blank: int, fastemit_lambda: float, clamp: float, num_threads: int, stream)\",\"Bases: GPURNNT\",\"Helper class to launch the CUDA Kernels to compute Multi-blank\",\"Transducer Loss(https://arxiv.org/pdf/2211.03541).\",\"Parameters:\",\"sigma – Hyper-parameter related to the logit-normalization method in training multi-blank transducers.\",\"num_big_blanks – Number of big blank symbols the model has. This should not include the standard blank symbol.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V + 1 + num-big-blanks\",\"workspace – An allocated chunk of memory that will be sliced off and reshaped into required blocks used as working memory.\",\"big_blank_workspace – An allocated chunk of memory that will be sliced off and reshaped into required blocks used as working memory specifically for the multi-blank related computations.\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of OMP threads to launch.\",\"stream – Numba Cuda Stream.\",\"compute_cost_and_score(acts: Tensor, grads: Tensor | None, costs: Tensor, labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\",\"Compute both the loss and the gradients.\",\"Parameters:\",\"acts – A flattened tensor of shape [B, T, U, V+1] representing the activation matrix.\",\"grad – A flattented zero tensor of same shape as acts.\",\"costs – A zero vector of length B which will be updated inplace with the log probability costs.\",\"flat_labels – A flattened matrix of labels of shape [B, U]\",\"label_lengths – A vector of length B that contains the original lengths of the acoustic sequence.\",\"input_lengths – A vector of length B that contains the original lengths of the target sequence.\",\"Updates: : This will launch kernels that will update inline the following variables:\",\"grads: Gradients of the activation matrix wrt the costs vector.\",\"costs: Negative log likelihood of the forward variable.\",\"Returns: An enum that either represents a successful RNNT operation or failure.\",\"cost_and_grad(acts: Tensor, grads: Tensor, costs: Tensor, pad_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\",\"score_forward(acts: Tensor, costs: Tensor, pad_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\"]},\"1211\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.MultiblankRNNTLossNumba\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.MultiblankRNNTLossNumba(blank, big_blank_durations, reduction='mean', fastemit_lambda: float = 0.0, clamp: float = -1, sigma: float = 0.0)\",\"Bases: Module\",\"Multiblank RNNT Loss Numba\",\"Parameters:\",\"blank (int) – standard blank label.\",\"big_blank_durations – list of durations for multi-blank transducer, e.g. [2, 4, 8].\",\"sigma – hyper-parameter for logit under-normalization method for training multi-blank transducers. Recommended value 0.05.\",\"https (Refer to) – //arxiv.org/pdf/2211.03541 for detailed explanations for the above parameters;\",\"reduction (string,optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(acts, labels, act_lens, label_lens)\",\"MultiblankRNNTLossNumba Forward.\",\"log_probs: Tensor of (batch x seqLength x labelLength x outputDim) : containing output from network\",\"labels: 2 dimensional Tensor containing all the targets of : the batch with zero padded\",\"act_lens: Tensor of size (batch) containing size of each output : sequence from the network\",\"label_lens: Tensor of (batch) containing label length of each example\",\"training : bool\"]},\"1212\":{\"h\":\"espnet2.asr.state_spaces.components.Normalization\",\"t\":[\"class espnet2.asr.state_spaces.components.Normalization(d, transposed=False, _name_='layer', **kwargs)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1213\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"step(x, **kwargs)\",\"training : bool\"]},\"1214\":{\"h\":\"espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder\",\"t\":[\"class espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder(vocab_size: int, encoder_output_size: int, dropout_rate: float = 0.0, whisper_model: str = 'small', download_dir: str | None = None, load_origin_token_embedding=False)\",\"Bases: AbsDecoder, BatchScorerInterface\",\"Transformer-based Speech-to-Text Decoder from OpenAI’s Whisper Model:\",\"URL: https://github.com/openai/whisper\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor)\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"forward_one_step(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, *, cache: List[Tensor] | None = None)\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (batch, maxlen_out)\",\"tgt_mask – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"NOTE (Shih-Lun): : cache implementation is ignored for now for simplicity & correctness\",\"score(ys, state, x)\",\"Score.\",\"training : bool\"]},\"1215\":{\"h\":\"espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder\",\"t\":[\"class espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder(input_size: int = 1, dropout_rate: float = 0.0, whisper_model: str = 'small', download_dir: str | None = None, use_specaug: bool = False, specaug_conf: dict | None = None, do_pad_trim: bool = False)\",\"Bases: AbsEncoder\",\"Transformer-based Speech Encoder from OpenAI’s Whisper Model:\",\"URL: https://github.com/openai/whisper\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1216\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"log_mel_spectrogram(audio: Tensor, ilens: Tensor | None = None)\",\"Use log-mel spectrogram computation native to Whisper training\",\"output_size()\",\"pad_or_trim(array: Tensor, length: int, axis: int = -1)\",\"Pad or trim the audio array to N_SAMPLES.\",\"Used in zero-shot inference cases.\",\"training : bool\",\"whisper_encode(input: Tensor, ilens: Tensor | None = None)\"]},\"1217\":{\"h\":\"espnet2.asr.state_spaces.s4.OptimModule\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.OptimModule -->\",\"class espnet2.asr.state_spaces.s4.OptimModule\",\"Bases: Module\",\"Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters. # noqa\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"register(name, tensor, lr=None)\",\"Register a tensor with a configurable learning rate and 0 weight decay.\",\"training : bool\"]},\"1218\":{\"h\":\"espnet2.asr.pit_espnet_model.PITLossWrapper\",\"t\":[\"class espnet2.asr.pit_espnet_model.PITLossWrapper(criterion_fn: Callable, num_ref: int)\",\"Bases: AbsLossWrapper\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inf: Tensor, inf_lens: Tensor, ref: Tensor, ref_lens: Tensor, others: Dict | None = None)\",\"PITLoss Wrapper function. Similar to espnet2/enh/loss/wrapper/pit_solver.py\",\"Parameters:\",\"inf – Iterable[torch.Tensor], (batch, num_inf, …)\",\"inf_lens – Iterable[torch.Tensor], (batch, num_inf, …)\",\"ref – Iterable[torch.Tensor], (batch, num_ref, …)\",\"ref_lens – Iterable[torch.Tensor], (batch, num_ref, …)\",\"permute_inf – If true, permute the inference and inference_lens according to the optimal permutation.\",\"classmethod permutate(perm, *args)\",\"training : bool\"]},\"1219\":{\"h\":\"espnet2.asr.partially_AR_model.PartiallyARInference\",\"t\":[\"class espnet2.asr.partially_AR_model.PartiallyARInference(ctc: CTC, decoder: AbsDecoder, threshold_probability: float, sos: int | None = None, eos: int | None = None, mask_token: int | None = None, token_list: List[int] | None = None, scorers: Dict[str, ScorerInterface] | None = None, weights: Dict[str, float] | None = None, beam_size: int = 10, max_seq_len: int = 5, max_mask_parallel: int = -1)\",\"Bases: Module\",\"Mask-CTC-based partially autoregressive inference\",\"Initialize Mask-CTC inference\",\"forward(enc_out: Tensor, *args, **kwargs)\",\"Perform Semi-AR inference\",\"set_hyp_primer(primer: List[int])\",\"training : bool\"]},\"1220\":{\"h\":\"espnet2.asr.decoder.rnn_decoder.RNNDecoder\",\"t\":[\"<!-- _espnet2.asr.decoder.rnn_decoder.RNNDecoder -->\",\"class espnet2.asr.decoder.rnn_decoder.RNNDecoder(vocab_size: int, encoder_output_size: int, rnn_type: str = 'lstm', num_layers: int = 1, hidden_size: int = 320, sampling_probability: float = 0.0, dropout: float = 0.0, context_residual: bool = False, replace_sos: bool = False, num_encs: int = 1, att_conf: dict = {'aconv_chans': 10, 'aconv_filts': 100, 'adim': 320, 'aheads': 4, 'atype': 'location', 'awin': 5, 'han_conv_chans': -1, 'han_conv_filts': 100, 'han_dim': 320, 'han_heads': 4, 'han_mode': False, 'han_type': None, 'han_win': 5, 'num_att': 1, 'num_encs': 1})\",\"Bases: AbsDecoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(hs_pad, hlens, ys_in_pad, ys_in_lens, strm_idx=0)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1221\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"init_state(x)\",\"Get an initial state for decoding (optional).\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"rnn_forward(ey, z_list, c_list, z_prev, c_prev)\",\"score(yseq, state, x)\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys.\",\"Returns: Tuple of : scores for next token that has a shape of (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"training : bool\",\"zero_state(hs_pad)\"]},\"1222\":{\"h\":\"espnet2.asr.encoder.rnn_encoder.RNNEncoder\",\"t\":[\"<!-- _espnet2.asr.encoder.rnn_encoder.RNNEncoder -->\",\"class espnet2.asr.encoder.rnn_encoder.RNNEncoder(input_size: int, rnn_type: str = 'lstm', bidirectional: bool = True, use_projection: bool = True, num_layers: int = 4, hidden_size: int = 320, output_size: int = 320, dropout: float = 0.0, subsample: Sequence[int] | None = (2, 2, 1, 1))\",\"Bases: AbsEncoder\",\"RNNEncoder class.\",\"Parameters:\",\"input_size – The number of expected features in the input\",\"output_size – The number of output features\",\"hidden_size – The number of hidden features\",\"bidirectional – If True becomes a bidirectional LSTM\",\"use_projection – Use projection layer or not\",\"num_layers – Number of recurrent layers\",\"dropout – dropout probability\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1223\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"1224\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.RNNTLossNumba\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.RNNTLossNumba(blank=0, reduction='mean', fastemit_lambda: float = 0.0, clamp: float = -1)\",\"Bases: Module\",\"RNNT Loss Numba\",\"Parameters:\",\"blank (int,optional) – blank label. Default: 0.\",\"reduction (string,optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(acts, labels, act_lens, label_lens)\",\"Forward RNNTLossNumba.\",\"log_probs: Tensor of (batch x seqLength x labelLength x outputDim) : containing output from network\",\"labels: 2 dimensional Tensor containing all the targets of the : batch with zero padded\",\"act_lens: Tensor of size (batch) containing size of each output : sequence from the network\",\"label_lens: Tensor of (batch) containing label length of each example\",\"training : bool\"]},\"1225\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.RNNTStatus\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.RNNTStatus(value)\",\"Bases: Enum\",\"An enumeration.\",\"RNNT_STATUS_INVALID_VALUE = 1\",\"RNNT_STATUS_SUCCESS = 0\"]},\"1226\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.R_Op\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.R_Op(value)\",\"Bases: Enum\",\"Represents a reduction operation performed on the input tensor\",\"ADD = 0\",\"MAXIMUM = 1\"]},\"1227\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.ReduceHelper\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.ReduceHelper(I_opid: int, R_opid: int, acts: Tensor, output: Tensor, num_rows: int, num_cols: int, minus: bool, stream)\",\"CUDA Warp reduction kernel helper which reduces via the R_Op.Add and writes\",\"the result to output according to I_op id.\",\"The result is stored in the blockIdx.\"]},\"1228\":{\"h\":\"NOTE\",\"t\":[\"Efficient warp occurs at input shapes of 2 ^ K.\",\"References\",\"Warp Primitives [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/]\",\"Parameters:\",\"I_opid – Operator ID for input. See I_Op for more information.\",\"R_opid – Operator ID for reduction. See R_Op for more information.\",\"acts – Flatened activation matrix of shape [B * T * U * (V+1)].\",\"output – Flatened output matrix of shape [B * T * U * (V+1)]. Data will be overwritten.\",\"num_rows – Vocabulary size (including blank token) - V+1. Represents the number of threads per block.\",\"num_cols – Flattened shape of activation matrix, without vocabulary dimension (B * T * U). Represents number of blocks per grid.\",\"minus – Bool flag whether to add or subtract as reduction. If minus is set; calls _reduce_minus, else calls _reduce_rows kernel.\",\"stream – CUDA Stream.\"]},\"1229\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.ResEncoder\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.ResEncoder(relu_type, weights)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1230\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"threeD_to_2D_tensor(x)\",\"training : bool\"]},\"1231\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.ResNet\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.ResNet(block, layers, num_classes=1000, relu_type='relu', gamma_zero=False, avg_pool_downsample=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1232\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1233\":{\"h\":\"espnet2.asr.state_spaces.residual.Residual\",\"t\":[\"<!-- _espnet2.asr.state_spaces.residual.Residual -->\",\"class espnet2.asr.state_spaces.residual.Residual(i_layer, d_input, d_model, alpha=1.0, beta=1.0)\",\"Bases: Module\",\"Residual connection with constant affine weights.\",\"Can simulate standard residual, no residual, and “constant gates”.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"forward(x, y, transposed)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1234\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1235\":{\"h\":\"espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dInput\",\"t\":[\"class espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dInput(d, transposed=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1236\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1237\":{\"h\":\"espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dOutput\",\"t\":[\"class espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dOutput(norm_input)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1238\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1239\":{\"h\":\"espnet2.asr.frontend.s3prl.S3prlFrontend\",\"t\":[\"<!-- _espnet2.asr.frontend.s3prl.S3prlFrontend -->\",\"class espnet2.asr.frontend.s3prl.S3prlFrontend(fs: int | str = 16000, frontend_conf: dict | None = {'badim': 320, 'bdropout_rate': 0.0, 'blayers': 3, 'bnmask': 2, 'bprojs': 320, 'btype': 'blstmp', 'bunits': 300, 'delay': 3, 'ref_channel': -1, 'taps': 5, 'use_beamformer': False, 'use_dnn_mask_for_wpe': True, 'use_wpe': False, 'wdropout_rate': 0.0, 'wlayers': 3, 'wprojs': 320, 'wtype': 'blstmp', 'wunits': 300}, download_dir: str | None = None, multilayer_feature: bool = False, layer: int = -1)\",\"Bases: AbsFrontend\",\"Speech Pretrained Representation frontend structure for ASR.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1240\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1241\":{\"h\":\"espnet2.asr.state_spaces.s4.S4\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.S4 -->\",\"class espnet2.asr.state_spaces.s4.S4(d_model, d_state=64, l_max=None, channels=1, bidirectional=False, activation='gelu', postact='glu', hyper_act=None, dropout=0.0, tie_dropout=False, bottleneck=None, gate=None, transposed=True, verbose=False, **kernel_args)\",\"Bases: Module\",\"Initialize S4 module.\",\"d_state: the dimension of the state, also denoted by N l_max: the maximum kernel length, also denoted by L.\",\"Set l_max=None to always use a global kernel\",\"channels: can be interpreted as a number of “heads”; : the SSM is a map from a 1-dim to C-dim sequence. It’s not recommended to change this unless desperate for things to tune; instead, increase d_model for larger models\",\"bidirectional: if True, convolution kernel will be two-sided\"]},\"1242\":{\"h\":\"Position-wise feedforward components:\",\"t\":[\"activation: activation in between SS and FF postact: activation after FF hyper_act: use a “hypernetwork” multiplication (experimental) dropout: standard dropout argument. tie_dropout=True ties the dropout\",\"mask across the sequence length, emulating nn.Dropout1d\"]},\"1243\":{\"h\":\"Other arguments:\",\"t\":[\"transposed: choose backbone axis ordering of : (B, L, H) (if False) or (B, H, L) (if True) [B=batch size, L=sequence length, H=hidden dimension]\",\"gate: add gated activation (GSS) bottleneck: reduce SSM dimension (GSS)\",\"See the class SSKernel for the kernel constructor which accepts kernel_args. Relevant options that are worth considering and tuning include “mode” + “measure”, “dt_min”, “dt_max”, “lr”\",\"Other options are all experimental and should not need to be configured\",\"property d_output\",\"default_state(*batch_shape, device=None)\",\"forward(u, state=None, rate=1.0, lengths=None, **kwargs)\",\"Forward pass.\",\"u: (B H L) if self.transposed else (B L H) state: (H N) never needed unless you know what you’re doing\",\"Returns: same shape as u\",\"setup_step(**kwargs)\",\"step(u, state, **kwargs)\",\"Step one time step as a recurrent model.\",\"Intended to be used during validation.\",\"u: (B H) state: (B H N) Returns: output (B H), state (B H N)\",\"training : bool\"]},\"1244\":{\"h\":\"espnet2.asr.decoder.s4_decoder.S4Decoder\",\"t\":[\"<!-- _espnet2.asr.decoder.s4_decoder.S4Decoder -->\",\"class espnet2.asr.decoder.s4_decoder.S4Decoder(vocab_size: int, encoder_output_size: int, input_layer: str = 'embed', dropinp: float = 0.0, dropout: float = 0.25, prenorm: bool = True, n_layers: int = 16, transposed: bool = False, tie_dropout: bool = False, n_repeat=1, layer=None, residual=None, norm=None, pool=None, track_norms=True, drop_path: float = 0.0)\",\"Bases: AbsDecoder, BatchScorerInterface\",\"S4 decoder module.\",\"Parameters:\",\"vocab_size – output dim\",\"encoder_output_size – dimension of hidden vector\",\"input_layer – input layer type\",\"dropinp – input dropout\",\"dropout – dropout parameter applied on every residual and every layer\",\"prenorm – pre-norm vs. post-norm\",\"n_layers – number of layers\",\"transposed – transpose inputs so each layer receives (batch, dim, length)\",\"tie_dropout – tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d\",\"n_repeat – each layer is repeated n times per stage before applying pooling\",\"layer – layer config, must be specified\",\"residual – residual config\",\"norm – normalization config (e.g. layer vs batch)\",\"pool – config for pooling layer per stage\",\"track_norms – log norms of each layer output\",\"drop_path – drop rate for stochastic depth\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor, state=None)\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"init_state(x: Tensor)\",\"Initialize state.\",\"score(ys, state, x)\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys.\",\"Returns: Tuple of : scores for next token that has a shape of (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"training : bool\"]},\"1245\":{\"h\":\"espnet2.asr.state_spaces.s4.SSKernel\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.SSKernel -->\",\"class espnet2.asr.state_spaces.s4.SSKernel(H, N=64, L=None, measure='legs', rank=1, channels=1, dt_min=0.001, dt_max=0.1, deterministic=False, lr=None, mode='nplr', n_ssm=None, verbose=False, measure_args={}, **kernel_args)\",\"Bases: Module\",\"Wrapper around SSKernel parameterizations.\",\"The SSKernel is expected to support the interface forward() default_state() _setup_step() step()\",\"State Space Kernel which computes the convolution kernel $\\\\bar{K}$.\",\"H: Number of independent SSM copies; : controls the size of the model. Also called d_model in the config.\",\"N: State size (dimensionality of parameters A, B, C). : Also called d_state in the config. Generally shouldn’t need to be adjusted and doens’t affect speed much.\",\"L: Maximum length of convolution kernel, if known. : Should work in the majority of cases even if not known.\",\"measure: Options for initialization of (A, B). : For NPLR mode, recommendations are “legs”, “fout”, “hippo” (combination of both). For Diag mode, recommendations are “diag-inv”, “diag-lin”, “diag-legs”, and “diag” (combination of diag-inv and diag-lin)\",\"rank: Rank of low-rank correction for NPLR mode. : Needs to be increased for measure “legt”\",\"channels: C channels turns the SSM from a 1-dim to C-dim map; : can think of it having C separate “heads” per SSM. This was partly a feature to make it easier to implement bidirectionality; it is recommended to set channels=1 and adjust H to control parameters instead\",\"dt_min, dt_max: min and max values for the step size dt (Delta) mode: Which kernel algorithm to use. ‘nplr’ is the full S4 model;\",\"‘diag’ is the simpler S4D; ‘slow’ is a dense version for testing\",\"n_ssm: Number of independent trainable (A, B) SSMs, : e.g. n_ssm=1 means all A/B parameters are tied across the H different instantiations of C. n_ssm=None means all H SSMs are completely independent. Generally, changing this option can save parameters but doesn’t affect performance or speed much. This parameter must divide H\",\"lr: Passing in a number (e.g. 0.001) sets : attributes of SSM parameers (A, B, dt). A custom optimizer hook is needed to configure the optimizer to set the learning rates appropriately for these parameters.\",\"default_state(*args, **kwargs)\",\"forward(state=None, L=None, rate=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1246\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_state(u, state)\",\"Forward the state through a sequence.\",\"i.e. computes the state after passing chunk through SSM\",\"state: (B, H, N) u: (B, H, L)\",\"Returns: (B, H, N)\",\"step(u, state, **kwargs)\",\"training : bool\"]},\"1247\":{\"h\":\"espnet2.asr.state_spaces.s4.SSKernelDiag\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.SSKernelDiag -->\",\"class espnet2.asr.state_spaces.s4.SSKernelDiag(A, B, C, log_dt, L=None, disc='bilinear', real_type='exp', lr=None, bandlimit=None)\",\"Bases: OptimModule\",\"Version using (complex) diagonal state matrix (S4D).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"default_state(*batch_shape)\",\"forward(L, state=None, rate=1.0, u=None)\",\"Forward pass.\",\"state: (B, H, N) initial state rate: sampling rate factor L: target length\",\"returns: (C, H, L) convolution kernel (generally C=1) (B, H, L) output from initial state\",\"forward_state(u, state)\",\"step(u, state)\",\"training : bool\"]},\"1248\":{\"h\":\"espnet2.asr.state_spaces.s4.SSKernelNPLR\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.SSKernelNPLR -->\",\"class espnet2.asr.state_spaces.s4.SSKernelNPLR(w, P, B, C, log_dt, L=None, lr=None, verbose=False, keops=False, real_type='exp', real_tolerance=0.001, bandlimit=None)\",\"Bases: OptimModule\",\"Stores a representation of and computes the SSKernel function.\",\"K_L(A^dt, B^dt, C) corresponding to a discretized state space, where A is Normal + Low Rank (NPLR)\",\"Initialize kernel.\",\"L: Maximum length; this module computes an SSM kernel of length L A is represented by diag(w) - PP^* w: (S, N) diagonal part P: (R, S, N) low-rank part\",\"B: (S, N) C: (C, H, N) dt: (H) timescale per feature lr: [dict | float | None] hook to set lr of special parameters (A, B, dt)\",\"Dimensions: N (or d_state): state size H (or d_model): total SSM copies S (or n_ssm): number of trainable copies of (A, B, dt); must divide H R (or rank): rank of low-rank part C (or channels): system is 1-dim to C-dim\",\"The forward pass of this Module returns a tensor of shape (C, H, L)\",\"Note: tensor shape N here denotes half the true state size, : because of conjugate symmetry\",\"default_state(*batch_shape)\",\"forward(state=None, rate=1.0, L=None)\",\"Forward pass.\",\"state: (B, H, N) initial state rate: sampling rate factor L: target length\",\"returns: (C, H, L) convolution kernel (generally C=1) (B, H, L) output from initial state\",\"step(u, state)\",\"Step one time step as a recurrent model.\",\"Must have called self._setup_step() and created state with self.default_state() before calling this\",\"training : bool\"]},\"1249\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.SamePad\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.SamePad(kernel_size, causal=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1250\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1251\":{\"h\":\"espnet2.asr.state_spaces.base.SequenceIdentity\",\"t\":[\"class espnet2.asr.state_spaces.base.SequenceIdentity(*args, transposed=False, **kwargs)\",\"Bases: SequenceIdentity\",\"Simple SequenceModule for testing purposes.\",\"Initialize SequenceModule.\",\"d_model: input dimension (sometimes denoted H for hidden dimension) transposed: if True, inputs have axis ordering (B, H, L) instead of (B, H, L)\",\"forward(x, state=None, **kwargs)\",\"Forward pass.\",\"training : bool\"]},\"1252\":{\"h\":\"espnet2.asr.state_spaces.model.SequenceModel\",\"t\":[\"class espnet2.asr.state_spaces.model.SequenceModel(d_model, n_layers=1, transposed=False, dropout=0.0, tie_dropout=False, prenorm=True, n_repeat=1, layer=None, residual=None, norm=None, pool=None, track_norms=True, dropinp=0.0, drop_path=0.0)\",\"Bases: SequenceModule\",\"Isotropic deep sequence model backbone, in the style of ResNets / Transformers.\",\"The SequenceModel class implements a generic (batch, length, d_input) -> (batch, length, d_output) transformation\",\"Parameters:\",\"d_model – Resize input (useful for deep models with residuals)\",\"n_layers – Number of layers\",\"transposed – Transpose inputs so each layer receives (batch, dim, length)\",\"dropout – Dropout parameter applied on every residual and every layer\",\"tie_dropout – Tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d\",\"prenorm – Pre-norm vs. post-norm\",\"n_repeat – Each layer is repeated n times per stage before applying pooling\",\"layer – Layer config, must be specified\",\"residual – Residual config\",\"norm – Normalization config (e.g. layer vs batch)\",\"pool – Config for pooling layer per stage\",\"track_norms – Log norms of each layer output\",\"dropinp – Input dropout\",\"drop_path – Stochastic depth for each residual path\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_state\",\"Return dimension of output of self.state_to_tensor.\",\"default_state(*batch_shape, device=None)\",\"Create initial state for a batch of inputs.\",\"forward(inputs, *args, state=None, **kwargs)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"property state_to_tensor\",\"Return a function mapping a state to a single tensor.\",\"This method should be implemented if one wants to use the hidden state insteadof the output sequence for final prediction. Currently only used with the StateDecoder.\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1253\":{\"h\":\"espnet2.asr.state_spaces.base.SequenceModule\",\"t\":[\"class espnet2.asr.state_spaces.base.SequenceModule\",\"Bases: Module\",\"Abstract sequence model class.\",\"All models must adhere to this interface\",\"A SequenceModule is generally a model that transforms an input of shape (n_batch, l_sequence, d_model) to (n_batch, l_sequence, d_output)\",\"REQUIRED methods and attributes forward, d_model, d_output: controls standard forward pass, a sequence-to-sequence transformation __init__ should also satisfy the following interface; see SequenceIdentity for an example\",\"def __init__(self, d_model, transposed=False,\",\"**\",\"kwargs)\",\"OPTIONAL methods default_state, step: allows stepping the model recurrently with a hidden state state_to_tensor, d_state: allows decoding from hidden state\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_model\",\"Model dimension (generally same as input dimension).\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, encoder) to track the internal shapes of the full model.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"property d_state\",\"Return dimension of output of self.state_to_tensor.\",\"default_state(*batch_shape, device=None)\",\"Create initial state for a batch of inputs.\",\"forward(x, state=None, **kwargs)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"property state_to_tensor\",\"Return a function mapping a state to a single tensor.\",\"This method should be implemented if one wants to use the hidden state insteadof the output sequence for final prediction. Currently only used with the StateDecoder.\",\"step(x, state=None, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1254\":{\"h\":\"espnet2.asr.state_spaces.block.SequenceResidualBlock\",\"t\":[\"class espnet2.asr.state_spaces.block.SequenceResidualBlock(d_input, i_layer=None, prenorm=True, dropout=0.0, tie_dropout=False, transposed=False, layer=None, residual=None, norm=None, pool=None, drop_path=0.0)\",\"Bases: SequenceModule\",\"Residual block wrapper for black box layer.\",\"The SequenceResidualBlock class implements a generic (batch, length, d_input) -> (batch, length, d_input) transformation\",\"Parameters:\",\"d_input – Input feature dimension\",\"i_layer – Layer index, only needs to be passed into certain residuals like Decay\",\"dropout – Dropout for black box module\",\"tie_dropout – Tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d\",\"transposed – Transpose inputs so each layer receives (batch, dim, length)\",\"layer – Config for black box module\",\"residual – Config for residual function\",\"norm – Config for normalization layer\",\"pool – Config for pooling layer per stage\",\"drop_path – Drop ratio for stochastic depth\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"property d_state\",\"Return dimension of output of self.state_to_tensor.\",\"default_state(*args, **kwargs)\",\"Create initial state for a batch of inputs.\",\"forward(x, state=None, **kwargs)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"property state_to_tensor\",\"Return a function mapping a state to a single tensor.\",\"This method should be implemented if one wants to use the hidden state insteadof the output sequence for final prediction. Currently only used with the StateDecoder.\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\",\"training : bool\"]},\"1255\":{\"h\":\"espnet2.asr.frontend.windowing.SlidingWindow\",\"t\":[\"class espnet2.asr.frontend.windowing.SlidingWindow(win_length: int = 400, hop_length: int = 160, channels: int = 1, padding: int | None = None, fs=None)\",\"Bases: AbsFrontend\",\"Sliding Window.\",\"Provides a sliding window over a batched continuous raw audio tensor. Optionally, provides padding (Currently not implemented). Combine this module with a pre-encoder compatible with raw audio data, for example Sinc convolutions.\",\"Known issues: Output length is calculated incorrectly if audio shorter than win_length. WARNING: trailing values are discarded - padding not implemented yet. There is currently no additional window function applied to input values.\",\"Initialize.\",\"Parameters:\",\"win_length – Length of frame.\",\"hop_length – Relative starting point of next frame.\",\"channels – Number of input channels.\",\"padding – Padding (placeholder, currently not implemented).\",\"fs – Sampling rate (placeholder for compatibility, not used).\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Apply a sliding window on the input.\",\"Parameters:\",\"input – Input (B, T, C*D) or (B, T*C*D), with D=C=1.\",\"input_lengths – Input lengths within batch.\",\"Returns: Output with dimensions (B, T, C, D), with D=win_length. Tensor: Output lengths within batch.\",\"Return type: Tensor\",\"output_size()\",\"Return output length of feature dimension D, i.e. the window length.\",\"training : bool\"]},\"1256\":{\"h\":\"espnet2.asr.preencoder.sinc.SpatialDropout\",\"t\":[\"<!-- _espnet2.asr.preencoder.sinc.SpatialDropout -->\",\"class espnet2.asr.preencoder.sinc.SpatialDropout(dropout_probability: float = 0.15, shape: tuple | list | None = None)\",\"Bases: Module\",\"Spatial dropout module.\",\"Apply dropout to full channels on tensors of input (B, C, D)\",\"Initialize.\",\"Parameters:\",\"dropout_probability – Dropout probability.\",\"shape (tuple,list) – Shape of input tensors.\",\"forward(x: Tensor)\",\"Forward of spatial dropout module.\",\"training : bool\"]},\"1257\":{\"h\":\"espnet2.asr.specaug.specaug.SpecAug\",\"t\":[\"<!-- _espnet2.asr.specaug.specaug.SpecAug -->\",\"class espnet2.asr.specaug.specaug.SpecAug(apply_time_warp: bool = True, time_warp_window: int = 5, time_warp_mode: str = 'bicubic', apply_freq_mask: bool = True, freq_mask_width_range: int | Sequence[int] = (0, 20), num_freq_mask: int = 2, apply_time_mask: bool = True, time_mask_width_range: int | Sequence[int] | None = None, time_mask_width_ratio_range: float | Sequence[float] | None = None, num_time_mask: int = 2, replace_with_zero: bool = True)\",\"Bases: AbsSpecAug\",\"Implementation of SpecAug.\",\"Reference: : Daniel S. Park et al. “SpecAugment: A Simple Data <br/>\",\"Augmentation Method for Automatic Speech Recognition”\",\"WARNING\",\"When using cuda mode, time_warp doesn’t have reproducibility due to torch.nn.functional.interpolate.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, x_lengths=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1258\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1259\":{\"h\":\"espnet2.asr.state_spaces.components.SquaredReLU\",\"t\":[\"class espnet2.asr.state_spaces.components.SquaredReLU\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1260\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1261\":{\"h\":\"espnet2.asr.state_spaces.components.StochasticDepth\",\"t\":[\"class espnet2.asr.state_spaces.components.StochasticDepth(p: float, mode: str)\",\"Bases: Module\",\"Stochastic depth module.\",\"See stochastic_depth().\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1262\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1263\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.SubModel\",\"t\":[\"class espnet2.asr.encoder.avhubert_encoder.SubModel(resnet=None, input_dim=None, cfg=None)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1264\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1265\":{\"h\":\"espnet2.asr.state_spaces.components.TSInverseNormalization\",\"t\":[\"class espnet2.asr.state_spaces.components.TSInverseNormalization(method, normalizer)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1266\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1267\":{\"h\":\"espnet2.asr.state_spaces.components.TSNormalization\",\"t\":[\"class espnet2.asr.state_spaces.components.TSNormalization(method, horizon)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1268\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1269\":{\"h\":\"espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder\",\"t\":[\"class espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder(input_size: int | None = None, extractor_mode: str = 'group_norm', extractor_conv_layer_config: List[List[int]] | None = [[512, 10, 5], [512, 3, 2], [512, 3, 2], [512, 3, 2], [512, 3, 2], [512, 2, 2], [512, 2, 2]], extractor_conv_bias: bool = False, encoder_embed_dim: int = 768, encoder_projection_dropout: float = 0.1, encoder_pos_conv_kernel: int = 128, encoder_pos_conv_groups: int = 16, encoder_num_layers: int = 12, encoder_num_heads: int = 12, encoder_attention_dropout: float = 0.1, encoder_ff_interm_features: int = 3072, encoder_ff_interm_dropout: float = 0.0, encoder_dropout: float = 0.1, encoder_layer_norm_first: bool = False, encoder_layer_drop: float = 0.05, mask_prob: float = 0.8, mask_selection: str = 'static', mask_other: float = 0.0, mask_length: int = 10, no_mask_overlap: bool = False, mask_min_space: int = 1, mask_channel_prob: float = 0.0, mask_channel_selection: str = 'static', mask_channel_other: float = 0.0, mask_channel_length: int = 10, no_mask_channel_overlap: bool = False, mask_channel_min_space: int = 1, skip_masked: bool = False, skip_nomask: bool = False, num_classes: int = 100, final_dim: int = 256, feature_grad_mult: float | None = 0.1, finetuning: bool = False, freeze_encoder_updates: int = 0)\",\"Bases: AbsEncoder\",\"Torch Audio Hubert encoder module.\",\"Parameters:\",\"extractor_mode – Operation mode of feature extractor. Valid values are “group_norm” or “layer_norm”.\",\"extractor_conv_layer_config – Configuration of convolution layers in feature extractor. List of convolution configuration, i.e. [[output_channel, kernel_size, stride], …]\",\"extractor_conv_bias – Whether to include bias term to each convolution operation.\",\"encoder_embed_dim – The dimension of embedding in encoder.\",\"encoder_projection_dropout – The dropout probability applied after the input feature is projected to “encoder_embed_dim”.\",\"encoder_pos_conv_kernel – Kernel size of convolutional positional embeddings.\",\"encoder_pos_conv_groups – Number of groups of convolutional positional embeddings.\",\"encoder_num_layers – Number of self attention layers in transformer block.\",\"encoder_num_heads – Number of heads in self attention layers.\",\"encoder_attention_dropout – Dropout probability applied after softmax in self-attention layer.\",\"encoder_ff_interm_features – Dimension of hidden features in feed forward layer.\",\"encoder_ff_interm_dropout – Dropout probability applied in feedforward layer.\",\"encoder_dropout – Dropout probability applied at the end of feed forward layer.\",\"encoder_layer_norm_first – Control the order of layer norm in transformer layer and each encoder layer. If True, in transformer layer, layer norm is applied before features are fed to encoder layers.\",\"encoder_layer_drop – Probability to drop each encoder layer during training.\",\"mask_prob – Probability for each token to be chosen as start of the span to be masked.\",\"mask_selection – How to choose the mask length. Options: [static, uniform, normal, poisson].\",\"mask_other – Secondary mask argument (used for more complex distributions).\",\"mask_length – The lengths of the mask.\",\"no_mask_overlap – Whether to allow masks to overlap.\",\"mask_min_space – Minimum space between spans (if no overlap is enabled).\",\"mask_channel_prob – (float): The probability of replacing a feature with 0.\",\"mask_channel_selection – How to choose the mask length for channel masking. Options: [static, uniform, normal, poisson].\",\"mask_channel_other – Secondary mask argument for channel masking(used for more complex distributions).\",\"mask_channel_length – Minimum space between spans (if no overlap is enabled) for channel masking.\",\"no_mask_channel_overlap – Whether to allow channel masks to overlap.\",\"mask_channel_min_space – Minimum space between spans for channel masking(if no overlap is enabled).\",\"skip_masked – If True, skip computing losses over masked frames.\",\"skip_nomask – If True, skip computing losses over unmasked frames.\",\"num_classes – The number of classes in the labels.\",\"final_dim – Project final representations and targets to final_dim.\",\"feature_grad_mult – The factor to scale the convolutional feature extraction layer gradients by. The scale factor will not affect the forward pass.\",\"finetuning – Whether to finetuning the model with ASR or other tasks.\",\"freeze_encoder_updates – The number of steps to freeze the encoder parameters in ASR finetuning.\",\"Hubert specific Args: : Please refer to: https://pytorch.org/audio/stable/generated/torchaudio.models.hubert_pretrain_model.html#torchaudio.models.hubert_pretrain_model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, ys_pad: Tensor | None = None, ys_pad_length: Tensor | None = None, prev_states: Tensor | None = None)\",\"Forward Hubert Pretrain Encoder.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1270\":{\"h\":\"espnet2.asr.decoder.transducer_decoder.TransducerDecoder\",\"t\":[\"class espnet2.asr.decoder.transducer_decoder.TransducerDecoder(vocab_size: int, rnn_type: str = 'lstm', num_layers: int = 1, hidden_size: int = 320, dropout: float = 0.0, dropout_embed: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"(RNN-)Transducer decoder module.\",\"Parameters:\",\"vocab_size – Output dimension.\",\"layers_type – (RNN-)Decoder layers type.\",\"num_layers – Number of decoder layers.\",\"hidden_size – Number of decoder units per layer.\",\"dropout – Dropout rate for decoder layers.\",\"dropout_embed – Dropout rate for embedding layer.\",\"embed_pad – Embed/Blank symbol ID.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(hyps: List[Hypothesis] | List[ExtendedHypothesis], dec_states: Tuple[Tensor, Tensor | None], cache: Dict[str, Any], use_lm: bool)\",\"One-step forward hypotheses.\",\"Parameters:\",\"hyps – Hypotheses.\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"cache – Pairs of (dec_out, dec_states) for each label sequences. (keys)\",\"use_lm – Whether to compute label ID sequences for LM.\",\"Returns: Decoder output sequences. (B, D_dec) dec_states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec)) lm_labels: Label ID sequences for LM. (B,)\",\"Return type: dec_out\",\"create_batch_states(states: Tuple[Tensor, Tensor | None], new_states: List[Tuple[Tensor, Tensor | None]], check_list: List | None = None)\",\"Create decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"new_states – Decoder hidden states. [N x ((1, D_dec), (1, D_dec))]\",\"Returns: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"Return type: states\",\"forward(labels: Tensor)\",\"Encode source label sequences.\",\"Parameters:labels – Label ID sequences. (B, L)\",\"Returns: Decoder output sequences. (B, T, U, D_dec)\",\"Return type: dec_out\",\"init_state(batch_size: int)\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"rnn_forward(sequence: Tensor, state: Tuple[Tensor, Tensor | None])\",\"Encode source label sequences.\",\"Parameters:\",\"sequence – RNN input sequences. (B, D_emb)\",\"state – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"Returns: RNN output sequences. (B, D_dec) (h_next, c_next): Decoder hidden states. (N, B, D_dec), (N, B, D_dec))\",\"Return type: sequence\",\"score(hyp: Hypothesis, cache: Dict[str, Any])\",\"One-step forward hypothesis.\",\"Parameters:\",\"hyp – Hypothesis.\",\"cache – Pairs of (dec_out, state) for each label sequence. (key)\",\"Returns: Decoder output sequence. (1, D_dec) new_state: Decoder hidden states. ((N, 1, D_dec), (N, 1, D_dec)) label: Label ID for LM. (1,)\",\"Return type: dec_out\",\"select_state(states: Tuple[Tensor, Tensor | None], idx: int)\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. : ((N, 1, D_dec), (N, 1, D_dec))\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"training : bool\"]},\"1271\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.TransformerDecoder\",\"t\":[\"class espnet2.asr.decoder.transformer_decoder.TransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, layer_drop_rate: float = 0.0)\",\"Bases: BaseTransformerDecoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"training : bool\"]},\"1272\":{\"h\":\"espnet2.asr.encoder.transformer_encoder.TransformerEncoder\",\"t\":[\"class espnet2.asr.encoder.transformer_encoder.TransformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, padding_idx: int = -1, interctc_layer_idx: ~typing.List[int] = [], interctc_use_conditioning: bool = False, layer_drop_rate: float = 0.0)\",\"Bases: AbsEncoder\",\"Transformer encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"input_layer – input layer type\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type – linear of conv1d\",\"positionwise_conv_kernel_size – kernel size of positionwise conv1d layer\",\"padding_idx – padding_idx for input_layer=embed\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, ctc: CTC | None = None, return_all_hs: bool = False)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"ctc (CTC) – ctc module for intermediate CTC loss\",\"return_all_hs (bool) – whether to return all hidden states\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"training : bool\"]},\"1273\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.TransformerMDDecoder\",\"t\":[\"class espnet2.asr.decoder.transformer_decoder.TransformerMDDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, use_speech_attn: bool = True)\",\"Bases: BaseTransformerDecoder\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor, speech: Tensor | None = None)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor, speech: Tensor | None = None, speech_lens: Tensor | None = None, return_hs: bool = False)\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"return_hs – dec hidden state corresponding to ys, used for searchable hidden ints\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"forward_one_step(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, memory_mask: Tensor | None = None, *, speech: Tensor | None = None, speech_mask: Tensor | None = None, cache: List[Tensor] | None = None, return_hs: bool = False)\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (batch, maxlen_out)\",\"tgt_mask – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"memory_mask – encoded memory mask (batch, 1, maxlen_in)\",\"speech – encoded speech, float32 (batch, maxlen_in, feat)\",\"speech_mask – encoded memory mask (batch, 1, maxlen_in)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"return_hs – dec hidden state corresponding to ys, used for searchable hidden ints\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"score(ys, state, x, speech=None)\",\"Score.\",\"training : bool\"]},\"1274\":{\"h\":\"espnet2.asr.state_spaces.components.TransposedLN\",\"t\":[\"class espnet2.asr.state_spaces.components.TransposedLN(d, scalar=True)\",\"Bases: Module\",\"Transposed LayerNorm module.\",\"LayerNorm module over second dimension Assumes shape (B, D, L), where L can be 1 or more axis\",\"This is slow and a dedicated CUDA/Triton implementation shuld provide substantial end-to-end speedup\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1275\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1276\":{\"h\":\"espnet2.asr.state_spaces.components.TransposedLinear\",\"t\":[\"class espnet2.asr.state_spaces.components.TransposedLinear(d_input, d_output, bias=True)\",\"Bases: Module\",\"Transposed linear module.\",\"Linear module on the second-to-last dimension Assumes shape (B, D, L), where L can be 1 or more axis\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1277\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1278\":{\"h\":\"espnet2.asr.state_spaces.base.TransposedModule\",\"t\":[\"espnet2.asr.state_spaces.base.TransposedModule(module)\",\"Transpose module.\",\"Wrap a SequenceModule class to accept transposed parameter, handle state, absorb kwargs\"]},\"1279\":{\"h\":\"espnet2.asr.state_spaces.pool.UpPool\",\"t\":[\"<!-- _espnet2.asr.state_spaces.pool.UpPool -->\",\"class espnet2.asr.state_spaces.pool.UpPool(d_input, d_output, stride, transposed=True, weight_norm=True, initializer=None, activation=None)\",\"Bases: SequenceModule\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"default_state(*batch_shape, device=None)\",\"Create initial state for a batch of inputs.\",\"forward(x, skip=None)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step one time step as a recurrent model.\",\"x: (…, H)\",\"training : bool\"]},\"1280\":{\"h\":\"espnet2.asr.state_spaces.pool.UpSample\",\"t\":[\"<!-- _espnet2.asr.state_spaces.pool.UpSample -->\",\"class espnet2.asr.state_spaces.pool.UpSample(d_input, stride=1, expand=1, transposed=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1281\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"step(x, state, **kwargs)\",\"training : bool\"]},\"1282\":{\"h\":\"espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder\",\"t\":[\"class espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder(input_size: int, rnn_type: str = 'lstm', bidirectional: bool = True, use_projection: bool = True, num_layers: int = 4, hidden_size: int = 320, output_size: int = 320, dropout: float = 0.0, in_channel: int = 1)\",\"Bases: AbsEncoder\",\"VGGRNNEncoder class.\",\"Parameters:\",\"input_size – The number of expected features in the input\",\"bidirectional – If True becomes a bidirectional LSTM\",\"use_projection – Use projection layer or not\",\"num_layers – Number of recurrent layers\",\"hidden_size – The number of hidden features\",\"output_size – The number of output features\",\"dropout – dropout probability\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1283\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"1284\":{\"h\":\"espnet2.asr.frontend.whisper.WhisperFrontend\",\"t\":[\"class espnet2.asr.frontend.whisper.WhisperFrontend(whisper_model: str = 'small', fs: int | str = 16000, freeze_weights: bool = True, download_dir: str | None = None)\",\"Bases: AbsFrontend\",\"Speech Representation Using Encoder Outputs from OpenAI’s Whisper Model:\",\"URL: https://github.com/openai/whisper\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1285\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"log_mel_spectrogram(audio: Tensor, ilens: Tensor | None = None)\",\"output_size()\",\"training : bool\",\"whisper_encode(input: Tensor, ilens: Tensor | None = None)\"]},\"1286\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank._MultiblankRNNTNumba\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank._MultiblankRNNTNumba(*args, **kwargs)\",\"Bases: Function\",\"Numba class for multi-blank transducer loss\",\"(https://arxiv.org/pdf/2211.03541.pdf)\",\"static backward(ctx, grad_output)\",\"Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).\",\"This function is to be overridden by all subclasses.\",\"It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.\",\"The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output.\",\"static forward(ctx, acts, labels, act_lens, label_lens, blank, big_blank_durations, reduction, fastemit_lambda, clamp, sigma)\",\"MultiblankRNNTNumba Forward.\",\"big_blank_durations: list of durations for multi-blank transducer, e.g. : [2, 4, 8].\",\"sigma: hyper-parameter for logit under-normalization method for training : multi-blank transducers. Recommended value 0.05.\",\"Refer to https://arxiv.org/pdf/2211.03541 for detailed explanations for : the above parameters;\",\"For other parameters for this class, refer to comment for class _RNNTNumba\"]},\"1287\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank._RNNTNumba\",\"t\":[\"class espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank._RNNTNumba(*args, **kwargs)\",\"Bases: Function\",\"static backward(ctx, grad_output)\",\"Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).\",\"This function is to be overridden by all subclasses.\",\"It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.\",\"The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output.\",\"static forward(ctx, acts, labels, act_lens, label_lens, blank, reduction, fastemit_lambda, clamp)\",\"RNNTNumba Forward.\",\"log_probs: Tensor of (batch x seqLength x labelLength x outputDim) : containing output from network\",\"labels: 2 dimensional Tensor containing all the targets of : the batch with zero padded\",\"act_lens: Tensor of size (batch) containing size of each : output sequence from the network\",\"label_lens: Tensor of (batch) containing label length of each example fastemit_lambda: Float scaling factor for FastEmit regularization. Refer to\",\"FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\"]},\"1288\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.add\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.add(x, y)\"]},\"1289\":{\"h\":\"espnet2.asr.decoder.rnn_decoder.build_attention_list\",\"t\":[\"espnet2.asr.decoder.rnn_decoder.build_attention_list(eprojs: int, dunits: int, atype: str = 'location', num_att: int = 1, num_encs: int = 1, aheads: int = 4, adim: int = 320, awin: int = 5, aconv_chans: int = 10, aconv_filts: int = 100, han_mode: bool = False, han_type=None, han_heads: int = 4, han_dim: int = 320, han_conv_chans: int = -1, han_conv_filts: int = 100, han_win: int = 5)\"]},\"1290\":{\"h\":\"espnet2.asr.state_spaces.cauchy.cauchy_mult\"},\"1291\":{\"h\":\"espnet2.asr.state_spaces.cauchy.cauchy_mult_keops\"},\"1292\":{\"h\":\"espnet2.asr.state_spaces.cauchy.cauchy_mult_torch\"},\"1293\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.certify_inputs\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.certify_inputs(log_probs, labels, lengths, label_lengths)\"]},\"1294\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_contiguous\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_contiguous(var, name)\"]},\"1295\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_dim\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_dim(var, dim, name)\"]},\"1296\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_type\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_type(var, t, name)\"]},\"1297\":{\"h\":\"espnet2.asr.state_spaces.s4.combination\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.combination -->\",\"espnet2.asr.state_spaces.s4.combination(measures, N, R, S, **ssm_args)\"]},\"1298\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_alphas_kernel\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_alphas_kernel(acts: Tensor, denom: Tensor, alphas: Tensor, llForward: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int)\",\"Compute alpha (forward variable) probabilities over the transduction step.\",\"Parameters:\",\"acts – Tensor of shape [B, T, U, V+1] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"alphas – Zero tensor of shape [B, T, U]. Will be updated inside the kernel with the forward variable probabilities.\",\"llForward – Zero tensor of shape [B]. Represents the log-likelihood of the forward pass. Returned as the forward pass loss that is reduced by the optimizer.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"Updates: : Kernel inplace updates the following inputs:\",\"alphas: forward variable scores.\",\"llForward: log-likelihood of forward variable.\"]},\"1299\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_betas_kernel\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_betas_kernel(acts: Tensor, denom: Tensor, betas: Tensor, llBackward: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int)\",\"Compute beta (backward variable) probabilities over the transduction step.\",\"Parameters:\",\"acts – Tensor of shape [B, T, U, V+1] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"betas – Zero tensor of shape [B, T, U]. Will be updated inside the kernel with the backward variable probabilities.\",\"llBackward – Zero tensor of shape [B]. Represents the log-likelihood of the backward pass. Returned as the backward pass loss that is reduced by the optimizer.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"Updates: : Kernel inplace updates the following inputs:\",\"betas: backward variable scores.\",\"llBackward: log-likelihood of backward variable.\"]},\"1300\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.compute_costs_data\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.compute_costs_data(source: Tensor, dest: Tensor, fastemit_lambda: float)\"]},\"1301\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_grad_kernel\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_grad_kernel(grads: Tensor, acts: Tensor, denom: Tensor, alphas: Tensor, betas: Tensor, logll: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int, fastemit_lambda: float, clamp: float)\",\"Compute gradients over the transduction step.\",\"Parameters:\",\"grads – Zero Tensor of shape [B, T, U, V+1]. Is updated by this kernel to contain the gradients of this batch of samples.\",\"acts – Tensor of shape [B, T, U, V+1] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"alphas – Alpha variable, contains forward probabilities. A tensor of shape [B, T, U].\",\"betas – Beta varoable, contains backward probabilities. A tensor of shape [B, T, U].\",\"logll – Log-likelihood of the forward variable, represented as a vector of shape [B]. Represents the log-likelihood of the forward pass.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"Updates: : Kernel inplace updates the following inputs:\",\"grads: Gradients with respect to the log likelihood (logll).\"]},\"1302\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_alphas_kernel\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_alphas_kernel(acts: Tensor, denom: Tensor, sigma: float, alphas: Tensor, llForward: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int, big_blank_duration: Tensor, num_big_blanks: int)\",\"Compute alpha (forward variable) probabilities for multi-blank transducuer loss\",\"(https://arxiv.org/pdf/2211.03541).\",\"Parameters:\",\"acts – Tensor of shape [B, T, U, V + 1 + num_big_blanks] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"sigma – Hyper-parameter for logit-undernormalization technique for training multi-blank transducers.\",\"alphas – Zero tensor of shape [B, T, U]. Will be updated inside the kernel with the forward variable probabilities.\",\"llForward – Zero tensor of shape [B]. Represents the log-likelihood of the forward pass. Returned as the forward pass loss that is reduced by the optimizer.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT standard blank token in the vocabulary.\",\"big_blank_durations – Vector of supported big blank durations of the model.\",\"num_big_blanks – Number of big blanks of the model.\",\"Updates: : Kernel inplace updates the following inputs:\",\"alphas: forward variable scores.\",\"llForward: log-likelihood of forward variable.\"]},\"1303\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_betas_kernel\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_betas_kernel(acts: Tensor, denom: Tensor, sigma: float, betas: Tensor, llBackward: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int, big_blank_duration: Tensor, num_big_blanks: int)\",\"Compute beta (backward variable) probabilities for multi-blank transducer loss\",\"(https://arxiv.org/pdf/2211.03541).\",\"Parameters:\",\"acts – Tensor of shape [B, T, U, V + 1 + num-big-blanks] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"sigma – Hyper-parameter for logit-undernormalization technique for training multi-blank transducers.\",\"betas – Zero tensor of shape [B, T, U]. Will be updated inside the kernel with the backward variable probabilities.\",\"llBackward – Zero tensor of shape [B]. Represents the log-likelihood of the backward pass. Returned as the backward pass loss that is reduced by the optimizer.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT standard blank token in the vocabulary.\",\"big_blank_durations – Vector of supported big blank durations of the model.\",\"num_big_blanks – Number of big blanks of the model.\",\"Updates: : Kernel inplace updates the following inputs:\",\"betas: backward variable scores.\",\"llBackward: log-likelihood of backward variable.\"]},\"1304\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_grad_kernel\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_grad_kernel(grads: Tensor, acts: Tensor, denom: Tensor, sigma: float, alphas: Tensor, betas: Tensor, logll: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int, big_blank_duration: Tensor, num_big_blanks: int, fastemit_lambda: float, clamp: float)\",\"Compute gradients for multi-blank transducer loss\",\"(https://arxiv.org/pdf/2211.03541).\",\"Parameters:\",\"grads – Zero Tensor of shape [B, T, U, V + 1 + num_big_blanks]. Is updated by this kernel to contain the gradients of this batch of samples.\",\"acts – Tensor of shape [B, T, U, V + 1 + num_big_blanks] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"sigma – Hyper-parameter for logit-undernormalization technique for training multi-blank transducers.\",\"alphas – Alpha variable, contains forward probabilities. A tensor of shape [B, T, U].\",\"betas – Beta varoable, contains backward probabilities. A tensor of shape [B, T, U].\",\"logll – Log-likelihood of the forward variable, represented as a vector of shape [B]. Represents the log-likelihood of the forward pass.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"big_blank_durations – Vector of supported big blank durations of the model.\",\"num_big_blanks – Number of big blanks of the model.\",\"Updates: : Kernel inplace updates the following inputs:\",\"grads: Gradients with respect to the log likelihood (logll).\"]},\"1305\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.conv3x3\",\"t\":[\"espnet2.asr.encoder.avhubert_encoder.conv3x3(in_planes, out_planes, stride=1)\"]},\"1306\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.copy_data_1d\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.copy_data_1d(source: Tensor, dest: Tensor, idx: int)\"]},\"1307\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.div_up\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.div_up(x: int, y: int)\"]},\"1308\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.download_avhubert\",\"t\":[\"espnet2.asr.encoder.avhubert_encoder.download_avhubert(model_url, dir_path)\"]},\"1309\":{\"h\":\"espnet2.asr.encoder.hubert_encoder.download_hubert\",\"t\":[\"espnet2.asr.encoder.hubert_encoder.download_hubert(model_url, dir_path)\"]},\"1310\":{\"h\":\"espnet2.asr.encoder.wav2vec2_encoder.download_w2v\",\"t\":[\"espnet2.asr.encoder.wav2vec2_encoder.download_w2v(model_url, dir_path)\"]},\"1311\":{\"h\":\"espnet2.asr.state_spaces.pool.downsample\",\"t\":[\"<!-- _espnet2.asr.state_spaces.pool.downsample -->\",\"espnet2.asr.state_spaces.pool.downsample(x, stride=1, expand=1, transposed=False)\"]},\"1312\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.downsample_basic_block\",\"t\":[\"espnet2.asr.encoder.avhubert_encoder.downsample_basic_block(inplanes, outplanes, stride)\"]},\"1313\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.downsample_basic_block_v2\",\"t\":[\"espnet2.asr.encoder.avhubert_encoder.downsample_basic_block_v2(inplanes, outplanes, stride)\"]},\"1314\":{\"h\":\"espnet2.asr.state_spaces.s4.dplr\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.dplr -->\",\"espnet2.asr.state_spaces.s4.dplr(scaling, N, rank=1, H=1, dtype=torch.float32, real_scale=1.0, imag_scale=1.0, random_real=False, random_imag=False, normalize=False, diagonal=True, random_B=False)\"]},\"1315\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.dtype\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.dtype()\"]},\"1316\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.exponential\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.exponential(x)\"]},\"1317\":{\"h\":\"espnet2.asr.state_spaces.utils.extract_attrs_from_obj\",\"t\":[\"espnet2.asr.state_spaces.utils.extract_attrs_from_obj(obj, *attrs)\"]},\"1318\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.flatten_tensor\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.flatten_tensor(x: Tensor)\"]},\"1319\":{\"h\":\"espnet2.asr.state_spaces.utils.get_class\",\"t\":[\"<!-- _espnet2.asr.state_spaces.utils.get_class -->\",\"espnet2.asr.state_spaces.utils.get_class(registry, _name_)\"]},\"1320\":{\"h\":\"espnet2.asr.decoder.hugging_face_transformers_decoder.get_hugging_face_model_lm_head\",\"t\":[\"espnet2.asr.decoder.hugging_face_transformers_decoder.get_hugging_face_model_lm_head(model)\"]},\"1321\":{\"h\":\"espnet2.asr.decoder.hugging_face_transformers_decoder.get_hugging_face_model_network\",\"t\":[\"espnet2.asr.decoder.hugging_face_transformers_decoder.get_hugging_face_model_network(model)\"]},\"1322\":{\"h\":\"espnet2.asr.state_spaces.components.get_initializer\",\"t\":[\"espnet2.asr.state_spaces.components.get_initializer(name, activation=None)\"]},\"1323\":{\"h\":\"espnet2.asr.state_spaces.s4.get_logger\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.get_logger -->\",\"espnet2.asr.state_spaces.s4.get_logger(name='espnet2.asr.state_spaces.s4', level=20)\",\"Initialize multi-GPU-friendly python logger.\"]},\"1324\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.get_workspace_size\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.get_workspace_size(maxT: int, maxU: int, minibatch: int, gpu: bool)\"]},\"1325\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.identity\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.identity(x)\"]},\"1326\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.index_put\",\"t\":[\"espnet2.asr.encoder.avhubert_encoder.index_put(tensor, indices, value)\"]},\"1327\":{\"h\":\"espnet2.asr.state_spaces.utils.instantiate\",\"t\":[\"<!-- _espnet2.asr.state_spaces.utils.instantiate -->\",\"espnet2.asr.state_spaces.utils.instantiate(registry, config, *args, partial=False, wrap=None, **kwargs)\",\"Instantiate registered module.\",\"registry: Dictionary mapping names to functions or target paths : (e.g. {‘model’: ‘models.SequenceModel’})\",\"config: Dictionary with a ‘name’ key indicating which element of the registry : to grab, and kwargs to be passed into the target constructor\",\"wrap: wrap the target class (e.g. ema optimizer or tasks.wrap)\",\"*\",\"args,\",\"**\",\"kwargs: additional arguments\",\"to override the config to pass into the target constructor\"]},\"1328\":{\"h\":\"espnet2.asr.state_spaces.utils.is_dict\",\"t\":[\"<!-- _espnet2.asr.state_spaces.utils.is_dict -->\",\"espnet2.asr.state_spaces.utils.is_dict(x)\"]},\"1329\":{\"h\":\"espnet2.asr.state_spaces.utils.is_list\",\"t\":[\"<!-- _espnet2.asr.state_spaces.utils.is_list -->\",\"espnet2.asr.state_spaces.utils.is_list(x)\"]},\"1330\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.is_xla_tensor\",\"t\":[\"espnet2.asr.encoder.avhubert_encoder.is_xla_tensor(tensor)\"]},\"1331\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.log_plus\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.log_plus(p1: float, p2: float)\"]},\"1332\":{\"h\":\"espnet2.asr.bayes_risk_ctc.log_substraction_exp\",\"t\":[\"espnet2.asr.bayes_risk_ctc.log_substraction_exp(a, b)\",\"return (a.exp() - b.exp()).log(): a numeracal safe implementation\"]},\"1333\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.log_sum_exp\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.log_sum_exp(a: float, b: float)\"]},\"1334\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.logp\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.logp(denom: Tensor, acts: Tensor, maxT: int, maxU: int, alphabet_size: int, mb: int, t: int, u: int, v: int)\",\"Compute the sum of log probability from the activation tensor\",\"and its denominator.\",\"Parameters:\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"acts – Tensor of shape [B, T, U, V+1] flattened. Represents the logprobs activation tensor.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"mb – Batch indexer.\",\"t – Acoustic sequence timestep indexer.\",\"u – Target sequence timestep indexer.\",\"v – Vocabulary token indexer.\",\"Returns: The sum of logprobs[mb, t, u, v] + denom[mb, t, u]\"]},\"1335\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.maximum\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.maximum(x, y)\"]},\"1336\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.multiblank_rnnt_loss\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.multiblank_rnnt_loss(acts, labels, act_lens, label_lens, blank, big_blank_durations=[], reduction='mean', fastemit_lambda: float = 0.0, clamp: float = 0.0)\",\"Multi-blank RNN Transducer (https://arxiv.org/pdf/2211.03541.pdf)\",\"Loss (functional form) :param acts: Tensor of (batch x seqLength x labelLength x outputDim) containing :param output from network: :param labels: 2 dimensional Tensor containing all the targets of the batch with\",\"zero padded\",\"Parameters:\",\"act_lens – Tensor of size (batch) containing size of each output sequence from the network\",\"label_lens – Tensor of (batch) containing label length of each example\",\"blank (int) – standard blank label.\",\"big_blank_durations – list of durations for multi-blank transducer, e.g. [2, 4, 8].\",\"sigma – hyper-parameter for logit under-normalization method for training multi-blank transducers. Recommended value 0.05.\",\"https (Refer to) – //arxiv.org/pdf/2211.03541 for detailed explanations for the last two params.\",\"reduction (string,optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’\"]},\"1337\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.multiblank_rnnt_loss_gpu\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.multiblank_rnnt_loss_gpu(acts: Tensor, labels: Tensor, input_lengths: Tensor, label_lengths: Tensor, costs: Tensor, grads: Tensor, blank_label: int, big_blank_durations: list, fastemit_lambda: float, clamp: float, num_threads: int, sigma: float)\",\"Wrapper method for accessing GPU Multi-blank RNNT loss\",\"(https://arxiv.org/pdf/2211.03541.pdf).\",\"CUDA implementation ported from [HawkAaron/warp-transducer] : (https://github.com/HawkAaron/warp-transducer).\",\"Parameters:\",\"acts – Activation tensor of shape [B, T, U, V + num_big_blanks + 1].\",\"labels – Ground truth labels of shape [B, U].\",\"input_lengths – Lengths of the acoustic sequence as a vector of ints [B].\",\"label_lengths – Lengths of the target sequence as a vector of ints [B].\",\"costs – Zero vector of length [B] in which costs will be set.\",\"grads – Zero tensor of shape [B, T, U, V + num_big_blanks + 1] where the gradient will be set.\",\"blank_label – Index of the standard blank token in the vocabulary.\",\"big_blank_durations – A list of supported durations for big blank symbols in the model, e.g. [2, 4, 8]. Note we only include durations for\",\"``\",\"big blanks’’ here and it should not include 1 for the standard blank. Those big blanks have vocabulary indices after the standard blank index.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of threads for OpenMP.\",\"sigma – logit-undernormalization weight used in the multi-blank model. Refer to the multi-blank paper https://arxiv.org/pdf/2211.03541 for detailed explanations.\"]},\"1338\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.negate\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.negate(x)\"]},\"1339\":{\"h\":\"espnet2.asr.state_spaces.s4.nplr\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.nplr -->\",\"espnet2.asr.state_spaces.s4.nplr(measure, N, rank=1, dtype=torch.float32, diagonalize_precision=True)\",\"Decompose as Normal Plus Low-Rank (NPLR).\",\"Return w, p, q, V, B such that (w - p q^*, B) is unitarily equivalent to the original HiPPO A, B by the matrix V i.e. A = V[w - p q*]V*, B = V B\"]},\"1340\":{\"h\":\"espnet2.asr.state_spaces.utils.omegaconf_filter_keys\",\"t\":[\"espnet2.asr.state_spaces.utils.omegaconf_filter_keys(d, fn=None)\",\"Only keep keys where fn(key) is True. Support nested DictConfig.\"]},\"1341\":{\"h\":\"espnet2.asr.state_spaces.s4.power\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.power -->\",\"espnet2.asr.state_spaces.s4.power(L, A, v=None)\",\"Compute A^L and the scan sum_i A^i v_i.\",\"A: (…, N, N) v: (…, N, L)\"]},\"1342\":{\"h\":\"espnet2.asr.state_spaces.s4.rank_correction\",\"t\":[\"espnet2.asr.state_spaces.s4.rank_correction(measure, N, rank=1, dtype=torch.float32)\",\"Return low-rank matrix L such that A + L is normal.\"]},\"1343\":{\"h\":\"espnet2.asr.state_spaces.s4.rank_zero_only\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.rank_zero_only -->\",\"espnet2.asr.state_spaces.s4.rank_zero_only(fn: Callable)\",\"Decorator function from PyTorch Lightning.\",\"Function that can be used as a decorator to enable a function/method being called only on global rank 0.\"]},\"1344\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.reduce_exp\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.reduce_exp(acts: Tensor, denom, rows: int, cols: int, minus: bool, stream)\",\"Helper method to call the Warp Reduction Kernel to perform exp reduction.\"]},\"1345\":{\"h\":\"NOTE\",\"t\":[\"Efficient warp occurs at input shapes of 2 ^ K.\",\"References\",\"Warp Primitives [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/]\",\"Parameters:\",\"acts – Flatened activation matrix of shape [B * T * U * (V+1)].\",\"output – Flatened output matrix of shape [B * T * U * (V+1)]. Data will be overwritten.\",\"rows – Vocabulary size (including blank token) - V+1. Represents the number of threads per block.\",\"cols – Flattened shape of activation matrix, without vocabulary dimension (B * T * U). Represents number of blocks per grid.\",\"minus – Bool flag whether to add or subtract as reduction. If minus is set; calls _reduce_minus, else calls _reduce_rows kernel.\",\"stream – CUDA Stream.\"]},\"1346\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.reduce_max\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.reduce_max(acts: Tensor, denom, rows: int, cols: int, minus: bool, stream)\",\"Helper method to call the Warp Reduction Kernel to perform max reduction.\"]},\"1347\":{\"h\":\"NOTE\",\"t\":[\"Efficient warp occurs at input shapes of 2 ^ K.\",\"References\",\"Warp Primitives [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/]\",\"Parameters:\",\"acts – Flatened activation matrix of shape [B * T * U * (V+1)].\",\"output – Flatened output matrix of shape [B * T * U * (V+1)]. Data will be overwritten.\",\"rows – Vocabulary size (including blank token) - V+1. Represents the number of threads per block.\",\"cols – Flattened shape of activation matrix, without vocabulary dimension (B * T * U). Represents number of blocks per grid.\",\"minus – Bool flag whether to add or subtract as reduction. If minus is set; calls _reduce_minus, else calls _reduce_rows kernel.\",\"stream – CUDA Stream.\"]},\"1348\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.rnnt_loss\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.rnnt_loss(acts, labels, act_lens, label_lens, blank=0, reduction='mean', fastemit_lambda: float = 0.0, clamp: float = 0.0)\",\"RNN Transducer Loss (functional form)\",\"Parameters:\",\"acts – Tensor of (batch x seqLength x labelLength x outputDim) containing output from network\",\"labels – 2 dimensional Tensor containing all the targets of the batch with zero padded\",\"act_lens – Tensor of size (batch) containing size of each output sequence from the network\",\"label_lens – Tensor of (batch) containing label length of each example\",\"blank (int,optional) – blank label. Default: 0.\",\"reduction (string,optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’\"]},\"1349\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.rnnt_loss_cpu\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.rnnt_loss_cpu(acts: Tensor, labels: Tensor, input_lengths: Tensor, label_lengths: Tensor, costs: Tensor, grads: Tensor, blank_label: int, fastemit_lambda: float, clamp: float, num_threads: int)\",\"Wrapper method for accessing CPU RNNT loss.\",\"CPU implementation ported from [HawkAaron/warp-transducer] : (https://github.com/HawkAaron/warp-transducer).\",\"Parameters:\",\"acts – Activation tensor of shape [B, T, U, V+1].\",\"labels – Ground truth labels of shape [B, U].\",\"input_lengths – Lengths of the acoustic sequence as a vector of ints [B].\",\"label_lengths – Lengths of the target sequence as a vector of ints [B].\",\"costs – Zero vector of length [B] in which costs will be set.\",\"grads – Zero tensor of shape [B, T, U, V+1] where the gradient will be set.\",\"blank_label – Index of the blank token in the vocabulary.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of threads for OpenMP.\"]},\"1350\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.rnnt_loss_gpu\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.rnnt_loss_gpu(acts: Tensor, labels: Tensor, input_lengths: Tensor, label_lengths: Tensor, costs: Tensor, grads: Tensor, blank_label: int, fastemit_lambda: float, clamp: float, num_threads: int)\",\"Wrapper method for accessing GPU RNNT loss.\",\"CUDA implementation ported from [HawkAaron/warp-transducer] : (https://github.com/HawkAaron/warp-transducer).\",\"Parameters:\",\"acts – Activation tensor of shape [B, T, U, V+1].\",\"labels – Ground truth labels of shape [B, U].\",\"input_lengths – Lengths of the acoustic sequence as a vector of ints [B].\",\"label_lengths – Lengths of the target sequence as a vector of ints [B].\",\"costs – Zero vector of length [B] in which costs will be set.\",\"grads – Zero tensor of shape [B, T, U, V+1] where the gradient will be set.\",\"blank_label – Index of the blank token in the vocabulary.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of threads for OpenMP.\"]},\"1351\":{\"h\":\"espnet2.asr.state_spaces.s4.ssm\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.ssm -->\",\"espnet2.asr.state_spaces.s4.ssm(measure, N, R, H, **ssm_args)\",\"Dispatcher to create single SSM initialization.\",\"N: state size R: rank (for DPLR parameterization) H: number of independent SSM copies\"]},\"1352\":{\"h\":\"espnet2.asr.state_spaces.components.stochastic_depth\",\"t\":[\"espnet2.asr.state_spaces.components.stochastic_depth(input: tensor, p: float, mode: str, training: bool = True)\",\"Apply stochastic depth.\",\"Implements the Stochastic Depth from “Deep Networks with Stochastic Depth” used for randomly dropping residual branches of residual architectures.\",\"Parameters:\",\"input (Tensor *[*N,...]) – The input tensor or arbitrary dimensions with the first one being its batch i.e. a batch with N rows.\",\"p (float) – probability of the input to be zeroed.\",\"mode (str) – \\\"batch\\\" or \\\"row\\\". \\\"batch\\\" randomly zeroes the entire input, \\\"row\\\" zeroes randomly selected rows from the batch.\",\"training – apply stochastic depth if is True. Default: True\",\"Returns: The randomly zeroed tensor.\",\"Return type: Tensor[N, …]\"]},\"1353\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.threads_per_block\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.threads_per_block()\"]},\"1354\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.time_masking\",\"t\":[\"espnet2.asr.encoder.avhubert_encoder.time_masking(xs_pad, min_T=5, max_T=20)\",\"Masking Contiguous Frames with random length of [min_T, max_T]\"]},\"1355\":{\"h\":\"espnet2.asr.state_spaces.utils.to_dict\",\"t\":[\"<!-- _espnet2.asr.state_spaces.utils.to_dict -->\",\"espnet2.asr.state_spaces.utils.to_dict(x, recursive=True)\",\"Convert Sequence or Mapping object to dict.\",\"lists get converted to a dictionary .. rubric:: Examples\",\"python {0: x[0], 1: x[1], ...} \"]},\"1356\":{\"h\":\"espnet2.asr.state_spaces.utils.to_list\",\"t\":[\"<!-- _espnet2.asr.state_spaces.utils.to_list -->\",\"espnet2.asr.state_spaces.utils.to_list(x, recursive=False)\",\"Convert an object to list.\",\"If Sequence (e.g. list, tuple, Listconfig): just return it\",\"Special case: If non-recursive and not a list, wrap in list\"]},\"1357\":{\"h\":\"espnet2.asr.state_spaces.s4.transition\",\"t\":[\"<!-- _espnet2.asr.state_spaces.s4.transition -->\",\"espnet2.asr.state_spaces.s4.transition(measure, N)\",\"A, B transition matrices for different measures.\"]},\"1358\":{\"h\":\"espnet2.asr.state_spaces.pool.upsample\",\"t\":[\"<!-- _espnet2.asr.state_spaces.pool.upsample -->\",\"espnet2.asr.state_spaces.pool.upsample(x, stride=1, expand=1, transposed=False)\"]},\"1359\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.warp_size\",\"t\":[\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.warp_size()\"]},\"1360\":{\"h\":\"espnet2.diar.attractor.abs_attractor.AbsAttractor\",\"t\":[\"class espnet2.diar.attractor.abs_attractor.AbsAttractor\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(enc_input: Tensor, ilens: Tensor, dec_input: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1361\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1362\":{\"h\":\"espnet2.diar.decoder.abs_decoder.AbsDecoder\",\"t\":[\"class espnet2.diar.decoder.abs_decoder.AbsDecoder\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1363\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract property num_spk\",\"training : bool\"]},\"1364\":{\"h\":\"espnet2.diar.abs_diar.AbsDiarization\",\"t\":[\"<!-- _espnet2.diar.abs_diar.AbsDiarization -->\",\"class espnet2.diar.abs_diar.AbsDiarization\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1365\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract forward_rawwav(input: Tensor, ilens: Tensor)\",\"training : bool\"]},\"1366\":{\"h\":\"espnet2.diar.layers.abs_mask.AbsMask\",\"t\":[\"<!-- _espnet2.diar.layers.abs_mask.AbsMask -->\",\"class espnet2.diar.layers.abs_mask.AbsMask\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input, ilens, bottleneck_feat, num_spk)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1367\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract property max_num_spk : int\",\"training : bool\"]},\"1368\":{\"h\":\"espnet2.diar.layers.tcn_nomask.ChannelwiseLayerNorm\",\"t\":[\"class espnet2.diar.layers.tcn_nomask.ChannelwiseLayerNorm(channel_size)\",\"Bases: Module\",\"Channel-wise Layer Normalization (cLN).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, K], M is batch size, N is channel size, K is length\",\"Returns: [M, N, K]\",\"Return type: cLN_y\",\"reset_parameters()\",\"training : bool\"]},\"1369\":{\"h\":\"espnet2.diar.layers.tcn_nomask.Chomp1d\",\"t\":[\"<!-- _espnet2.diar.layers.tcn_nomask.Chomp1d -->\",\"class espnet2.diar.layers.tcn_nomask.Chomp1d(chomp_size)\",\"Bases: Module\",\"To ensure the output length is the same as the input.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, H, Kpad]\",\"Returns: [M, H, K]\",\"training : bool\"]},\"1370\":{\"h\":\"espnet2.diar.layers.tcn_nomask.DepthwiseSeparableConv\",\"t\":[\"class espnet2.diar.layers.tcn_nomask.DepthwiseSeparableConv(in_channels, out_channels, kernel_size, stride, padding, dilation, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, H, K]\",\"Returns: [M, B, K]\",\"Return type: result\",\"training : bool\"]},\"1371\":{\"h\":\"espnet2.diar.espnet_model.ESPnetDiarizationModel\",\"t\":[\"class espnet2.diar.espnet_model.ESPnetDiarizationModel(frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, label_aggregator: Module, encoder: AbsEncoder, decoder: AbsDecoder, attractor: AbsAttractor | None, diar_weight: float = 1.0, attractor_weight: float = 1.0)\",\"Bases: AbsESPnetModel\",\"Speaker Diarization model\",\"If “attractor” is “None”, SA-EEND will be used. Else if “attractor” is not “None”, EEND-EDA will be used. For the details about SA-EEND and EEND-EDA, refer to the following papers: SA-EEND: https://arxiv.org/pdf/1909.06247.pdf EEND-EDA: https://arxiv.org/pdf/2005.09921.pdf, https://arxiv.org/pdf/2106.10654.pdf\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"attractor_loss(att_prob, label)\",\"static calc_diarization_error(pred, label, length)\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, spk_labels: Tensor | None = None, spk_labels_lengths: Tensor | None = None, **kwargs)\",\"create_length_mask(length, max_len, num_output)\",\"encode(speech: Tensor, speech_lengths: Tensor, bottleneck_feats: Tensor, bottleneck_feats_lengths: Tensor)\",\"Frontend + Encoder\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch,)\",\"bottleneck_feats – (Batch, Length, …): used for enh + diar\",\"forward(speech: Tensor, speech_lengths: Tensor | None = None, spk_labels: Tensor | None = None, spk_labels_lengths: Tensor | None = None, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, samples)\",\"speech_lengths – (Batch,) default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"spk_labels – (Batch, )\",\"kwargs – “utt_id” is among the input.\",\"pit_loss(pred, label, lengths)\",\"pit_loss_single_permute(pred, label, length)\",\"training : bool\"]},\"1372\":{\"h\":\"espnet2.diar.layers.tcn_nomask.GlobalLayerNorm\",\"t\":[\"class espnet2.diar.layers.tcn_nomask.GlobalLayerNorm(channel_size)\",\"Bases: Module\",\"Global Layer Normalization (gLN).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, K], M is batch size, N is channel size, K is length\",\"Returns: [M, N, K]\",\"Return type: gLN_y\",\"reset_parameters()\",\"training : bool\"]},\"1373\":{\"h\":\"espnet2.diar.label_processor.LabelProcessor\",\"t\":[\"class espnet2.diar.label_processor.LabelProcessor(win_length: int = 512, hop_length: int = 128, center: bool = True)\",\"Bases: Module\",\"Label aggregator for speaker diarization\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor)\",\"Forward.\",\"Parameters:\",\"input – (Batch, Nsamples, Label_dim)\",\"ilens – (Batch)\",\"Returns: (Batch, Frames, Label_dim) olens: (Batch)\",\"Return type: output\",\"training : bool\"]},\"1374\":{\"h\":\"espnet2.diar.decoder.linear_decoder.LinearDecoder\",\"t\":[\"class espnet2.diar.decoder.linear_decoder.LinearDecoder(encoder_output_size: int, num_spk: int = 2)\",\"Bases: AbsDecoder\",\"Linear decoder for speaker diarization\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – hidden_space [Batch, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"property num_spk\",\"training : bool\"]},\"1375\":{\"h\":\"espnet2.diar.layers.multi_mask.MultiMask\",\"t\":[\"<!-- _espnet2.diar.layers.multi_mask.MultiMask -->\",\"class espnet2.diar.layers.multi_mask.MultiMask(input_dim: int, bottleneck_dim: int = 128, max_num_spk: int = 3, mask_nonlinear='relu')\",\"Bases: AbsMask\",\"Multiple 1x1 convolution layer Module.\",\"This module corresponds to the final 1x1 conv block and non-linear function in TCNSeparator. This module has multiple 1x1 conv blocks. One of them is selected according to the given num_spk to handle flexible num_spk.\",\"Parameters:\",\"input_dim – Number of filters in autoencoder\",\"bottleneck_dim – Number of channels in bottleneck 1 * 1-conv block\",\"max_num_spk – Number of mask_conv1x1 modules (>= Max number of speakers in the dataset)\",\"mask_nonlinear – use which non-linear function to generate mask\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, bottleneck_feat: Tensor, num_spk: int)\",\"Keep this API same with TasNet.\",\"Parameters:\",\"input – [M, K, N], M is batch size\",\"ilens (torch.Tensor) – (M,)\",\"bottleneck_feat – [M, K, B]\",\"num_spk – number of speakers\",\"**(**Training – oracle,\",\"Inference – estimated by other module (e.g, EEND-EDA))\",\"Returns: [(M, K, N), …] ilens (torch.Tensor): (M,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property max_num_spk : int\",\"training : bool\"]},\"1376\":{\"h\":\"espnet2.diar.attractor.rnn_attractor.RnnAttractor\",\"t\":[\"class espnet2.diar.attractor.rnn_attractor.RnnAttractor(encoder_output_size: int, layer: int = 1, unit: int = 512, dropout: float = 0.1, attractor_grad: bool = True)\",\"Bases: AbsAttractor\",\"encoder decoder attractor for speaker diarization\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_input: Tensor, ilens: Tensor, dec_input: Tensor)\",\"Forward.\",\"Parameters:\",\"enc_input (torch.Tensor) – hidden_space [Batch, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"dec_input (torch.Tensor) – decoder input (zeros) [Batch, num_spk + 1, F]\",\"Returns: [Batch, num_spk + 1, F] att_prob: [Batch, num_spk + 1, 1]\",\"Return type: attractor\",\"training : bool\"]},\"1377\":{\"h\":\"espnet2.diar.separator.tcn_separator_nomask.TCNSeparatorNomask\",\"t\":[\"class espnet2.diar.separator.tcn_separator_nomask.TCNSeparatorNomask(input_dim: int, layer: int = 8, stack: int = 3, bottleneck_dim: int = 128, hidden_dim: int = 512, kernel: int = 3, causal: bool = False, norm_type: str = 'gLN')\",\"Bases: AbsSeparator\",\"Temporal Convolution Separator\",\"Note that this separator is equivalent to TCNSeparator except for not having the mask estimation part. This separator outputs the intermediate bottleneck feats (which is used as the input to diarization branch in enh_diar task). This separator is followed by MultiMask module, which estimates the masks.\",\"Parameters:\",\"input_dim – input feature dimension\",\"layer – int, number of layers in each stack.\",\"stack – int, number of stacks\",\"bottleneck_dim – bottleneck dimension\",\"hidden_dim – number of convolution channel\",\"kernel – int, kernel size.\",\"causal – bool, defalut False.\",\"norm_type – str, choose from ‘BN’, ‘gLN’, ‘cLN’\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"Returns: [B, T, bottleneck_dim] ilens (torch.Tensor): (B,)\",\"Return type: feats (torch.Tensor)\",\"property num_spk\",\"property output_dim : int\",\"training : bool\"]},\"1378\":{\"h\":\"espnet2.diar.layers.tcn_nomask.TemporalBlock\",\"t\":[\"class espnet2.diar.layers.tcn_nomask.TemporalBlock(in_channels, out_channels, kernel_size, stride, padding, dilation, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, B, K]\",\"Returns: [M, B, K]\",\"training : bool\"]},\"1379\":{\"h\":\"espnet2.diar.layers.tcn_nomask.TemporalConvNet\",\"t\":[\"class espnet2.diar.layers.tcn_nomask.TemporalConvNet(N, B, H, P, X, R, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Basic Module of tasnet.\",\"Parameters:\",\"N – Number of filters in autoencoder\",\"B – Number of channels in bottleneck 1 * 1-conv block\",\"H – Number of channels in convolutional blocks\",\"P – Kernel size in convolutional blocks\",\"X – Number of convolutional blocks in each repeat\",\"R – Number of repeats\",\"norm_type – BN, gLN, cLN\",\"causal – causal or non-causal\",\"forward(mixture_w)\",\"Keep this API same with TasNet.\",\"Parameters:mixture_w – [M, N, K], M is batch size\",\"Returns: [M, B, K]\",\"Return type: bottleneck_feature\",\"training : bool\"]},\"1380\":{\"h\":\"espnet2.diar.layers.tcn_nomask.check_nonlinear\",\"t\":[\"espnet2.diar.layers.tcn_nomask.check_nonlinear(nolinear_type)\"]},\"1381\":{\"h\":\"espnet2.diar.layers.tcn_nomask.chose_norm\",\"t\":[\"<!-- _espnet2.diar.layers.tcn_nomask.chose_norm -->\",\"espnet2.diar.layers.tcn_nomask.chose_norm(norm_type, channel_size)\",\"The input of normalization will be (M, C, K), where M is batch size.\",\"C is channel size and K is sequence length.\"]},\"1382\":{\"h\":\"espnet2.fileio.datadir_writer.DatadirWriter\",\"t\":[\"class espnet2.fileio.datadir_writer.DatadirWriter(p: Path | str)\",\"Bases: object\",\"Writer class to create kaldi like data directory.\"]},\"1383\":{\"h\":\"Examples\",\"t\":[\">>> with DatadirWriter(\\\"output\\\") as writer: ... # output/sub.txt is created here ... subwriter = writer[\\\"sub.txt\\\"] ... # Write \\\"uttidA some/where/a.wav\\\" ... subwriter[\\\"uttidA\\\"] = \\\"some/where/a.wav\\\" ... subwriter[\\\"uttidB\\\"] = \\\"some/where/b.wav\\\"\",\"close()\"]},\"1384\":{\"h\":\"espnet2.fileio.rand_gen_dataset.FloatRandomGenerateDataset\",\"t\":[\"class espnet2.fileio.rand_gen_dataset.FloatRandomGenerateDataset(shape_file: Path | str, dtype: str | dtype = 'float32', loader_type: str = 'csv_int')\",\"Bases: Mapping\",\"Generate float array from shape.txt.\"]},\"1385\":{\"h\":\"Examples\",\"t\":[\"shape.txt uttA 123,83 uttB 34,83\",\"dataset = FloatRandomGenerateDataset(“shape.txt”) array = dataset[“uttA”] assert array.shape == (123, 83) array = dataset[“uttB”] assert array.shape == (34, 83)\"]},\"1386\":{\"h\":\"espnet2.fileio.rand_gen_dataset.IntRandomGenerateDataset\",\"t\":[\"class espnet2.fileio.rand_gen_dataset.IntRandomGenerateDataset(shape_file: Path | str, low: int, high: int | None = None, dtype: str | dtype = 'int64', loader_type: str = 'csv_int')\",\"Bases: Mapping\",\"Generate float array from shape.txt\"]},\"1387\":{\"h\":\"Examples\",\"t\":[\"shape.txt uttA 123,83 uttB 34,83\",\"dataset = IntRandomGenerateDataset(“shape.txt”, low=0, high=10) array = dataset[“uttA”] assert array.shape == (123, 83) array = dataset[“uttB”] assert array.shape == (34, 83)\"]},\"1388\":{\"h\":\"espnet2.fileio.score_scp.MIDReader\",\"t\":[\"<!-- _espnet2.fileio.score_scp.MIDReader -->\",\"class espnet2.fileio.score_scp.MIDReader(fname, add_rest=True, dtype=<class 'numpy.int16'>)\",\"Bases: Mapping\",\"Reader class for ‘mid.scp’.\"]},\"1389\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.mid key2 /some/path/b.mid key3 /some/path/c.mid key4 /some/path/d.mid …\",\">>> reader = XMLScpReader('mid.scp') >>> tempo, note_list = reader['key1']\",\"get_path(key)\",\"keys()\"]},\"1390\":{\"h\":\"espnet2.fileio.multi_sound_scp.MultiSoundScpReader\",\"t\":[\"class espnet2.fileio.multi_sound_scp.MultiSoundScpReader(fname, dtype=None, always_2d: bool = False, stack_axis=0, pad=nan)\",\"Bases: Mapping\",\"Reader class for ‘wav.scp’ containing multiple sounds.\",\"This is useful when loading variable numbers of audios for different samples.\"]},\"1391\":{\"h\":\"Examples\",\"t\":[\"wav.scp is a text file that looks like the following:\",\"key1 /some/path/a1.wav /another/path/a2.wav /yet/another/path/a3.wav key2 /some/path/b1.wav /another/path/b2.wav key3 /some/path/c1.wav /another/path/c2.wav /yet/another/path/c3.wav key4 /some/path/d1.wav …\",\">>> reader = SoundScpReader('wav.scp', stack_axis=0) >>> rate, stacked_arrays = reader['key1'] >>> assert stacked_arrays.shape[0] == 3\"]},\"1392\":{\"h\":\"NOTE\",\"t\":[\"All audios in each sample must have the same sampling rates. Audios of different lengths in each sample will be right-padded with np.nan\",\"to the same length.\",\"get_path(key)\",\"keys()\",\"pad_to_same_length(arrays, pad=nan, axis=0)\",\"Right-pad arrays to the same length.\",\"Parameters:\",\"arrays (List *[*np.ndarray]) – List of arrays to pad\",\"pad (float) – Value to pad\",\"axis (int) – Axis to pad\",\"Returns: Padded array\",\"Return type: np.ndarray\"]},\"1393\":{\"h\":\"espnet2.fileio.score_scp.NOTE\",\"t\":[\"<!-- _espnet2.fileio.score_scp.NOTE -->\",\"class espnet2.fileio.score_scp.NOTE(lyric, midi, st, et)\",\"Bases: object\"]},\"1394\":{\"h\":\"espnet2.fileio.npy_scp.NpyScpReader\",\"t\":[\"<!-- _espnet2.fileio.npy_scp.NpyScpReader -->\",\"class espnet2.fileio.npy_scp.NpyScpReader(fname: Path | str)\",\"Bases: Mapping\",\"Reader class for a scp file of numpy file.\"]},\"1395\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.npy key2 /some/path/b.npy key3 /some/path/c.npy key4 /some/path/d.npy …\",\">>> reader = NpyScpReader('npy.scp') >>> array = reader['key1']\",\"get_path(key)\",\"keys()\"]},\"1396\":{\"h\":\"espnet2.fileio.npy_scp.NpyScpWriter\",\"t\":[\"<!-- _espnet2.fileio.npy_scp.NpyScpWriter -->\",\"class espnet2.fileio.npy_scp.NpyScpWriter(outdir: Path | str, scpfile: Path | str)\",\"Bases: object\",\"Writer class for a scp file of numpy file.\"]},\"1397\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.npy key2 /some/path/b.npy key3 /some/path/c.npy key4 /some/path/d.npy …\",\">>> writer = NpyScpWriter('./data/', './data/feat.scp') >>> writer['aa'] = numpy_array >>> writer['bb'] = numpy_array\",\"close()\",\"get_path(key)\"]},\"1398\":{\"h\":\"espnet2.fileio.read_text.RandomTextReader\",\"t\":[\"<!-- _espnet2.fileio.read_text.RandomTextReader -->\",\"class espnet2.fileio.read_text.RandomTextReader(text_and_scp: str)\",\"Bases: Mapping\",\"Reader class for random access to text.\",\"Simple text reader for non-pair text data (for unsupervised ASR) : Instead of loading the whole text into memory (often large for UASR), the reader consumes text which stores in byte-offset of each text file and randomly selected unpaired text from it for training using mmap.\",\"Examples: : text : text1line text2line text3line <br/> scp : 11 00000000000000000010 00000000110000000020 00000000210000000030 <br/> scp explanation : (number of digits per int value) (text start at bytes 0 and end at bytes 10 (including “\",\"“)) : (text start at bytes 11 and end at bytes 20 (including “\",\"“)) : (text start at bytes 21 and end at bytes 30 (including “\",\"“))\",\"keys()\"]},\"1399\":{\"h\":\"espnet2.fileio.rttm.RttmReader\",\"t\":[\"<!-- _espnet2.fileio.rttm.RttmReader -->\",\"class espnet2.fileio.rttm.RttmReader(fname: str)\",\"Bases: Mapping\",\"Reader class for ‘rttm.scp’.\"]},\"1400\":{\"h\":\"Examples\",\"t\":[\"SPEAKER file1 1 0 1023 <NA> <NA> spk1 <NA> SPEAKER file1 2 4000 3023 <NA> <NA> spk2 <NA> SPEAKER file1 3 500 4023 <NA> <NA> spk1 <NA> END file1 <NA> 4023 <NA> <NA> <NA> <NA>\",\"This is an extend version of standard RTTM format for espnet. The difference including:\",\"Use sample number instead of absolute time\",\"has a END label to represent the duration of a recording\",\"replace duration (5th field) with end time (For standard RTTM,\",\"see https://catalog.ldc.upenn.edu/docs/LDC2004T12/RTTM-format-v13.pdf)\",\"…\",\">>> reader = RttmReader('rttm') >>> spk_label = reader[\\\"file1\\\"]\",\"keys()\"]},\"1401\":{\"h\":\"espnet2.fileio.score_scp.SingingScoreReader\",\"t\":[\"class espnet2.fileio.score_scp.SingingScoreReader(fname, dtype=<class 'numpy.int16'>)\",\"Bases: Mapping\",\"Reader class for ‘score.scp’.\"]},\"1402\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/score.json key2 /some/path/score.json key3 /some/path/score.json key4 /some/path/score.json …\",\">>> reader = SoundScpReader('score.scp') >>> score = reader['key1']\",\"get_path(key)\",\"keys()\"]},\"1403\":{\"h\":\"espnet2.fileio.score_scp.SingingScoreWriter\",\"t\":[\"class espnet2.fileio.score_scp.SingingScoreWriter(outdir: Path | str, scpfile: Path | str)\",\"Bases: object\",\"Writer class for ‘score.scp’\"]},\"1404\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/score.json key2 /some/path/score.json key3 /some/path/score.json key4 /some/path/score.json …\",\">>> writer = SingingScoreWriter('./data/', './data/score.scp') >>> writer['aa'] = score_obj >>> writer['bb'] = score_obj\",\"close()\",\"get_path(key)\"]},\"1405\":{\"h\":\"espnet2.fileio.sound_scp.SoundScpReader\",\"t\":[\"<!-- _espnet2.fileio.sound_scp.SoundScpReader -->\",\"class espnet2.fileio.sound_scp.SoundScpReader(fname, dtype=None, always_2d: bool = False, multi_columns: bool = False, concat_axis=1)\",\"Bases: Mapping\",\"Reader class for ‘wav.scp’.\"]},\"1406\":{\"h\":\"Examples\",\"t\":[\"wav.scp is a text file that looks like the following:\",\"key1 /some/path/a.wav key2 /some/path/b.wav key3 /some/path/c.wav key4 /some/path/d.wav …\",\">>> reader = SoundScpReader('wav.scp') >>> rate, array = reader['key1']\",\"If multi_columns=True is given and multiple files are given in one line with space delimiter, and the output array are concatenated along channel direction\",\"key1 /some/path/a.wav /some/path/a2.wav key2 /some/path/b.wav /some/path/b2.wav …\",\">>> reader = SoundScpReader('wav.scp', multi_columns=True) >>> rate, array = reader['key1']\",\"In the above case, a.wav and a2.wav are concatenated.\",\"Note that even if multi_columns=True is given, SoundScpReader still supports a normal wav.scp, i.e., a wav file is given per line, but this option is disable by default because dict[str, list[str]] object is needed to be kept, but it increases the required amount of memory.\",\"get_path(key)\",\"keys()\"]},\"1407\":{\"h\":\"espnet2.fileio.sound_scp.SoundScpWriter\",\"t\":[\"<!-- _espnet2.fileio.sound_scp.SoundScpWriter -->\",\"class espnet2.fileio.sound_scp.SoundScpWriter(outdir: Path | str, scpfile: Path | str, format='wav', multi_columns: bool = False, output_name_format: str = '{key}.{audio_format}', output_name_format_multi_columns: str = '{key}-CH{channel}.{audio_format}', subtype: str | None = None)\",\"Bases: object\",\"Writer class for ‘wav.scp’\",\"Parameters:\",\"outdir –\",\"scpfile –\",\"format – The output audio format\",\"multi_columns – Save multi channel data as multiple monaural audio files\",\"output_name_format – The naming formam of generated audio files\",\"output_name_format_multi_columns – The naming formam of generated audio files when multi_columns is given\",\"dtype –\",\"subtype –\"]},\"1408\":{\"h\":\"Examples\",\"t\":[\">>> writer = SoundScpWriter('./data/', './data/wav.scp') >>> writer['aa'] = 16000, numpy_array >>> writer['bb'] = 16000, numpy_array\",\"aa ./data/aa.wav bb ./data/bb.wav\",\">>> writer = SoundScpWriter( './data/', './data/feat.scp', multi_columns=True, ) >>> numpy_array.shape (100, 2) >>> writer['aa'] = 16000, numpy_array\",\"aa ./data/aa-CH0.wav ./data/aa-CH1.wav\",\"close()\",\"get_path(key)\"]},\"1409\":{\"h\":\"espnet2.fileio.vad_scp.VADScpReader\",\"t\":[\"<!-- _espnet2.fileio.vad_scp.VADScpReader -->\",\"class espnet2.fileio.vad_scp.VADScpReader(fname, dtype=<class 'numpy.float32'>)\",\"Bases: Mapping\",\"Reader class for ‘vad.scp’.\",\"Different from segments, the vad.scp would focus on utterance-level, while the segments are expected to focus on a whole session. The major usage in ESPnet is to guide the silence trim for UASR.\"]},\"1410\":{\"h\":\"Examples\",\"t\":[\"key1 0:1.2000 key2 3.0000:4.5000 7.0000:9:0000 …\",\">>> reader = VADScpReader('wav.scp') >>> array = reader['key1']\",\"keys()\"]},\"1411\":{\"h\":\"espnet2.fileio.vad_scp.VADScpWriter\",\"t\":[\"<!-- _espnet2.fileio.vad_scp.VADScpWriter -->\",\"class espnet2.fileio.vad_scp.VADScpWriter(scpfile: Path | str, dtype=None)\",\"Bases: object\",\"Writer class for ‘vad.scp’\"]},\"1412\":{\"h\":\"Examples\",\"t\":[\"key1 0:1.2000 key2 3.0000:4.5000 7.0000:9:0000 …\",\">>> writer = VADScpWriter('./data/vad.scp') >>> writer['aa'] = list of tuples >>> writer['bb'] = list of tuples\",\"close()\"]},\"1413\":{\"h\":\"espnet2.fileio.score_scp.XMLReader\",\"t\":[\"<!-- _espnet2.fileio.score_scp.XMLReader -->\",\"class espnet2.fileio.score_scp.XMLReader(fname, dtype=<class 'numpy.int16'>)\",\"Bases: Mapping\",\"Reader class for ‘xml.scp’.\"]},\"1414\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.xml key2 /some/path/b.xml key3 /some/path/c.xml key4 /some/path/d.xml …\",\">>> reader = XMLScpReader('xml.scp') >>> tempo, note_list = reader['key1']\",\"get_path(key)\",\"keys()\"]},\"1415\":{\"h\":\"espnet2.fileio.score_scp.XMLWriter\",\"t\":[\"<!-- _espnet2.fileio.score_scp.XMLWriter -->\",\"class espnet2.fileio.score_scp.XMLWriter(outdir: Path | str, scpfile: Path | str)\",\"Bases: object\",\"Writer class for ‘midi.scp’\"]},\"1416\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.musicxml key2 /some/path/b.musicxml key3 /some/path/c.musicxml key4 /some/path/d.musicxml …\",\">>> writer = XMLScpWriter('./data/', './data/xml.scp') >>> writer['aa'] = xml_obj >>> writer['bb'] = xml_obj\",\"close()\",\"get_path(key)\"]},\"1417\":{\"h\":\"espnet2.fileio.read_text.load_num_sequence_text\",\"t\":[\"espnet2.fileio.read_text.load_num_sequence_text(path: Path | str, loader_type: str = 'csv_int')\",\"Read a text file indicating sequences of number\"]},\"1418\":{\"h\":\"Examples\",\"t\":[\"key1 1 2 3 key2 34 5 6\",\">>> d = load_num_sequence_text('text') >>> np.testing.assert_array_equal(d[\\\"key1\\\"], np.array([1, 2, 3]))\"]},\"1419\":{\"h\":\"espnet2.fileio.rttm.load_rttm_text\",\"t\":[\"<!-- _espnet2.fileio.rttm.load_rttm_text -->\",\"espnet2.fileio.rttm.load_rttm_text(path: Path | str)\",\"Read a RTTM file\",\"Note: only support speaker information now\"]},\"1420\":{\"h\":\"espnet2.fileio.read_text.read_2columns_text\",\"t\":[\"espnet2.fileio.read_text.read_2columns_text(path: Path | str)\",\"Read a text file having 2 columns as dict object.\"]},\"1421\":{\"h\":\"Examples\",\"t\":[\"wav.scp key1 /some/path/a.wav key2 /some/path/b.wav\",\"``python\",\"read_2columns_text('wav.scp') {'key1': -'/some/path/a.wav', 'key2': '/some/path/b.wav'} ``\"]},\"1422\":{\"h\":\"espnet2.fileio.read_text.read_label\",\"t\":[\"<!-- _espnet2.fileio.read_text.read_label -->\",\"espnet2.fileio.read_text.read_label(path: Path | str)\",\"Read a text file indicating sequences of number\"]},\"1423\":{\"h\":\"Examples\",\"t\":[\"key1 start_time_1 end_time_1 phone_1 start_time_2 end_time_2 phone_2 ….\",\"key2 start_time_1 end_time_1 phone_1\",\">>> d = load_num_sequence_text('label') >>> np.testing.assert_array_equal(d[\\\"key1\\\"], [0.1, 0.2, \\\"啊\\\"]))\"]},\"1424\":{\"h\":\"espnet2.fileio.read_text.read_multi_columns_text\",\"t\":[\"espnet2.fileio.read_text.read_multi_columns_text(path: Path | str, return_unsplit: bool = False)\",\"Read a text file having 2 or more columns as dict object.\"]},\"1425\":{\"h\":\"Examples\",\"t\":[\"wav.scp: : key1 /some/path/a1.wav /some/path/a2.wav key2 /some/path/b1.wav /some/path/b2.wav /some/path/b3.wav key3 /some/path/c1.wav …\",\">>> read_multi_columns_text('wav.scp') {'key1': ['/some/path/a1.wav', '/some/path/a2.wav'], 'key2': ['/some/path/b1.wav', '/some/path/b2.wav', '/some/path/b3.wav'], 'key3': ['/some/path/c1.wav']}\"]},\"1426\":{\"h\":\"espnet2.fileio.sound_scp.soundfile_read\",\"t\":[\"<!-- _espnet2.fileio.sound_scp.soundfile_read -->\",\"espnet2.fileio.sound_scp.soundfile_read(wavs: str | List[str], dtype=None, always_2d: bool = False, concat_axis: int = 1, start: int = 0, end: int | None = None, return_subtype: bool = False)\"]},\"1427\":{\"h\":\"espnet2.fst.lm_rescore.compute_am_scores_and_lm_scores\",\"t\":[\"espnet2.fst.lm_rescore.compute_am_scores_and_lm_scores(lats: Fsa, word_fsas_with_epsilon_loops: Fsa, path_to_seq_map: Tensor, device: str = 'cuda', batch_size: int = 500)\",\"Compute AM and LM scores of n-best lists (represented as word_fsas).\",\"Parameters:\",\"lats – An FsaVec, which is the output of k2.intersect_dense_pruned. It must have the attribute lm_scores.\",\"word_fsas_with_epsilon_loops – An FsaVec representing a n-best list. Note that it has been processed by k2.add_epsilon_self_loops.\",\"path_to_seq_map – A 1-D torch.Tensor with dtype torch.int32. path_to_seq_map[i] indicates which sequence the i-th Fsa in word_fsas_with_epsilon_loops belongs to. path_to_seq_map.numel() == word_fsas_with_epsilon_loops.arcs.dim0().\",\"batch_size – Batchify the n-best list when intersecting with inverted_lats. You could tune this to avoid GPU OOM issue or increase the GPU usage.\",\"Returns: Return a tuple of (1-D torch.Tensor, 1-D torch.Tensor) containing the AM and LM scores of each path. am_scores.numel() == word_fsas_with_epsilon_loops.shape[0] lm_scores.numel() == word_fsas_with_epsilon_loops.shape[0]\"]},\"1428\":{\"h\":\"espnet2.fst.lm_rescore.nbest_am_lm_scores\",\"t\":[\"<!-- _espnet2.fst.lm_rescore.nbest_am_lm_scores -->\",\"espnet2.fst.lm_rescore.nbest_am_lm_scores(lats: Fsa, num_paths: int, device: str = 'cuda', batch_size: int = 500)\",\"Compute am scores with word_seqs\",\"Compatible with both ctc_decoding or TLG decoding.\"]},\"1429\":{\"h\":\"espnet2.fst.lm_rescore.remove_repeated_and_leq\",\"t\":[\"espnet2.fst.lm_rescore.remove_repeated_and_leq(tokens: List[int], blank_id: int = 0)\",\"Generate valid token sequence.\",\"Result may be used as input of transformer decoder and neural language model. Fristly, remove repeated token from a “token alignment” seqs; Then remove blank symbols.\",\"This fuction may be replaced by tokenizing word_seqs with tokenizer or composeing word_seqs_fsas with L_inv.fst or composing token_seqs with ctc_topo. Current method is slelected other than previous three methods because it won’t need an extra object, i.e. tokenizer, L.fst or ctc_topo.\"]},\"1430\":{\"h\":\"espnet2.enh.layers.uses.ATFBlock\",\"t\":[\"<!-- _espnet2.enh.layers.uses.ATFBlock -->\",\"class espnet2.enh.layers.uses.ATFBlock(input_size, rnn_type='lstm', hidden_size=128, att_heads=4, dropout=0.0, activation='relu', bidirectional=True, norm_type='cLN', ch_mode='att', ch_att_dim=256, eps=1e-05, with_channel_modeling=True)\",\"Bases: Module\",\"Container module for a single Attentive Time-Frequency Block.\",\"Parameters:\",\"input_size (int) – dimension of the input feature.\",\"rnn_type (str) – type of the RNN cell in the improved Transformer layer.\",\"hidden_size (int) – hidden dimension of the RNN cell.\",\"att_heads (int) – number of attention heads in Transformer.\",\"dropout (float) – dropout ratio. Default is 0.\",\"activation (str) – non-linear activation function applied in each block.\",\"bidirectional (bool) – whether the RNN layers are bidirectional.\",\"norm_type (str) – normalization type in the improved Transformer layer.\",\"ch_mode (str) – mode of channel modeling. Select from “att” and “tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"eps (float) – epsilon for layer normalization.\",\"with_channel_modeling (bool) – whether to use channel modeling.\",\"forward(input, ref_channel=None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – feature sequence (batch, C, N, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel. if None, simply average all channels. if int, take the specified channel instead of averaging.\",\"Returns: output sequence (batch, C, N, freq, time)\",\"Return type: output (torch.Tensor)\",\"freq_path_process(x)\",\"time_path_process(x)\",\"training : bool\"]},\"1431\":{\"h\":\"espnet2.enh.decoder.abs_decoder.AbsDecoder\",\"t\":[\"<!-- _espnet2.enh.decoder.abs_decoder.AbsDecoder -->\",\"class espnet2.enh.decoder.abs_decoder.AbsDecoder\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1432\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_streaming(input_frame: Tensor)\",\"streaming_merge(chunks: Tensor, ilens: tensor | None = None)\",\"Stream merge.\",\"It merges the frame-level processed audio chunks in the streaming simulation. It is noted that, in real applications, the processed audio should be sent to the output channel frame by frame. You may refer to this function to manage your streaming output buffer.\",\"Parameters:\",\"chunks – List [(B, frame_size),]\",\"ilens – [B]\",\"Returns: [B, T]\",\"Return type: merge_audio\",\"training : bool\"]},\"1433\":{\"h\":\"espnet2.enh.diffusion.abs_diffusion.AbsDiffusion\",\"t\":[\"class espnet2.enh.diffusion.abs_diffusion.AbsDiffusion\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract enhance(input: Tensor)\",\"abstract forward(input: Tensor, ilens: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1434\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1435\":{\"h\":\"espnet2.enh.encoder.abs_encoder.AbsEncoder\",\"t\":[\"<!-- _espnet2.enh.encoder.abs_encoder.AbsEncoder -->\",\"class espnet2.enh.encoder.abs_encoder.AbsEncoder\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1436\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_streaming(input: Tensor)\",\"abstract property output_dim : int\",\"streaming_frame(audio: Tensor)\",\"Stream frame.\",\"It splits the continuous audio into frame-level audio chunks in the streaming simulation. It is noted that this function takes the entire long audio as input for a streaming simulation. You may refer to this function to manage your streaming input buffer in a real streaming application.\",\"Parameters:audio – (B, T)\",\"Returns: List [(B, frame_size),]\",\"Return type: chunked\",\"training : bool\"]},\"1437\":{\"h\":\"espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss\",\"t\":[\"class espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss\",\"Bases: Module, ABC\",\"Base class for all Enhancement loss modules.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(ref, inf)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1438\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\",\"property only_for_test : bool\",\"training : bool\"]},\"1439\":{\"h\":\"espnet2.enh.abs_enh.AbsEnhancement\",\"t\":[\"<!-- _espnet2.enh.abs_enh.AbsEnhancement -->\",\"class espnet2.enh.abs_enh.AbsEnhancement\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1440\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract forward_rawwav(input: Tensor, ilens: Tensor)\",\"training : bool\"]},\"1441\":{\"h\":\"espnet2.enh.extractor.abs_extractor.AbsExtractor\",\"t\":[\"class espnet2.enh.extractor.abs_extractor.AbsExtractor\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor, input_aux: Tensor, ilens_aux: Tensor, suffix_tag: str = '', additional: Dict | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1442\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1443\":{\"h\":\"espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper\",\"t\":[\"class espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper\",\"Bases: Module, ABC\",\"Base class for all Enhancement loss wrapper modules.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(ref: List, inf: List, others: Dict)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1444\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\",\"weight = 1.0\"]},\"1445\":{\"h\":\"espnet2.enh.separator.abs_separator.AbsSeparator\",\"t\":[\"class espnet2.enh.separator.abs_separator.AbsSeparator\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor, additional: Dict | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1446\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_streaming(input_frame: Tensor, buffer=None)\",\"abstract property num_spk\",\"training : bool\"]},\"1447\":{\"h\":\"espnet2.enh.separator.tfgridnetv3_separator.AllHeadPReLULayerNormalization4DC\",\"t\":[\"class espnet2.enh.separator.tfgridnetv3_separator.AllHeadPReLULayerNormalization4DC(input_dimension, eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1448\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1449\":{\"h\":\"espnet2.enh.separator.tfgridnetv2_separator.AllHeadPReLULayerNormalization4DCF\",\"t\":[\"class espnet2.enh.separator.tfgridnetv2_separator.AllHeadPReLULayerNormalization4DCF(input_dimension, eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1450\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1451\":{\"h\":\"espnet2.enh.diffusion.sampling.correctors.AnnealedLangevinDynamics\",\"t\":[\"class espnet2.enh.diffusion.sampling.correctors.AnnealedLangevinDynamics(sde, score_fn, snr, n_steps)\",\"Bases: Corrector\",\"The original annealed Langevin dynamics predictor in NCSN/NCSNv2.\",\"update_fn(x, t, *args)\",\"One update of the corrector.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A PyTorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1452\":{\"h\":\"espnet2.enh.layers.dcunet.ArgsComplexMultiplicationWrapper\",\"t\":[\"class espnet2.enh.layers.dcunet.ArgsComplexMultiplicationWrapper(module_cls, *args, **kwargs)\",\"Bases: Module\",\"Adapted from asteroid’s complex_nn.py, allowing\",\"args/kwargs to be passed through forward().\",\"Make a complex-valued module F from a real-valued module f by applying complex multiplication rules:\",\"F(a + i b) = f1(a) - f1(b) + i (f2(b) + f2(a))\",\"where f1, f2 are instances of f that do not share weights.\",\"Parameters:module_cls (callable) – A class or function that returns a Torch module/functional. Constructor of f in the formula above. Called 2x with *args, **kwargs, to construct the real and imaginary component modules.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, *args, **kwargs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1453\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1454\":{\"h\":\"espnet2.enh.separator.asteroid_models.AsteroidModel_Converter\",\"t\":[\"class espnet2.enh.separator.asteroid_models.AsteroidModel_Converter(encoder_output_dim: int, model_name: str, num_spk: int, pretrained_path: str = '', loss_type: str = 'si_snr', **model_related_kwargs)\",\"Bases: AbsSeparator\",\"The class to convert the models from asteroid to AbsSeprator.\",\"Parameters:\",\"encoder_output_dim – input feature dimension, default=1 after the NullEncoder\",\"num_spk – number of speakers\",\"loss_type – loss type of enhancement\",\"model_name – Asteroid model names, e.g. ConvTasNet, DPTNet. Refers to https://github.com/asteroid-team/asteroid/ blob/master/asteroid/models/_init_.py\",\"pretrained_path – the name of pretrained model from Asteroid in HF hub. Refers to: https://github.com/asteroid-team/asteroid/ blob/master/docs/source/readmes/pretrained_models.md and https://huggingface.co/models?filter=asteroid\",\"model_related_kwargs – more args towards each specific asteroid model.\",\"forward(input: Tensor, ilens: Tensor | None = None, additional: Dict | None = None)\",\"Whole forward of asteroid models.\",\"Parameters:\",\"input (torch.Tensor) – Raw Waveforms [B, T]\",\"ilens (torch.Tensor) – input lengths [B]\",\"additional (DictorNone) – other data included in model\",\"Returns: [(B, T), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, T), ‘mask_spk2’: torch.Tensor(Batch, T), … ‘mask_spkn’: torch.Tensor(Batch, T),\",\"]\",\"Return type: estimated Waveforms(List[Union(torch.Tensor])\",\"forward_rawwav(input: Tensor, ilens: Tensor | None = None)\",\"Output with waveforms.\",\"property num_spk\",\"training : bool\"]},\"1455\":{\"h\":\"espnet2.enh.layers.dnn_beamformer.AttentionReference\",\"t\":[\"class espnet2.enh.layers.dnn_beamformer.AttentionReference(bidim, att_dim, eps=1e-06)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(psd_in: Tensor | ComplexTensor, ilens: LongTensor, scaling: float = 2.0)\",\"Attention-based reference forward function.\",\"Parameters:\",\"psd_in (torch.complex64/ComplexTensor) – (B, F, C, C)\",\"ilens (torch.Tensor) – (B,)\",\"scaling (float) –\",\"Returns: (B, C) ilens (torch.Tensor): (B,)\",\"Return type: u (torch.Tensor)\",\"training : bool\"]},\"1456\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.AttnBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.AttnBlock(channels)\",\"Bases: Module\",\"Channel-wise self-attention block.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1457\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1458\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.AttnBlockpp\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layerspp.AttnBlockpp(channels, skip_rescale=False, init_scale=0.0)\",\"Bases: Module\",\"Channel-wise self-attention block. Modified from DDPM.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1459\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1460\":{\"h\":\"espnet2.enh.layers.fasnet.BF_module\",\"t\":[\"<!-- _espnet2.enh.layers.fasnet.BF_module -->\",\"class espnet2.enh.layers.fasnet.BF_module(input_dim, feature_dim, hidden_dim, output_dim, num_spk=2, layer=4, segment_size=100, bidirectional=True, dropout=0.0, fasnet_type='ifasnet')\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1461\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1462\":{\"h\":\"espnet2.enh.layers.bsrnn.BSRNN\",\"t\":[\"<!-- _espnet2.enh.layers.bsrnn.BSRNN -->\",\"class espnet2.enh.layers.bsrnn.BSRNN(input_dim=481, num_channel=16, num_layer=6, target_fs=48000, causal=True)\",\"Bases: Module\",\"Band-Split RNN (BSRNN).\",\"References\",\"[1] J. Yu, H. Chen, Y. Luo, R. Gu, and C. Weng, “High fidelity speech enhancement with band-split RNN,” in Proc. ISCA Interspeech, 2023. https://isca-speech.org/archive/interspeech_2023/yu23b_interspeech.html [2] J. Yu, and Y. Luo, “Efficient monaural speech enhancement with universal sample rate band-split RNN,” in Proc. ICASSP, 2023. https://ieeexplore.ieee.org/document/10096020\",\"Parameters:\",\"input_dim (int) – maximum number of frequency bins corresponding to target_fs\",\"num_channel (int) – embedding dimension of each time-frequency bin\",\"num_layer (int) – number of time and frequency RNN layers\",\"target_fs (int) – maximum sampling frequency supported by the model\",\"causal (bool) – Whether or not to adopt causal processing if True, LSTM will be used instead of BLSTM for time modeling\",\"forward(x, fs=None)\",\"BSRNN forward.\",\"Parameters:\",\"x (torch.Tensor) – input tensor of shape (B, T, F, 2)\",\"fs (int,optional) – sampling rate of the input signal. if not None, the input signal will be truncated to only process the effective frequency subbands. if None, the input signal is assumed to be already truncated to only contain effective frequency subbands.\",\"Returns: output tensor of shape (B, T, F, 2)\",\"Return type: out (torch.Tensor)\",\"training : bool\"]},\"1463\":{\"h\":\"espnet2.enh.separator.bsrnn_separator.BSRNNSeparator\",\"t\":[\"class espnet2.enh.separator.bsrnn_separator.BSRNNSeparator(input_dim: int, num_spk: int = 1, num_channels: int = 16, num_layers: int = 6, target_fs: int = 48000, causal: bool = True, ref_channel: int | None = None)\",\"Bases: AbsSeparator\",\"Band-split RNN (BSRNN) separator.\",\"Reference: : [1] J. Yu, H. Chen, Y. Luo, R. Gu, and C. Weng, “High fidelity speech enhancement with band-split RNN,” in Proc. ISCA Interspeech, 2023. https://isca-speech.org/archive/interspeech_2023/yu23b_interspeech.html [2] J. Yu, and Y. Luo, “Efficient monaural speech enhancement with universal sample rate band-split RNN,” in Proc. ICASSP, 2023. https://ieeexplore.ieee.org/document/10096020\",\"Parameters:\",\"input_dim – (int) maximum number of frequency bins corresponding to target_fs\",\"num_spk – (int) number of speakers.\",\"num_channels – (int) feature dimension in the BandSplit block.\",\"num_layers – (int) number of processing layers.\",\"target_fs – (int) max sampling frequency that the model can handle.\",\"causal (bool) – whether or not to apply causal modeling. if True, LSTM will be used instead of BLSTM for time modeling\",\"ref_channel – (int) reference channel. not used for now.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"BSRNN Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – STFT spectrum [B, T, (C,) F (,2)]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model. unused in this model.\",\"Returns: [(B, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1464\":{\"h\":\"espnet2.enh.layers.bsrnn.BandSplit\",\"t\":[\"<!-- _espnet2.enh.layers.bsrnn.BandSplit -->\",\"class espnet2.enh.layers.bsrnn.BandSplit(input_dim, target_fs=48000, channels=128)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, fs=None)\",\"BandSplit forward.\",\"Parameters:\",\"x (torch.Tensor) – input tensor of shape (B, T, F, 2)\",\"fs (int,optional) – sampling rate of the input signal. if not None, the input signal will be truncated to only process the effective frequency subbands. if None, the input signal is assumed to be already truncated to only contain effective frequency subbands.\",\"Returns: output tensor of shape (B, N, T, K’) : K’ might be smaller than len(self.subbands) if fs < self.target_fs.\",\"Return type: z (torch.Tensor)\",\"training : bool\"]},\"1465\":{\"h\":\"espnet2.enh.layers.dcunet.BatchNorm\",\"t\":[\"<!-- _espnet2.enh.layers.dcunet.BatchNorm -->\",\"class espnet2.enh.layers.dcunet.BatchNorm(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, device=None, dtype=None)\",\"Bases: _BatchNorm\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"affine : bool\",\"eps : float\",\"momentum : float\",\"num_features : int\",\"track_running_stats : bool\"]},\"1466\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.CISDRLoss\",\"t\":[\"class espnet2.enh.loss.criterions.time_domain.CISDRLoss(filter_length=512, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"CI-SDR loss\",\"Reference: : Convolutive Transfer Function Invariant SDR Training Criteria for Multi-Channel Reverberant Speech Separation; C. Boeddeker et al., 2021; https://arxiv.org/abs/2011.15003\",\"Parameters:\",\"ref – (Batch, samples)\",\"inf – (Batch, samples)\",\"filter_length (int) – a time-invariant filter that allows slight distortion via filtering\",\"Returns: (Batch,)\",\"Return type: loss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref: Tensor, inf: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1467\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1468\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CRPBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.CRPBlock(features, n_stages, act=ReLU(), maxpool=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1469\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1470\":{\"h\":\"espnet2.enh.layers.uses.ChannelAttention\",\"t\":[\"<!-- _espnet2.enh.layers.uses.ChannelAttention -->\",\"class espnet2.enh.layers.uses.ChannelAttention(input_dim, att_heads=4, att_dim=256, activation='relu', eps=1e-05)\",\"Bases: Module\",\"Channel Attention module.\",\"Parameters:\",\"input_dim (int) – dimension of the input feature.\",\"att_heads (int) – number of attention heads in self-attention.\",\"att_dim (int) – projection dimension for query and key before self-attention.\",\"activation (str) – non-linear activation function.\",\"eps (float) – epsilon for layer normalization.\",\"forward(x, ref_channel=None)\",\"ChannelAttention Forward.\",\"Parameters:\",\"x (torch.Tensor) – input feature (batch, C, N, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"Returns: output feature (batch, C, N, freq, time)\",\"Return type: output (torch.Tensor)\",\"training : bool\"]},\"1471\":{\"h\":\"espnet2.enh.layers.uses.ChannelTAC\",\"t\":[\"<!-- _espnet2.enh.layers.uses.ChannelTAC -->\",\"class espnet2.enh.layers.uses.ChannelTAC(input_dim, eps=1e-05)\",\"Bases: Module\",\"Channel Transform-Average-Concatenate (TAC) module.\",\"Parameters:\",\"input_dim (int) – dimension of the input feature.\",\"eps (float) – epsilon for layer normalization.\",\"forward(x, ref_channel=None)\",\"ChannelTAC Forward.\",\"Parameters:\",\"x (torch.Tensor) – input feature (batch, C, N, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"Returns: output feature (batch, C, N, freq, time)\",\"Return type: output (torch.Tensor)\",\"training : bool\"]},\"1472\":{\"h\":\"espnet2.enh.layers.tcn.ChannelwiseLayerNorm\",\"t\":[\"class espnet2.enh.layers.tcn.ChannelwiseLayerNorm(channel_size, shape='BDT')\",\"Bases: Module\",\"Channel-wise Layer Normalization (cLN).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, K], M is batch size, N is channel size, K is length\",\"Returns: [M, N, K]\",\"Return type: cLN_y\",\"reset_parameters()\",\"training : bool\"]},\"1473\":{\"h\":\"espnet2.enh.layers.tcn.Chomp1d\",\"t\":[\"<!-- _espnet2.enh.layers.tcn.Chomp1d -->\",\"class espnet2.enh.layers.tcn.Chomp1d(chomp_size)\",\"Bases: Module\",\"To ensure the output length is the same as the input.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, H, Kpad]\",\"Returns: [M, H, K]\",\"training : bool\"]},\"1474\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.Combine\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layerspp.Combine(dim1, dim2, method='cat')\",\"Bases: Module\",\"Combine information from skip connections.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1475\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1476\":{\"h\":\"espnet2.enh.layers.complexnn.ComplexBatchNorm\",\"t\":[\"class espnet2.enh.layers.complexnn.ComplexBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, complex_axis=1)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(inputs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1477\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"reset_parameters()\",\"reset_running_stats()\",\"training : bool\"]},\"1478\":{\"h\":\"espnet2.enh.layers.complexnn.ComplexConv2d\",\"t\":[\"<!-- _espnet2.enh.layers.complexnn.ComplexConv2d -->\",\"class espnet2.enh.layers.complexnn.ComplexConv2d(in_channels, out_channels, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=1, groups=1, causal=True, complex_axis=1)\",\"Bases: Module\",\"ComplexConv2d.\",\"in_channels: real+imag out_channels: real+imag kernel_size : input [B,C,D,T] kernel size in [D,T] padding : input [B,C,D,T] padding in [D,T] causal: if causal, will padding time dimension’s left side,\",\"otherwise both\",\"forward(inputs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1479\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1480\":{\"h\":\"espnet2.enh.layers.complexnn.ComplexConvTranspose2d\",\"t\":[\"class espnet2.enh.layers.complexnn.ComplexConvTranspose2d(in_channels, out_channels, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), output_padding=(0, 0), causal=False, complex_axis=1, groups=1)\",\"Bases: Module\",\"ComplexConvTranspose2d.\",\"in_channels: real+imag out_channels: real+imag\",\"forward(inputs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1481\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1482\":{\"h\":\"espnet2.enh.layers.dcunet.ComplexLinear\",\"t\":[\"<!-- _espnet2.enh.layers.dcunet.ComplexLinear -->\",\"class espnet2.enh.layers.dcunet.ComplexLinear(input_dim, output_dim, complex_valued)\",\"Bases: Module\",\"A potentially complex-valued linear layer. Reduces to a regular linear\",\"layer if complex_valued=False.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1483\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1484\":{\"h\":\"espnet2.enh.layers.adapt_layers.ConcatAdaptLayer\",\"t\":[\"class espnet2.enh.layers.adapt_layers.ConcatAdaptLayer(indim, enrolldim, ninputs=1)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(main, enroll)\",\"ConcatAdaptLayer forward.\",\"Parameters:\",\"main –\",\"tensor or tuple or list activations in the main neural network, which are adapted tuple/list may be useful when we want to apply the adaptation\",\"to both normal and skip connection at once\",\"enroll –\",\"tensor or tuple or list embedding extracted from enrollment tuple/list may be useful when we want to apply the adaptation\",\"to both normal and skip connection at once\",\"training : bool\"]},\"1485\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CondCRPBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.CondCRPBlock(features, n_stages, num_classes, normalizer, act=ReLU())\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1486\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1487\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CondMSFBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.CondMSFBlock(in_planes, features, num_classes, normalizer)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs, y, shape)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1488\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1489\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CondRCUBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.CondRCUBlock(features, n_blocks, n_stages, num_classes, normalizer, act=ReLU())\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1490\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1491\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CondRefineBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.CondRefineBlock(in_planes, features, num_classes, normalizer, act=ReLU(), start=False, end=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs, y, output_shape)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1492\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1493\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalBatchNorm2d\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalBatchNorm2d(num_features, num_classes, bias=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1494\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1495\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2d\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2d(num_features, num_classes, bias=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1496\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1497\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2dPlus\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2dPlus(num_features, num_classes, bias=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1498\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1499\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalNoneNorm2d\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalNoneNorm2d(num_features, num_classes, bias=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1500\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1501\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ConditionalResidualBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.ConditionalResidualBlock(input_dim, output_dim, num_classes, resample=1, act=ELU(alpha=1.0), normalization=<class 'espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2dPlus'>, adjust_padding=False, dilation=None)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1502\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1503\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalVarianceNorm2d\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalVarianceNorm2d(num_features, num_classes, bias=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1504\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1505\":{\"h\":\"espnet2.enh.separator.conformer_separator.ConformerSeparator\",\"t\":[\"class espnet2.enh.separator.conformer_separator.ConformerSeparator(input_dim: int, num_spk: int = 2, predict_noise: bool = False, adim: int = 384, aheads: int = 4, layers: int = 6, linear_units: int = 1536, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, normalize_before: bool = False, concat_after: bool = False, dropout_rate: float = 0.1, input_layer: str = 'linear', positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, nonlinear: str = 'relu', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, conformer_enc_kernel_size: int = 7, padding_idx: int = -1)\",\"Bases: AbsSeparator\",\"Conformer separator.\",\"Parameters:\",\"input_dim – input feature dimension\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"adim (int) – Dimension of attention.\",\"aheads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"layers (int) – The number of transformer blocks.\",\"dropout_rate (float) – Dropout rate.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"conformer_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"conformer_self_attn_layer_type (str) – Encoder attention layer type.\",\"conformer_activation_type (str) – Encoder activation function type.\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"use_macaron_style_in_conformer (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_in_conformer (bool) – Whether to use convolution module.\",\"conformer_enc_kernel_size (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1506\":{\"h\":\"espnet2.enh.layers.tcndenseunet.Conv2DActNorm\",\"t\":[\"class espnet2.enh.layers.tcndenseunet.Conv2DActNorm(in_channels, out_channels, ksz=(3, 3), stride=(1, 2), padding=(1, 0), upsample=False, activation=<class 'torch.nn.modules.activation.ELU'>)\",\"Bases: Module\",\"Basic Conv2D + activation + instance norm building block.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inp)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1507\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1508\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.Conv2d\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.Conv2d(in_ch, out_ch, kernel, up=False, down=False, resample_kernel=(1, 3, 3, 1), use_bias=True, kernel_init=None)\",\"Bases: Module\",\"Conv2d layer with optimal upsampling and downsampling (StyleGAN2).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1509\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1510\":{\"h\":\"espnet2.enh.decoder.conv_decoder.ConvDecoder\",\"t\":[\"class espnet2.enh.decoder.conv_decoder.ConvDecoder(channel: int, kernel_size: int, stride: int)\",\"Bases: AbsDecoder\",\"Transposed Convolutional decoder for speech enhancement and separation\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – spectrum [Batch, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz (Not used)\",\"forward_streaming(input_frame: Tensor)\",\"streaming_merge(chunks: Tensor, ilens: tensor | None = None)\",\"Stream Merge.\",\"It merges the frame-level processed audio chunks in the streaming simulation. It is noted that, in real applications, the processed audio should be sent to the output channel frame by frame. You may refer to this function to manage your streaming output buffer.\",\"Parameters:\",\"chunks – List [(B, frame_size),]\",\"ilens – [B]\",\"Returns: [B, T]\",\"Return type: merge_audio\",\"training : bool\"]},\"1511\":{\"h\":\"espnet2.enh.encoder.conv_encoder.ConvEncoder\",\"t\":[\"class espnet2.enh.encoder.conv_encoder.ConvEncoder(channel: int, kernel_size: int, stride: int)\",\"Bases: AbsEncoder\",\"Convolutional encoder for speech enhancement and separation\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – mixed speech [Batch, sample]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz (Not used)\",\"Returns: mixed feature after encoder [Batch, flens, channel]\",\"Return type: feature (torch.Tensor)\",\"forward_streaming(input: Tensor)\",\"property output_dim : int\",\"streaming_frame(audio: Tensor)\",\"Stream frame.\",\"It splits the continuous audio into frame-level audio chunks in the streaming simulation. It is noted that this function takes the entire long audio as input for a streaming simulation. You may refer to this function to manage your streaming input buffer in a real streaming application.\",\"Parameters:audio – (B, T)\",\"Returns: List [(B, frame_size),]\",\"Return type: chunked\",\"training : bool\"]},\"1512\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ConvMeanPool\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.ConvMeanPool(input_dim, output_dim, kernel_size=3, biases=True, adjust_padding=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inputs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1513\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1514\":{\"h\":\"espnet2.enh.diffusion.sampling.correctors.Corrector\",\"t\":[\"class espnet2.enh.diffusion.sampling.correctors.Corrector(sde, score_fn, snr, n_steps)\",\"Bases: ABC\",\"The abstract class for a corrector algorithm.\",\"abstract update_fn(x, t, *args)\",\"One update of the corrector.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A PyTorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1515\":{\"h\":\"espnet2.enh.separator.dan_separator.DANSeparator\",\"t\":[\"class espnet2.enh.separator.dan_separator.DANSeparator(input_dim: int, rnn_type: str = 'blstm', num_spk: int = 2, nonlinear: str = 'tanh', layer: int = 2, unit: int = 512, emb_D: int = 40, dropout: float = 0.0)\",\"Bases: AbsSeparator\",\"Deep Attractor Network Separator\",\"Reference: : DEEP ATTRACTOR NETWORK FOR SINGLE-MICROPHONE SPEAKER SEPARATION; Zhuo Chen. et al., 2017; https://pubmed.ncbi.nlm.nih.gov/29430212/\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘blstm’, ‘lstm’ etc.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"emb_D – int, dimension of the attribute vector for one tf-bin.\",\"dropout – float, dropout ratio. Default is 0.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model e.g. “feature_ref”: list of reference spectra List[(B, T, F)]\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1516\":{\"h\":\"espnet2.enh.separator.dccrn_separator.DCCRNSeparator\",\"t\":[\"class espnet2.enh.separator.dccrn_separator.DCCRNSeparator(input_dim: int, num_spk: int = 1, rnn_layer: int = 2, rnn_units: int = 256, masking_mode: str = 'E', use_clstm: bool = True, bidirectional: bool = False, use_cbn: bool = False, kernel_size: int = 5, kernel_num: List[int] = [32, 64, 128, 256, 256, 256], use_builtin_complex: bool = True, use_noise_mask: bool = False)\",\"Bases: AbsSeparator\",\"DCCRN separator.\",\"Parameters:\",\"input_dim (int) – input dimension。\",\"num_spk (int,optional) – number of speakers. Defaults to 1.\",\"rnn_layer (int,optional) – number of lstm layers in the crn. Defaults to 2.\",\"rnn_units (int,optional) – rnn units. Defaults to 128.\",\"masking_mode (str,optional) – usage of the estimated mask. Defaults to “E”.\",\"use_clstm (bool,optional) – whether use complex LSTM. Defaults to False.\",\"bidirectional (bool,optional) – whether use BLSTM. Defaults to False.\",\"use_cbn (bool,optional) – whether use complex BN. Defaults to False.\",\"kernel_size (int,optional) – convolution kernel size. Defaults to 5.\",\"kernel_num (list,optional) – output dimension of each layer of the encoder.\",\"use_builtin_complex (bool,optional) – torch.complex if True, else ComplexTensor.\",\"use_noise_mask (bool,optional) – whether to estimate the mask of noise.\",\"apply_masks(masks: List[Tensor | ComplexTensor], real: Tensor, imag: Tensor)\",\"apply masks\",\"Parameters:\",\"masks – est_masks, [(B, T, F), …]\",\"real (torch.Tensor) – real part of the noisy spectrum, (B, F, T)\",\"imag (torch.Tensor) – imag part of the noisy spectrum, (B, F, T)\",\"Returns: [(B, T, F), …]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"create_masks(mask_tensor: Tensor)\",\"create estimated mask for each speaker\",\"Parameters:mask_tensor (torch.Tensor) – output of decoder, shape(B, 2*num_spk, F-1, T)\",\"flatten_parameters()\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1517\":{\"h\":\"espnet2.enh.layers.dcunet.DCUNet\",\"t\":[\"<!-- _espnet2.enh.layers.dcunet.DCUNet -->\",\"class espnet2.enh.layers.dcunet.DCUNet(dcunet_architecture: str = 'DilDCUNet-v2', dcunet_time_embedding: str = 'gfp', dcunet_temb_layers_global: int = 2, dcunet_temb_layers_local: int = 1, dcunet_temb_activation: str = 'silu', dcunet_time_embedding_complex: bool = False, dcunet_fix_length: str = 'pad', dcunet_mask_bound: str = 'none', dcunet_norm_type: str = 'bN', dcunet_activation: str = 'relu', embed_dim: int = 128, **kwargs)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"fix_input_dims(x)\",\"fix_output_dims(out, x)\",\"forward(spec, t)\",\"Input shape is expected to be $(batch, nfreqs, time)$, with $nfreqs - 1$\",\"divisible by $f_0 * f_1 * … * f_N$ where $f_k$ are the frequency strides of the encoders, and $time - 1$ is divisible by $t_0 * t_1 * … * t_N$ where $t_N$ are the time strides of the encoders. :param spec: complex spectrogram tensor. 1D, 2D or 3D tensor, time last. :type spec: Tensor\",\"Returns: Tensor, of shape (batch, time) or (time).\",\"training : bool\"]},\"1518\":{\"h\":\"espnet2.enh.layers.dcunet.DCUNetComplexDecoderBlock\",\"t\":[\"class espnet2.enh.layers.dcunet.DCUNetComplexDecoderBlock(in_chan, out_chan, kernel_size, stride, padding, dilation, output_padding=(0, 0), norm_type='bN', activation='leaky_relu', embed_dim=None, temb_layers=1, temb_activation='swish', complex_time_embedding=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, t_embed, output_size=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1519\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1520\":{\"h\":\"espnet2.enh.layers.dcunet.DCUNetComplexEncoderBlock\",\"t\":[\"class espnet2.enh.layers.dcunet.DCUNetComplexEncoderBlock(in_chan, out_chan, kernel_size, stride, padding, dilation, norm_type='bN', activation='leaky_relu', embed_dim=None, complex_time_embedding=False, temb_layers=1, temb_activation='silu')\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, t_embed)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1521\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1522\":{\"h\":\"espnet2.enh.layers.dc_crn.DC_CRN\",\"t\":[\"<!-- _espnet2.enh.layers.dc_crn.DC_CRN -->\",\"class espnet2.enh.layers.dc_crn.DC_CRN(input_dim, input_channels: List = [2, 16, 32, 64, 128, 256], enc_hid_channels=8, enc_kernel_size=(1, 3), enc_padding=(0, 1), enc_last_kernel_size=(1, 4), enc_last_stride=(1, 2), enc_last_padding=(0, 1), enc_layers=5, skip_last_kernel_size=(1, 3), skip_last_stride=(1, 1), skip_last_padding=(0, 1), glstm_groups=2, glstm_layers=2, glstm_bidirectional=False, glstm_rearrange=False, output_channels=2)\",\"Bases: Module\",\"Densely-Connected Convolutional Recurrent Network (DC-CRN).\",\"Reference: Fig. 3 and Section III-B in [1]\",\"Parameters:\",\"input_dim (int) – input feature dimension\",\"input_channels (list) – number of input channels for the stacked DenselyConnectedBlock layers Its length should be (number of DenselyConnectedBlock layers). It is recommended to use even number of channels to avoid AssertError when glstm_bidirectional=True.\",\"enc_hid_channels (int) – common number of intermediate channels for all DenselyConnectedBlock of the encoder\",\"enc_kernel_size (tuple) – common kernel size for all DenselyConnectedBlock of the encoder\",\"enc_padding (tuple) – common padding for all DenselyConnectedBlock of the encoder\",\"enc_last_kernel_size (tuple) – common kernel size for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_last_stride (tuple) – common stride for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_last_padding (tuple) – common padding for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_layers (int) – common total number of Conv layers for all DenselyConnectedBlock layers of the encoder\",\"skip_last_kernel_size (tuple) – common kernel size for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"skip_last_stride (tuple) – common stride for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"skip_last_padding (tuple) – common padding for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"glstm_groups (int) – number of groups in each Grouped LSTM layer\",\"glstm_layers (int) – number of Grouped LSTM layers\",\"glstm_bidirectional (bool) – whether to use BLSTM or unidirectional LSTM in Grouped LSTM layers\",\"glstm_rearrange (bool) – whether to apply the rearrange operation after each grouped LSTM layer\",\"output_channels (int) – number of output channels (must be an even number to recover both real and imaginary parts)\",\"forward(x)\",\"DC-CRN forward.\",\"Parameters:x (torch.Tensor) – Concatenated real and imaginary spectrum features (B, input_channels[0], T, F)\",\"Returns: (B, 2, output_channels, T, F)\",\"Return type: out (torch.Tensor)\",\"training : bool\"]},\"1523\":{\"h\":\"espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator\",\"t\":[\"class espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator(input_dim: int, num_spk: int = 2, predict_noise: bool = False, input_channels: List = [2, 16, 32, 64, 128, 256], enc_hid_channels: int = 8, enc_kernel_size: Tuple = (1, 3), enc_padding: Tuple = (0, 1), enc_last_kernel_size: Tuple = (1, 4), enc_last_stride: Tuple = (1, 2), enc_last_padding: Tuple = (0, 1), enc_layers: int = 5, skip_last_kernel_size: Tuple = (1, 3), skip_last_stride: Tuple = (1, 1), skip_last_padding: Tuple = (0, 1), glstm_groups: int = 2, glstm_layers: int = 2, glstm_bidirectional: bool = False, glstm_rearrange: bool = False, mode: str = 'masking', ref_channel: int = 0)\",\"Bases: AbsSeparator\",\"Densely-Connected Convolutional Recurrent Network (DC-CRN) Separator\",\"Reference: : Deep Learning Based Real-Time Speech Enhancement for Dual-Microphone Mobile Phones; Tan et al., 2020 https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf\",\"Parameters:\",\"input_dim – input feature dimension\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"input_channels (list) – number of input channels for the stacked DenselyConnectedBlock layers Its length should be (number of DenselyConnectedBlock layers).\",\"enc_hid_channels (int) – common number of intermediate channels for all DenselyConnectedBlock of the encoder\",\"enc_kernel_size (tuple) – common kernel size for all DenselyConnectedBlock of the encoder\",\"enc_padding (tuple) – common padding for all DenselyConnectedBlock of the encoder\",\"enc_last_kernel_size (tuple) – common kernel size for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_last_stride (tuple) – common stride for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_last_padding (tuple) – common padding for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_layers (int) – common total number of Conv layers for all DenselyConnectedBlock layers of the encoder\",\"skip_last_kernel_size (tuple) – common kernel size for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"skip_last_stride (tuple) – common stride for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"skip_last_padding (tuple) – common padding for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"glstm_groups (int) – number of groups in each Grouped LSTM layer\",\"glstm_layers (int) – number of Grouped LSTM layers\",\"glstm_bidirectional (bool) – whether to use BLSTM or unidirectional LSTM in Grouped LSTM layers\",\"glstm_rearrange (bool) – whether to apply the rearrange operation after each grouped LSTM layer\",\"output_channels (int) – number of output channels (even number)\",\"mode (str) – one of (“mapping”, “masking”) “mapping”: complex spectral mapping “masking”: complex masking\",\"ref_channel (int) – index of the reference microphone\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"DC-CRN Separator Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [Batch, T, F] or [Batch, T, C, F]\",\"ilens (torch.Tensor) – input lengths [Batch,]\",\"Returns: [(Batch, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1524\":{\"h\":\"espnet2.enh.layers.dnn_beamformer.DNN_Beamformer\",\"t\":[\"class espnet2.enh.layers.dnn_beamformer.DNN_Beamformer(bidim, btype: str = 'blstmp', blayers: int = 3, bunits: int = 300, bprojs: int = 320, num_spk: int = 1, use_noise_mask: bool = True, nonlinear: str = 'sigmoid', dropout_rate: float = 0.0, badim: int = 320, ref_channel: int = -1, beamformer_type: str = 'mvdr_souden', rtf_iterations: int = 2, mwf_mu: float = 1.0, eps: float = 1e-06, diagonal_loading: bool = True, diag_eps: float = 1e-07, mask_flooring: bool = False, flooring_thres: float = 1e-06, use_torch_solver: bool = True, use_torchaudio_api: bool = False, btaps: int = 5, bdelay: int = 3)\",\"Bases: Module\",\"DNN mask based Beamformer.\",\"Citation: : Multichannel End-to-end Speech Recognition; T. Ochiai et al., 2017; http://proceedings.mlr.press/v70/ochiai17a/ochiai17a.pdf\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"apply_beamforming(data, ilens, psd_n, psd_speech, psd_distortion=None, rtf_mat=None, spk=0)\",\"Beamforming with the provided statistics.\",\"Parameters:\",\"data (torch.complex64/ComplexTensor) – (B, F, C, T)\",\"ilens (torch.Tensor) – (B,)\",\"psd_n (torch.complex64/ComplexTensor) – Noise covariance matrix for MVDR (B, F, C, C) Observation covariance matrix for MPDR/wMPDR (B, F, C, C) Stacked observation covariance for WPD (B,F,(btaps+1)*C,(btaps+1)*C)\",\"psd_speech (torch.complex64/ComplexTensor) – Speech covariance matrix (B, F, C, C)\",\"psd_distortion (torch.complex64/ComplexTensor) – Noise covariance matrix (B, F, C, C)\",\"rtf_mat (torch.complex64/ComplexTensor) – RTF matrix (B, F, C, num_spk)\",\"spk (int) – speaker index\",\"Returns: (B, F, T) ws (torch.complex64/ComplexTensor): (B, F) or (B, F, (btaps+1)*C)\",\"Return type: enhanced (torch.complex64/ComplexTensor)\",\"forward(data: Tensor | ComplexTensor, ilens: LongTensor, powers: List[Tensor] | None = None, oracle_masks: List[Tensor] | None = None)\",\"DNN_Beamformer forward function.\",\"Notation: : B: Batch C: Channel T: Time or Sequence length F: Freq\",\"Parameters:\",\"data (torch.complex64/ComplexTensor) – (B, T, C, F)\",\"ilens (torch.Tensor) – (B,)\",\"powers (List *[*torch.Tensor] orNone) – used for wMPDR or WPD (B, F, T)\",\"oracle_masks (List *[*torch.Tensor] orNone) – oracle masks (B, F, C, T) if not None, oracle_masks will be used instead of self.mask\",\"Returns: (B, T, F) ilens (torch.Tensor): (B,) masks (torch.Tensor): (B, T, C, F)\",\"Return type: enhanced (torch.complex64/ComplexTensor)\",\"predict_mask(data: Tensor | ComplexTensor, ilens: LongTensor)\",\"Predict masks for beamforming.\",\"Parameters:\",\"data (torch.complex64/ComplexTensor) – (B, T, C, F), double precision\",\"ilens (torch.Tensor) – (B,)\",\"Returns: (B, T, C, F) ilens (torch.Tensor): (B,)\",\"Return type: masks (torch.Tensor)\",\"training : bool\"]},\"1525\":{\"h\":\"espnet2.enh.layers.dnn_wpe.DNN_WPE\",\"t\":[\"<!-- _espnet2.enh.layers.dnn_wpe.DNN_WPE -->\",\"class espnet2.enh.layers.dnn_wpe.DNN_WPE(wtype: str = 'blstmp', widim: int = 257, wlayers: int = 3, wunits: int = 300, wprojs: int = 320, dropout_rate: float = 0.0, taps: int = 5, delay: int = 3, use_dnn_mask: bool = True, nmask: int = 1, nonlinear: str = 'sigmoid', iterations: int = 1, normalization: bool = False, eps: float = 1e-06, diagonal_loading: bool = True, diag_eps: float = 1e-07, mask_flooring: bool = False, flooring_thres: float = 1e-06, use_torch_solver: bool = True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(data: Tensor | ComplexTensor, ilens: LongTensor)\",\"DNN_WPE forward function.\",\"Notation: : B: Batch C: Channel T: Time or Sequence length F: Freq or Some dimension of the feature vector\",\"Parameters:\",\"data – (B, T, C, F)\",\"ilens – (B,)\",\"Returns: (B, T, C, F) ilens: (B,) masks (torch.Tensor or List[torch.Tensor]): (B, T, C, F) power (List[torch.Tensor]): (B, F, T)\",\"Return type: enhanced (torch.Tensor or List[torch.Tensor])\",\"predict_mask(data: Tensor | ComplexTensor, ilens: LongTensor)\",\"Predict mask for WPE dereverberation.\",\"Parameters:\",\"data (torch.complex64/ComplexTensor) – (B, T, C, F), double precision\",\"ilens (torch.Tensor) – (B,)\",\"Returns: (B, T, C, F) ilens (torch.Tensor): (B,)\",\"Return type: masks (torch.Tensor or List[torch.Tensor])\",\"training : bool\"]},\"1526\":{\"h\":\"espnet2.enh.layers.dnsmos.DNSMOS_local\",\"t\":[\"<!-- _espnet2.enh.layers.dnsmos.DNSMOS_local -->\",\"class espnet2.enh.layers.dnsmos.DNSMOS_local(primary_model_path, p808_model_path, use_gpu=False, convert_to_torch=False)\",\"Bases: object\",\"audio_melspec(audio, n_mels=120, frame_size=320, hop_length=160, sr=16000, to_db=True)\",\"get_polyfit_val(sig, bak, ovr, is_personalized_MOS)\"]},\"1527\":{\"h\":\"espnet2.enh.layers.dnsmos.DNSMOS_web\",\"t\":[\"<!-- _espnet2.enh.layers.dnsmos.DNSMOS_web -->\",\"class espnet2.enh.layers.dnsmos.DNSMOS_web(auth_key)\",\"Bases: object\"]},\"1528\":{\"h\":\"espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator\",\"t\":[\"class espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator(input_dim: int, rnn_type: str = 'blstm', num_spk: int = 2, predict_noise: bool = False, nonlinear: str = 'tanh', layer: int = 2, unit: int = 512, emb_D: int = 40, dropout: float = 0.0, alpha: float = 5.0, max_iteration: int = 500, threshold: float = 1e-05)\",\"Bases: AbsSeparator\",\"Deep Clustering End-to-End Separator\",\"References\",\"Single-Channel Multi-Speaker Separation using Deep Clustering; Yusuf Isik. et al., 2016; https://www.isca-speech.org/archive/interspeech_2016/isik16_interspeech.html\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘blstm’, ‘lstm’ etc.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"emb_D – int, dimension of the feature vector for a tf-bin.\",\"dropout – float, dropout ratio. Default is 0.\",\"alpha – float, the clustering hardness parameter.\",\"max_iteration – int, the max iterations of soft kmeans.\",\"threshold – float, the threshold to end the soft k-means process.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. V: OrderedDict[\",\"others predicted data, e.g. masks: OrderedDict[ ‘mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1529\":{\"h\":\"espnet2.enh.separator.dpcl_separator.DPCLSeparator\",\"t\":[\"class espnet2.enh.separator.dpcl_separator.DPCLSeparator(input_dim: int, rnn_type: str = 'blstm', num_spk: int = 2, nonlinear: str = 'tanh', layer: int = 2, unit: int = 512, emb_D: int = 40, dropout: float = 0.0)\",\"Bases: AbsSeparator\",\"Deep Clustering Separator.\",\"References\",\"[1] Deep clustering: Discriminative embeddings for segmentation and : separation; John R. Hershey. et al., 2016; https://ieeexplore.ieee.org/document/7471631\",\"[2] Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding : Vectors Based on Regular Simplex; Tanaka, K. et al., 2021; https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘blstm’, ‘lstm’ etc.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"emb_D – int, dimension of the feature vector for a tf-bin.\",\"dropout – float, dropout ratio. Default is 0.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. tf_embedding: OrderedDict[\",\"’tf_embedding’: learned embedding of all T-F bins (B, T * F, D),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1530\":{\"h\":\"espnet2.enh.loss.wrappers.dpcl_solver.DPCLSolver\",\"t\":[\"class espnet2.enh.loss.wrappers.dpcl_solver.DPCLSolver(criterion: AbsEnhLoss, weight=1.0)\",\"Bases: AbsLossWrapper\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref, inf, others={})\",\"A naive DPCL solver\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"inf (List *[*torch.Tensor]) – [(batch, …), …]\",\"others (List) – other data included in this solver e.g. “tf_embedding” learned embedding of all T-F bins (B, T * F, D)\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: (dict), for collecting training status others: reserved\",\"Return type: loss\",\"training : bool\"]},\"1531\":{\"h\":\"espnet2.enh.layers.dpmulcat.DPMulCat\",\"t\":[\"<!-- _espnet2.enh.layers.dpmulcat.DPMulCat -->\",\"class espnet2.enh.layers.dpmulcat.DPMulCat(input_size: int, hidden_size: int, output_size: int, num_spk: int, dropout: float = 0.0, num_layers: int = 4, bidirectional: bool = True, input_normalize: bool = False)\",\"Bases: Module\",\"Dual-path RNN module with MulCat blocks.\",\"Parameters:\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"output_size – int, dimension of the output size.\",\"num_spk – int, the number of speakers in the output.\",\"dropout – float, the dropout rate in the LSTM layer. (Default: 0.0)\",\"bidirectional – bool, whether the RNN layers are bidirectional. (Default: True)\",\"num_layers – int, number of stacked MulCat blocks. (Default: 4)\",\"input_normalize – bool, whether to apply GroupNorm on the input Tensor. (Default: False)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Compute output after DPMulCat module.\",\"Parameters:input (torch.Tensor) – The input feature. Tensor of shape (batch, N, dim1, dim2) Apply RNN on dim1 first and then dim2\",\"Returns: (list(torch.Tensor) or list(list(torch.Tensor)) : In training mode, the module returns output of each DPMulCat block. In eval mode, the module only returns output in the last block.\",\"training : bool\"]},\"1532\":{\"h\":\"espnet2.enh.layers.dprnn.DPRNN\",\"t\":[\"<!-- _espnet2.enh.layers.dprnn.DPRNN -->\",\"class espnet2.enh.layers.dprnn.DPRNN(rnn_type, input_size, hidden_size, output_size, dropout=0, num_layers=1, bidirectional=True)\",\"Bases: Module\",\"Deep dual-path RNN.\",\"Parameters:\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"output_size – int, dimension of the output size.\",\"dropout – float, dropout ratio. Default is 0.\",\"num_layers – int, number of stacked RNN layers. Default is 1.\",\"bidirectional – bool, whether the RNN layers are bidirectional. Default is True.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1533\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1534\":{\"h\":\"espnet2.enh.separator.dprnn_separator.DPRNNSeparator\",\"t\":[\"class espnet2.enh.separator.dprnn_separator.DPRNNSeparator(input_dim: int, rnn_type: str = 'lstm', bidirectional: bool = True, num_spk: int = 2, predict_noise: bool = False, nonlinear: str = 'relu', layer: int = 3, unit: int = 512, segment_size: int = 20, dropout: float = 0.0)\",\"Bases: AbsSeparator\",\"Dual-Path RNN (DPRNN) Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"segment_size – dual-path segment size\",\"dropout – float, dropout ratio. Default is 0.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1535\":{\"h\":\"espnet2.enh.layers.dprnn.DPRNN_TAC\",\"t\":[\"<!-- _espnet2.enh.layers.dprnn.DPRNN_TAC -->\",\"class espnet2.enh.layers.dprnn.DPRNN_TAC(rnn_type, input_size, hidden_size, output_size, dropout=0, num_layers=1, bidirectional=True)\",\"Bases: Module\",\"Deep duaL-path RNN with TAC applied to each layer/block.\",\"Parameters:\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"output_size – int, dimension of the output size.\",\"dropout – float, dropout ratio. Default is 0.\",\"num_layers – int, number of stacked RNN layers. Default is 1.\",\"bidirectional – bool, whether the RNN layers are bidirectional. Default is False.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1536\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1537\":{\"h\":\"espnet2.enh.layers.dptnet.DPTNet\",\"t\":[\"<!-- _espnet2.enh.layers.dptnet.DPTNet -->\",\"class espnet2.enh.layers.dptnet.DPTNet(rnn_type, input_size, hidden_size, output_size, att_heads=4, dropout=0, activation='relu', num_layers=1, bidirectional=True, norm_type='gLN')\",\"Bases: Module\",\"Dual-path transformer network.\",\"Parameters:\",\"rnn_type (str) – select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size (int) – dimension of the input feature. Input size must be a multiple of att_heads.\",\"hidden_size (int) – dimension of the hidden state.\",\"output_size (int) – dimension of the output size.\",\"att_heads (int) – number of attention heads.\",\"dropout (float) – dropout ratio. Default is 0.\",\"activation (str) – activation function applied at the output of RNN.\",\"num_layers (int) – number of stacked RNN layers. Default is 1.\",\"bidirectional (bool) – whether the RNN layers are bidirectional. Default is True.\",\"norm_type (str) – type of normalization to use after each inter- or intra-chunk Transformer block.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1538\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"inter_chunk_process(x, layer_index)\",\"intra_chunk_process(x, layer_index)\",\"training : bool\"]},\"1539\":{\"h\":\"espnet2.enh.separator.dptnet_separator.DPTNetSeparator\",\"t\":[\"class espnet2.enh.separator.dptnet_separator.DPTNetSeparator(input_dim: int, post_enc_relu: bool = True, rnn_type: str = 'lstm', bidirectional: bool = True, num_spk: int = 2, predict_noise: bool = False, unit: int = 256, att_heads: int = 4, dropout: float = 0.0, activation: str = 'relu', norm_type: str = 'gLN', layer: int = 6, segment_size: int = 20, nonlinear: str = 'relu')\",\"Bases: AbsSeparator\",\"Dual-Path Transformer Network (DPTNet) Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"unit – int, dimension of the hidden state.\",\"att_heads – number of attention heads.\",\"dropout – float, dropout ratio. Default is 0.\",\"activation – activation function applied at the output of RNN.\",\"norm_type – type of normalization to use after each inter- or intra-chunk Transformer block.\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"segment_size – dual-path segment size\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"merge_feature(x, length=None)\",\"property num_spk\",\"split_feature(x)\",\"training : bool\"]},\"1540\":{\"h\":\"espnet2.enh.separator.svoice_separator.Decoder\",\"t\":[\"class espnet2.enh.separator.svoice_separator.Decoder(kernel_size)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(est_source)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1541\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1542\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.Dense\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.Dense\",\"Bases: Module\",\"Linear layer with default_init.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"training : bool\"]},\"1543\":{\"h\":\"espnet2.enh.layers.tcndenseunet.DenseBlock\",\"t\":[\"<!-- _espnet2.enh.layers.tcndenseunet.DenseBlock -->\",\"class espnet2.enh.layers.tcndenseunet.DenseBlock(in_channels, out_channels, num_freqs, pre_blocks=2, freq_proc_blocks=1, post_blocks=2, ksz=(3, 3), activation=<class 'torch.nn.modules.activation.ELU'>, hid_chans=32)\",\"Bases: Module\",\"single DenseNet block as used in iNeuBe model.\",\"Parameters:\",\"in_channels – number of input channels (image axis).\",\"out_channels – number of output channels (image axis).\",\"num_freqs – number of complex frequencies in the input STFT complex image-like tensor. The input is batch, image_channels, frames, freqs.\",\"pre_blocks – dense block before point-wise convolution block over frequency axis.\",\"freq_proc_blocks – number of frequency axis processing blocks.\",\"post_blocks – dense block after point-wise convolution block over frequency axis.\",\"ksz – kernel size used in densenet Conv2D layers.\",\"activation – activation function to use in the whole iNeuBe model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"hid_chans – number of hidden channels in densenet Conv2D.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1544\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1545\":{\"h\":\"espnet2.enh.layers.dc_crn.DenselyConnectedBlock\",\"t\":[\"class espnet2.enh.layers.dc_crn.DenselyConnectedBlock(in_channels, out_channels, hid_channels=8, kernel_size=(1, 3), padding=(0, 1), last_kernel_size=(1, 4), last_stride=(1, 2), last_padding=(0, 1), last_output_padding=(0, 0), layers=5, transposed=False)\",\"Bases: Module\",\"Densely-Connected Convolutional Block.\",\"Parameters:\",\"in_channels (int) – number of input channels\",\"out_channels (int) – number of output channels\",\"hid_channels (int) – number of output channels in intermediate Conv layers\",\"kernel_size (tuple) – kernel size for all but the last Conv layers\",\"padding (tuple) – padding for all but the last Conv layers\",\"last_kernel_size (tuple) – kernel size for the last GluConv layer\",\"last_stride (tuple) – stride for the last GluConv layer\",\"last_padding (tuple) – padding for the last GluConv layer\",\"last_output_padding (tuple) – output padding for the last GluConvTranspose2d (only used when transposed=True)\",\"layers (int) – total number of Conv layers\",\"transposed (bool) – True to use GluConvTranspose2d in the last layer False to use GluConv2d in the last layer\",\"forward(input)\",\"DenselyConnectedBlock forward.\",\"Parameters:input (torch.Tensor) – (B, C, T_in, F_in)\",\"Returns: (B, C, T_out, F_out)\",\"Return type: out (torch.Tensor)\",\"training : bool\"]},\"1546\":{\"h\":\"espnet2.enh.layers.tcn.DepthwiseSeparableConv\",\"t\":[\"class espnet2.enh.layers.tcn.DepthwiseSeparableConv(in_channels, out_channels, skip_channels, kernel_size, stride, padding, dilation, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, H, K]\",\"Returns: [M, B, K] skip_out: [M, Sc, K]\",\"Return type: res_out\",\"training : bool\"]},\"1547\":{\"h\":\"espnet2.enh.layers.dcunet.DiffusionStepEmbedding\",\"t\":[\"class espnet2.enh.layers.dcunet.DiffusionStepEmbedding(embed_dim, complex_valued=False)\",\"Bases: Module\",\"Diffusion-Step embedding as in DiffWave / Vaswani et al. 2017.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(t)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1548\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1549\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.Downsample\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.Downsample(channels, with_conv=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1550\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1551\":{\"h\":\"espnet2.enh.diffusion_enh.ESPnetDiffusionModel\",\"t\":[\"class espnet2.enh.diffusion_enh.ESPnetDiffusionModel(encoder: AbsEncoder, diffusion: AbsDiffusion, decoder: AbsDecoder, num_spk: int = 1, normalize: bool = False, **kwargs)\",\"Bases: ESPnetEnhancementModel\",\"Target Speaker Extraction Frontend model\",\"Main entry of speech enhancement/separation model training.\",\"Parameters:\",\"encoder – waveform encoder that converts waveforms to feature representations\",\"separator – separator that enhance or separate the feature representations\",\"decoder – waveform decoder that converts the feature back to waveforms\",\"mask_module – mask module that converts the feature to masks NOTE: Only used for compatibility with joint speaker diarization. See test/espnet2/enh/test_espnet_enh_s2t_model.py for details.\",\"loss_wrappers – list of loss wrappers Each loss wrapper contains a criterion for loss calculation and the corresonding loss weight. The losses will be calculated in the order of the list and summed up.\",\"------------------------------------------------------------------ –\",\"stft_consistency – (deprecated, kept for compatibility) whether to compute the TF-domain loss while enforcing STFT consistency NOTE: STFT consistency is now always used for frequency-domain spectrum losses.\",\"loss_type – (deprecated, kept for compatibility) loss type\",\"mask_type – (deprecated, kept for compatibility) mask type in TF-domain model\",\"------------------------------------------------------------------ –\",\"flexible_numspk – whether to allow the model to predict a variable number of speakers in its output. NOTE: This should be used when training a speech separation model for unknown number of speakers.\",\"------------------------------------------------------------------ –\",\"extract_feats_in_collect_stats – used in espnet2/tasks/abs_task.py for determining whether or not to skip model building in collect_stats stage (stage 5 in egs2/\",\"*\",\"/enh1/enh.sh).\",\"normalize_variance – whether to normalize the signal variance before model forward, and revert it back after.\",\"normalize_variance_per_ch – whether to normalize the signal variance for each channel instead of the whole signal. NOTE: normalize_variance and normalize_variance_per_ch cannot be True at the same time.\",\"------------------------------------------------------------------ –\",\"categories – list of all possible categories of minibatches (order matters!) (e.g. [“1ch_8k_reverb”, “1ch_8k_both”] for multi-condition training) NOTE: this will be used to convert category index to the corresponding name for logging in forward_loss. Different categories will have different loss name suffixes.\",\"category_weights – list of weights for each category. Used to set loss weights for batches of different categories.\",\"------------------------------------------------------------------ –\",\"always_forward_in_48k – whether to always upsample the input speech to 48kHz for forward, and then downsample to the original sample rate for loss calculation. NOTE: this can be useful to train a model capable of handling various sampling rates while unifying bandwidth extension + speech enhancement.\",\"collect_feats(speech_mix: Tensor, speech_mix_lengths: Tensor, **kwargs)\",\"enhance(feature_mix)\",\"forward(speech_mix: Tensor, speech_mix_lengths: Tensor | None = None, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech_mix – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref1 – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref2 – (Batch, samples) or (Batch, samples, channels)\",\"... –\",\"speech_mix_lengths – (Batch,), default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"enroll_ref1 – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 1\",\"enroll_ref2 – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 2\",\"... –\",\"kwargs – “utt_id” is among the input.\",\"forward_loss(speech_ref, speech_mix, speech_lengths)\",\"training : bool\"]},\"1552\":{\"h\":\"espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel\",\"t\":[\"class espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel(enh_model: ESPnetEnhancementModel, s2t_model: ESPnetASRModel | ESPnetSTModel | ESPnetDiarizationModel, calc_enh_loss: bool = True, bypass_enh_prob: float = 0)\",\"Bases: AbsESPnetModel\",\"Joint model Enhancement and Speech to Text.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"asr_pit_loss(speech, speech_lengths, text, text_lengths)\",\"batchify_nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor, batch_size: int = 100)\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param encoder_out: (Batch, Length, Dim) :param encoder_out_lens: (Batch,) :param ys_pad: (Batch, Length) :param ys_pad_lens: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase GPU memory usage\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor)\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"encode_diar(speech: Tensor, speech_lengths: Tensor, num_spk: int)\",\"Frontend + Encoder. Note that this method is used by diar_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"num_spk – int\",\"forward(speech: Tensor, speech_lengths: Tensor | None = None, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, ) default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"task (For Enh+ASR) – text_spk1: (Batch, Length) text_spk2: (Batch, Length) … text_spk1_lengths: (Batch,) text_spk2_lengths: (Batch,) …\",\"tasks (For other) –\",\"text: (Batch, Length) default None just to keep the argument order text_lengths: (Batch,)\",\"default None for the same reason as speech_lengths\",\"inherite_attributes(inherite_enh_attrs: List[str] = [], inherite_s2t_attrs: List[str] = [])\",\"nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor)\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"Normally, this function is called in batchify_nll.\",\"Parameters:\",\"encoder_out – (Batch, Length, Dim)\",\"encoder_out_lens – (Batch,)\",\"ys_pad – (Batch, Length)\",\"ys_pad_lens – (Batch,)\",\"permutation_invariant_training(losses: Tensor)\",\"Compute PIT loss.\",\"Parameters:losses (torch.Tensor) – (batch, nref, nhyp)\",\"Returns: list: (batch, n_spk) loss: torch.Tensor: (batch)\",\"Return type: perm\",\"training : bool\"]},\"1553\":{\"h\":\"espnet2.enh.espnet_model.ESPnetEnhancementModel\",\"t\":[\"class espnet2.enh.espnet_model.ESPnetEnhancementModel(encoder: AbsEncoder, separator: AbsSeparator | None, decoder: AbsDecoder, mask_module: AbsMask | None, loss_wrappers: List[AbsLossWrapper] | None, stft_consistency: bool = False, loss_type: str = 'mask_mse', mask_type: str | None = None, flexible_numspk: bool = False, extract_feats_in_collect_stats: bool = False, normalize_variance: bool = False, normalize_variance_per_ch: bool = False, categories: list = [], category_weights: list = [], always_forward_in_48k: bool = False)\",\"Bases: AbsESPnetModel\",\"Speech enhancement or separation Frontend model\",\"Main entry of speech enhancement/separation model training.\",\"Parameters:\",\"encoder – waveform encoder that converts waveforms to feature representations\",\"separator – separator that enhance or separate the feature representations\",\"decoder – waveform decoder that converts the feature back to waveforms\",\"mask_module – mask module that converts the feature to masks NOTE: Only used for compatibility with joint speaker diarization. See test/espnet2/enh/test_espnet_enh_s2t_model.py for details.\",\"loss_wrappers – list of loss wrappers Each loss wrapper contains a criterion for loss calculation and the corresonding loss weight. The losses will be calculated in the order of the list and summed up.\",\"------------------------------------------------------------------ –\",\"stft_consistency – (deprecated, kept for compatibility) whether to compute the TF-domain loss while enforcing STFT consistency NOTE: STFT consistency is now always used for frequency-domain spectrum losses.\",\"loss_type – (deprecated, kept for compatibility) loss type\",\"mask_type – (deprecated, kept for compatibility) mask type in TF-domain model\",\"------------------------------------------------------------------ –\",\"flexible_numspk – whether to allow the model to predict a variable number of speakers in its output. NOTE: This should be used when training a speech separation model for unknown number of speakers.\",\"------------------------------------------------------------------ –\",\"extract_feats_in_collect_stats – used in espnet2/tasks/abs_task.py for determining whether or not to skip model building in collect_stats stage (stage 5 in egs2/\",\"*\",\"/enh1/enh.sh).\",\"normalize_variance – whether to normalize the signal variance before model forward, and revert it back after.\",\"normalize_variance_per_ch – whether to normalize the signal variance for each channel instead of the whole signal. NOTE: normalize_variance and normalize_variance_per_ch cannot be True at the same time.\",\"------------------------------------------------------------------ –\",\"categories – list of all possible categories of minibatches (order matters!) (e.g. [“1ch_8k_reverb”, “1ch_8k_both”] for multi-condition training) NOTE: this will be used to convert category index to the corresponding name for logging in forward_loss. Different categories will have different loss name suffixes.\",\"category_weights – list of weights for each category. Used to set loss weights for batches of different categories.\",\"------------------------------------------------------------------ –\",\"always_forward_in_48k – whether to always upsample the input speech to 48kHz for forward, and then downsample to the original sample rate for loss calculation. NOTE: this can be useful to train a model capable of handling various sampling rates while unifying bandwidth extension + speech enhancement.\",\"collect_feats(speech_mix: Tensor, speech_mix_lengths: Tensor, **kwargs)\",\"forward(speech_mix: Tensor, speech_mix_lengths: Tensor | None = None, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech_mix – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref – (Batch, num_speaker, samples) or (Batch, num_speaker, samples, channels)\",\"speech_mix_lengths – (Batch,), default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"kwargs – “utt_id” is among the input.\",\"forward_enhance(speech_mix: Tensor, speech_lengths: Tensor, additional: Dict | None = None, fs: int | None = None)\",\"forward_loss(speech_pre: Tensor, speech_lengths: Tensor, feature_mix: Tensor, feature_pre: List[Tensor], others: OrderedDict, speech_ref: List[Tensor], noise_ref: List[Tensor] | None = None, dereverb_speech_ref: List[Tensor] | None = None, category: Tensor | None = None, num_spk: int | None = None, fs: int | None = None)\",\"static sort_by_perm(nn_output, perm)\",\"Sort the input list of tensors by the specified permutation.\",\"Parameters:\",\"nn_output – List[torch.Tensor(Batch, …)], len(nn_output) == num_spk\",\"perm – (Batch, num_spk) or List[torch.Tensor(num_spk)]\",\"Returns: List[torch.Tensor(Batch, …)]\",\"Return type: nn_output_new\",\"training : bool\"]},\"1554\":{\"h\":\"espnet2.enh.espnet_model_tse.ESPnetExtractionModel\",\"t\":[\"class espnet2.enh.espnet_model_tse.ESPnetExtractionModel(encoder: AbsEncoder, extractor: AbsExtractor, decoder: AbsDecoder, loss_wrappers: List[AbsLossWrapper], num_spk: int = 1, flexible_numspk: bool = False, share_encoder: bool = True, extract_feats_in_collect_stats: bool = False)\",\"Bases: AbsESPnetModel\",\"Target Speaker Extraction Frontend model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech_mix: Tensor, speech_mix_lengths: Tensor, **kwargs)\",\"forward(speech_mix: Tensor, speech_mix_lengths: Tensor | None = None, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech_mix – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref1 – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref2 – (Batch, samples) or (Batch, samples, channels)\",\"... –\",\"speech_mix_lengths – (Batch,), default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"enroll_ref1 – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 1\",\"enroll_ref2 – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 2\",\"... –\",\"kwargs – “utt_id” is among the input.\",\"forward_enhance(speech_mix: Tensor, speech_lengths: Tensor, enroll_ref: Tensor, enroll_ref_lengths: Tensor, additional: Dict | None = None)\",\"forward_loss(speech_pre: Tensor, speech_lengths: Tensor, feature_mix: Tensor, feature_pre: Tensor, others: OrderedDict, speech_ref: Tensor)\",\"training : bool\"]},\"1555\":{\"h\":\"espnet2.enh.separator.svoice_separator.Encoder\",\"t\":[\"class espnet2.enh.separator.svoice_separator.Encoder(enc_kernel_size: int, enc_feat_dim: int)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(mixture)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1556\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1557\":{\"h\":\"espnet2.enh.diffusion.sampling.predictors.EulerMaruyamaPredictor\",\"t\":[\"class espnet2.enh.diffusion.sampling.predictors.EulerMaruyamaPredictor(sde, score_fn, probability_flow=False)\",\"Bases: Predictor\",\"update_fn(x, t, *args)\",\"One update of the predictor.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A Pytorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1558\":{\"h\":\"espnet2.enh.separator.fasnet_separator.FaSNetSeparator\",\"t\":[\"class espnet2.enh.separator.fasnet_separator.FaSNetSeparator(input_dim: int, enc_dim: int, feature_dim: int, hidden_dim: int, layer: int, segment_size: int, num_spk: int, win_len: int, context_len: int, fasnet_type: str, dropout: float = 0.0, sr: int = 16000, predict_noise: bool = False)\",\"Bases: AbsSeparator\",\"Filter-and-sum Network (FaSNet) Separator\",\"Parameters:\",\"input_dim – required by AbsSeparator. Not used in this model.\",\"enc_dim – encoder dimension\",\"feature_dim – feature dimension\",\"hidden_dim – hidden dimension in DPRNN\",\"layer – number of DPRNN blocks in iFaSNet\",\"segment_size – dual-path segment size\",\"num_spk – number of speakers\",\"win_len – window length in millisecond\",\"context_len – context length in millisecond\",\"fasnet_type – ‘fasnet’ or ‘ifasnet’. Select from origin fasnet or Implicit fasnet\",\"dropout – dropout rate. Default is 0.\",\"sr – samplerate of input audio\",\"predict_noise – whether to output the estimated noise signal\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – (Batch, samples, channels)\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: separated (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1559\":{\"h\":\"espnet2.enh.layers.fasnet.FaSNet_TAC\",\"t\":[\"<!-- _espnet2.enh.layers.fasnet.FaSNet_TAC -->\",\"class espnet2.enh.layers.fasnet.FaSNet_TAC(*args, **kwargs)\",\"Bases: FaSNet_base\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"abstract forward function\",\"input: shape (batch, max_num_ch, T) num_mic: shape (batch, ), the number of channels for each input.\",\"Zero for fixed geometry configuration.\",\"training : bool\"]},\"1560\":{\"h\":\"espnet2.enh.layers.fasnet.FaSNet_base\",\"t\":[\"<!-- _espnet2.enh.layers.fasnet.FaSNet_base -->\",\"class espnet2.enh.layers.fasnet.FaSNet_base(enc_dim, feature_dim, hidden_dim, layer, segment_size=24, nspk=2, win_len=16, context_len=16, dropout=0.0, sr=16000)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"abstract forward function\",\"input: shape (batch, max_num_ch, T) num_mic: shape (batch, ), the number of channels for each input.\",\"Zero for fixed geometry configuration.\",\"pad_input(input, window)\",\"Zero-padding input according to window/stride size.\",\"seg_signal_context(x, window, context)\",\"Segmenting the signal into chunks with specific context.\",\"input: : x: size (B, ch, T) window: int context: int\",\"seq_cos_sim(ref, target)\",\"Cosine similarity between some reference mics and some target mics\",\"ref: shape (nmic1, L, seg1) target: shape (nmic2, L, seg2)\",\"signal_context(x, context)\",\"signal context function\",\"Segmenting the signal into chunks with specific context. input:\",\"x: size (B, dim, nframe) context: int\",\"training : bool\"]},\"1561\":{\"h\":\"espnet2.enh.layers.dcunet.FeatureMapDense\",\"t\":[\"<!-- _espnet2.enh.layers.dcunet.FeatureMapDense -->\",\"class espnet2.enh.layers.dcunet.FeatureMapDense(input_dim, output_dim, complex_valued=False)\",\"Bases: Module\",\"A fully connected layer that reshapes outputs to feature maps.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1562\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1563\":{\"h\":\"espnet2.enh.loss.wrappers.fixed_order.FixedOrderSolver\",\"t\":[\"class espnet2.enh.loss.wrappers.fixed_order.FixedOrderSolver(criterion: AbsEnhLoss, weight=1.0)\",\"Bases: AbsLossWrapper\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref, inf, others={})\",\"An naive fixed-order solver\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"inf (List *[*torch.Tensor]) – [(batch, …), …]\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: dict, for collecting training status others: reserved\",\"Return type: loss\",\"training : bool\"]},\"1564\":{\"h\":\"espnet2.enh.layers.tcndenseunet.FreqWiseBlock\",\"t\":[\"class espnet2.enh.layers.tcndenseunet.FreqWiseBlock(in_channels, num_freqs, out_channels, activation=<class 'torch.nn.modules.activation.ELU'>)\",\"Bases: Module\",\"FreqWiseBlock, see iNeuBe paper.\",\"Block that applies pointwise 2D convolution over STFT-like image tensor on frequency axis. The input is assumed to be [batch, image_channels, frames, freq].\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inp)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1565\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1566\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence\",\"t\":[\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence(compute_on_mask=False, mask_type=None, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf)\",\"time-frequency absolute coherence loss.\",\"Reference: : Independent Vector Analysis with Deep Neural Network Source Priors; Li et al 2020; https://arxiv.org/abs/2008.11273\",\"Parameters:\",\"ref – (Batch, T, F) or (Batch, T, C, F)\",\"inf – (Batch, T, F) or (Batch, T, C, F)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\",\"training : bool\"]},\"1567\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy\",\"t\":[\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy(compute_on_mask=False, mask_type=None, ignore_id=-100, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf)\",\"time-frequency cross-entropy loss.\",\"Parameters:\",\"ref – (Batch, T) or (Batch, T, C)\",\"inf – (Batch, T, nclass) or (Batch, T, C, nclass)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\",\"training : bool\"]},\"1568\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL\",\"t\":[\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL(compute_on_mask=False, mask_type='IBM', loss_type='dpcl', name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf)\",\"time-frequency Deep Clustering loss.\",\"References\",\"[1] Deep clustering: Discriminative embeddings for segmentation and : separation; John R. Hershey. et al., 2016; https://ieeexplore.ieee.org/document/7471631\",\"[2] Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding : Vectors Based on Regular Simplex; Tanaka, K. et al., 2021; https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html\",\"Parameters:\",\"ref – List[(Batch, T, F) * spks]\",\"inf – (Batch, T*F, D)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\",\"training : bool\"]},\"1569\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1\",\"t\":[\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1(compute_on_mask=False, mask_type='IBM', name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf)\",\"time-frequency L1 loss.\",\"Parameters:\",\"ref – (Batch, T, F) or (Batch, T, C, F)\",\"inf – (Batch, T, F) or (Batch, T, C, F)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\",\"training : bool\"]},\"1570\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss\",\"t\":[\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss(name, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: AbsEnhLoss, ABC\",\"Base class for all frequence-domain Enhancement loss modules.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract property compute_on_mask : bool\",\"create_mask_label(mix_spec, ref_spec, noise_spec=None)\",\"property is_dereverb_loss : bool\",\"property is_noise_loss : bool\",\"abstract property mask_type : str\",\"property name : str\",\"property only_for_test : bool\",\"training : bool\"]},\"1571\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE\",\"t\":[\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE(compute_on_mask=False, mask_type='IBM', name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf)\",\"time-frequency MSE loss.\",\"Parameters:\",\"ref – (Batch, T, F) or (Batch, T, C, F)\",\"inf – (Batch, T, F) or (Batch, T, C, F)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\",\"training : bool\"]},\"1572\":{\"h\":\"espnet2.enh.layers.dc_crn.GLSTM\",\"t\":[\"<!-- _espnet2.enh.layers.dc_crn.GLSTM -->\",\"class espnet2.enh.layers.dc_crn.GLSTM(hidden_size=1024, groups=2, layers=2, bidirectional=False, rearrange=False)\",\"Bases: Module\",\"Grouped LSTM.\",\"Reference: : Efficient Sequence Learning with Group Recurrent Networks; Gao et al., 2018\",\"Parameters:\",\"hidden_size (int) – total hidden size of all LSTMs in each grouped LSTM layer i.e., hidden size of each LSTM is hidden_size // groups\",\"groups (int) – number of LSTMs in each grouped LSTM layer\",\"layers (int) – number of grouped LSTM layers\",\"bidirectional (bool) – whether to use BLSTM or unidirectional LSTM\",\"rearrange (bool) – whether to apply the rearrange operation after each grouped LSTM layer\",\"forward(x)\",\"Grouped LSTM forward.\",\"Parameters:x (torch.Tensor) – (B, C, T, D)\",\"Returns: (B, C, T, D)\",\"Return type: out (torch.Tensor)\",\"training : bool\"]},\"1573\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.GaussianFourierProjection\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layerspp.GaussianFourierProjection(embedding_size=256, scale=1.0)\",\"Bases: Module\",\"Gaussian Fourier embeddings for noise levels.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1574\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1575\":{\"h\":\"espnet2.enh.layers.tcn.GlobalLayerNorm\",\"t\":[\"<!-- _espnet2.enh.layers.tcn.GlobalLayerNorm -->\",\"class espnet2.enh.layers.tcn.GlobalLayerNorm(channel_size, shape='BDT')\",\"Bases: Module\",\"Global Layer Normalization (gLN).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, K], M is batch size, N is channel size, K is length\",\"Returns: [M, N, K]\",\"Return type: gLN_y\",\"reset_parameters()\",\"training : bool\"]},\"1576\":{\"h\":\"espnet2.enh.layers.dc_crn.GluConv2d\",\"t\":[\"<!-- _espnet2.enh.layers.dc_crn.GluConv2d -->\",\"class espnet2.enh.layers.dc_crn.GluConv2d(in_channels, out_channels, kernel_size, stride, padding=0)\",\"Bases: Module\",\"Conv2d with Gated Linear Units (GLU).\",\"Input and output shapes are the same as regular Conv2d layers.\",\"Reference: Section III-B in [1]\",\"Parameters:\",\"in_channels (int) – number of input channels\",\"out_channels (int) – number of output channels\",\"kernel_size (int/tuple) – kernel size in Conv2d\",\"stride (int/tuple) – stride size in Conv2d\",\"padding (int/tuple) – padding size in Conv2d\",\"forward(x)\",\"ConvGLU forward.\",\"Parameters:x (torch.Tensor) – (B, C_in, H_in, W_in)\",\"Returns: (B, C_out, H_out, W_out)\",\"Return type: out (torch.Tensor)\",\"training : bool\"]},\"1577\":{\"h\":\"espnet2.enh.layers.dc_crn.GluConvTranspose2d\",\"t\":[\"class espnet2.enh.layers.dc_crn.GluConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding=0, output_padding=(0, 0))\",\"Bases: Module\",\"ConvTranspose2d with Gated Linear Units (GLU).\",\"Input and output shapes are the same as regular ConvTranspose2d layers.\",\"Reference: Section III-B in [1]\",\"Parameters:\",\"in_channels (int) – number of input channels\",\"out_channels (int) – number of output channels\",\"kernel_size (int/tuple) – kernel size in ConvTranspose2d\",\"stride (int/tuple) – stride size in ConvTranspose2d\",\"padding (int/tuple) – padding size in ConvTranspose2d\",\"output_padding (int/tuple) – Additional size added to one side of each dimension in the output shape\",\"forward(x)\",\"DeconvGLU forward.\",\"Parameters:x (torch.Tensor) – (B, C_in, H_in, W_in)\",\"Returns: (B, C_out, H_out, W_out)\",\"Return type: out (torch.Tensor)\",\"training : bool\"]},\"1578\":{\"h\":\"espnet2.enh.separator.tfgridnet_separator.GridNetBlock\",\"t\":[\"class espnet2.enh.separator.tfgridnet_separator.GridNetBlock(emb_dim, emb_ks, emb_hs, n_freqs, hidden_channels, n_head=4, approx_qk_dim=512, activation='prelu', eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"GridNetBlock Forward.\",\"Parameters:\",\"x – [B, C, T, Q]\",\"out – [B, C, T, Q]\",\"training : bool\"]},\"1579\":{\"h\":\"espnet2.enh.separator.tfgridnetv2_separator.GridNetV2Block\",\"t\":[\"class espnet2.enh.separator.tfgridnetv2_separator.GridNetV2Block(emb_dim, emb_ks, emb_hs, n_freqs, hidden_channels, n_head=4, approx_qk_dim=512, activation='prelu', eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"GridNetV2Block Forward.\",\"Parameters:\",\"x – [B, C, T, Q]\",\"out – [B, C, T, Q]\",\"training : bool\"]},\"1580\":{\"h\":\"espnet2.enh.separator.tfgridnetv3_separator.GridNetV3Block\",\"t\":[\"class espnet2.enh.separator.tfgridnetv3_separator.GridNetV3Block(emb_dim, emb_ks, emb_hs, hidden_channels, n_head=4, qk_output_channel=4, activation='prelu', eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"GridNetV2Block Forward.\",\"Parameters:\",\"x – [B, C, T, Q]\",\"out – [B, C, T, Q]\",\"training : bool\"]},\"1581\":{\"h\":\"espnet2.enh.layers.dptnet.ImprovedTransformerLayer\",\"t\":[\"class espnet2.enh.layers.dptnet.ImprovedTransformerLayer(rnn_type, input_size, att_heads, hidden_size, dropout=0.0, activation='relu', bidirectional=True, norm='gLN')\",\"Bases: Module\",\"Container module of the (improved) Transformer proposed in [1].\",\"Reference: : Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation; Chen et al, Interspeech 2020.\",\"Parameters:\",\"rnn_type (str) – select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size (int) – Dimension of the input feature.\",\"att_heads (int) – Number of attention heads.\",\"hidden_size (int) – Dimension of the hidden state.\",\"dropout (float) – Dropout ratio. Default is 0.\",\"activation (str) – activation function applied at the output of RNN.\",\"bidirectional (bool,optional) – True for bidirectional Inter-Chunk RNN (Intra-Chunk is always bidirectional).\",\"norm (str,optional) – Type of normalization to use.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, attn_mask=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1582\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1583\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.InstanceNorm2dPlus\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.normalization.InstanceNorm2dPlus(num_features, bias=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1584\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1585\":{\"h\":\"espnet2.enh.diffusion.sampling.correctors.LangevinCorrector\",\"t\":[\"class espnet2.enh.diffusion.sampling.correctors.LangevinCorrector(sde, score_fn, snr, n_steps)\",\"Bases: Corrector\",\"update_fn(x, t, *args)\",\"One update of the corrector.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A PyTorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1586\":{\"h\":\"espnet2.enh.layers.uses.LayerNormalization\",\"t\":[\"<!-- _espnet2.enh.layers.uses.LayerNormalization -->\",\"class espnet2.enh.layers.uses.LayerNormalization(input_dim, dim=1, total_dim=4, eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1587\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1588\":{\"h\":\"espnet2.enh.separator.tfgridnet_separator.LayerNormalization4D\",\"t\":[\"class espnet2.enh.separator.tfgridnet_separator.LayerNormalization4D(input_dimension, eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1589\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1590\":{\"h\":\"espnet2.enh.separator.tfgridnetv2_separator.LayerNormalization4DCF\",\"t\":[\"class espnet2.enh.separator.tfgridnetv2_separator.LayerNormalization4DCF(input_dimension, eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1591\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1592\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.MSFBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.MSFBlock(in_planes, features)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs, shape)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1593\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1594\":{\"h\":\"espnet2.enh.layers.bsrnn.MaskDecoder\",\"t\":[\"<!-- _espnet2.enh.layers.bsrnn.MaskDecoder -->\",\"class espnet2.enh.layers.bsrnn.MaskDecoder(freq_dim, subbands, channels=128)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"MaskDecoder forward.\",\"Parameters:x (torch.Tensor) – input tensor of shape (B, N, T, K)\",\"Returns: output mask of shape (B, T, F, 2) r (torch.Tensor): output residual of shape (B, T, F, 2)\",\"Return type: m (torch.Tensor)\",\"training : bool\"]},\"1595\":{\"h\":\"espnet2.enh.layers.mask_estimator.MaskEstimator\",\"t\":[\"class espnet2.enh.layers.mask_estimator.MaskEstimator(type, idim, layers, units, projs, dropout, nmask=1, nonlinear='sigmoid')\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs: Tensor | ComplexTensor, ilens: LongTensor)\",\"Mask estimator forward function.\",\"Parameters:\",\"xs – (B, F, C, T)\",\"ilens – (B,)\",\"Returns: The hidden vector (B, F, C, T) masks: A tuple of the masks. (B, F, C, T) ilens: (B,)\",\"Return type: hs (torch.Tensor)\",\"training : bool\"]},\"1596\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.MeanPoolConv\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.MeanPoolConv(input_dim, output_dim, kernel_size=3, biases=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inputs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1597\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1598\":{\"h\":\"espnet2.enh.layers.skim.MemLSTM\",\"t\":[\"<!-- _espnet2.enh.layers.skim.MemLSTM -->\",\"class espnet2.enh.layers.skim.MemLSTM(hidden_size, dropout=0.0, bidirectional=False, mem_type='hc', norm_type='cLN')\",\"Bases: Module\",\"the Mem-LSTM of SkiM\",\"Parameters:\",\"hidden_size – int, dimension of the hidden state.\",\"dropout – float, dropout ratio. Default is 0.\",\"bidirectional – bool, whether the LSTM layers are bidirectional. Default is False.\",\"mem_type – ‘hc’, ‘h’, ‘c’ or ‘id’. It controls whether the hidden (or cell) state of SegLSTM will be processed by MemLSTM. In ‘id’ mode, both the hidden and cell states will be identically returned.\",\"norm_type – gLN, cLN. cLN is for causal implementation.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(hc, S)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1599\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_one_step(hc, state)\",\"training : bool\"]},\"1600\":{\"h\":\"espnet2.enh.loss.wrappers.mixit_solver.MixITSolver\",\"t\":[\"class espnet2.enh.loss.wrappers.mixit_solver.MixITSolver(criterion: AbsEnhLoss, weight: float = 1.0)\",\"Bases: AbsLossWrapper\",\"Mixture Invariant Training Solver.\",\"Parameters:\",\"criterion (AbsEnhLoss) – an instance of AbsEnhLoss\",\"weight (float) – weight (between 0 and 1) of current loss for multi-task learning.\",\"forward(ref: List[Tensor] | List[ComplexTensor], inf: List[Tensor] | List[ComplexTensor], others: Dict = {})\",\"MixIT solver.\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"inf (List *[*torch.Tensor]) – [(batch, …), …] x n_est\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: dict, for collecting training status others: dict, in this PIT solver, permutation order will be returned\",\"Return type: loss\",\"property name\",\"training : bool\"]},\"1601\":{\"h\":\"espnet2.enh.layers.adapt_layers.MulAddAdaptLayer\",\"t\":[\"class espnet2.enh.layers.adapt_layers.MulAddAdaptLayer(indim, enrolldim, ninputs=1, do_addition=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(main, enroll)\",\"MulAddAdaptLayer Forward.\",\"Parameters:\",\"main –\",\"tensor or tuple or list activations in the main neural network, which are adapted tuple/list may be useful when we want to apply the adaptation\",\"to both normal and skip connection at once\",\"enroll –\",\"tensor or tuple or list embedding extracted from enrollment tuple/list may be useful when we want to apply the adaptation\",\"to both normal and skip connection at once\",\"training : bool\"]},\"1602\":{\"h\":\"espnet2.enh.layers.dpmulcat.MulCatBlock\",\"t\":[\"<!-- _espnet2.enh.layers.dpmulcat.MulCatBlock -->\",\"class espnet2.enh.layers.dpmulcat.MulCatBlock(input_size: int, hidden_size: int, dropout: float = 0.0, bidirectional: bool = True)\",\"Bases: Module\",\"The MulCat block.\",\"Parameters:\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"dropout – float, the dropout rate in the LSTM layer. (Default: 0.0)\",\"bidirectional – bool, whether the RNN layers are bidirectional. (Default: True)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Compute output after MulCatBlock.\",\"Parameters:input (torch.Tensor) – The input feature. Tensor of shape (batch, time, feature_dim)\",\"Returns: The output feature after MulCatBlock. : Tensor of shape (batch, time, feature_dim)\",\"Return type: (torch.Tensor)\",\"training : bool\"]},\"1603\":{\"h\":\"espnet2.enh.loss.wrappers.multilayer_pit_solver.MultiLayerPITSolver\",\"t\":[\"class espnet2.enh.loss.wrappers.multilayer_pit_solver.MultiLayerPITSolver(criterion: AbsEnhLoss, weight=1.0, independent_perm=True, layer_weights=None)\",\"Bases: AbsLossWrapper\",\"Multi-Layer Permutation Invariant Training Solver.\",\"Compute the PIT loss given inferences of multiple layers and a single reference. It also support single inference and single reference in evaluation stage.\",\"Parameters:\",\"criterion (AbsEnhLoss) – an instance of AbsEnhLoss\",\"weight (float) – weight (between 0 and 1) of current loss for multi-task learning.\",\"independent_perm (bool) – If True, PIT will be performed in forward to find the best permutation; If False, the permutation from the last LossWrapper output will be inherited. Note: You should be careful about the ordering of loss wrappers defined in the yaml config, if this argument is False.\",\"layer_weights (Optional *[*List *[*float]]) – weights for each layer If not None, the loss of each layer will be weighted-summed using the specified weights.\",\"forward(ref, infs, others={})\",\"Permutation invariant training solver.\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"infs (Union *[*List *[*torch.Tensor],List *[*List *[*torch.Tensor]]]) – [(batch, …), …]\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: dict, for collecting training status others: dict, in this PIT solver, permutation order will be returned\",\"Return type: loss\",\"training : bool\"]},\"1604\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.MultiResL1SpecLoss\",\"t\":[\"class espnet2.enh.loss.criterions.time_domain.MultiResL1SpecLoss(window_sz=[512], hop_sz=None, eps=1e-08, time_domain_weight=0.5, normalize_variance=False, reduction='sum', name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"Multi-Resolution L1 time-domain + STFT mag loss\",\"Reference: Lu, Y. J., Cornell, S., Chang, X., Zhang, W., Li, C., Ni, Z., … & Watanabe, S. Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNET-Se Submission to the L3DAS22 Challenge. ICASSP 2022 p. 9201-9205.\",\"window_sz\",\"(list) list of STFT window sizes.\",\"hop_sz\",\"(list, optional) list of hop_sizes, default is each window_sz // 2.\",\"eps\",\"(float) stability epsilon\",\"time_domain_weight\",\"(float) weight for time domain loss.\",\"normalize_variance\",\"whether or not to normalize the variance when calculating the loss.\",\"Type: bool\",\"reduction\",\"select from “sum” and “mean”\",\"Type: str\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(target: Tensor, estimate: Tensor)\",\"forward.\",\"Parameters:\",\"target – (Batch, T)\",\"estimate – (Batch, T)\",\"Returns: (Batch,)\",\"Return type: loss\",\"get_magnitude(stft, eps=1e-06)\",\"property name : str\",\"training : bool\"]},\"1605\":{\"h\":\"espnet2.enh.layers.ncsnpp.NCSNpp\",\"t\":[\"<!-- _espnet2.enh.layers.ncsnpp.NCSNpp -->\",\"class espnet2.enh.layers.ncsnpp.NCSNpp(scale_by_sigma=True, nonlinearity='swish', nf=128, ch_mult=(1, 1, 2, 2, 2, 2, 2), num_res_blocks=2, attn_resolutions=(16,), resamp_with_conv=True, conditional=True, fir=True, fir_kernel=[1, 3, 3, 1], skip_rescale=True, resblock_type='biggan', progressive='output_skip', progressive_input='input_skip', progressive_combine='sum', init_scale=0.0, fourier_scale=16, image_size=256, embedding_type='fourier', dropout=0.0, centered=True, **unused_kwargs)\",\"Bases: Module\",\"NCSN++ model, adapted from https://github.com/yang-song/score_sde and\",\"https://github.com/sp-uhh/sgmse repository\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, time_cond)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1606\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"pad_spec(Y)\",\"training : bool\"]},\"1607\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.NIN\",\"t\":[\"<!-- _espnet2.enh.layers.ncsnpp_utils.layers.NIN -->\",\"class espnet2.enh.layers.ncsnpp_utils.layers.NIN(in_dim, num_units, init_scale=0.1)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1608\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1609\":{\"h\":\"espnet2.enh.layers.complexnn.NavieComplexLSTM\",\"t\":[\"class espnet2.enh.layers.complexnn.NavieComplexLSTM(input_size, hidden_size, projection_dim=None, bidirectional=False, batch_first=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"flatten_parameters()\",\"forward(inputs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1610\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1611\":{\"h\":\"espnet2.enh.separator.neural_beamformer.NeuralBeamformer\",\"t\":[\"class espnet2.enh.separator.neural_beamformer.NeuralBeamformer(input_dim: int, num_spk: int = 1, loss_type: str = 'mask_mse', use_wpe: bool = False, wnet_type: str = 'blstmp', wlayers: int = 3, wunits: int = 300, wprojs: int = 320, wdropout_rate: float = 0.0, taps: int = 5, delay: int = 3, use_dnn_mask_for_wpe: bool = True, wnonlinear: str = 'crelu', multi_source_wpe: bool = True, wnormalization: bool = False, use_beamformer: bool = True, bnet_type: str = 'blstmp', blayers: int = 3, bunits: int = 300, bprojs: int = 320, badim: int = 320, ref_channel: int = -1, use_noise_mask: bool = True, bnonlinear: str = 'sigmoid', beamformer_type: str = 'mvdr_souden', rtf_iterations: int = 2, bdropout_rate: float = 0.0, shared_power: bool = True, use_torchaudio_api: bool = False, diagonal_loading: bool = True, diag_eps_wpe: float = 1e-07, diag_eps_bf: float = 1e-07, mask_flooring: bool = False, flooring_thres_wpe: float = 1e-06, flooring_thres_bf: float = 1e-06, use_torch_solver: bool = True)\",\"Bases: AbsSeparator\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.complex64/ComplexTensor) – mixed speech [Batch, Frames, Channel, Freq]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: List[torch.complex64/ComplexTensor] output lengths other predcited data: OrderedDict[\",\"’dereverb1’: ComplexTensor(Batch, Frames, Channel, Freq), ‘mask_dereverb1’: torch.Tensor(Batch, Frames, Channel, Freq), ‘mask_noise1’: torch.Tensor(Batch, Frames, Channel, Freq), ‘mask_spk1’: torch.Tensor(Batch, Frames, Channel, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Channel, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Channel, Freq),\",\"]\",\"Return type: enhanced speech (single-channel)\",\"property num_spk\",\"training : bool\"]},\"1612\":{\"h\":\"espnet2.enh.diffusion.sampling.correctors.NoneCorrector\",\"t\":[\"class espnet2.enh.diffusion.sampling.correctors.NoneCorrector(*args, **kwargs)\",\"Bases: Corrector\",\"An empty corrector that does nothing.\",\"update_fn(x, t, *args)\",\"One update of the corrector.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A PyTorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1613\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.NoneNorm2d\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.normalization.NoneNorm2d(num_features, bias=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1614\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1615\":{\"h\":\"espnet2.enh.diffusion.sampling.predictors.NonePredictor\",\"t\":[\"class espnet2.enh.diffusion.sampling.predictors.NonePredictor(*args, **kwargs)\",\"Bases: Predictor\",\"An empty predictor that does nothing.\",\"update_fn(x, t, *args)\",\"One update of the predictor.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A Pytorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1616\":{\"h\":\"espnet2.enh.decoder.null_decoder.NullDecoder\",\"t\":[\"class espnet2.enh.decoder.null_decoder.NullDecoder\",\"Bases: AbsDecoder\",\"Null decoder, return the same args.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Forward. The input should be the waveform already.\",\"Parameters:\",\"input (torch.Tensor) – wav [Batch, sample]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz (Not used)\",\"training : bool\"]},\"1617\":{\"h\":\"espnet2.enh.encoder.null_encoder.NullEncoder\",\"t\":[\"class espnet2.enh.encoder.null_encoder.NullEncoder\",\"Bases: AbsEncoder\",\"Null encoder.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – mixed speech [Batch, sample]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz (Not used)\",\"property output_dim : int\",\"training : bool\"]},\"1618\":{\"h\":\"espnet2.enh.diffusion.sdes.OUVESDE\",\"t\":[\"<!-- _espnet2.enh.diffusion.sdes.OUVESDE -->\",\"class espnet2.enh.diffusion.sdes.OUVESDE(theta=1.5, sigma_min=0.05, sigma_max=0.5, N=1000, **ignored_kwargs)\",\"Bases: SDE\",\"Construct an Ornstein-Uhlenbeck Variance Exploding SDE.\",\"Note that the “steady-state mean” y is not provided at construction, but must rather be given as an argument to the methods which require it (e.g., sde or marginal_prob).\",\"dx = -theta (y-x) dt + sigma(t) dw\",\"with\",\"sigma(t) = sigma_min (sigma_max/sigma_min)^t * sqrt(2 log(sigma_max/sigma_min))\",\"Parameters:\",\"theta – stiffness parameter.\",\"sigma_min – smallest sigma.\",\"sigma_max – largest sigma.\",\"N – number of discretization steps\",\"property T\",\"End time of the SDE.\",\"copy()\",\"marginal_prob(x0, t, y)\",\"Parameters to determine the marginal distribution of\",\"the SDE, $p_t(x|args)$.\",\"prior_logp(z)\",\"Compute log-density of the prior distribution.\",\"Useful for computing the log-likelihood via probability flow ODE.\",\"Parameters:z – latent code\",\"Returns: log probability density\",\"prior_sampling(shape, y)\",\"Generate one sample from the prior distribution,\",\"$p_T(x|args)$ with shape shape.\",\"sde(x, t, y)\"]},\"1619\":{\"h\":\"espnet2.enh.diffusion.sdes.OUVPSDE\",\"t\":[\"<!-- _espnet2.enh.diffusion.sdes.OUVPSDE -->\",\"class espnet2.enh.diffusion.sdes.OUVPSDE(beta_min, beta_max, stiffness=1, N=1000, **ignored_kwargs)\",\"Bases: SDE\",\"OUVPSDE class.\",\"!!! SGMSE authors observed instabilities around t=0.2. !!!\",\"Construct an Ornstein-Uhlenbeck Variance Preserving SDE:\",\"dx = -1/2 * beta(t) * stiffness * (y-x) dt + sqrt(beta(t)) * dw\",\"with\",\"beta(t) = beta_min + t(beta_max - beta_min)\",\"Note that the “steady-state mean” y is not provided at construction, but must rather be given as an argument to the methods which require it (e.g., sde or marginal_prob).\",\"Parameters:\",\"beta_min – smallest sigma.\",\"beta_max – largest sigma.\",\"stiffness – stiffness factor of the drift. 1 by default.\",\"N – number of discretization steps\",\"property T\",\"End time of the SDE.\",\"copy()\",\"marginal_prob(x0, t, y)\",\"Parameters to determine the marginal distribution of\",\"the SDE, $p_t(x|args)$.\",\"prior_logp(z)\",\"Compute log-density of the prior distribution.\",\"Useful for computing the log-likelihood via probability flow ODE.\",\"Parameters:z – latent code\",\"Returns: log probability density\",\"prior_sampling(shape, y)\",\"Generate one sample from the prior distribution,\",\"$p_T(x|args)$ with shape shape.\",\"sde(x, t, y)\"]},\"1620\":{\"h\":\"espnet2.enh.layers.dcunet.OnReIm\",\"t\":[\"<!-- _espnet2.enh.layers.dcunet.OnReIm -->\",\"class espnet2.enh.layers.dcunet.OnReIm(module_cls, *args, **kwargs)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1621\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1622\":{\"h\":\"espnet2.enh.loss.wrappers.pit_solver.PITSolver\",\"t\":[\"class espnet2.enh.loss.wrappers.pit_solver.PITSolver(criterion: AbsEnhLoss, weight=1.0, independent_perm=True, flexible_numspk=False)\",\"Bases: AbsLossWrapper\",\"Permutation Invariant Training Solver.\",\"Parameters:\",\"criterion (AbsEnhLoss) – an instance of AbsEnhLoss\",\"weight (float) – weight (between 0 and 1) of current loss for multi-task learning.\",\"independent_perm (bool) –\",\"If True, PIT will be performed in forward to find the best permutation; If False, the permutation from the last LossWrapper output will be inherited. NOTE (wangyou): You should be careful about the ordering of loss\",\"wrappers defined in the yaml config, if this argument is False.\",\"flexible_numspk (bool) – If True, num_spk will be taken from inf to handle flexible numbers of speakers. This is because ref may include dummy data in this case.\",\"forward(ref, inf, others={})\",\"PITSolver forward.\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"inf (List *[*torch.Tensor]) – [(batch, …), …]\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: dict, for collecting training status others: dict, in this PIT solver, permutation order will be returned\",\"Return type: loss\",\"training : bool\"]},\"1623\":{\"h\":\"espnet2.enh.diffusion.sampling.predictors.Predictor\",\"t\":[\"class espnet2.enh.diffusion.sampling.predictors.Predictor(sde, score_fn, probability_flow=False)\",\"Bases: ABC\",\"The abstract class for a predictor algorithm.\",\"debug_update_fn(x, t, *args)\",\"abstract update_fn(x, t, *args)\",\"One update of the predictor.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A Pytorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1624\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.RCUBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.RCUBlock(features, n_blocks, n_stages, act=ReLU())\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1625\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1626\":{\"h\":\"espnet2.enh.separator.rnn_separator.RNNSeparator\",\"t\":[\"class espnet2.enh.separator.rnn_separator.RNNSeparator(input_dim: int, rnn_type: str = 'blstm', num_spk: int = 2, predict_noise: bool = False, nonlinear: str = 'sigmoid', layer: int = 3, unit: int = 512, dropout: float = 0.0)\",\"Bases: AbsSeparator\",\"RNN Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘blstm’, ‘lstm’ etc.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"dropout – float, dropout ratio. Default is 0.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"forward_streaming(input_frame: Tensor, states=None)\",\"property num_spk\",\"training : bool\"]},\"1627\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.RefineBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.RefineBlock(in_planes, features, act=ReLU(), start=False, end=False, maxpool=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs, output_shape)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1628\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1629\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ResidualBlock\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.ResidualBlock(input_dim, output_dim, resample=None, act=ELU(alpha=1.0), normalization=<class 'torch.nn.modules.instancenorm.InstanceNorm2d'>, adjust_padding=False, dilation=1)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1630\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1631\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.ResnetBlockBigGANpp\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layerspp.ResnetBlockBigGANpp(act, in_ch, out_ch=None, temb_dim=None, up=False, down=False, dropout=0.1, fir=False, fir_kernel=(1, 3, 3, 1), skip_rescale=True, init_scale=0.0)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, temb=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1632\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1633\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ResnetBlockDDPM\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.ResnetBlockDDPM(act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1)\",\"Bases: Module\",\"The ResNet Blocks used in DDPM.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, temb=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1634\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1635\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.ResnetBlockDDPMpp\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layerspp.ResnetBlockDDPMpp(act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1, skip_rescale=False, init_scale=0.0)\",\"Bases: Module\",\"ResBlock adapted from DDPM.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, temb=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1636\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1637\":{\"h\":\"espnet2.enh.diffusion.sampling.predictors.ReverseDiffusionPredictor\",\"t\":[\"class espnet2.enh.diffusion.sampling.predictors.ReverseDiffusionPredictor(sde, score_fn, probability_flow=False)\",\"Bases: Predictor\",\"update_fn(x, t, *args)\",\"One update of the predictor.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A Pytorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1638\":{\"h\":\"espnet2.enh.diffusion.sdes.SDE\",\"t\":[\"<!-- _espnet2.enh.diffusion.sdes.SDE -->\",\"class espnet2.enh.diffusion.sdes.SDE(N)\",\"Bases: ABC\",\"SDE abstract class. Functions are designed for a mini-batch of inputs.\",\"Construct an SDE.\",\"Parameters:N – number of discretization time steps.\",\"abstract property T\",\"End time of the SDE.\",\"abstract copy()\",\"discretize(x, t, *args)\",\"Discretize the SDE in the form: x_{i+1} = x_i + f_i(x_i) + G_i z_i.\",\"Useful for reverse diffusion sampling and probabiliy flow sampling. Defaults to Euler-Maruyama discretization.\",\"Parameters:\",\"x – a torch tensor\",\"t – a torch float representing the time step (from 0 to self.T)\",\"Returns: f, G\",\"abstract marginal_prob(x, t, *args)\",\"Parameters to determine the marginal distribution of\",\"the SDE, $p_t(x|args)$.\",\"abstract prior_logp(z)\",\"Compute log-density of the prior distribution.\",\"Useful for computing the log-likelihood via probability flow ODE.\",\"Parameters:z – latent code\",\"Returns: log probability density\",\"abstract prior_sampling(shape, *args)\",\"Generate one sample from the prior distribution,\",\"$p_T(x|args)$ with shape shape.\",\"reverse(score_model, probability_flow=False)\",\"Create the reverse-time SDE/ODE.\",\"Parameters:\",\"score_model – A function that takes x, t and y and returns the score.\",\"probability_flow – If True, create the reverse-time ODE used for probability flow sampling.\",\"abstract sde(x, t, *args)\"]},\"1639\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.SDRLoss\",\"t\":[\"class espnet2.enh.loss.criterions.time_domain.SDRLoss(filter_length=512, use_cg_iter=None, clamp_db=None, zero_mean=True, load_diag=None, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"SDR loss.\",\"filter_length: int : The length of the distortion filter allowed (default: 512)\",\"use_cg_iter: : If provided, an iterative method is used to solve for the distortion filter coefficients instead of direct Gaussian elimination. This can speed up the computation of the metrics in case the filters are long. Using a value of 10 here has been shown to provide good accuracy in most cases and is sufficient when using this loss to train neural separation networks.\",\"clamp_db: float : clamp the output value in [-clamp_db, clamp_db]\",\"zero_mean: bool : When set to True, the mean of all signals is subtracted prior.\",\"load_diag: : If provided, this small value is added to the diagonal coefficients of the system metrices when solving for the filter coefficients. This can help stabilize the metric in the case where some of the reference signals may sometimes be zero\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref: Tensor, est: Tensor)\",\"SDR forward.\",\"Parameters:\",\"ref – Tensor, (…, n_samples) reference signal\",\"est – Tensor (…, n_samples) estimated signal\",\"Returns: (…,) : the SDR loss (negative sdr)\",\"Return type: loss\",\"training : bool\"]},\"1640\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.SISNRLoss\",\"t\":[\"class espnet2.enh.loss.criterions.time_domain.SISNRLoss(clamp_db=None, zero_mean=True, eps=None, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"SI-SNR (or named SI-SDR) loss\",\"A more stable SI-SNR loss with clamp from fast_bss_eval.\",\"clamp_db\",\"float clamp the output value in [-clamp_db, clamp_db]\",\"zero_mean\",\"bool When set to True, the mean of all signals is subtracted prior.\",\"eps\",\"float Deprecated. Kept for compatibility.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref: Tensor, est: Tensor)\",\"SI-SNR forward.\",\"Parameters:\",\"ref – Tensor, (…, n_samples) reference signal\",\"est – Tensor (…, n_samples) estimated signal\",\"Returns: (…,) : the SI-SDR loss (negative si-sdr)\",\"Return type: loss\",\"training : bool\"]},\"1641\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.SNRLoss\",\"t\":[\"class espnet2.enh.loss.criterions.time_domain.SNRLoss(eps=1.1920928955078125e-07, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref: Tensor, inf: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1642\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1643\":{\"h\":\"espnet2.enh.decoder.stft_decoder.STFTDecoder\",\"t\":[\"class espnet2.enh.decoder.stft_decoder.STFTDecoder(n_fft: int = 512, win_length: int | None = None, hop_length: int = 128, window='hann', center: bool = True, normalized: bool = False, onesided: bool = True, default_fs: int = 16000, spec_transform_type: str | None = None, spec_factor: float = 0.15, spec_abs_exponent: float = 0.5)\",\"Bases: AbsDecoder\",\"STFT decoder for speech enhancement and separation\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: ComplexTensor, ilens: Tensor, fs: int = None)\",\"Forward.\",\"Parameters:\",\"input (ComplexTensor) – spectrum [Batch, T, (C,) F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz If not None, reconfigure iSTFT window and hop lengths for a new sampling rate while keeping their duration fixed.\",\"forward_streaming(input_frame: Tensor)\",\"Forward.\",\"Parameters:\",\"input (ComplexTensor) – spectrum [Batch, 1, F]\",\"output – wavs [Batch, 1, self.win_length]\",\"spec_back(spec)\",\"streaming_merge(chunks, ilens=None)\",\"streaming_merge. It merges the frame-level processed audio chunks in the streaming simulation. It is noted that, in real applications, the processed audio should be sent to the output channel frame by frame. You may refer to this function to manage your streaming output buffer.\",\"Parameters:\",\"chunks – List [(B, frame_size),]\",\"ilens – [B]\",\"Returns: [B, T]\",\"Return type: merge_audio\",\"training : bool\"]},\"1644\":{\"h\":\"espnet2.enh.encoder.stft_encoder.STFTEncoder\",\"t\":[\"class espnet2.enh.encoder.stft_encoder.STFTEncoder(n_fft: int = 512, win_length: int | None = None, hop_length: int = 128, window='hann', center: bool = True, normalized: bool = False, onesided: bool = True, use_builtin_complex: bool = True, default_fs: int = 16000, spec_transform_type: str | None = None, spec_factor: float = 0.15, spec_abs_exponent: float = 0.5)\",\"Bases: AbsEncoder\",\"STFT encoder for speech enhancement and separation\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – mixed speech [Batch, sample]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz If not None, reconfigure STFT window and hop lengths for a new sampling rate while keeping their duration fixed.\",\"Returns: [Batch, T, (C,) F] flens (torch.Tensor): [Batch]\",\"Return type: spectrum (ComplexTensor)\",\"forward_streaming(input: Tensor)\",\"Forward.\",\"Parameters:input (torch.Tensor) – mixed speech [Batch, frame_length]\",\"Returns: B, 1, F\",\"property output_dim : int\",\"spec_transform_func(spec)\",\"streaming_frame(audio)\",\"streaming_frame. It splits the continuous audio into frame-level audio chunks in the streaming simulation. It is noted that this function takes the entire long audio as input for a streaming simulation. You may refer to this function to manage your streaming input buffer in a real streaming application.\",\"Parameters:audio – (B, T)\",\"Returns: List [(B, frame_size),]\",\"Return type: chunked\",\"training : bool\"]},\"1645\":{\"h\":\"espnet2.enh.separator.svoice_separator.SVoiceSeparator\",\"t\":[\"class espnet2.enh.separator.svoice_separator.SVoiceSeparator(input_dim: int, enc_dim: int, kernel_size: int, hidden_size: int, num_spk: int = 2, num_layers: int = 4, segment_size: int = 20, bidirectional: bool = True, input_normalize: bool = False)\",\"Bases: AbsSeparator\",\"SVoice model for speech separation.\",\"Reference: : Voice Separation with an Unknown Number of Multiple Speakers; E. Nachmani et al., 2020; https://arxiv.org/abs/2003.01531\",\"Parameters:\",\"enc_dim – int, dimension of the encoder module’s output. (Default: 128)\",\"kernel_size – int, the kernel size of Conv1D layer in both encoder and decoder modules. (Default: 8)\",\"hidden_size – int, dimension of the hidden state in RNN layers. (Default: 128)\",\"num_spk – int, the number of speakers in the output. (Default: 2)\",\"num_layers – int, number of stacked MulCat blocks. (Default: 4)\",\"segment_size – dual-path segment size. (Default: 20)\",\"bidirectional – bool, whether the RNN layers are bidirectional. (Default: True)\",\"input_normalize – bool, whether to apply GroupNorm on the input Tensor. (Default: False)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1646\":{\"h\":\"espnet2.enh.diffusion.score_based_diffusion.ScoreModel\",\"t\":[\"class espnet2.enh.diffusion.score_based_diffusion.ScoreModel(**kwargs)\",\"Bases: AbsDiffusion\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"enhance(noisy_specturm, sampler_type='pc', predictor='reverse_diffusion', corrector='ald', N=30, corrector_steps=1, snr=0.5, **kwargs)\",\"Enhance function.\",\"Parameters:\",\"noisy_specturm (torch.Tensor) – noisy feature in [Batch, T, F]\",\"sampler_type (str) – sampler, ‘pc’ for Predictor-Corrector and ‘ode’ for ODE sampler.\",\"predictor (str) – the name of Predictor. ‘reverse_diffusion’, ‘euler_maruyama’, or ‘none’\",\"corrector (str) – the name of Corrector. ‘langevin’, ‘ald’ or ‘none’\",\"N (int) – The number of reverse sampling steps.\",\"corrector_steps (int) – number of steps in the Corrector.\",\"snr (float) – The SNR to use for the corrector.\",\"Returns: enhanced feature in [Batch, T, F]\",\"Return type: X_Hat (torch.Tensor)\",\"forward(feature_ref, feature_mix)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1647\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_ode_sampler(y, N=None, minibatch=None, **kwargs)\",\"get_pc_sampler(predictor_name, corrector_name, y, N=None, minibatch=None, **kwargs)\",\"score_fn(x, t, y)\",\"training : bool\"]},\"1648\":{\"h\":\"espnet2.enh.layers.skim.SegLSTM\",\"t\":[\"<!-- _espnet2.enh.layers.skim.SegLSTM -->\",\"class espnet2.enh.layers.skim.SegLSTM(input_size, hidden_size, dropout=0.0, bidirectional=False, norm_type='cLN')\",\"Bases: Module\",\"the Seg-LSTM of SkiM\",\"Parameters:\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"dropout – float, dropout ratio. Default is 0.\",\"bidirectional – bool, whether the LSTM layers are bidirectional. Default is False.\",\"norm_type – gLN, cLN. cLN is for causal implementation.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, hc)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1649\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1650\":{\"h\":\"espnet2.enh.layers.dprnn.SingleRNN\",\"t\":[\"<!-- _espnet2.enh.layers.dprnn.SingleRNN -->\",\"class espnet2.enh.layers.dprnn.SingleRNN(rnn_type, input_size, hidden_size, dropout=0, bidirectional=False)\",\"Bases: Module\",\"Container module for a single RNN layer.\",\"Parameters:\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"dropout – float, dropout ratio. Default is 0.\",\"bidirectional – bool, whether the RNN layers are bidirectional. Default is False.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, state=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1651\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1652\":{\"h\":\"espnet2.enh.layers.skim.SkiM\",\"t\":[\"<!-- _espnet2.enh.layers.skim.SkiM -->\",\"class espnet2.enh.layers.skim.SkiM(input_size, hidden_size, output_size, dropout=0.0, num_blocks=2, segment_size=20, bidirectional=True, mem_type='hc', norm_type='gLN', seg_overlap=False)\",\"Bases: Module\",\"Skipping Memory Net\",\"Parameters:\",\"input_size – int, dimension of the input feature. Input shape shoud be (batch, length, input_size)\",\"hidden_size – int, dimension of the hidden state.\",\"output_size – int, dimension of the output size.\",\"dropout – float, dropout ratio. Default is 0.\",\"num_blocks – number of basic SkiM blocks\",\"segment_size – segmentation size for splitting long features\",\"bidirectional – bool, whether the RNN layers are bidirectional.\",\"mem_type – ‘hc’, ‘h’, ‘c’, ‘id’ or None. It controls whether the hidden (or cell) state of SegLSTM will be processed by MemLSTM. In ‘id’ mode, both the hidden and cell states will be identically returned. When mem_type is None, the MemLSTM will be removed.\",\"norm_type – gLN, cLN. cLN is for causal implementation.\",\"seg_overlap – Bool, whether the segmentation will reserve 50% overlap for adjacent segments.Default is False.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1653\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_stream(input_frame, states)\",\"training : bool\"]},\"1654\":{\"h\":\"espnet2.enh.separator.skim_separator.SkiMSeparator\",\"t\":[\"class espnet2.enh.separator.skim_separator.SkiMSeparator(input_dim: int, causal: bool = True, num_spk: int = 2, predict_noise: bool = False, nonlinear: str = 'relu', layer: int = 3, unit: int = 512, segment_size: int = 20, dropout: float = 0.0, mem_type: str = 'hc', seg_overlap: bool = False)\",\"Bases: AbsSeparator\",\"Skipping Memory (SkiM) Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"causal – bool, whether the system is causal.\",\"num_spk – number of target speakers.\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of SkiM blocks. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"segment_size – segmentation size for splitting long features\",\"dropout – float, dropout ratio. Default is 0.\",\"mem_type – ‘hc’, ‘h’, ‘c’, ‘id’ or None. It controls whether the hidden (or cell) state of SegLSTM will be processed by MemLSTM. In ‘id’ mode, both the hidden and cell states will be identically returned. When mem_type is None, the MemLSTM will be removed.\",\"seg_overlap – Bool, whether the segmentation will reserve 50% overlap for adjacent segments. Default is False.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"forward_streaming(input_frame: Tensor, states=None)\",\"property num_spk\",\"training : bool\"]},\"1655\":{\"h\":\"espnet2.enh.layers.tcndenseunet.TCNDenseUNet\",\"t\":[\"class espnet2.enh.layers.tcndenseunet.TCNDenseUNet(n_spk=1, in_freqs=257, mic_channels=1, hid_chans=32, hid_chans_dense=32, ksz_dense=(3, 3), ksz_tcn=3, tcn_repeats=4, tcn_blocks=7, tcn_channels=384, activation=<class 'torch.nn.modules.activation.ELU'>)\",\"Bases: Module\",\"TCNDenseNet block from iNeuBe\",\"Reference: Lu, Y. J., Cornell, S., Chang, X., Zhang, W., Li, C., Ni, Z., … & Watanabe, S. Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNET-Se Submission to the L3DAS22 Challenge. ICASSP 2022 p. 9201-9205.\",\"Parameters:\",\"n_spk – number of output sources/speakers.\",\"in_freqs – number of complex STFT frequencies.\",\"mic_channels – number of microphones channels (only fixed-array geometry supported).\",\"hid_chans – number of channels in the subsampling/upsampling conv layers.\",\"hid_chans_dense – number of channels in the densenet layers (reduce this to reduce VRAM requirements).\",\"ksz_dense – kernel size in the densenet layers thorough iNeuBe.\",\"ksz_tcn – kernel size in the TCN submodule.\",\"tcn_repeats – number of repetitions of blocks in the TCN submodule.\",\"tcn_blocks – number of blocks in the TCN submodule.\",\"tcn_channels – number of channels in the TCN submodule.\",\"activation – activation function to use in the whole iNeuBe model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(tf_rep)\",\"forward.\",\"Parameters:tf_rep (torch.Tensor) – 4D tensor (multi-channel complex STFT of mixture) of shape [B, T, C, F] batch, frames, microphones, frequencies.\",\"Returns: complex 3D tensor monaural STFT of the targets : shape is [B, T, F] batch, frames, frequencies.\",\"Return type: out (torch.Tensor)\",\"training : bool\"]},\"1656\":{\"h\":\"espnet2.enh.layers.tcndenseunet.TCNResBlock\",\"t\":[\"class espnet2.enh.layers.tcndenseunet.TCNResBlock(in_chan, out_chan, ksz=3, stride=1, dilation=1, activation=<class 'torch.nn.modules.activation.ELU'>)\",\"Bases: Module\",\"single depth-wise separable TCN block as used in iNeuBe TCN.\",\"Parameters:\",\"in_chan – number of input feature channels.\",\"out_chan – number of output feature channels.\",\"ksz – kernel size.\",\"stride – stride in depth-wise convolution.\",\"dilation – dilation in depth-wise convolution.\",\"activation – activation function to use in the whole iNeuBe model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inp)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1657\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1658\":{\"h\":\"espnet2.enh.separator.tcn_separator.TCNSeparator\",\"t\":[\"class espnet2.enh.separator.tcn_separator.TCNSeparator(input_dim: int, num_spk: int = 2, predict_noise: bool = False, layer: int = 8, stack: int = 3, bottleneck_dim: int = 128, hidden_dim: int = 512, kernel: int = 3, causal: bool = False, norm_type: str = 'gLN', nonlinear: str = 'relu', pre_mask_nonlinear: str = 'prelu', masking: bool = True)\",\"Bases: AbsSeparator\",\"Temporal Convolution Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"layer – int, number of layers in each stack.\",\"stack – int, number of stacks\",\"bottleneck_dim – bottleneck dimension\",\"hidden_dim – number of convolution channel\",\"kernel – int, kernel size.\",\"causal – bool, defalut False.\",\"norm_type – str, choose from ‘BN’, ‘gLN’, ‘cLN’\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’, ‘linear’\",\"pre_mask_nonlinear – the non-linear function before masknet\",\"masking – whether to use the masking or mapping based method\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"forward_streaming(input_frame: Tensor, buffer=None)\",\"property num_spk\",\"training : bool\"]},\"1659\":{\"h\":\"espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor\",\"t\":[\"class espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor(input_dim: int, layer: int = 8, stack: int = 3, bottleneck_dim: int = 128, hidden_dim: int = 512, skip_dim: int = 128, kernel: int = 3, causal: bool = False, norm_type: str = 'gLN', pre_nonlinear: str = 'prelu', nonlinear: str = 'relu', i_adapt_layer: int = 7, adapt_layer_type: str = 'mul', adapt_enroll_dim: int = 128, use_spk_emb: bool = False, spk_emb_dim: int = 256)\",\"Bases: AbsExtractor\",\"Time-Domain SpeakerBeam Extractor.\",\"Parameters:\",\"input_dim – input feature dimension\",\"layer – int, number of layers in each stack\",\"stack – int, number of stacks\",\"bottleneck_dim – bottleneck dimension\",\"hidden_dim – number of convolution channel\",\"skip_dim – int, number of skip connection channels\",\"kernel – int, kernel size.\",\"causal – bool, defalut False.\",\"norm_type – str, choose from ‘BN’, ‘gLN’, ‘cLN’\",\"pre_nonlinear – the nonlinear function right before mask estimation select from ‘prelu’, ‘relu’, ‘tanh’, ‘sigmoid’, ‘linear’\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’, ‘linear’\",\"i_adapt_layer – int, index of adaptation layer\",\"adapt_layer_type – str, type of adaptation layer see espnet2.enh.layers.adapt_layers for options\",\"adapt_enroll_dim – int, dimensionality of the speaker embedding\",\"use_spk_emb – bool, whether to use speaker embeddings as enrollment\",\"spk_emb_dim – int, dimension of input speaker embeddings only used when use_spk_emb is True\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, input_aux: Tensor, ilens_aux: Tensor, suffix_tag: str = '', additional: Dict | None = None)\",\"TD-SpeakerBeam Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"input_aux (torch.TensororComplexTensor) – Encoded auxiliary feature for the target speaker [B, T, N] or [B, N]\",\"ilens_aux (torch.Tensor) – input lengths of auxiliary input for the target speaker [Batch]\",\"suffix_tag (str) – suffix to append to the keys in others\",\"additional (Noneordict) – additional parameters not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"f’mask{suffix_tag}’: torch.Tensor(Batch, Frames, Freq), f’enroll_emb{suffix_tag}’: torch.Tensor(Batch, adapt_enroll_dim/adapt_enroll_dim*2),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"training : bool\"]},\"1660\":{\"h\":\"espnet2.enh.separator.tfgridnet_separator.TFGridNet\",\"t\":[\"class espnet2.enh.separator.tfgridnet_separator.TFGridNet(input_dim, n_srcs=2, n_fft=128, stride=64, window='hann', n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_approx_qk_dim=512, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05, use_builtin_complex=False, ref_channel=-1)\",\"Bases: AbsSeparator\",\"Offline TFGridNet\",\"Reference: [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation”, in arXiv preprint arXiv:2211.12433, 2022. [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation”, in arXiv preprint arXiv:2209.03952, 2022.\",\"NOTES: As outlined in the Reference, this model works best when trained with variance normalized mixture input and target, e.g., with mixture of shape [batch, samples, microphones], you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signals. It is encouraged to do so when not using scale-invariant loss functions such as SI-SDR.\",\"Parameters:\",\"input_dim – placeholder, not used\",\"n_srcs – number of output sources/speakers.\",\"n_fft – stft window size.\",\"stride – stft stride.\",\"window – stft window type choose between ‘hamming’, ‘hanning’ or None.\",\"n_imics – number of microphones channels (only fixed-array geometry supported).\",\"n_layers – number of TFGridNet blocks.\",\"lstm_hidden_units – number of hidden units in LSTM.\",\"attn_n_head – number of heads in self-attention\",\"attn_approx_qk_dim – approximate dimention of frame-level key and value tensors\",\"emb_dim – embedding dimension\",\"emb_ks – kernel size for unfolding and deconv1D\",\"emb_hs – hop size for unfolding and deconv1D\",\"activation – activation function to use in the whole TFGridNet model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"eps – small epsilon for normalization layers.\",\"use_builtin_complex – whether to use builtin complex type or not.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – batched multi-channel audio tensor with M audio channels and N samples [B, N, M]\",\"ilens (torch.Tensor) – input lengths [B]\",\"additional (DictorNone) – other data, currently unused in this model.\",\"Returns: [(B, T), …] list of len n_srcs : of mono audio tensors with T samples.\",\"ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,\",\"we return it also in output.\",\"Return type: enhanced (List[Union(torch.Tensor)])\",\"property num_spk\",\"static pad2(input_tensor, target_len)\",\"training : bool\"]},\"1661\":{\"h\":\"espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2\",\"t\":[\"class espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2(input_dim, n_srcs=2, n_fft=128, stride=64, window='hann', n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_approx_qk_dim=512, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05, use_builtin_complex=False)\",\"Bases: AbsSeparator\",\"Offline TFGridNetV2. Compared with TFGridNet, TFGridNetV2 speeds up the code\",\"by vectorizing multiple heads in self-attention, and better dealing with Deconv1D in each intra- and inter-block when emb_ks == emb_hs.\",\"Reference: [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation”, in TASLP, 2023. [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation”, in ICASSP, 2023.\",\"NOTES: As outlined in the Reference, this model works best when trained with variance normalized mixture input and target, e.g., with mixture of shape [batch, samples, microphones], you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signals. It is encouraged to do so when not using scale-invariant loss functions such as SI-SDR. Specifically, use:\",\"std_\",\"= std(mix) mix = mix /\",\"std_\",\"tgt = tgt /\",\"std_\",\"Parameters:\",\"input_dim – placeholder, not used\",\"n_srcs – number of output sources/speakers.\",\"n_fft – stft window size.\",\"stride – stft stride.\",\"window – stft window type choose between ‘hamming’, ‘hanning’ or None.\",\"n_imics – number of microphones channels (only fixed-array geometry supported).\",\"n_layers – number of TFGridNetV2 blocks.\",\"lstm_hidden_units – number of hidden units in LSTM.\",\"attn_n_head – number of heads in self-attention\",\"attn_approx_qk_dim – approximate dimention of frame-level key and value tensors\",\"emb_dim – embedding dimension\",\"emb_ks – kernel size for unfolding and deconv1D\",\"emb_hs – hop size for unfolding and deconv1D\",\"activation – activation function to use in the whole TFGridNetV2 model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"eps – small epsilon for normalization layers.\",\"use_builtin_complex – whether to use builtin complex type or not.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – batched multi-channel audio tensor with M audio channels and N samples [B, N, M]\",\"ilens (torch.Tensor) – input lengths [B]\",\"additional (DictorNone) – other data, currently unused in this model.\",\"Returns: [(B, T), …] list of len n_srcs : of mono audio tensors with T samples.\",\"ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,\",\"we return it also in output.\",\"Return type: enhanced (List[Union(torch.Tensor)])\",\"property num_spk\",\"static pad2(input_tensor, target_len)\",\"training : bool\"]},\"1662\":{\"h\":\"espnet2.enh.separator.tfgridnetv3_separator.TFGridNetV3\",\"t\":[\"class espnet2.enh.separator.tfgridnetv3_separator.TFGridNetV3(input_dim, n_srcs=2, n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_qk_output_channel=4, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05)\",\"Bases: AbsSeparator\",\"Offline TFGridNetV3.\",\"On top of TFGridNetV2, TFGridNetV3 slightly modifies the internal architecture to make the model sampling-frequency-independent (SFI). This is achieved by making all network layers independent of the input time and frequency dimensions.\",\"Reference: [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation”, in TASLP, 2023. [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation”, in ICASSP, 2023.\",\"NOTES: As outlined in the Reference, this model works best when trained with variance normalized mixture input and target, e.g., with mixture of shape [batch, samples, microphones], you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signals. It is encouraged to do so when not using scale-invariant loss functions such as SI-SDR. Specifically, use:\",\"std_\",\"= std(mix) mix = mix /\",\"std_\",\"tgt = tgt /\",\"std_\",\"Parameters:\",\"input_dim – placeholder, not used\",\"n_srcs – number of output sources/speakers.\",\"n_fft – stft window size.\",\"stride – stft stride.\",\"window – stft window type choose between ‘hamming’, ‘hanning’ or None.\",\"n_imics – number of microphones channels (only fixed-array geometry supported).\",\"n_layers – number of TFGridNetV3 blocks.\",\"lstm_hidden_units – number of hidden units in LSTM.\",\"attn_n_head – number of heads in self-attention\",\"attn_attn_qk_output_channel – output channels of point-wise conv2d for getting key and query\",\"emb_dim – embedding dimension\",\"emb_ks – kernel size for unfolding and deconv1D\",\"emb_hs – hop size for unfolding and deconv1D\",\"activation – activation function to use in the whole TFGridNetV3 model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"eps – small epsilon for normalization layers.\",\"use_builtin_complex – whether to use builtin complex type or not.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – batched multi-channel audio tensor with M audio channels and N samples [B, T, F]\",\"ilens (torch.Tensor) – input lengths [B]\",\"additional (DictorNone) – other data, currently unused in this model.\",\"Returns: [(B, T), …] list of len n_srcs : of mono audio tensors with T samples.\",\"ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,\",\"we return it also in output.\",\"Return type: enhanced (List[Union(torch.Tensor)])\",\"property num_spk\",\"training : bool\"]},\"1663\":{\"h\":\"espnet2.enh.layers.tcn.TemporalBlock\",\"t\":[\"<!-- _espnet2.enh.layers.tcn.TemporalBlock -->\",\"class espnet2.enh.layers.tcn.TemporalBlock(in_channels, out_channels, skip_channels, kernel_size, stride, padding, dilation, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, B, K]\",\"Returns: [M, B, K]\",\"training : bool\"]},\"1664\":{\"h\":\"espnet2.enh.layers.tcn.TemporalConvNet\",\"t\":[\"<!-- _espnet2.enh.layers.tcn.TemporalConvNet -->\",\"class espnet2.enh.layers.tcn.TemporalConvNet(N, B, H, P, X, R, C, Sc=None, out_channel=None, norm_type='gLN', causal=False, pre_mask_nonlinear='linear', mask_nonlinear='relu')\",\"Bases: Module\",\"Basic Module of tasnet.\",\"Parameters:\",\"N – Number of filters in autoencoder\",\"B – Number of channels in bottleneck 1 * 1-conv block\",\"H – Number of channels in convolutional blocks\",\"P – Kernel size in convolutional blocks\",\"X – Number of convolutional blocks in each repeat\",\"R – Number of repeats\",\"C – Number of speakers\",\"Sc – Number of channels in skip-connection paths’ 1x1-conv blocks\",\"out_channel – Number of output channels if it is None, N will be used instead.\",\"norm_type – BN, gLN, cLN\",\"causal – causal or non-causal\",\"pre_mask_nonlinear – the non-linear function before masknet\",\"mask_nonlinear – use which non-linear function to generate mask\",\"forward(mixture_w)\",\"Keep this API same with TasNet.\",\"Parameters:mixture_w – [M, N, K], M is batch size\",\"Returns: [M, C, N, K]\",\"Return type: est_mask\",\"training : bool\"]},\"1665\":{\"h\":\"espnet2.enh.layers.tcn.TemporalConvNetInformed\",\"t\":[\"class espnet2.enh.layers.tcn.TemporalConvNetInformed(N, B, H, P, X, R, Sc=None, out_channel=None, norm_type='gLN', causal=False, pre_mask_nonlinear='prelu', mask_nonlinear='relu', i_adapt_layer: int = 7, adapt_layer_type: str = 'mul', adapt_enroll_dim: int = 128, **adapt_layer_kwargs)\",\"Bases: TemporalConvNet\",\"Basic Module of TasNet with adaptation layers.\",\"Parameters:\",\"N – Number of filters in autoencoder\",\"B – Number of channels in bottleneck 1 * 1-conv block\",\"H – Number of channels in convolutional blocks\",\"P – Kernel size in convolutional blocks\",\"X – Number of convolutional blocks in each repeat\",\"R – Number of repeats\",\"Sc – Number of channels in skip-connection paths’ 1x1-conv blocks\",\"out_channel – Number of output channels if it is None, N will be used instead.\",\"norm_type – BN, gLN, cLN\",\"causal – causal or non-causal\",\"pre_mask_nonlinear – the non-linear function before masknet\",\"mask_nonlinear – use which non-linear function to generate mask\",\"i_adapt_layer – int, index of the adaptation layer\",\"adapt_layer_type – str, type of adaptation layer see espnet2.enh.layers.adapt_layers for options\",\"adapt_enroll_dim – int, dimensionality of the speaker embedding\",\"forward(mixture_w, enroll_emb)\",\"TasNet forward with adaptation layers.\",\"Parameters:\",\"mixture_w – [M, N, K], M is batch size\",\"enroll_emb – [M, 2*adapt_enroll_dim] if self.skip_connection [M, adapt_enroll_dim] if not self.skip_connection\",\"Returns: [M, N, K]\",\"Return type: est_mask\",\"training : bool\"]},\"1666\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.TimeDomainL1\",\"t\":[\"class espnet2.enh.loss.criterions.time_domain.TimeDomainL1(name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref, inf)\",\"Time-domain L1 loss forward.\",\"Parameters:\",\"ref – (Batch, T) or (Batch, T, C)\",\"inf – (Batch, T) or (Batch, T, C)\",\"Returns: (Batch,)\",\"Return type: loss\",\"training : bool\"]},\"1667\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.TimeDomainLoss\",\"t\":[\"class espnet2.enh.loss.criterions.time_domain.TimeDomainLoss(name, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: AbsEnhLoss, ABC\",\"Base class for all time-domain Enhancement loss modules.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"property is_dereverb_loss : bool\",\"property is_noise_loss : bool\",\"property name : str\",\"property only_for_test : bool\",\"training : bool\"]},\"1668\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.TimeDomainMSE\",\"t\":[\"class espnet2.enh.loss.criterions.time_domain.TimeDomainMSE(name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref, inf)\",\"Time-domain MSE loss forward.\",\"Parameters:\",\"ref – (Batch, T) or (Batch, T, C)\",\"inf – (Batch, T) or (Batch, T, C)\",\"Returns: (Batch,)\",\"Return type: loss\",\"training : bool\"]},\"1669\":{\"h\":\"espnet2.enh.separator.transformer_separator.TransformerSeparator\",\"t\":[\"class espnet2.enh.separator.transformer_separator.TransformerSeparator(input_dim: int, num_spk: int = 2, predict_noise: bool = False, adim: int = 384, aheads: int = 4, layers: int = 6, linear_units: int = 1536, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, normalize_before: bool = False, concat_after: bool = False, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, use_scaled_pos_enc: bool = True, nonlinear: str = 'relu')\",\"Bases: AbsSeparator\",\"Transformer separator.\",\"Parameters:\",\"input_dim – input feature dimension\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"adim (int) – Dimension of attention.\",\"aheads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"layers (int) – The number of transformer blocks.\",\"dropout_rate (float) – Dropout rate.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"use_scaled_pos_enc (bool) – use scaled positional encoding or not\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1670\":{\"h\":\"espnet2.enh.layers.uses.USES\",\"t\":[\"<!-- _espnet2.enh.layers.uses.USES -->\",\"class espnet2.enh.layers.uses.USES(input_size, output_size, bottleneck_size=64, num_blocks=6, num_spatial_blocks=3, segment_size=64, memory_size=20, memory_types=1, rnn_type='lstm', hidden_size=128, att_heads=4, dropout=0.0, activation='relu', bidirectional=True, norm_type='cLN', ch_mode='att', ch_att_dim=256, eps=1e-05)\",\"Bases: Module\",\"Unconstrained Speech Enhancement and Separation (USES) Network.\",\"Reference: : [1] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.\",\"Parameters:\",\"input_size (int) – dimension of the input feature.\",\"output_size (int) – dimension of the output.\",\"bottleneck_size (int) – dimension of the bottleneck feature. Must be a multiple of att_heads.\",\"num_blocks (int) – number of processing blocks.\",\"num_spatial_blocks (int) – number of processing blocks with channel modeling.\",\"segment_size (int) – number of frames in each non-overlapping segment. This is used to segment long utterances into smaller segments for efficient processing.\",\"memory_size (int) – group size of global memory tokens. The basic use of memory tokens is to store the history information from previous segments. The memory tokens are updated by the output of the last block after processing each segment.\",\"memory_types (int) –\",\"numbre of memory token groups. Each group corresponds to a different type of processing, i.e.,\",\"the first group is used for denoising without dereverberation, the second group is used for denoising with dereverberation,\",\"rnn_type (str) – type of the RNN cell in the improved Transformer layer.\",\"hidden_size (int) – hidden dimension of the RNN cell.\",\"att_heads (int) – number of attention heads in Transformer.\",\"dropout (float) – dropout ratio. Default is 0.\",\"activation (str) – non-linear activation function applied in each block.\",\"bidirectional (bool) – whether the RNN layers are bidirectional.\",\"norm_type (str) – normalization type in the improved Transformer layer.\",\"ch_mode (str) – mode of channel modeling. Select from “att” and “tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"eps (float) – epsilon for layer normalization.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, ref_channel=None, mem_idx=None)\",\"USES forward.\",\"Parameters:\",\"input (torch.Tensor) – input feature (batch, mics, input_size, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel. if None, simply average all channels. if int, take the specified channel instead of averaging.\",\"mem_idx (Noneorint) – index of the memory token group. if None, use the only group of memory tokens in the model. if int, use the specified group from multiple existing groups.\",\"Returns: output feature (batch, output_size, freq, time)\",\"Return type: output (torch.Tensor)\",\"training : bool\"]},\"1671\":{\"h\":\"espnet2.enh.separator.uses_separator.USESSeparator\",\"t\":[\"class espnet2.enh.separator.uses_separator.USESSeparator(input_dim: int, num_spk: int = 2, enc_channels: int = 256, bottleneck_size: int = 64, num_blocks: int = 6, num_spatial_blocks: int = 3, ref_channel: int | None = None, segment_size: int = 64, memory_size: int = 20, memory_types: int = 1, rnn_type: str = 'lstm', bidirectional: bool = True, hidden_size: int = 128, att_heads: int = 4, dropout: float = 0.0, norm_type: str = 'cLN', activation: str = 'relu', ch_mode: str | List[str] = 'att', ch_att_dim: int = 256, eps: float = 1e-05, additional: dict = {})\",\"Bases: AbsSeparator\",\"Unconstrained Speech Enhancement and Separation (USES) Network.\",\"Reference: : [1] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.\",\"Parameters:\",\"input_dim (int) – input feature dimension. Not used as the model is independent of the input size.\",\"num_spk (int) – number of speakers.\",\"enc_channels (int) – feature dimension after the Conv1D encoder.\",\"bottleneck_size (int) – dimension of the bottleneck feature. Must be a multiple of att_heads.\",\"num_blocks (int) – number of processing blocks.\",\"num_spatial_blocks (int) – number of processing blocks with channel modeling.\",\"ref_channel (int) – reference channel (used in channel modeling modules).\",\"segment_size (int) – number of frames in each non-overlapping segment. This is used to segment long utterances into smaller chunks for efficient processing.\",\"memory_size (int) – group size of global memory tokens. The basic use of memory tokens is to store the history information from previous segments. The memory tokens are updated by the output of the last block after processing each segment.\",\"memory_types (int) –\",\"numbre of memory token groups. Each group corresponds to a different type of processing, i.e.,\",\"the first group is used for denoising without dereverberation, the second group is used for denoising with dereverberation,\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"bidirectional (bool) – whether the inter-chunk RNN layers are bidirectional.\",\"hidden_size (int) – dimension of the hidden state.\",\"att_heads (int) – number of attention heads.\",\"dropout (float) – dropout ratio. Default is 0.\",\"norm_type – type of normalization to use after each inter- or intra-chunk NN block.\",\"activation – the nonlinear activation function.\",\"ch_mode – str or list, mode of channel modeling. Select from “att” and “tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"ref_channel – Optional[int], index of the reference channel.\",\"eps (float) – epsilon for layer normalization.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – STFT spectrum [B, T, (C,) F (,2)] B is the batch size T is the number of time frames C is the number of microphone channels (optional) F is the number of frequency bins 2 is real and imaginary parts (optional if input is a complex tensor)\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) –\",\"other data included in model “mode”: one of (“no_dereverb”, “dereverb”, “both”)\",\"“no_dereverb”: only use the first memory group for denoising\",\"without dereverberation\",\"”dereverb”: only use the second memory group for denoising : with dereverberation\",\"”both”: use both memory groups for denoising with and without : dereverberation\",\"Returns: [(B, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\",\"training : bool\"]},\"1672\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.Upsample\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.Upsample(channels, with_conv=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1673\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1674\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.UpsampleConv\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.layers.UpsampleConv(input_dim, output_dim, kernel_size=3, biases=True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inputs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1675\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1676\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.VarianceNorm2d\",\"t\":[\"class espnet2.enh.layers.ncsnpp_utils.normalization.VarianceNorm2d(num_features, bias=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1677\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1678\":{\"h\":\"espnet2.enh.layers.beamformer.apply_beamforming_vector\",\"t\":[\"espnet2.enh.layers.beamformer.apply_beamforming_vector(beamform_vector: Tensor | ComplexTensor, mix: Tensor | ComplexTensor)\"]},\"1679\":{\"h\":\"espnet2.enh.diffusion.sdes.batch_broadcast\",\"t\":[\"<!-- _espnet2.enh.diffusion.sdes.batch_broadcast -->\",\"espnet2.enh.diffusion.sdes.batch_broadcast(a, x)\",\"Broadcasts a over all dimensions of x, except the batch dimension,\",\"which must match.\"]},\"1680\":{\"h\":\"espnet2.enh.layers.beamformer.blind_analytic_normalization\",\"t\":[\"espnet2.enh.layers.beamformer.blind_analytic_normalization(ws, psd_noise, eps=1e-08)\",\"Blind analytic normalization (BAN) for post-filtering\",\"Parameters:\",\"ws (torch.complex64/ComplexTensor) – beamformer vector (…, F, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise PSD matrix (…, F, C, C)\",\"eps (float) –\",\"Returns: normalized beamformer vector (…, F)\",\"Return type: ws_ban (torch.complex64/ComplexTensor)\"]},\"1681\":{\"h\":\"espnet2.enh.layers.complex_utils.cat\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.cat -->\",\"espnet2.enh.layers.complex_utils.cat(seq: Sequence[ComplexTensor | Tensor], *args, **kwargs)\"]},\"1682\":{\"h\":\"espnet2.enh.layers.tcn.check_nonlinear\",\"t\":[\"<!-- _espnet2.enh.layers.tcn.check_nonlinear -->\",\"espnet2.enh.layers.tcn.check_nonlinear(nolinear_type)\"]},\"1683\":{\"h\":\"espnet2.enh.layers.tcn.choose_norm\",\"t\":[\"<!-- _espnet2.enh.layers.tcn.choose_norm -->\",\"espnet2.enh.layers.tcn.choose_norm(norm_type, channel_size, shape='BDT')\",\"The input of normalization will be (M, C, K), where M is batch size.\",\"C is channel size and K is sequence length.\"]},\"1684\":{\"h\":\"espnet2.enh.layers.complexnn.complex_cat\",\"t\":[\"<!-- _espnet2.enh.layers.complexnn.complex_cat -->\",\"espnet2.enh.layers.complexnn.complex_cat(inputs, axis)\"]},\"1685\":{\"h\":\"espnet2.enh.layers.complex_utils.complex_norm\",\"t\":[\"espnet2.enh.layers.complex_utils.complex_norm(c: Tensor | ComplexTensor, dim=-1, keepdim=False)\"]},\"1686\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.contract_inner\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.layers.contract_inner(x, y)\",\"tensordot(x, y, 1).\"]},\"1687\":{\"h\":\"espnet2.enh.layers.conv_utils.conv2d_output_shape\",\"t\":[\"espnet2.enh.layers.conv_utils.conv2d_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1)\"]},\"1688\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.conv_downsample_2d\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.conv_downsample_2d(x, w, k=None, factor=2, gain=1)\",\"Fused tf.nn.conv2d() followed by downsample_2d().\",\"Padding is performed only once at the beginning, not between the operations. The fused op is considerably more efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of arbitrary order. :param x: Input tensor of the shape [N, C, H, W] or\",\"`\",\"[N, H, W,\",\"C]`.\",\"Parameters:\",\"w – Weight tensor of the shape [filterH, filterW, inChannels, outChannels]. Grouped convolution can be performed by inChannels = x.shape[0] // numGroups.\",\"k – FIR filter of the shape [firH, firW] or [firN] (separable). The default is [1] * factor, which corresponds to average pooling.\",\"factor – Integer downsampling factor (default: 2).\",\"gain – Scaling factor for signal magnitude (default: 1.0).\",\"Returns: Tensor of the shape [N, C, H // factor, W // factor] or [N, H // factor, W // factor, C], and same datatype as x.\"]},\"1689\":{\"h\":\"espnet2.enh.layers.conv_utils.convtransp2d_output_shape\",\"t\":[\"espnet2.enh.layers.conv_utils.convtransp2d_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0)\"]},\"1690\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ddpm_conv1x1\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.layers.ddpm_conv1x1(in_planes, out_planes, stride=1, bias=True, init_scale=1.0, padding=0)\",\"1x1 convolution with DDPM initialization.\"]},\"1691\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ddpm_conv3x3\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.layers.ddpm_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1.0, padding=1)\",\"3x3 convolution with DDPM initialization.\"]},\"1692\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.default_init\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.layers.default_init(scale=1.0)\",\"The same initialization used in DDPM.\"]},\"1693\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.downsample_2d\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.downsample_2d(x, k=None, factor=2, gain=1)\",\"Downsample a batch of 2D images with the given filter.\",\"Accepts a batch of 2D images of the shape [N, C, H, W] or [N, H, W, C] and downsamples each image with the given filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the specified gain. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its shape is a multiple of the downsampling factor. :param x: Input tensor of the shape [N, C, H, W] or\",\"`\",\"[N, H, W,\",\"C]`.\",\"Parameters:\",\"k – FIR filter of the shape [firH, firW] or [firN] (separable). The default is [1] * factor, which corresponds to average pooling.\",\"factor – Integer downsampling factor (default: 2).\",\"gain – Scaling factor for signal magnitude (default: 1.0).\",\"Returns: Tensor of the shape [N, C, H // factor, W // factor]\"]},\"1694\":{\"h\":\"espnet2.enh.layers.complex_utils.einsum\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.einsum -->\",\"espnet2.enh.layers.complex_utils.einsum(equation, *operands)\"]},\"1695\":{\"h\":\"espnet2.enh.layers.beamformer.generalized_eigenvalue_decomposition\",\"t\":[\"espnet2.enh.layers.beamformer.generalized_eigenvalue_decomposition(a: Tensor, b: Tensor, eps=1e-06)\",\"Solves the generalized eigenvalue decomposition through Cholesky decomposition.\",\"ported from https://github.com/asteroid-team/asteroid/blob/master/asteroid/dsp/beamforming.py#L464\",\"a @ e_vec = e_val * b @ e_vec | | Cholesky decomposition on b: | b = L @ L^H, where L is a lower triangular matrix | | Let C = L^-1 @ a @ L^-H, it is Hermitian. | => C @ y = lambda * y => e_vec = L^-H @ y\",\"Reference: https://www.netlib.org/lapack/lug/node54.html\",\"Parameters:\",\"a – A complex Hermitian or real symmetric matrix whose eigenvalues and eigenvectors will be computed. (…, C, C)\",\"b – A complex Hermitian or real symmetric definite positive matrix. (…, C, C)\",\"Returns: generalized eigenvalues (ascending order) e_vec: generalized eigenvectors\",\"Return type: e_val\"]},\"1696\":{\"h\":\"espnet2.enh.layers.beamformer.get_WPD_filter\",\"t\":[\"espnet2.enh.layers.beamformer.get_WPD_filter(Phi: Tensor | ComplexTensor, Rf: Tensor | ComplexTensor, reference_vector: Tensor, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the WPD vector.\",\"WPD is the Weighted Power minimization Distortionless response convolutional beamformer. As follows:\",\"h = (Rf^-1 @ Phi_{xx}) / tr[(Rf^-1) @ Phi_{xx}] @ u\",\"Reference: : T. Nakatani and K. Kinoshita, “A Unified Convolutional Beamformer for Simultaneous Denoising and Dereverberation,” in IEEE Signal Processing Letters, vol. 26, no. 6, pp. 903-907, June 2019, doi: 10.1109/LSP.2019.2911179. https://ieeexplore.ieee.org/document/8691481\",\"Parameters:\",\"Phi (torch.complex64/ComplexTensor) – (B, F, (btaps+1) * C, (btaps+1) * C) is the PSD of zero-padded speech [x^T(t,f) 0 … 0]^T.\",\"Rf (torch.complex64/ComplexTensor) – (B, F, (btaps+1) * C, (btaps+1) * C) is the power normalized spatio-temporal covariance matrix.\",\"reference_vector (torch.Tensor) – (B, (btaps+1) * C) is the reference_vector.\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (B, F, (btaps + 1) * C)\",\"Return type: filter_matrix (torch.complex64/ComplexTensor)\"]},\"1697\":{\"h\":\"espnet2.enh.layers.beamformer.get_WPD_filter_v2\",\"t\":[\"espnet2.enh.layers.beamformer.get_WPD_filter_v2(Phi: Tensor | ComplexTensor, Rf: Tensor | ComplexTensor, reference_vector: Tensor, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the WPD vector (v2).\",\"This implementation is more efficient than get_WPD_filter as : it skips unnecessary computation with zeros.\",\"Parameters:\",\"Phi (torch.complex64/ComplexTensor) – (B, F, C, C) is speech PSD.\",\"Rf (torch.complex64/ComplexTensor) – (B, F, (btaps+1) * C, (btaps+1) * C) is the power normalized spatio-temporal covariance matrix.\",\"reference_vector (torch.Tensor) – (B, C) is the reference_vector.\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (B, F, (btaps+1) * C)\",\"Return type: filter_matrix (torch.complex64/ComplexTensor)\"]},\"1698\":{\"h\":\"espnet2.enh.layers.beamformer.get_WPD_filter_with_rtf\",\"t\":[\"espnet2.enh.layers.beamformer.get_WPD_filter_with_rtf(psd_observed_bar: Tensor | ComplexTensor, psd_speech: Tensor | ComplexTensor, psd_noise: Tensor | ComplexTensor, iterations: int = 3, reference_vector: int | Tensor | None = None, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-15)\",\"Return the WPD vector calculated with RTF.\",\"WPD is the Weighted Power minimization Distortionless response convolutional beamformer. As follows:\",\"h = (Rf^-1 @ vbar) / (vbar^H @ R^-1 @ vbar)\",\"Reference: : T. Nakatani and K. Kinoshita, “A Unified Convolutional Beamformer for Simultaneous Denoising and Dereverberation,” in IEEE Signal Processing Letters, vol. 26, no. 6, pp. 903-907, June 2019, doi: 10.1109/LSP.2019.2911179. https://ieeexplore.ieee.org/document/8691481\",\"Parameters:\",\"psd_observed_bar (torch.complex64/ComplexTensor) – stacked observation covariance matrix\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"iterations (int) – number of iterations in power method\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)r\"]},\"1699\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.get_act\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.layers.get_act(config)\",\"Get activation functions from the config file.\"]},\"1700\":{\"h\":\"espnet2.enh.layers.dcunet.get_activation\",\"t\":[\"<!-- _espnet2.enh.layers.dcunet.get_activation -->\",\"espnet2.enh.layers.dcunet.get_activation(name)\"]},\"1701\":{\"h\":\"espnet2.enh.layers.wpe.get_correlations\",\"t\":[\"<!-- _espnet2.enh.layers.wpe.get_correlations -->\",\"espnet2.enh.layers.wpe.get_correlations(Y: Tensor | ComplexTensor, inverse_power: Tensor, taps, delay)\",\"Calculates weighted correlations of a window of length taps\",\"Parameters:\",\"Y – Complex-valued STFT signal with shape (F, C, T)\",\"inverse_power – Weighting factor with shape (F, T)\",\"taps (int) – Lenghts of correlation window\",\"delay (int) – Delay for the weighting factor\",\"Returns: Correlation matrix of shape (F, taps*C, taps*C) Correlation vector of shape (F, taps, C, C)\"]},\"1702\":{\"h\":\"espnet2.enh.layers.beamformer.get_covariances\",\"t\":[\"espnet2.enh.layers.beamformer.get_covariances(Y: Tensor | ComplexTensor, inverse_power: Tensor, bdelay: int, btaps: int, get_vector: bool = False)\",\"Calculates the power normalized spatio-temporal covariance : matrix of the framed signal.\",\"Parameters:\",\"Y – Complex STFT signal with shape (B, F, C, T)\",\"inverse_power – Weighting factor with shape (B, F, T)\",\"Returns: (B, F, (btaps+1) * C, (btaps+1) * C) Correlation vector: (B, F, btaps + 1, C, C)\",\"Return type: Correlation matrix\"]},\"1703\":{\"h\":\"espnet2.enh.layers.wpe.get_filter_matrix_conj\",\"t\":[\"espnet2.enh.layers.wpe.get_filter_matrix_conj(correlation_matrix: Tensor | ComplexTensor, correlation_vector: Tensor | ComplexTensor, eps: float = 1e-10)\",\"Calculate (conjugate) filter matrix based on correlations for one freq.\",\"Parameters:\",\"correlation_matrix – Correlation matrix (F, taps * C, taps * C)\",\"correlation_vector – Correlation vector (F, taps, C, C)\",\"eps –\",\"Returns: (F, taps, C, C)\",\"Return type: filter_matrix_conj (torch.complex/ComplexTensor)\"]},\"1704\":{\"h\":\"espnet2.enh.layers.beamformer.get_gev_vector\",\"t\":[\"espnet2.enh.layers.beamformer.get_gev_vector(psd_noise: Tensor | ComplexTensor, psd_speech: Tensor | ComplexTensor, mode='power', reference_vector: int | Tensor = 0, iterations: int = 3, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the generalized eigenvalue (GEV) beamformer vector:\",\"psd_speech @ h = lambda * psd_noise @ h\",\"Reference: : Blind acoustic beamforming based on generalized eigenvalue decomposition; E. Warsitz and R. Haeb-Umbach, 2007.\",\"Parameters:\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"mode (str) – one of (“power”, “evd”) “power”: power method “evd”: eigenvalue decomposition (only for torch builtin complex tensors)\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"iterations (int) – number of iterations in power method\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1705\":{\"h\":\"espnet2.enh.layers.beamformer.get_lcmv_vector_with_rtf\",\"t\":[\"espnet2.enh.layers.beamformer.get_lcmv_vector_with_rtf(psd_n: Tensor | ComplexTensor, rtf_mat: Tensor | ComplexTensor, reference_vector: int | Tensor | None = None, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the LCMV (Linearly Constrained Minimum Variance) vector : calculated with RTF: <br/> h = (Npsd^-1 @ rtf_mat) @ (rtf_mat^H @ Npsd^-1 @ rtf_mat)^-1 @ p\",\"Reference: : H. L. Van Trees, “Optimum array processing: Part IV of detection, estimation, and modulation theory,” John Wiley & Sons, 2004. (Chapter 6.7)\",\"Parameters:\",\"psd_n (torch.complex64/ComplexTensor) – observation/noise covariance matrix (…, F, C, C)\",\"rtf_mat (torch.complex64/ComplexTensor) – RTF matrix (…, F, C, num_spk)\",\"reference_vector (torch.Tensororint) – (…, num_spk) or scalar\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1706\":{\"h\":\"espnet2.enh.layers.beamformer.get_mvdr_vector\",\"t\":[\"espnet2.enh.layers.beamformer.get_mvdr_vector(psd_s, psd_n, reference_vector: Tensor, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the MVDR (Minimum Variance Distortionless Response) vector:\",\"h = (Npsd^-1 @ Spsd) / (Tr(Npsd^-1 @ Spsd)) @ u\",\"Reference: : On optimal frequency-domain multichannel linear filtering for noise reduction; M. Souden et al., 2010; https://ieeexplore.ieee.org/document/5089420\",\"Parameters:\",\"psd_s (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_n (torch.complex64/ComplexTensor) – observation/noise covariance matrix (…, F, C, C)\",\"reference_vector (torch.Tensor) – (…, C)\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1707\":{\"h\":\"espnet2.enh.layers.beamformer.get_mvdr_vector_with_rtf\",\"t\":[\"espnet2.enh.layers.beamformer.get_mvdr_vector_with_rtf(psd_n: Tensor | ComplexTensor, psd_speech: Tensor | ComplexTensor, psd_noise: Tensor | ComplexTensor, iterations: int = 3, reference_vector: int | Tensor | None = None, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the MVDR (Minimum Variance Distortionless Response) vector : calculated with RTF: <br/> h = (Npsd^-1 @ rtf) / (rtf^H @ Npsd^-1 @ rtf)\",\"Reference: : On optimal frequency-domain multichannel linear filtering for noise reduction; M. Souden et al., 2010; https://ieeexplore.ieee.org/document/5089420\",\"Parameters:\",\"psd_n (torch.complex64/ComplexTensor) – observation/noise covariance matrix (…, F, C, C)\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"iterations (int) – number of iterations in power method\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1708\":{\"h\":\"espnet2.enh.layers.beamformer.get_mwf_vector\",\"t\":[\"espnet2.enh.layers.beamformer.get_mwf_vector(psd_s, psd_n, reference_vector: Tensor | int, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the MWF (Minimum Multi-channel Wiener Filter) vector:\",\"h = (Npsd^-1 @ Spsd) @ u\",\"Parameters:\",\"psd_s (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_n (torch.complex64/ComplexTensor) – power-normalized observation covariance matrix (…, F, C, C)\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1709\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.get_normalization\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.normalization.get_normalization(config, conditional=False)\",\"Obtain normalization modules from the config file.\"]},\"1710\":{\"h\":\"espnet2.enh.layers.wpe.get_power\",\"t\":[\"<!-- _espnet2.enh.layers.wpe.get_power -->\",\"espnet2.enh.layers.wpe.get_power(signal, dim=-2)\",\"Calculates power for signal\",\"Parameters:\",\"signal – Single frequency signal with shape (F, C, T).\",\"axis – reduce_mean axis\",\"Returns: Power with shape (F, T)\"]},\"1711\":{\"h\":\"espnet2.enh.layers.beamformer.get_power_spectral_density_matrix\",\"t\":[\"espnet2.enh.layers.beamformer.get_power_spectral_density_matrix(xs, mask, normalization=True, reduction='mean', eps: float = 1e-15)\",\"Return cross-channel power spectral density (PSD) matrix\",\"Parameters:\",\"xs (torch.complex64/ComplexTensor) – (…, F, C, T)\",\"reduction (str) – “mean” or “median”\",\"mask (torch.Tensor) – (…, F, C, T)\",\"normalization (bool) –\",\"eps (float) –\",\"Returns : psd (torch.complex64/ComplexTensor): (…, F, C, C)\"]},\"1712\":{\"h\":\"espnet2.enh.layers.beamformer.get_rank1_mwf_vector\",\"t\":[\"espnet2.enh.layers.beamformer.get_rank1_mwf_vector(psd_speech, psd_noise, reference_vector: Tensor | int, denoising_weight: float = 1.0, approx_low_rank_psd_speech: bool = False, iterations: int = 3, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the R1-MWF (Rank-1 Multi-channel Wiener Filter) vector\",\"h = (Npsd^-1 @ Spsd) / (mu + Tr(Npsd^-1 @ Spsd)) @ u\",\"Reference: : [1] Rank-1 constrained multichannel Wiener filter for speech recognition in noisy environments; Z. Wang et al, 2018 https://hal.inria.fr/hal-01634449/document [2] Low-rank approximation based multichannel Wiener filter algorithms for noise reduction with application in cochlear implants; R. Serizel, 2014 https://ieeexplore.ieee.org/document/6730918\",\"Parameters:\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"denoising_weight (float) – a trade-off parameter between noise reduction and speech distortion. A larger value leads to more noise reduction at the expense of more speech distortion. When denoising_weight = 0, it corresponds to MVDR beamformer.\",\"approx_low_rank_psd_speech (bool) – whether to replace original input psd_speech with its low-rank approximation as in [1]\",\"iterations (int) – number of iterations in power method, only used when approx_low_rank_psd_speech = True\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1713\":{\"h\":\"espnet2.enh.layers.beamformer.get_rtf\",\"t\":[\"<!-- _espnet2.enh.layers.beamformer.get_rtf -->\",\"espnet2.enh.layers.beamformer.get_rtf(psd_speech, psd_noise, mode='power', reference_vector: int | Tensor = 0, iterations: int = 3)\",\"Calculate the relative transfer function (RTF)\",\"Algorithm of power method: : 1. rtf = reference_vector 2. for i in range(iterations): : rtf = (psd_noise^-1 @ psd_speech) @ rtf rtf = rtf / ||rtf||_2 # this normalization can be skipped 3. rtf = psd_noise @ rtf 4. rtf = rtf / rtf[…, ref_channel, :]\",\"Note: 4) Normalization at the reference channel is not performed here.\",\"Parameters:\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"mode (str) – one of (“power”, “evd”) “power”: power method “evd”: eigenvalue decomposition\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"iterations (int) – number of iterations in power method\",\"Returns: (…, F, C, 1)\",\"Return type: rtf (torch.complex64/ComplexTensor)\"]},\"1714\":{\"h\":\"espnet2.enh.layers.beamformer.get_rtf_matrix\",\"t\":[\"espnet2.enh.layers.beamformer.get_rtf_matrix(psd_speeches, psd_noises, diagonal_loading: bool = True, ref_channel: int = 0, rtf_iterations: int = 3, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Calculate the RTF matrix with each column the relative transfer function of the corresponding source.\"]},\"1715\":{\"h\":\"espnet2.enh.layers.beamformer.get_sdw_mwf_vector\",\"t\":[\"espnet2.enh.layers.beamformer.get_sdw_mwf_vector(psd_speech, psd_noise, reference_vector: Tensor | int, denoising_weight: float = 1.0, approx_low_rank_psd_speech: bool = False, iterations: int = 3, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the SDW-MWF (Speech Distortion Weighted Multi-channel Wiener Filter) vector\",\"h = (Spsd + mu * Npsd)^-1 @ Spsd @ u\",\"Reference: : [1] Spatially pre-processed speech distortion weighted multi-channel Wiener filtering for noise reduction; A. Spriet et al, 2004 https://dl.acm.org/doi/abs/10.1016/j.sigpro.2004.07.028 [2] Rank-1 constrained multichannel Wiener filter for speech recognition in noisy environments; Z. Wang et al, 2018 https://hal.inria.fr/hal-01634449/document [3] Low-rank approximation based multichannel Wiener filter algorithms for noise reduction with application in cochlear implants; R. Serizel, 2014 https://ieeexplore.ieee.org/document/6730918\",\"Parameters:\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"denoising_weight (float) – a trade-off parameter between noise reduction and speech distortion. A larger value leads to more noise reduction at the expense of more speech distortion. The plain MWF is obtained with denoising_weight = 1 (by default).\",\"approx_low_rank_psd_speech (bool) – whether to replace original input psd_speech with its low-rank approximation as in [2]\",\"iterations (int) – number of iterations in power method, only used when approx_low_rank_psd_speech = True\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float) –\",\"eps (float) –\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1716\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.get_weight\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.get_weight(module, shape, weight_var='weight', kernel_init=None)\",\"Get/create weight tensor for a convolution or fully-connected layer.\"]},\"1717\":{\"h\":\"espnet2.enh.layers.beamformer.gev_phase_correction\",\"t\":[\"espnet2.enh.layers.beamformer.gev_phase_correction(vector)\",\"Phase correction to reduce distortions due to phase inconsistencies.\",\"ported from https://github.com/fgnt/nn-gev/blob/master/fgnt/beamforming.py#L169\",\"Parameters:vector – Beamforming vector with shape (…, F, C)\",\"Returns: Phase corrected beamforming vectors\",\"Return type: w\"]},\"1718\":{\"h\":\"espnet2.enh.layers.ifasnet.iFaSNet\",\"t\":[\"<!-- _espnet2.enh.layers.ifasnet.iFaSNet -->\",\"class espnet2.enh.layers.ifasnet.iFaSNet(*args, **kwargs)\",\"Bases: FaSNet_base\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"abstract forward function\",\"input: shape (batch, max_num_ch, T) num_mic: shape (batch, ), the number of channels for each input.\",\"Zero for fixed geometry configuration.\",\"training : bool\"]},\"1719\":{\"h\":\"espnet2.enh.separator.ineube_separator.iNeuBe\",\"t\":[\"class espnet2.enh.separator.ineube_separator.iNeuBe(n_spk=1, n_fft=512, stride=128, window='hann', mic_channels=1, hid_chans=32, hid_chans_dense=32, ksz_dense=(3, 3), ksz_tcn=3, tcn_repeats=4, tcn_blocks=7, tcn_channels=384, activation='elu', output_from='dnn1', n_chunks=3, freeze_dnn1=False, tik_eps=1e-08)\",\"Bases: AbsSeparator\",\"iNeuBe, iterative neural/beamforming enhancement\",\"Reference: Lu, Y. J., Cornell, S., Chang, X., Zhang, W., Li, C., Ni, Z., … & Watanabe, S. Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNET-Se Submission to the L3DAS22 Challenge. ICASSP 2022 p. 9201-9205.\",\"NOTES: As outlined in the Reference, this model works best when coupled with the MultiResL1SpecLoss defined in criterions/time_domain.py. The model is trained with variance normalized mixture input and target. e.g. with mixture of shape [batch, microphones, samples] you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signal. In the Reference, the variance normalization was performed offline (we normalized by the std computed on the entire training set and not for each input separately). However we found out that also normalizing each input and target separately works well.\",\"Parameters:\",\"n_spk – number of output sources/speakers.\",\"n_fft – stft window size.\",\"stride – stft stride.\",\"window – stft window type choose between ‘hamming’, ‘hanning’ or None.\",\"mic_channels – number of microphones channels (only fixed-array geometry supported).\",\"hid_chans – number of channels in the subsampling/upsampling conv layers.\",\"hid_chans_dense – number of channels in the densenet layers (reduce this to reduce VRAM requirements).\",\"ksz_dense – kernel size in the densenet layers thorough iNeuBe.\",\"ksz_tcn – kernel size in the TCN submodule.\",\"tcn_repeats – number of repetitions of blocks in the TCN submodule.\",\"tcn_blocks – number of blocks in the TCN submodule.\",\"tcn_channels – number of channels in the TCN submodule.\",\"activation – activation function to use in the whole iNeuBe model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"output_from – output the estimate from ‘dnn1’, ‘mfmcwf’ or ‘dnn2’.\",\"n_chunks – number of future and past frames to consider for mfMCWF computation.\",\"freeze_dnn1 – whether or not freezing dnn1 parameters during training of dnn2.\",\"tik_eps – diagonal loading in the mfMCWF computation.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor/ComplexTensor) – batched multi-channel audio tensor with C audio channels and T samples [B, T, C]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data, currently unused in this model.\",\"Returns: [(B, T), …] list of len n_spk : of mono audio tensors with T samples.\",\"ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,\",\"we return it also in output.\",\"Return type: enhanced (List[Union[torch.Tensor, ComplexTensor]])\",\"static mfmcwf(mixture, estimate, n_chunks, tik_eps)\",\"multi-frame multi-channel wiener filter.\",\"Parameters:\",\"mixture (torch.Tensor) – multi-channel STFT complex mixture tensor, of shape [B, T, C, F] batch, frames, microphones, frequencies.\",\"estimate (torch.Tensor) – monaural STFT complex estimate of target source [B, T, F] batch, frames, frequencies.\",\"n_chunks (int) – number of past and future mfMCWF frames. If 0 then standard MCWF.\",\"tik_eps (float) – diagonal loading for matrix inversion in MCWF computation.\",\"Returns: monaural STFT complex estimate : of target source after MFMCWF [B, T, F] batch, frames, frequencies.\",\"Return type: beamformed (torch.Tensor)\",\"property num_spk\",\"static pad2(input_tensor, target_len)\",\"training : bool\",\"static unfold(tf_rep, chunk_size)\",\"unfolding STFT representation to add context in the mics channel.\",\"Parameters:\",\"mixture (torch.Tensor) – 3D tensor (monaural complex STFT) of shape [B, T, F] batch, frames, microphones, frequencies.\",\"n_chunks (int) – number of past and future to consider.\",\"Returns: complex 3D tensor STFT with context channel. : shape now is [B, T, C, F] batch, frames, context, frequencies. Basically same shape as a multi-channel STFT with C microphones.\",\"Return type: est_unfolded (torch.Tensor)\"]},\"1720\":{\"h\":\"espnet2.enh.layers.adapt_layers.into_orig_type\",\"t\":[\"espnet2.enh.layers.adapt_layers.into_orig_type(x, orig_type)\",\"Inverts into_tuple function.\"]},\"1721\":{\"h\":\"espnet2.enh.layers.adapt_layers.into_tuple\",\"t\":[\"<!-- _espnet2.enh.layers.adapt_layers.into_tuple -->\",\"espnet2.enh.layers.adapt_layers.into_tuple(x)\",\"Transforms tensor/list/tuple into tuple.\"]},\"1722\":{\"h\":\"espnet2.enh.layers.complex_utils.inverse\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.inverse -->\",\"espnet2.enh.layers.complex_utils.inverse(c: Tensor | ComplexTensor)\"]},\"1723\":{\"h\":\"espnet2.enh.layers.complex_utils.is_complex\",\"t\":[\"espnet2.enh.layers.complex_utils.is_complex(c)\"]},\"1724\":{\"h\":\"espnet2.enh.layers.complex_utils.is_torch_complex_tensor\",\"t\":[\"espnet2.enh.layers.complex_utils.is_torch_complex_tensor(c)\"]},\"1725\":{\"h\":\"espnet2.enh.layers.adapt_layers.make_adapt_layer\",\"t\":[\"espnet2.enh.layers.adapt_layers.make_adapt_layer(type, indim, enrolldim, ninputs=1)\"]},\"1726\":{\"h\":\"espnet2.enh.layers.dcunet.make_unet_encoder_decoder_args\",\"t\":[\"espnet2.enh.layers.dcunet.make_unet_encoder_decoder_args(encoder_args, decoder_args)\"]},\"1727\":{\"h\":\"espnet2.enh.layers.complex_utils.matmul\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.matmul -->\",\"espnet2.enh.layers.complex_utils.matmul(a: Tensor | ComplexTensor, b: Tensor | ComplexTensor)\"]},\"1728\":{\"h\":\"espnet2.enh.layers.dprnn.merge_feature\",\"t\":[\"<!-- _espnet2.enh.layers.dprnn.merge_feature -->\",\"espnet2.enh.layers.dprnn.merge_feature(input, rest)\"]},\"1729\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.naive_downsample_2d\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.naive_downsample_2d(x, factor=2)\"]},\"1730\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.naive_upsample_2d\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.naive_upsample_2d(x, factor=2)\"]},\"1731\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ncsn_conv1x1\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.layers.ncsn_conv1x1(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1.0, padding=0)\",\"1x1 convolution. Same as NCSNv1/v2.\"]},\"1732\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ncsn_conv3x3\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.layers.ncsn_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1.0, padding=1)\",\"3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\"]},\"1733\":{\"h\":\"espnet2.enh.layers.complex_utils.new_complex_like\",\"t\":[\"espnet2.enh.layers.complex_utils.new_complex_like(ref: Tensor | ComplexTensor, real_imag: Tuple[Tensor, Tensor])\"]},\"1734\":{\"h\":\"espnet2.enh.layers.conv_utils.num2tuple\",\"t\":[\"<!-- _espnet2.enh.layers.conv_utils.num2tuple -->\",\"espnet2.enh.layers.conv_utils.num2tuple(num)\"]},\"1735\":{\"h\":\"espnet2.enh.separator.svoice_separator.overlap_and_add\",\"t\":[\"espnet2.enh.separator.svoice_separator.overlap_and_add(signal, frame_step)\",\"Reconstructs a signal from a framed representation.\",\"Adds potentially overlapping frames of a signal with shape […, frames, frame_length], offsetting subsequent frames by frame_step. The resulting tensor has shape […, output_size] where\",\"output_size = (frames - 1) * frame_step + frame_length\",\"Args: : signal: A […, frames, frame_length] Tensor. All dimensions may be unknown, : and rank must be at least 2. <br/> frame_step: An integer denoting overlap offsets. : Must be less than or equal to frame_length.\",\"Returns: : A Tensor with shape […, output_size] containing the : overlap-added frames of signal’s inner-most two dimensions. <br/> output_size = (frames - 1) * frame_step + frame_length\",\"Based on\",\"https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/signal/python/ops/reconstruction_ops.py\"]},\"1736\":{\"h\":\"espnet2.enh.layers.beamformer.perform_WPD_filtering\",\"t\":[\"espnet2.enh.layers.beamformer.perform_WPD_filtering(filter_matrix: Tensor | ComplexTensor, Y: Tensor | ComplexTensor, bdelay: int, btaps: int)\",\"Perform WPD filtering.\",\"Parameters:\",\"filter_matrix – Filter matrix (B, F, (btaps + 1) * C)\",\"Y – Complex STFT signal with shape (B, F, C, T)\",\"Returns: (B, F, T)\",\"Return type: enhanced (torch.complex64/ComplexTensor)\"]},\"1737\":{\"h\":\"espnet2.enh.layers.wpe.perform_filter_operation\",\"t\":[\"espnet2.enh.layers.wpe.perform_filter_operation(Y: Tensor | ComplexTensor, filter_matrix_conj: Tensor | ComplexTensor, taps, delay)\",\"Parameters:\",\"Y – Complex-valued STFT signal of shape (F, C, T)\",\"Matrix (filter) –\"]},\"1738\":{\"h\":\"espnet2.enh.layers.dnsmos.poly1d\",\"t\":[\"<!-- _espnet2.enh.layers.dnsmos.poly1d -->\",\"espnet2.enh.layers.dnsmos.poly1d(coefficients, use_numpy=False)\"]},\"1739\":{\"h\":\"espnet2.enh.layers.beamformer.prepare_beamformer_stats\",\"t\":[\"espnet2.enh.layers.beamformer.prepare_beamformer_stats(signal, masks_speech, mask_noise, powers=None, beamformer_type='mvdr', bdelay=3, btaps=5, eps=1e-06)\",\"Prepare necessary statistics for constructing the specified beamformer.\",\"Parameters:\",\"signal (torch.complex64/ComplexTensor) – (…, F, C, T)\",\"masks_speech (List *[*torch.Tensor]) – (…, F, C, T) masks for all speech sources\",\"mask_noise (torch.Tensor) – (…, F, C, T) noise mask\",\"powers (List *[*torch.Tensor]) – powers for all speech sources (…, F, T) used for wMPDR or WPD beamformers\",\"beamformer_type (str) – one of the pre-defined beamformer types\",\"bdelay (int) – delay factor, used for WPD beamformser\",\"btaps (int) – number of filter taps, used for WPD beamformser\",\"eps (torch.Tensor) – tiny constant\",\"Returns: a dictionary containing all necessary statistics : e.g. “psd_n”, “psd_speech”, “psd_distortion” Note: * When masks_speech is a tensor or a single-element list, all returned \",\"statistics are tensors;\",\"When masks_speech is a multi-element list, some returned statistics can be a list, e.g., “psd_n” for MVDR, “psd_speech” and “psd_distortion”.\",\"Return type: beamformer_stats (dict)\"]},\"1740\":{\"h\":\"espnet2.enh.layers.complex_utils.reverse\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.reverse -->\",\"espnet2.enh.layers.complex_utils.reverse(a: Tensor | ComplexTensor, dim=0)\"]},\"1741\":{\"h\":\"espnet2.enh.layers.beamformer.signal_framing\",\"t\":[\"espnet2.enh.layers.beamformer.signal_framing(signal: Tensor | ComplexTensor, frame_length: int, frame_step: int, bdelay: int, do_padding: bool = False, pad_value: int = 0, indices: List | None = None)\",\"Expand signal into several frames, with each frame of length frame_length.\",\"Parameters:\",\"signal – (…, T)\",\"frame_length – length of each segment\",\"frame_step – step for selecting frames\",\"bdelay – delay for WPD\",\"do_padding – whether or not to pad the input signal at the beginning of the time dimension\",\"pad_value – value to fill in the padding\",\"Returns: if do_padding: (…, T, frame_length) else: (…, T - bdelay - frame_length + 2, frame_length)\",\"Return type: torch.Tensor\"]},\"1742\":{\"h\":\"espnet2.enh.layers.complex_utils.solve\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.solve -->\",\"espnet2.enh.layers.complex_utils.solve(b: Tensor | ComplexTensor, a: Tensor | ComplexTensor)\",\"Solve the linear equation ax = b.\"]},\"1743\":{\"h\":\"espnet2.enh.layers.dprnn.split_feature\",\"t\":[\"<!-- _espnet2.enh.layers.dprnn.split_feature -->\",\"espnet2.enh.layers.dprnn.split_feature(input, segment_size)\"]},\"1744\":{\"h\":\"espnet2.enh.layers.complex_utils.stack\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.stack -->\",\"espnet2.enh.layers.complex_utils.stack(seq: Sequence[ComplexTensor | Tensor], *args, **kwargs)\"]},\"1745\":{\"h\":\"espnet2.enh.layers.ifasnet.test_model\",\"t\":[\"<!-- _espnet2.enh.layers.ifasnet.test_model -->\",\"espnet2.enh.layers.ifasnet.test_model(model)\"]},\"1746\":{\"h\":\"espnet2.enh.layers.beamformer.tik_reg\",\"t\":[\"<!-- _espnet2.enh.layers.beamformer.tik_reg -->\",\"espnet2.enh.layers.beamformer.tik_reg(mat, reg: float = 1e-08, eps: float = 1e-08)\",\"Perform Tikhonov regularization (only modifying real part).\",\"Parameters:\",\"mat (torch.complex64/ComplexTensor) – input matrix (…, C, C)\",\"reg (float) – regularization factor\",\"eps (float) –\",\"Returns: regularized matrix (…, C, C)\",\"Return type: ret (torch.complex64/ComplexTensor)\"]},\"1747\":{\"h\":\"espnet2.enh.layers.complex_utils.to_complex\",\"t\":[\"espnet2.enh.layers.complex_utils.to_complex(c)\"]},\"1748\":{\"h\":\"espnet2.enh.layers.complex_utils.to_double\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.to_double -->\",\"espnet2.enh.layers.complex_utils.to_double(c)\"]},\"1749\":{\"h\":\"espnet2.enh.layers.complex_utils.to_float\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.to_float -->\",\"espnet2.enh.layers.complex_utils.to_float(c)\"]},\"1750\":{\"h\":\"espnet2.enh.layers.dcunet.torch_complex_from_reim\",\"t\":[\"espnet2.enh.layers.dcunet.torch_complex_from_reim(re, im)\"]},\"1751\":{\"h\":\"espnet2.enh.layers.complex_utils.trace\",\"t\":[\"<!-- _espnet2.enh.layers.complex_utils.trace -->\",\"espnet2.enh.layers.complex_utils.trace(a: Tensor | ComplexTensor)\"]},\"1752\":{\"h\":\"espnet2.enh.layers.dcunet.unet_decoder_args\",\"t\":[\"espnet2.enh.layers.dcunet.unet_decoder_args(encoders, *, skip_connections)\",\"Get list of decoder arguments for upsampling (right) side of a symmetric u-net,\",\"given the arguments used to construct the encoder. :param encoders (tuple of length N of tuples of: (in_chan, out_chan, kernel_size, stride, padding)):\",\"List of arguments used to construct the encoders\",\"Parameters:skip_connections (bool) – Whether to include skip connections in the calculation of decoder input channels.\",\"Returns: tuple of length N of tuples of : (in_chan, out_chan, kernel_size, stride, padding): Arguments to be used to construct decoders\"]},\"1753\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.upfirdn2d.upfirdn2d\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.upfirdn2d.upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0))\"]},\"1754\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.upfirdn2d.upfirdn2d_native\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.upfirdn2d.upfirdn2d_native(input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1)\"]},\"1755\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.upsample_2d\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.upsample_2d(x, k=None, factor=2, gain=1)\",\"Upsample a batch of 2D images with the given filter.\",\"Accepts a batch of 2D images of the shape [N, C, H, W] or [N, H, W, C] and upsamples each image with the given filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the specified gain. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its shape is a multiple of the upsampling factor. :param x: Input tensor of the shape [N, C, H, W] or\",\"`\",\"[N, H, W,\",\"C]`.\",\"Parameters:\",\"k – FIR filter of the shape [firH, firW] or [firN] (separable). The default is [1] * factor, which corresponds to nearest-neighbor upsampling.\",\"factor – Integer upsampling factor (default: 2).\",\"gain – Scaling factor for signal magnitude (default: 1.0).\",\"Returns: Tensor of the shape [N, C, H * factor, W * factor]\"]},\"1756\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.upsample_conv_2d\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.upsample_conv_2d(x, w, k=None, factor=2, gain=1)\",\"Fused upsample_2d() followed by tf.nn.conv2d().\",\"Padding is performed only once at the beginning, not between the operations. The fused op is considerably more efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of arbitrary order. :param x: Input tensor of the shape [N, C, H, W] or\",\"`\",\"[N, H, W,\",\"C]`.\",\"Parameters:\",\"w – Weight tensor of the shape [filterH, filterW, inChannels, outChannels]. Grouped convolution can be performed by inChannels = x.shape[0] // numGroups.\",\"k – FIR filter of the shape [firH, firW] or [firN] (separable). The default is [1] * factor, which corresponds to nearest-neighbor upsampling.\",\"factor – Integer upsampling factor (default: 2).\",\"gain – Scaling factor for signal magnitude (default: 1.0).\",\"Returns: Tensor of the shape [N, C, H * factor, W * factor] or [N, H * factor, W * factor, C], and same datatype as x.\"]},\"1757\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.variance_scaling\",\"t\":[\"espnet2.enh.layers.ncsnpp_utils.layers.variance_scaling(scale, mode, distribution, in_axis=1, out_axis=0, dtype=torch.float32, device='cpu')\",\"Ported from JAX.\"]},\"1758\":{\"h\":\"espnet2.enh.layers.wpe.wpe\",\"t\":[\"<!-- _espnet2.enh.layers.wpe.wpe -->\",\"espnet2.enh.layers.wpe.wpe(Y: Tensor | ComplexTensor, taps=10, delay=3, iterations=3)\",\"WPE\",\"Parameters:\",\"Y – Complex valued STFT signal with shape (F, C, T)\",\"taps – Number of filter taps\",\"delay – Delay as a guard interval, such that X does not become zero.\",\"iterations –\",\"Returns: (F, C, T)\",\"Return type: enhanced\"]},\"1759\":{\"h\":\"espnet2.enh.layers.wpe.wpe_one_iteration\",\"t\":[\"<!-- _espnet2.enh.layers.wpe.wpe_one_iteration -->\",\"espnet2.enh.layers.wpe.wpe_one_iteration(Y: Tensor | ComplexTensor, power: Tensor, taps: int = 10, delay: int = 3, eps: float = 1e-10, inverse_power: bool = True)\",\"WPE for one iteration\",\"Parameters:\",\"Y – Complex valued STFT signal with shape (…, C, T)\",\"power – : (…, T)\",\"taps – Number of filter taps\",\"delay – Delay as a guard interval, such that X does not become zero.\",\"eps –\",\"inverse_power (bool) –\",\"Returns: (…, C, T)\",\"Return type: enhanced\"]},\"1760\":{\"h\":\"espnet2.gan_svs.abs_gan_svs.AbsGANSVS\",\"t\":[\"<!-- _espnet2.gan_svs.abs_gan_svs.AbsGANSVS -->\",\"class espnet2.gan_svs.abs_gan_svs.AbsGANSVS\",\"Bases: AbsSVS, ABC\",\"GAN-based SVS model abstract class.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(forward_generator, *args, **kwargs)\",\"Return generator or discriminator loss.\",\"training : bool\"]},\"1761\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.AvocodoDiscriminator\",\"t\":[\"class espnet2.gan_svs.avocodo.avocodo.AvocodoDiscriminator(combd: Dict[str, Any] = {'combd_d_d': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'combd_d_g': [[1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1]], 'combd_d_k': [[7, 11, 11, 11, 11, 5], [11, 21, 21, 21, 21, 5], [15, 41, 41, 41, 41, 5]], 'combd_d_p': [[3, 5, 5, 5, 5, 2], [5, 10, 10, 10, 10, 2], [7, 20, 20, 20, 20, 2]], 'combd_d_s': [[1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1]], 'combd_h_u': [[16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024]], 'combd_op_f': [1, 1, 1], 'combd_op_g': [1, 1, 1], 'combd_op_k': [3, 3, 3]}, sbd: Dict[str, Any] = {'pqmf_config': {'fsbd': [64, 256, 0.1, 9.0], 'sbd': [16, 256, 0.03, 10.0]}, 'sbd_band_ranges': [[0, 6], [0, 11], [0, 16], [0, 64]], 'sbd_dilations': [[[5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11]], [[3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 3, 5], [2, 3, 5]]], 'sbd_filters': [[64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [32, 64, 128, 128, 128]], 'sbd_kernel_sizes': [[[7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]], [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]]], 'sbd_strides': [[1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1]], 'sbd_transpose': [False, False, False, True], 'segment_size': 8192, 'use_sbd': True}, pqmf_config: Dict[str, Any] = {'lv1': [2, 256, 0.25, 10.0], 'lv2': [4, 192, 0.13, 10.0]}, projection_filters: List[int] = [0, 1, 1, 1])\",\"Bases: Module\",\"Avocodo Discriminator module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y: Tensor, y_hats: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1762\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1763\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.AvocodoDiscriminatorPlus\",\"t\":[\"class espnet2.gan_svs.avocodo.avocodo.AvocodoDiscriminatorPlus(combd: Dict[str, Any] = {'combd_d_d': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'combd_d_g': [[1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1]], 'combd_d_k': [[7, 11, 11, 11, 11, 5], [11, 21, 21, 21, 21, 5], [15, 41, 41, 41, 41, 5]], 'combd_d_p': [[3, 5, 5, 5, 5, 2], [5, 10, 10, 10, 10, 2], [7, 20, 20, 20, 20, 2]], 'combd_d_s': [[1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1]], 'combd_h_u': [[16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024]], 'combd_op_f': [1, 1, 1], 'combd_op_g': [1, 1, 1], 'combd_op_k': [3, 3, 3]}, sbd: Dict[str, Any] = {'pqmf_config': {'fsbd': [64, 256, 0.1, 9.0], 'sbd': [16, 256, 0.03, 10.0]}, 'sbd_band_ranges': [[0, 6], [0, 11], [0, 16], [0, 64]], 'sbd_dilations': [[[5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11]], [[3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 3, 5], [2, 3, 5]]], 'sbd_filters': [[64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [32, 64, 128, 128, 128]], 'sbd_kernel_sizes': [[[7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]], [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]]], 'sbd_strides': [[1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1]], 'sbd_transpose': [False, False, False, True], 'segment_size': 8192, 'use_sbd': True}, pqmf_config: Dict[str, Any] = {'lv1': [2, 256, 0.25, 10.0], 'lv2': [4, 192, 0.13, 10.0]}, projection_filters: List[int] = [0, 1, 1, 1], sample_rate: int = 22050, multi_freq_disc_params: Dict[str, Any] = {'divisors': [32, 16, 8, 4, 2, 1, 1], 'domain': 'double', 'hidden_channels': [256, 512, 512], 'hop_length_factors': [4, 8, 16], 'mel_scale': True, 'strides': [1, 2, 1, 2, 1, 2, 1]})\",\"Bases: Module\",\"Avocodo discriminator with additional MFD.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y: Tensor, y_hats: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1764\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1765\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.AvocodoGenerator\",\"t\":[\"class espnet2.gan_svs.avocodo.avocodo.AvocodoGenerator(in_channels: int = 80, out_channels: int = 1, channels: int = 512, global_channels: int = -1, kernel_size: int = 7, upsample_scales: List[int] = [8, 8, 2, 2], upsample_kernel_sizes: List[int] = [16, 16, 4, 4], resblock_kernel_sizes: List[int] = [3, 7, 11], resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], projection_filters: List[int] = [0, 1, 1, 1], projection_kernels: List[int] = [0, 5, 7, 11], use_additional_convs: bool = True, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, use_weight_norm: bool = True)\",\"Bases: Module\",\"Avocodo generator module.\",\"Initialize AvocodoGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"channels (int) – Number of hidden representation channels.\",\"global_channels (int) – Number of global conditioning channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsample layers.\",\"resblock_kernel_sizes (List *[*int]) – List of kernel sizes for residual blocks.\",\"resblock_dilations (List *[*List *[*int]]) – List of list of dilations for residual blocks.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor, g: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Input tensor (B, in_channels, T).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: List of output tensors (B, out_channels, T).\",\"Return type: List[Tensor]\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows the official implementation manner. https://github.com/jik876/hifi-gan/blob/master/models.py\",\"training : bool\"]},\"1766\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.BaseFrequenceDiscriminator\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.BaseFrequenceDiscriminator(in_channels, hidden_channels=512, divisors=[32, 16, 8, 4, 2, 1, 1], strides=[1, 2, 1, 2, 1, 2, 1])\",\"Bases: Module\",\"Base Frequence Discriminator\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"hidden_channels (int,optional) – Number of channels in hidden layers. Defaults to 512.\",\"divisors (List *[*int],optional) – List of divisors for the number of channels in each layer. The length of the list determines the number of layers. Defaults to [32, 16, 8, 4, 2, 1, 1].\",\"strides (List *[*int],optional) – List of stride values for each layer. The length of the list determines the number of layers.Defaults to [1, 2, 1, 2, 1, 2, 1].\",\"forward(x)\",\"Perform forward pass through the base frequency discriminator.\",\"Parameters:x (torch.Tensor) – Input tensor of shape (B, in_channels, freq_bins, time_steps).\",\"Returns: List of output tensors from each layer of the : discriminator, where the first tensor corresponds to the output of the first layer, and so on.\",\"Return type: List[torch.Tensor]\",\"training : bool\"]},\"1767\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.CoMBD\",\"t\":[\"<!-- _espnet2.gan_svs.avocodo.avocodo.CoMBD -->\",\"class espnet2.gan_svs.avocodo.avocodo.CoMBD(h, pqmf_list=None, use_spectral_norm=False)\",\"Bases: Module\",\"CoMBD (Collaborative Multi-band Discriminator) module\",\"from from https://arxiv.org/abs/2206.13404\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ys, ys_hat)\",\"Forward CoMBD.\",\"Parameters:\",\"ys (List *[*Tensor]) – List of ground truth signals of shape (B, 1, T).\",\"ys_hat (List *[*Tensor]) – List of predicted signals of shape (B, 1, T).\",\"Returns: Tuple containing the list of output tensors of shape (B, C_out, T_out) for real and fake, respectively, and the list of feature maps of shape (B, C, T) at each Conv1d layer for real and fake, respectively.\",\"Return type: Tuple[List[Tensor], List[Tensor], List[List[Tensor]], List[List[Tensor]]]\",\"training : bool\"]},\"1768\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.CoMBDBlock\",\"t\":[\"<!-- _espnet2.gan_svs.avocodo.avocodo.CoMBDBlock -->\",\"class espnet2.gan_svs.avocodo.avocodo.CoMBDBlock(h_u: List[int], d_k: List[int], d_s: List[int], d_d: List[int], d_g: List[int], d_p: List[int], op_f: int, op_k: int, op_g: int, use_spectral_norm=False)\",\"Bases: Module\",\"CoMBD (Collaborative Multi-band Discriminator) block module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward pass through the CoMBD block.\",\"Parameters:x (Tensor) – Input tensor of shape (B, C_in, T_in).\",\"Returns: Tuple containing the output tensor of : shape (B, C_out, T_out)\",\"and a list of feature maps of shape (B, C, T) at each Conv1d layer.\",\"Return type: Tuple[Tensor, List[Tensor]]\",\"training : bool\"]},\"1769\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.ConvReluNorm\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.ConvReluNorm(in_channels, hidden_channels, out_channels, kernel_size, n_layers, dropout_rate)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1770\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1771\":{\"h\":\"espnet2.gan_svs.vits.pitch_predictor.Decoder\",\"t\":[\"class espnet2.gan_svs.vits.pitch_predictor.Decoder(out_channels: int = 192, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, pw_layer_type: str = 'conv1d', pw_conv_kernel_size: int = 3, pos_enc_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0, global_channels: int = -1)\",\"Bases: Module\",\"Pitch or Mel decoder module in VISinger 2.\",\"Initialize Decoder in VISinger 2.\",\"Parameters:\",\"out_channels (int) – The output dimension of the module.\",\"attention_dim (int) – The dimension of the attention mechanism.\",\"attention_heads (int) – The number of attention heads.\",\"linear_units (int) – The number of units in the linear layer.\",\"blocks (int) – The number of encoder blocks.\",\"pw_layer_type (str) – The type of position-wise layer to use.\",\"pw_conv_kernel_size (int) – The kernel size of the position-wise convolutional layer.\",\"pos_enc_layer_type (str) – The type of positional encoding layer to use.\",\"self_attention_layer_type (str) – The type of self-attention layer to use.\",\"activation_type (str) – The type of activation function to use.\",\"normalize_before (bool) – Whether to normalize the data before the position-wise layer or after.\",\"use_macaron_style (bool) – Whether to use the macaron style or not.\",\"use_conformer_conv (bool) – Whether to use Conformer style conv or not.\",\"conformer_kernel_size (int) – The kernel size of the conformer convolutional layer.\",\"dropout_rate (float) – The dropout rate to use.\",\"positional_dropout_rate (float) – The positional dropout rate to use.\",\"attention_dropout_rate (float) – The attention dropout rate to use.\",\"global_channels (int) – The number of channels to use for global conditioning.\",\"forward(x, x_lengths, g=None)\",\"Forward pass of the Decoder.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, 2 + attention_dim, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Tensor,optional) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Output tensor (B, 1, T). Tensor: Output mask (B, 1, T).\",\"Return type: Tensor\",\"training : bool\"]},\"1772\":{\"h\":\"espnet2.gan_svs.vits.duration_predictor.DurationPredictor\",\"t\":[\"class espnet2.gan_svs.vits.duration_predictor.DurationPredictor(channels, filter_channels, kernel_size, dropout_rate, global_channels=0)\",\"Bases: Module\",\"Initialize duration predictor module.\",\"Parameters:\",\"channels (int) – Number of input channels.\",\"filter_channels (int) – Number of filter channels.\",\"kernel_size (int) – Size of the convolutional kernel.\",\"dropout_rate (float) – Dropout rate.\",\"global_channels (int,optional) – Number of global conditioning channels.\",\"forward(x, x_mask, g=None)\",\"Forward pass through the duration predictor module.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"x_mask (Tensor) – Mask tensor (B, 1, T).\",\"g (Tensor,optional) – Global condition tensor (B, global_channels, 1).\",\"Returns: Predicted duration tensor (B, 2, T).\",\"Return type: Tensor\",\"training : bool\"]},\"1773\":{\"h\":\"espnet2.gan_svs.espnet_model.ESPnetGANSVSModel\",\"t\":[\"class espnet2.gan_svs.espnet_model.ESPnetGANSVSModel(postfrontend: AbsFrontend | None, text_extract: AbsFeatsExtract | None, feats_extract: AbsFeatsExtract | None, score_feats_extract: AbsFeatsExtract | None, label_extract: AbsFeatsExtract | None, pitch_extract: AbsFeatsExtract | None, ying_extract: AbsFeatsExtract | None, duration_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, svs: AbsGANSVS)\",\"Bases: AbsGANESPnetModel\",\"ESPnet model for GAN-based singing voice synthesis task.\",\"Initialize ESPnetGANSVSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs)\",\"Calculate features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (T_label).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (T_phone).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (T_phone).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_wav). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True, **kwargs)\",\"Return generator or discriminator loss with dict format.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (B, T_label).\",\"duration_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (B, T_phone).\",\"duration_ruled_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (B, T_syllable).\",\"duration_syb_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_wav). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"kwargs – “utt_id” is among the input.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, singing: Tensor | None = None, label: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, duration_phn: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_syb: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **decode_config)\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"singing (Tensor) – Singing waveform tensor (T_wav).\",\"label (Option *[*Tensor]) – Label tensor (T_label).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (T_l abel).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (T_label).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (T_phone).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (T_phone).\",\"slur (Optional *[*Tensor]) – slur tensor (T_phone).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"pitch (Optional *[*Tensor) – Pitch tensor (T_wav).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"1774\":{\"h\":\"espnet2.gan_svs.post_frontend.fused.FusedPostFrontends\",\"t\":[\"class espnet2.gan_svs.post_frontend.fused.FusedPostFrontends(postfrontends=None, align_method='linear_projection', proj_dim=100, fs=16000, input_fs=24000)\",\"Bases: AbsFrontend\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1775\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"1776\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.Generator_Harm\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.Generator_Harm(hidden_channels: int = 192, n_harmonic: int = 64, kernel_size: int = 3, padding: int = 1, dropout_rate: float = 0.1, sample_rate: int = 22050, hop_size: int = 256)\",\"Bases: Module\",\"Initialize harmonic generator module.\",\"Parameters:\",\"hidden_channels (int) – Number of channels in the input and hidden layers.\",\"n_harmonic (int) – Number of harmonic channels.\",\"kernel_size (int) – Size of the convolutional kernel.\",\"padding (int) – Amount of padding added to the input.\",\"dropout_rate (float) – Dropout rate.\",\"sample_rate (int) – Sampling rate of the input audio.\",\"hop_size (int) – Hop size used in the analysis of the input audio.\",\"forward(f0, harm, mask)\",\"Generate harmonics from F0 and harmonic data.\",\"Parameters:\",\"f0 (Tensor) – Pitch (F0) tensor (B, 1, T).\",\"harm (Tensor) – Harmonic data tensor (B, hidden_channels, T).\",\"mask (Tensor) – Mask tensor for harmonic data (B, 1, T).\",\"Returns: Harmonic signal tensor (B, n_harmonic, T * hop_length).\",\"Return type: Tensor\",\"training : bool\"]},\"1777\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.Generator_Noise\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.Generator_Noise(win_length: int = 1024, hop_length: int = 256, n_fft: int = 1024, hidden_channels: int = 192, kernel_size: int = 3, padding: int = 1, dropout_rate: float = 0.1)\",\"Bases: Module\",\"Initialize the Generator_Noise module.\",\"Parameters:\",\"win_length (int,optional) – Window length. If None, set to n_fft.\",\"hop_length (int) – Hop length.\",\"n_fft (int) – FFT size.\",\"hidden_channels (int) – Number of hidden representation channels.\",\"kernel_size (int) – Size of the convolutional kernel.\",\"padding (int) – Size of the padding applied to the input.\",\"dropout_rate (float) – Dropout rate.\",\"forward(x, mask)\",\"Forward Generator Noise.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, hidden_channels, T).\",\"mask (Tensor) – Mask tensor (B, 1, T).\",\"Returns: Output tensor (B, 1, T * hop_size).\",\"Return type: Tensor\",\"training : bool\"]},\"1778\":{\"h\":\"espnet2.gan_svs.joint.joint_score2wav.JointScore2Wav\",\"t\":[\"class espnet2.gan_svs.joint.joint_score2wav.JointScore2Wav(idim: int, odim: int, segment_size: int = 32, sampling_rate: int = 22050, score2mel_type: str = 'xiaoice', score2mel_params: Dict[str, Any] = {'adim': 384, 'aheads': 4, 'conformer_activation_type': 'swish', 'conformer_dec_kernel_size': 31, 'conformer_enc_kernel_size': 7, 'conformer_pos_enc_layer_type': 'rel_pos', 'conformer_rel_pos_type': 'latest', 'conformer_self_attn_layer_type': 'rel_selfattn', 'decoder_concat_after': False, 'decoder_normalize_before': True, 'decoder_type': 'transformer', 'dlayers': 6, 'dunits': 1536, 'duration_predictor_chans': 384, 'duration_predictor_dropout_rate': 0.1, 'duration_predictor_kernel_size': 3, 'duration_predictor_layers': 2, 'elayers': 6, 'encoder_concat_after': False, 'encoder_normalize_before': True, 'encoder_type': 'transformer', 'eunits': 1536, 'init_dec_alpha': 1.0, 'init_enc_alpha': 1.0, 'init_type': 'xavier_uniform', 'lambda_dur': 0.1, 'lambda_mel': 1, 'lambda_pitch': 0.01, 'lambda_vuv': 0.01, 'langs': None, 'loss_function': 'XiaoiceSing2', 'loss_type': 'L1', 'midi_dim': 129, 'positionwise_conv_kernel_size': 1, 'positionwise_layer_type': 'conv1d', 'postnet_chans': 512, 'postnet_dropout_rate': 0.5, 'postnet_filts': 5, 'postnet_layers': 5, 'reduction_factor': 1, 'spk_embed_dim': None, 'spk_embed_integration_type': 'add', 'spks': None, 'tempo_dim': 500, 'transformer_dec_attn_dropout_rate': 0.1, 'transformer_dec_dropout_rate': 0.1, 'transformer_dec_positional_dropout_rate': 0.1, 'transformer_enc_attn_dropout_rate': 0.1, 'transformer_enc_dropout_rate': 0.1, 'transformer_enc_positional_dropout_rate': 0.1, 'use_batch_norm': True, 'use_cnn_in_conformer': True, 'use_macaron_style_in_conformer': True, 'use_masking': False, 'use_scaled_pos_enc': True, 'use_weighted_masking': False, 'zero_triu': False}, vocoder_type: str = 'hifigan_generator', vocoder_params: Dict[str, Any] = {'bias': True, 'channels': 512, 'global_channels': -1, 'kernel_size': 7, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'resblock_kernel_sizes': [3, 7, 11], 'upsample_kernel_sizes': [16, 16, 4, 4], 'upsample_scales': [8, 8, 2, 2], 'use_additional_convs': True, 'use_weight_norm': True}, use_pqmf: bool = False, pqmf_params: Dict[str, Any] = {'beta': 9.0, 'cutoff_ratio': 0.142, 'subbands': 4, 'taps': 62}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_score2mel: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False)\",\"Bases: AbsGANSVS\",\"General class to jointly train score2mel and vocoder parts.\",\"Initialize JointScore2Wav module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since the model is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"segment_size (int) – Segment size for random windowed inputs.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"text2mel_type (str) – The text2mel model type.\",\"text2mel_params (Dict *[*str,Any]) – Parameter dict for text2mel model.\",\"use_pqmf (bool) – Whether to use PQMF for multi-band vocoder.\",\"pqmf_params (Dict *[*str,Any]) – Parameter dict for PQMF module.\",\"vocoder_type (str) – The vocoder model type.\",\"vocoder_params (Dict *[*str,Any]) – Parameter dict for vocoder model.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"use_feat_match_loss (bool) – Whether to use feat match loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"use_mel_loss (bool) – Whether to use mel loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_text2mel (float) – Loss scaling coefficient for text2mel model loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: LongTensor | None = None, duration: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True)\",\"Perform generator forward.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"slur (FloatTensor) – Batch of padded slur (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False)\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (T_text,).\",\"feats (Tensor) – Feature tensor (T_feats, aux_channels).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"sids (Tensor) – Speaker index tensor (1,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (spk_embed_dim,).\",\"lids (Tensor) – Language index tensor (1,).\",\"noise_scale (float) – Noise scale value for flow.\",\"noise_scale_dur (float) – Noise scale value for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated singing.\",\"max_len (Optional *[*int]) – Maximum length.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"feat_gan (Tensor): Generated feature tensor (T_text, C).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_singing\",\"Return whether or not singing is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"1779\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.LayerNorm\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.LayerNorm(channels, eps=1e-05)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1780\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1781\":{\"h\":\"espnet2.gan_svs.vits.length_regulator.LengthRegulator\",\"t\":[\"class espnet2.gan_svs.vits.length_regulator.LengthRegulator(pad_value=0.0)\",\"Bases: Module\",\"Length Regulator\",\"Initilize length regulator module.\",\"Parameters:pad_value (float,optional) – Value used for padding.\",\"LR(x, duration, use_state_info=False)\",\"Length regulates input mel-spectrograms to match duration.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, dim, T).\",\"duration (Tensor) – Duration tensor (B, T).\",\"use_state_info (bool,optional) – Whether to use position information or not.\",\"Returns: Output tensor (B, dim, D_frame). Tensor: Output length (B,).\",\"Return type: Tensor\",\"expand(batch, predicted, use_state_info=False)\",\"Expand input mel-spectrogram based on the predicted duration.\",\"Parameters:\",\"batch (Tensor) – Input tensor (T, dim).\",\"predicted (Tensor) – Predicted duration tensor (T,).\",\"use_state_info (bool,optional) – Whether to use position information or not.\",\"Returns: Output tensor (D_frame, dim).\",\"Return type: Tensor\",\"forward(x, duration, use_state_info=False)\",\"Forward pass through the length regulator module.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, dim, T).\",\"duration (Tensor) – Duration tensor (B, T).\",\"use_state_info (bool,optional) – Whether to use position information or not.\",\"Returns: Output tensor (B, dim, D_frame). Tensor: Output length (B,).\",\"Return type: Tensor\",\"training : bool\"]},\"1782\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.MDC\",\"t\":[\"<!-- _espnet2.gan_svs.avocodo.avocodo.MDC -->\",\"class espnet2.gan_svs.avocodo.avocodo.MDC(in_channels, out_channels, strides, kernel_size, dilations, use_spectral_norm=False)\",\"Bases: Module\",\"Multiscale Dilated Convolution from https://arxiv.org/pdf/1609.07093.pdf\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1783\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1784\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.MDCDConfig\",\"t\":[\"<!-- _espnet2.gan_svs.avocodo.avocodo.MDCDConfig -->\",\"class espnet2.gan_svs.avocodo.avocodo.MDCDConfig(h)\",\"Bases: object\"]},\"1785\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.MelScale\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.MelScale(n_mels: int = 128, sample_rate: int = 24000, f_min: float = 0.0, f_max: float | None = None, n_stft: int | None = None)\",\"Bases: Module\",\"Turn a normal STFT into a mel frequency STFT, using a conversion\",\"matrix. This uses triangular filter banks. User can control which device the filter bank (fb) is (e.g. fb.to(spec_f.device)). :param n_mels: Number of mel filterbanks. (Default: 128) :type n_mels: int, optional :param sample_rate: Sample rate of audio signal. (Default: 16000) :type sample_rate: int, optional :param f_min: Minimum frequency. (Default: 0.) :type f_min: float, optional :param f_max: Maximum frequency.\",\"(Default: sample_rate // 2)\",\"Parameters:n_stft (int,optional) – Number of bins in STFT. Calculated from first input if None is given. See n_fft in :class:Spectrogram. (Default: None)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(specgram: Tensor)\",\"Forward MelScale\",\"Parameters:specgram (Tensor) – A spectrogram STFT of dimension (…, freq, time).\",\"Returns: Mel frequency spectrogram of size (…, n_mels, time).\",\"Return type: Tensor\",\"training : bool\"]},\"1786\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.MultiFrequencyDiscriminator\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.MultiFrequencyDiscriminator(sample_rate: int = 22050, hop_lengths=[128, 256, 512], hidden_channels=[256, 512, 512], domain='double', mel_scale=True, divisors=[32, 16, 8, 4, 2, 1, 1], strides=[1, 2, 1, 2, 1, 2, 1])\",\"Bases: Module\",\"Multi-Frequency Discriminator module in UnivNet.\",\"Initialize Multi-Frequency Discriminator module.\",\"Parameters:\",\"hop_lengths (list) – List of hop lengths.\",\"hidden_channels (list) – List of number of channels in hidden layers.\",\"domain (str) – Domain of input signal. Default is “double”.\",\"mel_scale (bool) – Whether to use mel-scale frequency. Default is True.\",\"divisors (list) – List of divisors for each layer in the discriminator. Default is [32, 16, 8, 4, 2, 1, 1].\",\"strides (list) – List of strides for each layer in the discriminator. Default is [1, 2, 1, 2, 1, 2, 1].\",\"forward(x)\",\"Forward pass of Multi-Frequency Discriminator module.\",\"Parameters:x (Tensor) – Input tensor (B, 1, T * hop_size).\",\"Returns: List of feature maps.\",\"Return type: List[Tensor]\",\"training : bool\"]},\"1787\":{\"h\":\"espnet2.gan_svs.vits.phoneme_predictor.PhonemePredictor\",\"t\":[\"class espnet2.gan_svs.vits.phoneme_predictor.PhonemePredictor(vocabs: int, hidden_channels: int = 192, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 2, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0)\",\"Bases: Module\",\"Phoneme Predictor module in VISinger.\",\"Initialize PhonemePredictor module.\",\"Parameters:\",\"vocabs (int) – The number of vocabulary.\",\"hidden_channels (int) – The number of hidden channels.\",\"attention_dim (int) – The number of attention dimension.\",\"attention_heads (int) – The number of attention heads.\",\"linear_units (int) – The number of linear units.\",\"blocks (int) – The number of encoder blocks.\",\"positionwise_layer_type (str) – The type of position-wise layer.\",\"positionwise_conv_kernel_size (int) – The size of position-wise convolution kernel.\",\"positional_encoding_layer_type (str) – The type of positional encoding layer.\",\"self_attention_layer_type (str) – The type of self-attention layer.\",\"activation_type (str) – The type of activation function.\",\"normalize_before (bool) – Whether to apply normalization before the position-wise layer or not.\",\"use_macaron_style (bool) – Whether to use macaron style or not.\",\"use_conformer_conv (bool) – Whether to use Conformer convolution or not.\",\"conformer_kernel_size (int) – The size of Conformer kernel.\",\"dropout_rate (float) – The dropout rate.\",\"positional_dropout_rate (float) – The dropout rate for positional encoding.\",\"attention_dropout_rate (float) – The dropout rate for attention.\",\"forward(x, x_mask)\",\"Perform forward propagation.\",\"Parameters:\",\"x (Tensor) – The input tensor of shape (B, dim, length).\",\"x_mask (Tensor) – The mask tensor for the input tensor of shape (B, length).\",\"Returns: The predicted phoneme tensor of shape (length, B, vocab_size).\",\"Return type: Tensor\",\"training : bool\"]},\"1788\":{\"h\":\"espnet2.gan_svs.vits.prior_decoder.PriorDecoder\",\"t\":[\"class espnet2.gan_svs.vits.prior_decoder.PriorDecoder(out_channels: int = 384, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0, global_channels: int = 0)\",\"Bases: Module\",\"Initialize prior decoder module.\",\"Parameters:\",\"out_channels (int) – Output channels of the prior decoder. Defaults to 384.\",\"attention_dim (int) – Dimension of the attention mechanism. Defaults to 192.\",\"attention_heads (int) – Number of attention heads. Defaults to 2.\",\"linear_units (int) – Number of units in the linear layer. Defaults to 768.\",\"blocks (int) – Number of blocks in the encoder. Defaults to 6.\",\"positionwise_layer_type (str) – Type of the positionwise layer. Defaults to “conv1d”.\",\"positionwise_conv_kernel_size (int) – Kernel size of the positionwise convolutional layer. Defaults to 3.\",\"positional_encoding_layer_type (str) – Type of positional encoding layer. Defaults to “rel_pos”.\",\"self_attention_layer_type (str) – Type of self-attention layer. Defaults to “rel_selfattn”.\",\"activation_type (str) – Type of activation. Defaults to “swish”.\",\"normalize_before (bool) – Flag for normalization. Defaults to True.\",\"use_macaron_style (bool) – Flag for macaron style. Defaults to False.\",\"use_conformer_conv (bool) – Flag for using conformer convolution. Defaults to False.\",\"conformer_kernel_size (int) – Kernel size for conformer convolution. Defaults to 7.\",\"dropout_rate (float) – Dropout rate. Defaults to 0.1.\",\"positional_dropout_rate (float) – Dropout rate for positional encoding. Defaults to 0.0.\",\"attention_dropout_rate (float) – Dropout rate for attention. Defaults to 0.0.\",\"global_channels (int) – Number of global channels. Defaults to 0.\",\"forward(x, x_lengths, g=None)\",\"Forward pass of the PriorDecoder module.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, attention_dim + 2, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Tensor) – Tensor for multi-singer. (B, global_channels, 1)\",\"Returns: Output tensor (B, out_channels, T). Tensor: Output mask tensor (B, 1, T).\",\"Return type: Tensor\",\"training : bool\"]},\"1789\":{\"h\":\"espnet2.gan_svs.vits.modules.Projection\",\"t\":[\"<!-- _espnet2.gan_svs.vits.modules.Projection -->\",\"class espnet2.gan_svs.vits.modules.Projection(hidden_channels, out_channels)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, x_mask)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1790\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1791\":{\"h\":\"espnet2.gan_svs.post_frontend.s3prl.S3prlPostFrontend\",\"t\":[\"class espnet2.gan_svs.post_frontend.s3prl.S3prlPostFrontend(fs: int | str = 16000, input_fs: int | str = 24000, postfrontend_conf: dict | None = {'badim': 320, 'bdropout_rate': 0.0, 'blayers': 3, 'bnmask': 2, 'bprojs': 320, 'btype': 'blstmp', 'bunits': 300, 'delay': 3, 'ref_channel': -1, 'taps': 5, 'use_beamformer': False, 'use_dnn_mask_for_wpe': True, 'use_wpe': False, 'wdropout_rate': 0.0, 'wlayers': 3, 'wprojs': 320, 'wtype': 'blstmp', 'wunits': 300}, download_dir: str | None = None, multilayer_feature: bool = False, layer: int = -1)\",\"Bases: AbsFrontend\",\"Pretrained SSL model for VISinger2 Plus. Based on S3prlFrontend, S3prlPostFrontend added a resampler to resample the input audio to the sample rate of the pretrained model.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1792\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"reload_pretrained_parameters()\",\"training : bool\"]},\"1793\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.SBD\",\"t\":[\"<!-- _espnet2.gan_svs.avocodo.avocodo.SBD -->\",\"class espnet2.gan_svs.avocodo.avocodo.SBD(h, use_spectral_norm=False)\",\"Bases: Module\",\"SBD (Sub-band Discriminator) from https://arxiv.org/pdf/2206.13404.pdf\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y, y_hat)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1794\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1795\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.SBDBlock\",\"t\":[\"<!-- _espnet2.gan_svs.avocodo.avocodo.SBDBlock -->\",\"class espnet2.gan_svs.avocodo.avocodo.SBDBlock(segment_dim, strides, filters, kernel_size, dilations, use_spectral_norm=False)\",\"Bases: Module\",\"SBD (Sub-band Discriminator) Block\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1796\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1797\":{\"h\":\"espnet2.gan_svs.uhifigan.sine_generator.SineGen\",\"t\":[\"class espnet2.gan_svs.uhifigan.sine_generator.SineGen(sample_rate, harmonic_num=0, sine_amp=0.1, noise_std=0.003, voiced_threshold=0, flag_for_pulse=False)\",\"Bases: Module\",\"Definition of sine generator\",\"SineGen(samp_rate, harmonic_num = 0, : sine_amp = 0.1, noise_std = 0.003, voiced_threshold = 0, flag_for_pulse=False)\",\"sample_rate: sampling rate in Hz harmonic_num: number of harmonic overtones (default 0) sine_amp: amplitude of sine-wavefrom (default 0.1) noise_std: std of Gaussian noise (default 0.003) voiced_thoreshold: F0 threshold for U/V classification (default 0) flag_for_pulse: this SinGen is used inside PulseGen (default False)\",\"Note: when flag_for_pulse is True, the first time step of a voiced : segment is always sin(np.pi) or cos(0)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(f0)\",\"Forward SineGen.\",\"sine_tensor, uv = forward(f0) input F0: tensor(batchsize=1, length, dim=1)\",\"f0 for unvoiced steps should be 0\",\"output sine_tensor: tensor(batchsize=1, length, dim) output uv: tensor(batchsize=1, length, 1)\",\"training : bool\"]},\"1798\":{\"h\":\"espnet2.gan_svs.vits.text_encoder.TextEncoder\",\"t\":[\"class espnet2.gan_svs.vits.text_encoder.TextEncoder(vocabs: int, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0, use_slur=True)\",\"Bases: Module\",\"Text encoder module in VISinger.\",\"This is a module of text encoder described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Instead of the relative positional Transformer, we use conformer architecture as the encoder module, which contains additional convolution layers.\",\"Initialize TextEncoder module.\",\"Parameters:\",\"vocabs (int) – Vocabulary size.\",\"attention_dim (int) – Attention dimension.\",\"attention_heads (int) – Number of attention heads.\",\"linear_units (int) – Number of linear units of positionwise layers.\",\"blocks (int) – Number of encoder blocks.\",\"positionwise_layer_type (str) – Positionwise layer type.\",\"positionwise_conv_kernel_size (int) – Positionwise layer’s kernel size.\",\"positional_encoding_layer_type (str) – Positional encoding layer type.\",\"self_attention_layer_type (str) – Self-attention layer type.\",\"activation_type (str) – Activation function type.\",\"normalize_before (bool) – Whether to apply LayerNorm before attention.\",\"use_macaron_style (bool) – Whether to use macaron style components.\",\"use_conformer_conv (bool) – Whether to use conformer conv layers.\",\"conformer_kernel_size (int) – Conformer’s conv kernel size.\",\"dropout_rate (float) – Dropout rate.\",\"positional_dropout_rate (float) – Dropout rate for positional encoding.\",\"attention_dropout_rate (float) – Dropout rate for attention.\",\"use_slur (bool) – Whether to use slur embedding.\",\"forward(phone: Tensor, phone_lengths: Tensor, midi_id: Tensor, dur: Tensor, slur: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"phone (Tensor) – Input index tensor (B, T_text).\",\"phone_lengths (Tensor) – Length tensor (B,).\",\"midi_id (Tensor) – Input midi tensor (B, T_text).\",\"dur (Tensor) – Input duration tensor (B, T_text).\",\"Returns: Encoded hidden representation (B, attention_dim, T_text). Tensor: Mask tensor for padded part (B, 1, T_text). Tensor: Encoded hidden representation for duration\",\"(B, attention_dim, T_text).\",\"Tensor: Encoded hidden representation for pitch : (B, attention_dim, T_text).\",\"Return type: Tensor\",\"training : bool\"]},\"1799\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.TorchSTFT\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.TorchSTFT(sample_rate, fft_size, hop_size, win_size, normalized=False, domain='linear', mel_scale=False, ref_level_db=20, min_level_db=-100)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"complex(x)\",\"training : bool\",\"transform(x)\"]},\"1800\":{\"h\":\"espnet2.gan_svs.uhifigan.uhifigan.UHiFiGANGenerator\",\"t\":[\"class espnet2.gan_svs.uhifigan.uhifigan.UHiFiGANGenerator(in_channels=80, out_channels=1, channels=512, global_channels: int = -1, kernel_size=7, downsample_scales=(2, 2, 8, 8), downsample_kernel_sizes=(4, 4, 16, 16), upsample_scales=(8, 8, 2, 2), upsample_kernel_sizes=(16, 16, 4, 4), resblock_kernel_sizes=(3, 7, 11), resblock_dilations=[(1, 3, 5), (1, 3, 5), (1, 3, 5)], projection_filters: List[int] = [0, 1, 1, 1], projection_kernels: List[int] = [0, 5, 7, 11], dropout=0.3, use_additional_convs=True, bias=True, nonlinear_activation='LeakyReLU', nonlinear_activation_params={'negative_slope': 0.1}, use_causal_conv=False, use_weight_norm=True, use_avocodo=False)\",\"Bases: Module\",\"UHiFiGAN generator module.\",\"Initialize Unet-based HiFiGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"channels (int) – Number of hidden representation channels.\",\"global_channels (int) – Number of global conditioning channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"upsample_scales (list) – List of upsampling scales.\",\"upsample_kernel_sizes (list) – List of kernel sizes for upsampling layers.\",\"resblock_kernel_sizes (list) – List of kernel sizes for residual blocks.\",\"resblock_dilations (list) – List of dilation list for residual blocks.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (dict) – Hyperparameters for activation function.\",\"use_causal_conv (bool) – Whether to use causal structure.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c=None, f0=None, excitation=None, g: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Input tensor (B, in_channels, T).\",\"f0 (Tensor) – Input tensor (B, 1, T).\",\"excitation (Tensor) – Input tensor (B, frame_len, T).\",\"Returns: Output tensor (B, out_channels, T).\",\"Return type: Tensor\",\"inference(excitation=None, f0=None, c=None, normalize_before=False)\",\"Perform inference.\",\"Parameters:\",\"c (Union *[*Tensor,ndarray]) – Input tensor (T, in_channels).\",\"normalize_before (bool) – Whether to perform normalization.\",\"Returns: Output tensor (T ** prod(upsample_scales), out_channels).\",\"Return type: Tensor\",\"register_stats(stats)\",\"Register stats for de-normalization as buffer.\",\"Parameters:stats (str) – Path of statistics file (“.npy” or “.h5”).\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows the official implementation manner. https://github.com/jik876/hifi-gan/blob/master/models.py\",\"training : bool\"]},\"1801\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.VISinger2Discriminator\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.VISinger2Discriminator(scales: int = 1, scale_downsample_pooling: str = 'AvgPool1d', scale_downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, scale_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, follow_official_norm: bool = True, periods: List[int] = [2, 3, 5, 7, 11], period_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, multi_freq_disc_params: Dict[str, Any] = {'divisors': [32, 16, 8, 4, 2, 1, 1], 'domain': 'double', 'hidden_channels': [256, 512, 512], 'hop_length_factors': [4, 8, 16], 'mel_scale': True, 'sample_rate': 22050, 'strides': [1, 2, 1, 2, 1, 2, 1]})\",\"Bases: Module\",\"Discriminator module for VISinger2, including MSD, MPD, and MFD.\",\"Parameters:\",\"scales (int) – Number of scales to be used in the multi-scale discriminator.\",\"scale_downsample_pooling (str) – Type of pooling used for downsampling.\",\"scale_downsample_pooling_params (Dict *[*str,Any]) – Parameters for the downsampling pooling layer.\",\"scale_discriminator_params (Dict *[*str,Any]) – Parameters for the scale discriminator.\",\"follow_official_norm (bool) – Whether to follow the official normalization.\",\"periods (List *[*int]) – List of periods to be used in the multi-period discriminator.\",\"period_discriminator_params (Dict *[*str,Any]) – Parameters for the period discriminator.\",\"multi_freq_disc_params (Dict *[*str,Any]) – Parameters for the multi-frequency discriminator.\",\"use_spectral_norm (bool) – Whether to use spectral normalization or not.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1802\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1803\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.VISinger2VocoderGenerator\",\"t\":[\"class espnet2.gan_svs.visinger2.visinger2_vocoder.VISinger2VocoderGenerator(in_channels: int = 80, out_channels: int = 1, channels: int = 512, global_channels: int = -1, kernel_size: int = 7, upsample_scales: List[int] = [8, 8, 2, 2], upsample_kernel_sizes: List[int] = [16, 16, 4, 4], resblock_kernel_sizes: List[int] = [3, 7, 11], resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], n_harmonic: int = 64, use_additional_convs: bool = True, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, use_weight_norm: bool = True)\",\"Bases: Module\",\"Initialize HiFiGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"channels (int) – Number of hidden representation channels.\",\"global_channels (int) – Number of global conditioning channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsample layers.\",\"resblock_kernel_sizes (List *[*int]) – List of kernel sizes for residual blocks.\",\"resblock_dilations (List *[*List *[*int]]) – List of list of dilations for residual blocks.\",\"n_harmonic (int) – Number of harmonics used to synthesize a sound signal.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c, ddsp, g: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Input tensor (B, in_channels, T).\",\"ddsp (Tensor) – Input tensor (B, n_harmonic + 2, T * hop_length).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Output tensor (B, out_channels, T).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows the official implementation manner. https://github.com/jik876/hifi-gan/blob/master/models.py\",\"training : bool\"]},\"1804\":{\"h\":\"espnet2.gan_svs.vits.generator.VISingerGenerator\",\"t\":[\"class espnet2.gan_svs.vits.generator.VISingerGenerator(vocabs: int, aux_channels: int = 513, hidden_channels: int = 192, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, global_channels: int = -1, segment_size: int = 32, text_encoder_attention_heads: int = 2, text_encoder_ffn_expand: int = 4, text_encoder_blocks: int = 6, text_encoder_positionwise_layer_type: str = 'conv1d', text_encoder_positionwise_conv_kernel_size: int = 1, text_encoder_positional_encoding_layer_type: str = 'rel_pos', text_encoder_self_attention_layer_type: str = 'rel_selfattn', text_encoder_activation_type: str = 'swish', text_encoder_normalize_before: bool = True, text_encoder_dropout_rate: float = 0.1, text_encoder_positional_dropout_rate: float = 0.0, text_encoder_attention_dropout_rate: float = 0.0, text_encoder_conformer_kernel_size: int = 7, use_macaron_style_in_text_encoder: bool = True, use_conformer_conv_in_text_encoder: bool = True, decoder_kernel_size: int = 7, decoder_channels: int = 512, decoder_downsample_scales: List[int] = [2, 2, 8, 8], decoder_downsample_kernel_sizes: List[int] = [4, 4, 16, 16], decoder_upsample_scales: List[int] = [8, 8, 2, 2], decoder_upsample_kernel_sizes: List[int] = [16, 16, 4, 4], decoder_resblock_kernel_sizes: List[int] = [3, 7, 11], decoder_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], use_avocodo=False, projection_filters: List[int] = [0, 1, 1, 1], projection_kernels: List[int] = [0, 5, 7, 11], n_harmonic: int = 64, use_weight_norm_in_decoder: bool = True, posterior_encoder_kernel_size: int = 5, posterior_encoder_layers: int = 16, posterior_encoder_stacks: int = 1, posterior_encoder_base_dilation: int = 1, posterior_encoder_dropout_rate: float = 0.0, use_weight_norm_in_posterior_encoder: bool = True, flow_flows: int = 4, flow_kernel_size: int = 5, flow_base_dilation: int = 1, flow_layers: int = 4, flow_dropout_rate: float = 0.0, use_weight_norm_in_flow: bool = True, use_only_mean_in_flow: bool = True, generator_type: str = 'visinger', vocoder_generator_type: str = 'hifigan', fs: int = 22050, hop_length: int = 256, win_length: int | None = 1024, n_fft: int = 1024, use_phoneme_predictor: bool = False, expand_f0_method: str = 'repeat', hubert_channels: int = 0)\",\"Bases: Module\",\"Generator module in VISinger.\",\"Initialize VITS generator module.\",\"Parameters:\",\"vocabs (int) – Input vocabulary size.\",\"aux_channels (int) – Number of acoustic feature channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"global_channels (int) – Number of global conditioning channels.\",\"segment_size (int) – Segment size for decoder.\",\"text_encoder_attention_heads (int) – Number of heads in conformer block of text encoder.\",\"text_encoder_ffn_expand (int) – Expansion ratio of FFN in conformer block of text encoder.\",\"text_encoder_blocks (int) – Number of conformer blocks in text encoder.\",\"text_encoder_positionwise_layer_type (str) – Position-wise layer type in conformer block of text encoder.\",\"text_encoder_positionwise_conv_kernel_size (int) – Position-wise convolution kernel size in conformer block of text encoder. Only used when the above layer type is conv1d or conv1d-linear.\",\"text_encoder_positional_encoding_layer_type (str) – Positional encoding layer type in conformer block of text encoder.\",\"text_encoder_self_attention_layer_type (str) – Self-attention layer type in conformer block of text encoder.\",\"text_encoder_activation_type (str) – Activation function type in conformer block of text encoder.\",\"text_encoder_normalize_before (bool) – Whether to apply layer norm before self-attention in conformer block of text encoder.\",\"text_encoder_dropout_rate (float) – Dropout rate in conformer block of text encoder.\",\"text_encoder_positional_dropout_rate (float) – Dropout rate for positional encoding in conformer block of text encoder.\",\"text_encoder_attention_dropout_rate (float) – Dropout rate for attention in conformer block of text encoder.\",\"text_encoder_conformer_kernel_size (int) – Conformer conv kernel size. It will be used when only use_conformer_conv_in_text_encoder = True.\",\"use_macaron_style_in_text_encoder (bool) – Whether to use macaron style FFN in conformer block of text encoder.\",\"use_conformer_conv_in_text_encoder (bool) – Whether to use covolution in conformer block of text encoder.\",\"decoder_kernel_size (int) – Decoder kernel size.\",\"decoder_channels (int) – Number of decoder initial channels.\",\"decoder_downsample_scales (List *[*int]) – List of downsampling scales in decoder.\",\"decoder_downsample_kernel_sizes (List *[*int]) – List of kernel sizes for downsampling layers in decoder.\",\"decoder_upsample_scales (List *[*int]) – List of upsampling scales in decoder.\",\"decoder_upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsampling layers in decoder.\",\"decoder_resblock_kernel_sizes (List *[*int]) – List of kernel sizes for resblocks in decoder.\",\"decoder_resblock_dilations (List *[*List *[*int]]) – List of list of dilations for resblocks in decoder.\",\"use_avocodo (bool) – Whether to use Avocodo model in the generator.\",\"projection_filters (List *[*int]) – List of projection filter sizes.\",\"projection_kernels (List *[*int]) – List of projection kernel sizes.\",\"n_harmonic (int) – Number of harmonic components.\",\"use_weight_norm_in_decoder (bool) – Whether to apply weight normalization in decoder.\",\"posterior_encoder_kernel_size (int) – Posterior encoder kernel size.\",\"posterior_encoder_layers (int) – Number of layers of posterior encoder.\",\"posterior_encoder_stacks (int) – Number of stacks of posterior encoder.\",\"posterior_encoder_base_dilation (int) – Base dilation of posterior encoder.\",\"posterior_encoder_dropout_rate (float) – Dropout rate for posterior encoder.\",\"use_weight_norm_in_posterior_encoder (bool) – Whether to apply weight normalization in posterior encoder.\",\"flow_flows (int) – Number of flows in flow.\",\"flow_kernel_size (int) – Kernel size in flow.\",\"flow_base_dilation (int) – Base dilation in flow.\",\"flow_layers (int) – Number of layers in flow.\",\"flow_dropout_rate (float) – Dropout rate in flow\",\"use_weight_norm_in_flow (bool) – Whether to apply weight normalization in flow.\",\"use_only_mean_in_flow (bool) – Whether to use only mean in flow.\",\"generator_type (str) – Type of generator to use for the model.\",\"vocoder_generator_type (str) – Type of vocoder generator to use for the model.\",\"fs (int) – Sample rate of the audio.\",\"hop_length (int) – Number of samples between successive frames in STFT.\",\"win_length (int) – Window size of the STFT.\",\"n_fft (int) – Length of the FFT window to be used.\",\"use_phoneme_predictor (bool) – Whether to use phoneme predictor in the model.\",\"expand_f0_method (str) – The method used to expand F0. Use “repeat” or “interpolation”.\",\"hubert_channels (int) – Number of channels in the Hubert model. This is used in VISinger2 Plus.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Tensor | None = None, label_lengths: Tensor | None = None, melody: Tensor | None = None, gt_dur: Tensor | None = None, score_dur: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, ying: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (LongTensor) – Batch of padded label ids (B, Tmax).\",\"label_lengths (LongTensor) – Batch of the lengths of padded label ids (B, ).\",\"melody (LongTensor) – Batch of padded midi (B, Tmax).\",\"gt_dur (LongTensor) – Batch of padded ground truth duration (B, Tmax).\",\"score_dur (LongTensor) – Batch of padded score duration (B, Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"ying (Optional *[*Tensor]) – Batch of padded ying (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"Returns: Waveform tensor (B, 1, segment_size * upsample_factor). Tensor: Duration negative log-likelihood (NLL) tensor (B,). Tensor: Monotonic attention weight tensor (B, 1, T_feats, T_text). Tensor: Segments start index tensor (B,). Tensor: Text mask tensor (B, 1, T_text). Tensor: Feature mask tensor (B, 1, T_feats). tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]: \",\"Tensor: Posterior encoder hidden representation (B, H, T_feats).\",\"Tensor: Flow hidden representation (B, H, T_feats).\",\"Tensor: Expanded text encoder projected mean (B, H, T_feats).\",\"Tensor: Expanded text encoder projected scale (B, H, T_feats).\",\"Tensor: Posterior encoder projected mean (B, H, T_feats).\",\"Tensor: Posterior encoder projected scale (B, H, T_feats).\",\"Return type: Tensor\",\"inference(text: Tensor, text_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, label: Tensor | None = None, label_lengths: Tensor | None = None, melody: Tensor | None = None, score_dur: Tensor | None = None, slur: Tensor | None = None, gt_dur: Tensor | None = None, pitch: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False)\",\"Run inference.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (LongTensor) – Batch of padded label ids (B, Tmax).\",\"label_lengths (LongTensor) – Batch of the lengths of padded label ids (B, ).\",\"melody (LongTensor) – Batch of padded midi (B, Tmax).\",\"gt_dur (LongTensor) – Batch of padded ground truth duration (B, Tmax).\",\"score_dur (LongTensor) – Batch of padded score duration (B, Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"ying (Optional *[*Tensor]) – Batch of padded ying (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"noise_scale (float) – Noise scale parameter for flow.\",\"noise_scale_dur (float) – Noise scale parameter for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated speech.\",\"max_len (Optional *[*int]) – Maximum length of acoustic feature sequence.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Generated waveform tensor (B, T_wav).\",\"Return type: Tensor\",\"training : bool\"]},\"1805\":{\"h\":\"espnet2.gan_svs.vits.vits.VITS\",\"t\":[\"<!-- _espnet2.gan_svs.vits.vits.VITS -->\",\"class espnet2.gan_svs.vits.vits.VITS(idim: int, odim: int, sampling_rate: int = 22050, generator_type: str = 'visinger', vocoder_generator_type: str = 'hifigan', generator_params: Dict[str, Any] = {'decoder_channels': 512, 'decoder_kernel_size': 7, 'decoder_resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'decoder_resblock_kernel_sizes': [3, 7, 11], 'decoder_upsample_kernel_sizes': [16, 16, 4, 4], 'decoder_upsample_scales': [8, 8, 2, 2], 'expand_f0_method': 'repeat', 'flow_base_dilation': 1, 'flow_dropout_rate': 0.0, 'flow_flows': 4, 'flow_kernel_size': 5, 'flow_layers': 4, 'global_channels': -1, 'hidden_channels': 192, 'hubert_channels': 0, 'langs': None, 'posterior_encoder_base_dilation': 1, 'posterior_encoder_dropout_rate': 0.0, 'posterior_encoder_kernel_size': 5, 'posterior_encoder_layers': 16, 'posterior_encoder_stacks': 1, 'projection_filters': [0, 1, 1, 1], 'projection_kernels': [0, 5, 7, 11], 'segment_size': 32, 'spk_embed_dim': None, 'spks': None, 'text_encoder_activation_type': 'swish', 'text_encoder_attention_dropout_rate': 0.0, 'text_encoder_attention_heads': 2, 'text_encoder_blocks': 6, 'text_encoder_conformer_kernel_size': 7, 'text_encoder_dropout_rate': 0.1, 'text_encoder_ffn_expand': 4, 'text_encoder_normalize_before': True, 'text_encoder_positional_dropout_rate': 0.0, 'text_encoder_positional_encoding_layer_type': 'rel_pos', 'text_encoder_positionwise_conv_kernel_size': 1, 'text_encoder_positionwise_layer_type': 'conv1d', 'text_encoder_self_attention_layer_type': 'rel_selfattn', 'use_conformer_conv_in_text_encoder': True, 'use_macaron_style_in_text_encoder': True, 'use_only_mean_in_flow': True, 'use_phoneme_predictor': False, 'use_weight_norm_in_decoder': True, 'use_weight_norm_in_flow': True, 'use_weight_norm_in_posterior_encoder': True}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'avocodo': {'combd': {'combd_d_d': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'combd_d_g': [[1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1]], 'combd_d_k': [[7, 11, 11, 11, 11, 5], [11, 21, 21, 21, 21, 5], [15, 41, 41, 41, 41, 5]], 'combd_d_p': [[3, 5, 5, 5, 5, 2], [5, 10, 10, 10, 10, 2], [7, 20, 20, 20, 20, 2]], 'combd_d_s': [[1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1]], 'combd_h_u': [[16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024]], 'combd_op_f': [1, 1, 1], 'combd_op_g': [1, 1, 1], 'combd_op_k': [3, 3, 3]}, 'pqmf_config': {'lv1': [2, 256, 0.25, 10.0], 'lv2': [4, 192, 0.13, 10.0]}, 'sbd': {'pqmf_config': {'fsbd': [64, 256, 0.1, 9.0], 'sbd': [16, 256, 0.03, 10.0]}, 'sbd_band_ranges': [[0, 6], [0, 11], [0, 16], [0, 64]], 'sbd_dilations': [[[5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11]], [[3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 3, 5], [2, 3, 5]]], 'sbd_filters': [[64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [32, 64, 128, 128, 128]], 'sbd_kernel_sizes': [[[7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]], [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]]], 'sbd_strides': [[1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1]], 'sbd_transpose': [False, False, False, True], 'use_sbd': True}}, 'hifigan_multi_scale_multi_period_discriminator': {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_adv: float = 1.0, lambda_mel: float = 45.0, lambda_feat_match: float = 2.0, lambda_dur: float = 0.1, lambda_kl: float = 1.0, lambda_pitch: float = 10.0, lambda_phoneme: float = 1.0, lambda_c_yin: float = 45.0, cache_generator_outputs: bool = True)\",\"Bases: AbsGANSVS\",\"VITS module (generator + discriminator).\",\"This is a module of VITS described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initialize VITS module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since VITS is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"generator_type (str) – Generator type.\",\"vocoder_generator_type (str) – Type of vocoder generator to use in the model.\",\"generator_params (Dict *[*str,Any]) – Parameter dict for generator.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel spectrogram loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_dur (float) – Loss scaling coefficient for duration loss.\",\"lambda_kl (float) – Loss scaling coefficient for KL divergence loss.\",\"lambda_pitch (float) – Loss scaling coefficient for pitch loss.\",\"lambda_phoneme (float) – Loss scaling coefficient for phoneme loss.\",\"lambda_c_yin (float) – Loss scaling coefficient for yin loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, ssl_feats: Tensor | None = None, ssl_feats_lengths: Tensor | None = None, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: LongTensor | None = None, ying: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True)\",\"Perform generator forward.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"ssl_feats (Tensor) – SSL feature tensor (B, T_feats, hubert_channels).\",\"ssl_feats_lengths (Tensor) – SSL feature length tensor (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, T_text).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, T_text).\",\"pitch (FloatTensor) – Batch of padded f0 (B, T_feats).\",\"ying (Optional *[*Tensor]) – Batch of padded ying (B, T_feats).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, T_text).\",\"slur (FloatTensor) – Batch of padded slur (B, T_text).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, feats: Tensor | None = None, ssl_feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False)\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (T_text,).\",\"feats (Tensor) – Feature tensor (T_feats, aux_channels).\",\"ssl_feats (Tensor) – SSL Feature tensor (T_feats, hubert_channels).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, T_text).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, T_text).\",\"pitch (FloatTensor) – Batch of padded f0 (B, T_feats).\",\"slur (LongTensor) – Batch of padded slur (B, T_text).\",\"sids (Tensor) – Speaker index tensor (1,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (spk_embed_dim,).\",\"lids (Tensor) – Language index tensor (1,).\",\"noise_scale (float) – Noise scale value for flow.\",\"noise_scale_dur (float) – Noise scale value for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated singing.\",\"max_len (Optional *[*int]) – Maximum length.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, T_text).\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_singing\",\"Return whether or not singing is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"1806\":{\"h\":\"espnet2.gan_svs.pits.modules.WN\",\"t\":[\"<!-- _espnet2.gan_svs.pits.modules.WN -->\",\"class espnet2.gan_svs.pits.modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, x_mask, g=None, **kwargs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1807\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels)\",\"remove_weight_norm()\",\"training : bool\"]},\"1808\":{\"h\":\"espnet2.gan_svs.pits.ying_decoder.YingDecoder\",\"t\":[\"class espnet2.gan_svs.pits.ying_decoder.YingDecoder(hidden_channels, kernel_size, dilation_rate, n_layers, yin_start, yin_scope, yin_shift_range, gin_channels=0)\",\"Bases: Module\",\"Ying decoder module.\",\"Initialize the YingDecoder module.\",\"Parameters:\",\"hidden_channels (int) – Number of hidden channels.\",\"kernel_size (int) – Size of the convolutional kernel.\",\"dilation_rate (int) – Dilation rate of the convolutional layers.\",\"n_layers (int) – Number of convolutional layers.\",\"yin_start (int) – Start point of the yin target signal.\",\"yin_scope (int) – Scope of the yin target signal.\",\"yin_shift_range (int) – Maximum number of frames to shift the yin target signal.\",\"gin_channels (int,optional) – Number of global conditioning channels. Defaults to 0.\",\"crop_scope(x, yin_start, scope_shift)\",\"Crop the input tensor.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor of shape [B, C, T].\",\"yin_start (int) – Starting point of the yin target signal.\",\"scope_shift (torch.Tensor) – Shift tensor of shape [B].\",\"Returns: Cropped tensor of shape [B, C, yin_scope].\",\"Return type: torch.Tensor\",\"forward(z_yin, yin_gt, z_mask, g=None)\",\"Forward pass of the decoder.\",\"Parameters:\",\"z_yin (torch.Tensor) – The input yin note sequence of shape (B, C, T_yin).\",\"yin_gt (torch.Tensor) – The ground truth yin note sequence of shape (B, C, T_yin).\",\"z_mask (torch.Tensor) – The mask tensor of shape (B, 1, T_yin).\",\"g (torch.Tensor) – The global conditioning tensor.\",\"Returns: The predicted yin note sequence of shape (B, C, T_yin). torch.Tensor: The shifted ground truth yin note sequence of shape\",\"(B, C, T_yin).\",\"torch.Tensor: The cropped ground truth yin note sequence of shape : (B, C, T_yin).\",\"torch.Tensor: The cropped input yin note sequence of shape (B, C, T_yin). torch.Tensor: The scope shift tensor of shape (B,).\",\"Return type: torch.Tensor\",\"infer(z_yin, z_mask, g=None)\",\"Generate yin prediction.\",\"Parameters:\",\"z_yin (torch.Tensor) – Input yin target tensor of shape [B, yin_scope, C].\",\"z_mask (torch.Tensor) – Input mask tensor of shape [B, yin_scope, 1].\",\"g (torch.Tensor,optional) – Global conditioning tensor of shape [B, gin_channels, 1]. Defaults to None.\",\"Returns: Predicted yin tensor of shape [B, yin_scope, C].\",\"Return type: torch.Tensor\",\"training : bool\"]},\"1809\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.amp_to_impulse_response\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.amp_to_impulse_response(amp, target_size)\"]},\"1810\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.create_fb_matrix\",\"t\":[\"espnet2.gan_svs.visinger2.visinger2_vocoder.create_fb_matrix(n_freqs: int, f_min: float, f_max: float, n_mels: int, sample_rate: int, norm: str | None = None)\",\"Create a frequency bin conversion matrix.\",\"Parameters:\",\"n_freqs (int) – Number of frequencies to highlight/apply\",\"f_min (float) – Minimum frequency (Hz)\",\"f_max (float) – Maximum frequency (Hz)\",\"n_mels (int) – Number of mel filterbanks\",\"sample_rate (int) – Sample rate of the audio waveform\",\"norm (Optional *[*str]) – If ‘slaney’,\",\"band (divide the triangular mel weights by the widthofthe mel) –\",\"**(**Default ( *(*area normalization).) – None)\",\"Returns: Triangular filter banks (fb matrix) of size (n_freqs, n_mels) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (…, n_freqs), the applied result would be A * create_fb_matrix(A.size(-1), …).\",\"Return type: Tensor\"]},\"1811\":{\"h\":\"espnet2.gan_svs.utils.expand_f0.expand_f0\",\"t\":[\"<!-- _espnet2.gan_svs.utils.expand_f0.expand_f0 -->\",\"espnet2.gan_svs.utils.expand_f0.expand_f0(f0_frame, hop_length, method='interpolation')\",\"Expand f0 to output wave length.\",\"Parameters:\",\"f0_frame (Tensor) – Input tensor (B, 1, frame_len).\",\"hop_length (Tensor) – Hop length.\",\"method (str) – Method to expand f0. Choose either ‘interpolation’ or ‘repeat’.\",\"Returns: Output tensor (B, 1, wav_len).\",\"Return type: Tensor\"]},\"1812\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.extract_loudness\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.extract_loudness(signal, sampling_rate, block_size, n_fft=2048)\"]},\"1813\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.extract_pitch\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.extract_pitch(signal, sampling_rate, block_size)\"]},\"1814\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.fft_convolve\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.fft_convolve(signal, kernel)\"]},\"1815\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.get_padding\",\"t\":[\"espnet2.gan_svs.avocodo.avocodo.get_padding(kernel_size, dilation=1)\"]},\"1816\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.gru\",\"t\":[\"<!-- _espnet2.gan_svs.visinger2.ddsp.gru -->\",\"espnet2.gan_svs.visinger2.ddsp.gru(n_input, hidden_size)\"]},\"1817\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.harmonic_synth\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.harmonic_synth(pitch, amplitudes, sampling_rate)\"]},\"1818\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.init_kernels\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.init_kernels(win_len, win_inc, fft_len, win_type=None, invers=False)\"]},\"1819\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.mean_std_loudness\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.mean_std_loudness(dataset)\"]},\"1820\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.mlp\",\"t\":[\"<!-- _espnet2.gan_svs.visinger2.ddsp.mlp -->\",\"espnet2.gan_svs.visinger2.ddsp.mlp(in_size, hidden_size, n_layers)\"]},\"1821\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.multiscale_fft\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.multiscale_fft(signal, scales, overlap)\"]},\"1822\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.remove_above_nyquist\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.remove_above_nyquist(amplitudes, pitch, sampling_rate)\"]},\"1823\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.resample\",\"t\":[\"<!-- _espnet2.gan_svs.visinger2.ddsp.resample -->\",\"espnet2.gan_svs.visinger2.ddsp.resample(x, factor: int)\"]},\"1824\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.safe_log\",\"t\":[\"<!-- _espnet2.gan_svs.visinger2.ddsp.safe_log -->\",\"espnet2.gan_svs.visinger2.ddsp.safe_log(x)\"]},\"1825\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.scale_function\",\"t\":[\"espnet2.gan_svs.visinger2.ddsp.scale_function(x)\"]},\"1826\":{\"h\":\"espnet2.gan_svs.vits.modules.sequence_mask\",\"t\":[\"<!-- _espnet2.gan_svs.vits.modules.sequence_mask -->\",\"espnet2.gan_svs.vits.modules.sequence_mask(length, max_length=None)\"]},\"1827\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.upsample\",\"t\":[\"<!-- _espnet2.gan_svs.visinger2.ddsp.upsample -->\",\"espnet2.gan_svs.visinger2.ddsp.upsample(signal, factor)\"]},\"1828\":{\"h\":\"espnet2.gan_tts.abs_gan_tts.AbsGANTTS\",\"t\":[\"<!-- _espnet2.gan_tts.abs_gan_tts.AbsGANTTS -->\",\"class espnet2.gan_tts.abs_gan_tts.AbsGANTTS\",\"Bases: AbsTTS, ABC\",\"GAN-based TTS model abstract class.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(forward_generator, *args, **kwargs)\",\"Return generator or discriminator loss.\",\"training : bool\"]},\"1829\":{\"h\":\"espnet2.gan_tts.jets.alignments.AlignmentModule\",\"t\":[\"class espnet2.gan_tts.jets.alignments.AlignmentModule(adim, odim, cache_prior=True)\",\"Bases: Module\",\"Alignment Learning Framework proposed for parallel TTS models in:\",\"https://arxiv.org/abs/2108.10447\",\"Initialize AlignmentModule.\",\"Parameters:\",\"adim (int) – Dimension of attention.\",\"odim (int) – Dimension of feats.\",\"cache_prior (bool) – Whether to cache beta-binomial prior.\",\"forward(text, feats, text_lengths, feats_lengths, x_masks=None)\",\"Calculate alignment loss.\",\"Parameters:\",\"text (Tensor) – Batched text embedding (B, T_text, adim).\",\"feats (Tensor) – Batched acoustic feature (B, T_feats, odim).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"x_masks (Tensor) – Mask tensor (B, T_text).\",\"Returns: Log probability of attention matrix (B, T_feats, T_text).\",\"Return type: Tensor\",\"training : bool\"]},\"1830\":{\"h\":\"espnet2.gan_tts.wavenet.residual_block.Conv1d\",\"t\":[\"class espnet2.gan_tts.wavenet.residual_block.Conv1d(*args, **kwargs)\",\"Bases: Conv1d\",\"Conv1d module with customized initialization.\",\"Initialize Conv1d module.\",\"bias : Tensor | None\",\"dilation : Tuple[int, ...]\",\"groups : int\",\"in_channels : int\",\"kernel_size : Tuple[int, ...]\",\"out_channels : int\",\"output_padding : Tuple[int, ...]\",\"padding : str | Tuple[int, ...]\",\"padding_mode : str\",\"reset_parameters()\",\"Reset parameters.\",\"stride : Tuple[int, ...]\",\"transposed : bool\",\"weight : Tensor\"]},\"1831\":{\"h\":\"espnet2.gan_tts.wavenet.residual_block.Conv1d1x1\",\"t\":[\"class espnet2.gan_tts.wavenet.residual_block.Conv1d1x1(in_channels: int, out_channels: int, bias: bool)\",\"Bases: Conv1d\",\"1x1 Conv1d with customized initialization.\",\"Initialize 1x1 Conv1d module.\",\"bias : Tensor | None\",\"dilation : Tuple[int, ...]\",\"groups : int\",\"in_channels : int\",\"kernel_size : Tuple[int, ...]\",\"out_channels : int\",\"output_padding : Tuple[int, ...]\",\"padding : str | Tuple[int, ...]\",\"padding_mode : str\",\"stride : Tuple[int, ...]\",\"training : bool\",\"transposed : bool\",\"weight : Tensor\"]},\"1832\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.upsample.Conv2d\",\"t\":[\"class espnet2.gan_tts.parallel_wavegan.upsample.Conv2d(*args, **kwargs)\",\"Bases: Conv2d\",\"Conv2d module with customized initialization.\",\"Initialize Conv2d module.\",\"bias : Tensor | None\",\"dilation : Tuple[int, ...]\",\"groups : int\",\"in_channels : int\",\"kernel_size : Tuple[int, ...]\",\"out_channels : int\",\"output_padding : Tuple[int, ...]\",\"padding : str | Tuple[int, ...]\",\"padding_mode : str\",\"reset_parameters()\",\"Reset parameters.\",\"stride : Tuple[int, ...]\",\"transposed : bool\",\"weight : Tensor\"]},\"1833\":{\"h\":\"espnet2.gan_tts.vits.flow.ConvFlow\",\"t\":[\"<!-- _espnet2.gan_tts.vits.flow.ConvFlow -->\",\"class espnet2.gan_tts.vits.flow.ConvFlow(in_channels: int, hidden_channels: int, kernel_size: int, layers: int, bins: int = 10, tail_bound: float = 5.0)\",\"Bases: Module\",\"Convolutional flow module.\",\"Initialize ConvFlow module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size.\",\"layers (int) – Number of layers.\",\"bins (int) – Number of bins.\",\"tail_bound (float) – Tail bound value.\",\"forward(x: Tensor, x_mask: Tensor, g: Tensor | None = None, inverse: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T).\",\"x_mask (Tensor) – Mask tensor (B,).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, channels, 1).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Output tensor (B, channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\",\"training : bool\"]},\"1834\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.upsample.ConvInUpsampleNetwork\",\"t\":[\"class espnet2.gan_tts.parallel_wavegan.upsample.ConvInUpsampleNetwork(upsample_scales: List[int], nonlinear_activation: str | None = None, nonlinear_activation_params: Dict[str, Any] = {}, interpolate_mode: str = 'nearest', freq_axis_kernel_size: int = 1, aux_channels: int = 80, aux_context_window: int = 0)\",\"Bases: Module\",\"Convolution + upsampling network module.\",\"Initialize ConvInUpsampleNetwork module.\",\"Parameters:\",\"upsample_scales (list) – List of upsampling scales.\",\"nonlinear_activation (Optional *[*str]) – Activation function name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Arguments for the specified activation function.\",\"mode (str) – Interpolation mode.\",\"freq_axis_kernel_size (int) – Kernel size in the direction of frequency axis.\",\"aux_channels (int) – Number of channels of pre-conv layer.\",\"aux_context_window (int) – Context window size of the pre-conv layer.\",\"forward(c: Tensor)\",\"Calculate forward propagation.\",\"Parameters:c (Tensor) – Input tensor (B, C, T_feats).\",\"Returns: Upsampled tensor (B, C, T_wav), : where T_wav = T_feats * prod(upsample_scales).\",\"Return type: Tensor\",\"training : bool\"]},\"1835\":{\"h\":\"espnet2.gan_tts.vits.flow.DilatedDepthSeparableConv\",\"t\":[\"class espnet2.gan_tts.vits.flow.DilatedDepthSeparableConv(channels: int, kernel_size: int, layers: int, dropout_rate: float = 0.0, eps: float = 1e-05)\",\"Bases: Module\",\"Dilated depth-separable conv module.\",\"Initialize DilatedDepthSeparableConv module.\",\"Parameters:\",\"channels (int) – Number of channels.\",\"kernel_size (int) – Kernel size.\",\"layers (int) – Number of layers.\",\"dropout_rate (float) – Dropout rate.\",\"eps (float) – Epsilon for layer norm.\",\"forward(x: Tensor, x_mask: Tensor, g: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"x_mask (Tensor) – Mask tensor (B, 1, T).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Output tensor (B, channels, T).\",\"Return type: Tensor\",\"training : bool\"]},\"1836\":{\"h\":\"espnet2.gan_tts.hifigan.loss.DiscriminatorAdversarialLoss\",\"t\":[\"class espnet2.gan_tts.hifigan.loss.DiscriminatorAdversarialLoss(average_by_discriminators: bool = True, loss_type: str = 'mse')\",\"Bases: Module\",\"Discriminator adversarial loss module.\",\"Initialize DiscriminatorAversarialLoss module.\",\"Parameters:\",\"average_by_discriminators (bool) – Whether to average the loss by the number of discriminators.\",\"loss_type (str) – Loss type, “mse” or “hinge”.\",\"forward(outputs_hat: List[List[Tensor]] | List[Tensor] | Tensor, outputs: List[List[Tensor]] | List[Tensor] | Tensor)\",\"Calcualate discriminator adversarial loss.\",\"Parameters:\",\"outputs_hat (Union *[*List *[*List *[*Tensor]],List *[*Tensor],Tensor]) – Discriminator outputs, list of discriminator outputs, or list of list of discriminator outputs calculated from generator.\",\"outputs (Union *[*List *[*List *[*Tensor]],List *[*Tensor],Tensor]) – Discriminator outputs, list of discriminator outputs, or list of list of discriminator outputs calculated from groundtruth.\",\"Returns: Discriminator real loss value. Tensor: Discriminator fake loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"1837\":{\"h\":\"espnet2.gan_tts.espnet_model.ESPnetGANTTSModel\",\"t\":[\"class espnet2.gan_tts.espnet_model.ESPnetGANTTSModel(feats_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_extract: AbsFeatsExtract | None, pitch_normalize: InversibleInterface | None, energy_extract: AbsFeatsExtract | None, energy_normalize: InversibleInterface | None, tts: AbsGANTTS)\",\"Bases: AbsGANESPnetModel\",\"ESPnet model for GAN-based text-to-speech task.\",\"Initialize ESPnetGANTTSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs)\",\"Calculate features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B, 1).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"durations_lengths (Optional *[*Tensor) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True, **kwargs)\",\"Return generator or discriminator loss with dict format.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"duration (Optional *[*Tensor]) – Duration tensor.\",\"duration_lengths (Optional *[*Tensor]) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"kwargs – “utt_id” is among the input.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"training : bool\"]},\"1838\":{\"h\":\"espnet2.gan_tts.vits.flow.ElementwiseAffineFlow\",\"t\":[\"class espnet2.gan_tts.vits.flow.ElementwiseAffineFlow(channels: int)\",\"Bases: Module\",\"Elementwise affine flow module.\",\"Initialize ElementwiseAffineFlow module.\",\"Parameters:channels (int) – Number of channels.\",\"forward(x: Tensor, x_mask: Tensor, inverse: bool = False, **kwargs)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Output tensor (B, channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\",\"training : bool\"]},\"1839\":{\"h\":\"espnet2.gan_tts.hifigan.loss.FeatureMatchLoss\",\"t\":[\"class espnet2.gan_tts.hifigan.loss.FeatureMatchLoss(average_by_layers: bool = True, average_by_discriminators: bool = True, include_final_outputs: bool = False)\",\"Bases: Module\",\"Feature matching loss module.\",\"Initialize FeatureMatchLoss module.\",\"Parameters:\",\"average_by_layers (bool) – Whether to average the loss by the number of layers.\",\"average_by_discriminators (bool) – Whether to average the loss by the number of discriminators.\",\"include_final_outputs (bool) – Whether to include the final output of each discriminator for loss calculation.\",\"forward(feats_hat: List[List[Tensor]] | List[Tensor], feats: List[List[Tensor]] | List[Tensor])\",\"Calculate feature matching loss.\",\"Parameters:\",\"feats_hat (Union *[*List *[*List *[*Tensor]],List *[*Tensor]]) – List of list of discriminator outputs or list of discriminator outputs calcuated from generator’s outputs.\",\"feats (Union *[*List *[*List *[*Tensor]],List *[*Tensor]]) – List of list of discriminator outputs or list of discriminator outputs calcuated from groundtruth..\",\"Returns: Feature matching loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"1840\":{\"h\":\"espnet2.gan_tts.vits.flow.FlipFlow\",\"t\":[\"<!-- _espnet2.gan_tts.vits.flow.FlipFlow -->\",\"class espnet2.gan_tts.vits.flow.FlipFlow\",\"Bases: Module\",\"Flip flow module.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, *args, inverse: bool = False, **kwargs)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Flipped tensor (B, channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\",\"training : bool\"]},\"1841\":{\"h\":\"espnet2.gan_tts.jets.loss.ForwardSumLoss\",\"t\":[\"<!-- _espnet2.gan_tts.jets.loss.ForwardSumLoss -->\",\"class espnet2.gan_tts.jets.loss.ForwardSumLoss\",\"Bases: Module\",\"Forwardsum loss described at https://openreview.net/forum?id=0NQwnnwAORi\",\"Initialize forwardsum loss module.\",\"forward(log_p_attn: Tensor, ilens: Tensor, olens: Tensor, blank_prob: float = 0.36787944117144233)\",\"Calculate forward propagation.\",\"Parameters:\",\"log_p_attn (Tensor) – Batch of log probability of attention matrix (B, T_feats, T_text).\",\"ilens (Tensor) – Batch of the lengths of each input (B,).\",\"olens (Tensor) – Batch of the lengths of each target (B,).\",\"blank_prob (float) – Blank symbol probability.\",\"Returns: forwardsum loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"1842\":{\"h\":\"espnet2.gan_tts.jets.length_regulator.GaussianUpsampling\",\"t\":[\"class espnet2.gan_tts.jets.length_regulator.GaussianUpsampling(delta=0.1)\",\"Bases: Module\",\"Gaussian upsampling with fixed temperature as in:\",\"https://arxiv.org/abs/2010.04301\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(hs, ds, h_masks=None, d_masks=None)\",\"Upsample hidden states according to durations.\",\"Parameters:\",\"hs (Tensor) – Batched hidden state to be expanded (B, T_text, adim).\",\"ds (Tensor) – Batched token duration (B, T_text).\",\"h_masks (Tensor) – Mask tensor (B, T_feats).\",\"d_masks (Tensor) – Mask tensor (B, T_text).\",\"Returns: Expanded hidden state (B, T_feat, adim).\",\"Return type: Tensor\",\"training : bool\"]},\"1843\":{\"h\":\"espnet2.gan_tts.hifigan.loss.GeneratorAdversarialLoss\",\"t\":[\"class espnet2.gan_tts.hifigan.loss.GeneratorAdversarialLoss(average_by_discriminators: bool = True, loss_type: str = 'mse')\",\"Bases: Module\",\"Generator adversarial loss module.\",\"Initialize GeneratorAversarialLoss module.\",\"Parameters:\",\"average_by_discriminators (bool) – Whether to average the loss by the number of discriminators.\",\"loss_type (str) – Loss type, “mse” or “hinge”.\",\"forward(outputs: List[List[Tensor]] | List[Tensor] | Tensor)\",\"Calcualate generator adversarial loss.\",\"Parameters:outputs (Union *[*List *[*List *[*Tensor]],List *[*Tensor],Tensor]) – Discriminator outputs, list of discriminator outputs, or list of list of discriminator outputs..\",\"Returns: Generator adversarial loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"1844\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANGenerator\",\"t\":[\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANGenerator(in_channels: int = 80, out_channels: int = 1, channels: int = 512, global_channels: int = -1, kernel_size: int = 7, upsample_scales: List[int] = [8, 8, 2, 2], upsample_kernel_sizes: List[int] = [16, 16, 4, 4], resblock_kernel_sizes: List[int] = [3, 7, 11], resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], use_additional_convs: bool = True, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, use_weight_norm: bool = True)\",\"Bases: Module\",\"HiFiGAN generator module.\",\"Initialize HiFiGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"channels (int) – Number of hidden representation channels.\",\"global_channels (int) – Number of global conditioning channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsample layers.\",\"resblock_kernel_sizes (List *[*int]) – List of kernel sizes for residual blocks.\",\"resblock_dilations (List *[*List *[*int]]) – List of list of dilations for residual blocks.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor, g: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Input tensor (B, in_channels, T).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Output tensor (B, out_channels, T).\",\"Return type: Tensor\",\"inference(c: Tensor, g: Tensor | None = None)\",\"Perform inference.\",\"Parameters:\",\"c (torch.Tensor) – Input tensor (T, in_channels).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (global_channels, 1).\",\"Returns: Output tensor (T ** upsample_factor, out_channels).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows the official implementation manner. https://github.com/jik876/hifi-gan/blob/master/models.py\",\"training : bool\"]},\"1845\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiPeriodDiscriminator\",\"t\":[\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiPeriodDiscriminator(periods: List[int] = [2, 3, 5, 7, 11], discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True})\",\"Bases: Module\",\"HiFiGAN multi-period discriminator module.\",\"Initialize HiFiGANMultiPeriodDiscriminator module.\",\"Parameters:\",\"periods (List *[*int]) – List of periods.\",\"discriminator_params (Dict *[*str,Any]) – Parameters for hifi-gan period discriminator module. The period parameter will be overwritten.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, which consists of each : layer output tensors.\",\"Return type: List\",\"training : bool\"]},\"1846\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiScaleDiscriminator\",\"t\":[\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiScaleDiscriminator(scales: int = 3, downsample_pooling: str = 'AvgPool1d', downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, follow_official_norm: bool = False)\",\"Bases: Module\",\"HiFi-GAN multi-scale discriminator module.\",\"Initilize HiFiGAN multi-scale discriminator module.\",\"Parameters:\",\"scales (int) – Number of multi-scales.\",\"downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"downsample_pooling_params (Dict *[*str,Any]) – Parameters for the above pooling module.\",\"discriminator_params (Dict *[*str,Any]) – Parameters for hifi-gan scale discriminator module.\",\"follow_official_norm (bool) – Whether to follow the norm setting of the official implementaion. The first discriminator uses spectral norm and the other discriminators use weight norm.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of eachlayer output tensors.\",\"Return type: List[List[torch.Tensor]]\",\"training : bool\"]},\"1847\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiScaleMultiPeriodDiscriminator\",\"t\":[\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiScaleMultiPeriodDiscriminator(scales: int = 3, scale_downsample_pooling: str = 'AvgPool1d', scale_downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, scale_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, follow_official_norm: bool = True, periods: List[int] = [2, 3, 5, 7, 11], period_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True})\",\"Bases: Module\",\"HiFi-GAN multi-scale + multi-period discriminator module.\",\"Initilize HiFiGAN multi-scale + multi-period discriminator module.\",\"Parameters:\",\"scales (int) – Number of multi-scales.\",\"scale_downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"scale_downsample_pooling_params (dict) – Parameters for the above pooling module.\",\"scale_discriminator_params (dict) – Parameters for hifi-gan scale discriminator module.\",\"follow_official_norm (bool) – Whether to follow the norm setting of the official implementaion. The first discriminator uses spectral norm and the other discriminators use weight norm.\",\"periods (list) – List of periods.\",\"period_discriminator_params (dict) – Parameters for hifi-gan period discriminator module. The period parameter will be overwritten.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of each layer output tensors. Multi scale and multi period ones are concatenated.\",\"Return type: List[List[Tensor]]\",\"training : bool\"]},\"1848\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANPeriodDiscriminator\",\"t\":[\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANPeriodDiscriminator(in_channels: int = 1, out_channels: int = 1, period: int = 3, kernel_sizes: List[int] = [5, 3], channels: int = 32, downsample_scales: List[int] = [3, 3, 3, 3, 1], max_downsample_channels: int = 1024, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, use_weight_norm: bool = True, use_spectral_norm: bool = False)\",\"Bases: Module\",\"HiFiGAN period discriminator module.\",\"Initialize HiFiGANPeriodDiscriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"period (int) – Period.\",\"kernel_sizes (list) – Kernel sizes of initial conv layers and the final conv layer.\",\"channels (int) – Number of initial channels.\",\"downsample_scales (List *[*int]) – List of downsampling scales.\",\"max_downsample_channels (int) – Number of maximum downsampling channels.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"use_spectral_norm (bool) – Whether to use spectral norm. If set to true, it will be applied to all of the conv layers.\",\"apply_spectral_norm()\",\"Apply spectral normalization module from all of the layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:c (Tensor) – Input tensor (B, in_channels, T).\",\"Returns: List of each layer’s tensors.\",\"Return type: list\",\"training : bool\"]},\"1849\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANScaleDiscriminator\",\"t\":[\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANScaleDiscriminator(in_channels: int = 1, out_channels: int = 1, kernel_sizes: List[int] = [15, 41, 5, 3], channels: int = 128, max_downsample_channels: int = 1024, max_groups: int = 16, bias: int = True, downsample_scales: List[int] = [2, 2, 4, 4, 1], nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, use_weight_norm: bool = True, use_spectral_norm: bool = False)\",\"Bases: Module\",\"HiFi-GAN scale discriminator module.\",\"Initilize HiFiGAN scale discriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_sizes (List *[*int]) – List of four kernel sizes. The first will be used for the first conv layer, and the second is for downsampling part, and the remaining two are for the last two output layers.\",\"channels (int) – Initial number of channels for conv layer.\",\"max_downsample_channels (int) – Maximum number of channels for downsampling layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"downsample_scales (List *[*int]) – List of downsampling scales.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"use_spectral_norm (bool) – Whether to use spectral norm. If set to true, it will be applied to all of the conv layers.\",\"apply_spectral_norm()\",\"Apply spectral normalization module from all of the layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of output tensors of each layer.\",\"Return type: List[Tensor]\",\"remove_spectral_norm()\",\"Remove spectral normalization module from all of the layers.\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"training : bool\"]},\"1850\":{\"h\":\"espnet2.gan_tts.jets.jets.JETS\",\"t\":[\"<!-- _espnet2.gan_tts.jets.jets.JETS -->\",\"class espnet2.gan_tts.jets.jets.JETS(idim: int, odim: int, sampling_rate: int = 22050, generator_type: str = 'jets_generator', generator_params: Dict[str, Any] = {'adim': 256, 'aheads': 2, 'conformer_activation_type': 'swish', 'conformer_dec_kernel_size': 31, 'conformer_enc_kernel_size': 7, 'conformer_pos_enc_layer_type': 'rel_pos', 'conformer_rel_pos_type': 'latest', 'conformer_self_attn_layer_type': 'rel_selfattn', 'decoder_concat_after': False, 'decoder_normalize_before': True, 'decoder_type': 'transformer', 'dlayers': 4, 'dunits': 1024, 'duration_predictor_chans': 384, 'duration_predictor_dropout_rate': 0.1, 'duration_predictor_kernel_size': 3, 'duration_predictor_layers': 2, 'elayers': 4, 'encoder_concat_after': False, 'encoder_normalize_before': True, 'encoder_type': 'transformer', 'energy_embed_dropout': 0.5, 'energy_embed_kernel_size': 1, 'energy_predictor_chans': 384, 'energy_predictor_dropout': 0.5, 'energy_predictor_kernel_size': 3, 'energy_predictor_layers': 2, 'eunits': 1024, 'generator_bias': True, 'generator_channels': 512, 'generator_global_channels': -1, 'generator_kernel_size': 7, 'generator_nonlinear_activation': 'LeakyReLU', 'generator_nonlinear_activation_params': {'negative_slope': 0.1}, 'generator_out_channels': 1, 'generator_resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'generator_resblock_kernel_sizes': [3, 7, 11], 'generator_upsample_kernel_sizes': [16, 16, 4, 4], 'generator_upsample_scales': [8, 8, 2, 2], 'generator_use_additional_convs': True, 'generator_use_weight_norm': True, 'gst_conv_chans_list': [32, 32, 64, 64, 128, 128], 'gst_conv_kernel_size': 3, 'gst_conv_layers': 6, 'gst_conv_stride': 2, 'gst_gru_layers': 1, 'gst_gru_units': 128, 'gst_heads': 4, 'gst_tokens': 10, 'init_dec_alpha': 1.0, 'init_enc_alpha': 1.0, 'init_type': 'xavier_uniform', 'langs': -1, 'pitch_embed_dropout': 0.5, 'pitch_embed_kernel_size': 1, 'pitch_predictor_chans': 384, 'pitch_predictor_dropout': 0.5, 'pitch_predictor_kernel_size': 5, 'pitch_predictor_layers': 5, 'positionwise_conv_kernel_size': 1, 'positionwise_layer_type': 'conv1d', 'reduction_factor': 1, 'segment_size': 64, 'spk_embed_dim': None, 'spk_embed_integration_type': 'add', 'spks': -1, 'stop_gradient_from_energy_predictor': False, 'stop_gradient_from_pitch_predictor': True, 'transformer_dec_attn_dropout_rate': 0.1, 'transformer_dec_dropout_rate': 0.1, 'transformer_dec_positional_dropout_rate': 0.1, 'transformer_enc_attn_dropout_rate': 0.1, 'transformer_enc_dropout_rate': 0.1, 'transformer_enc_positional_dropout_rate': 0.1, 'use_batch_norm': True, 'use_cnn_in_conformer': True, 'use_gst': False, 'use_macaron_style_in_conformer': True, 'use_masking': False, 'use_scaled_pos_enc': True, 'use_weighted_masking': False, 'zero_triu': False}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_adv: float = 1.0, lambda_mel: float = 45.0, lambda_feat_match: float = 2.0, lambda_var: float = 1.0, lambda_align: float = 2.0, cache_generator_outputs: bool = True, plot_pred_mos: bool = False, mos_pred_tool: str = 'utmos')\",\"Bases: AbsGANTTS\",\"JETS module (generator + discriminator).\",\"This is a module of JETS described in\",\"`\",\"JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech’_.\",\"<!-- : https://arxiv.org/abs/2203.16852 -->\",\"Initialize JETS module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since JETS is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"generator_type (str) – Generator type.\",\"generator_params (Dict *[*str,Any]) – Parameter dict for generator.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel spectrogram loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_var (float) – Loss scaling coefficient for variance loss.\",\"lambda_align (float) – Loss scaling coefficient for alignment loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"plot_pred_mos (bool) – Whether to plot predicted MOS during the training.\",\"mos_pred_tool (str) – MOS prediction tool name.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True, **kwargs)\",\"Perform generator forward.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, feats: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, use_teacher_forcing: bool = False, **kwargs)\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (T_text,).\",\"feats (Tensor) – Feature tensor (T_feats, aux_channels).\",\"pitch (Tensor) – Pitch tensor (T_feats, 1).\",\"energy (Tensor) – Energy tensor (T_feats, 1).\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"duration (Tensor): Predicted duration tensor (T_text,).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_speech\",\"Return whether or not speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"1851\":{\"h\":\"espnet2.gan_tts.jets.generator.JETSGenerator\",\"t\":[\"class espnet2.gan_tts.jets.generator.JETSGenerator(idim: int, odim: int, adim: int = 256, aheads: int = 2, elayers: int = 4, eunits: int = 1024, dlayers: int = 4, dunits: int = 1024, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, segment_size: int = 64, generator_out_channels: int = 1, generator_channels: int = 512, generator_global_channels: int = -1, generator_kernel_size: int = 7, generator_upsample_scales: List[int] = [8, 8, 2, 2], generator_upsample_kernel_sizes: List[int] = [16, 16, 4, 4], generator_resblock_kernel_sizes: List[int] = [3, 7, 11], generator_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], generator_use_additional_convs: bool = True, generator_bias: bool = True, generator_nonlinear_activation: str = 'LeakyReLU', generator_nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, generator_use_weight_norm: bool = True)\",\"Bases: Module\",\"Generator module in JETS.\",\"Initialize JETS generator module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"pitch_predictor_layers (int) – Number of pitch predictor layers.\",\"pitch_predictor_chans (int) – Number of pitch predictor channels.\",\"pitch_predictor_kernel_size (int) – Kernel size of pitch predictor.\",\"pitch_predictor_dropout_rate (float) – Dropout rate in pitch predictor.\",\"pitch_embed_kernel_size (float) – Kernel size of pitch embedding.\",\"pitch_embed_dropout_rate (float) – Dropout rate for pitch embedding.\",\"stop_gradient_from_pitch_predictor – Whether to stop gradient from pitch predictor to encoder.\",\"energy_predictor_layers (int) – Number of energy predictor layers.\",\"energy_predictor_chans (int) – Number of energy predictor channels.\",\"energy_predictor_kernel_size (int) – Kernel size of energy predictor.\",\"energy_predictor_dropout_rate (float) – Dropout rate in energy predictor.\",\"energy_embed_kernel_size (float) – Kernel size of energy embedding.\",\"energy_embed_dropout_rate (float) – Dropout rate for energy embedding.\",\"stop_gradient_from_energy_predictor – Whether to stop gradient from energy predictor to encoder.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – The number of GST embeddings.\",\"gst_heads (int) – The number of heads in GST multihead attention.\",\"gst_conv_layers (int) – The number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – The number of GRU layers in GST.\",\"gst_gru_units (int) – The number of GRU units in GST.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"segment_size (int) – Segment size for random windowed discriminator\",\"generator_out_channels (int) – Number of output channels.\",\"generator_channels (int) – Number of hidden representation channels.\",\"generator_global_channels (int) – Number of global conditioning channels.\",\"generator_kernel_size (int) – Kernel size of initial and final conv layer.\",\"generator_upsample_scales (List *[*int]) – List of upsampling scales.\",\"generator_upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsample layers.\",\"generator_resblock_kernel_sizes (List *[*int]) – List of kernel sizes for residual blocks.\",\"generator_resblock_dilations (List *[*List *[*int]]) – List of list of dilations for residual blocks.\",\"generator_use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"generator_bias (bool) – Whether to add bias parameter in convolution layers.\",\"generator_nonlinear_activation (str) – Activation function module name.\",\"generator_nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"generator_use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"pitch (Tensor) – Batch of padded token-averaged pitch (B, T_text, 1).\",\"pitch_lengths (LongTensor) – Batch of pitch lengths (B, T_text).\",\"energy (Tensor) – Batch of padded token-averaged energy (B, T_text, 1).\",\"energy_lengths (LongTensor) – Batch of energy lengths (B, T_text).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"Returns: Waveform tensor (B, 1, segment_size * upsample_factor). Tensor: Binarization loss (). Tensor: Log probability attention matrix (B, T_feats, T_text). Tensor: Segments start index tensor (B,). Tensor: predicted duration (B, T_text). Tensor: ground-truth duration obtained from an alignment module (B, T_text). Tensor: predicted pitch (B, T_text,1). Tensor: ground-truth averaged pitch (B, T_text, 1). Tensor: predicted energy (B, T_text, 1). Tensor: ground-truth averaged energy (B, T_text, 1).\",\"Return type: Tensor\",\"inference(text: Tensor, text_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, use_teacher_forcing: bool = False)\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (B, T_text,).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"pitch (Tensor) – Pitch tensor (B, T_feats, 1)\",\"energy (Tensor) – Energy tensor (B, T_feats, 1)\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Generated waveform tensor (B, T_wav). Tensor: Duration tensor (B, T_text).\",\"Return type: Tensor\",\"training : bool\"]},\"1852\":{\"h\":\"espnet2.gan_tts.joint.joint_text2wav.JointText2Wav\",\"t\":[\"class espnet2.gan_tts.joint.joint_text2wav.JointText2Wav(idim: int, odim: int, segment_size: int = 32, sampling_rate: int = 22050, text2mel_type: str = 'fastspeech2', text2mel_params: Dict[str, Any] = {'adim': 384, 'aheads': 2, 'conformer_activation_type': 'swish', 'conformer_dec_kernel_size': 31, 'conformer_enc_kernel_size': 7, 'conformer_pos_enc_layer_type': 'rel_pos', 'conformer_rel_pos_type': 'latest', 'conformer_self_attn_layer_type': 'rel_selfattn', 'decoder_concat_after': False, 'decoder_normalize_before': True, 'decoder_type': 'conformer', 'dlayers': 4, 'dunits': 1536, 'duration_predictor_chans': 384, 'duration_predictor_dropout_rate': 0.1, 'duration_predictor_kernel_size': 3, 'duration_predictor_layers': 2, 'elayers': 4, 'encoder_concat_after': False, 'encoder_normalize_before': True, 'encoder_type': 'conformer', 'energy_embed_dropout': 0.5, 'energy_embed_kernel_size': 1, 'energy_predictor_chans': 384, 'energy_predictor_dropout': 0.5, 'energy_predictor_kernel_size': 3, 'energy_predictor_layers': 2, 'eunits': 1536, 'gst_conv_chans_list': [32, 32, 64, 64, 128, 128], 'gst_conv_kernel_size': 3, 'gst_conv_layers': 6, 'gst_conv_stride': 2, 'gst_gru_layers': 1, 'gst_gru_units': 128, 'gst_heads': 4, 'gst_tokens': 10, 'init_dec_alpha': 1.0, 'init_enc_alpha': 1.0, 'init_type': 'xavier_uniform', 'langs': -1, 'pitch_embed_dropout': 0.5, 'pitch_embed_kernel_size': 1, 'pitch_predictor_chans': 384, 'pitch_predictor_dropout': 0.5, 'pitch_predictor_kernel_size': 5, 'pitch_predictor_layers': 5, 'positionwise_conv_kernel_size': 1, 'positionwise_layer_type': 'conv1d', 'postnet_chans': 512, 'postnet_dropout_rate': 0.5, 'postnet_filts': 5, 'postnet_layers': 5, 'reduction_factor': 1, 'spk_embed_dim': None, 'spk_embed_integration_type': 'add', 'spks': -1, 'stop_gradient_from_energy_predictor': False, 'stop_gradient_from_pitch_predictor': True, 'transformer_dec_attn_dropout_rate': 0.1, 'transformer_dec_dropout_rate': 0.1, 'transformer_dec_positional_dropout_rate': 0.1, 'transformer_enc_attn_dropout_rate': 0.1, 'transformer_enc_dropout_rate': 0.1, 'transformer_enc_positional_dropout_rate': 0.1, 'use_batch_norm': True, 'use_cnn_in_conformer': True, 'use_gst': False, 'use_macaron_style_in_conformer': True, 'use_masking': False, 'use_scaled_pos_enc': True, 'use_weighted_masking': False, 'zero_triu': False}, vocoder_type: str = 'hifigan_generator', vocoder_params: Dict[str, Any] = {'bias': True, 'channels': 512, 'global_channels': -1, 'kernel_size': 7, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'resblock_kernel_sizes': [3, 7, 11], 'upsample_kernel_sizes': [16, 16, 4, 4], 'upsample_scales': [8, 8, 2, 2], 'use_additional_convs': True, 'use_weight_norm': True}, use_pqmf: bool = False, pqmf_params: Dict[str, Any] = {'beta': 9.0, 'cutoff_ratio': 0.142, 'subbands': 4, 'taps': 62}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_text2mel: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False)\",\"Bases: AbsGANTTS\",\"General class to jointly train text2mel and vocoder parts.\",\"Initialize JointText2Wav module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since the model is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"segment_size (int) – Segment size for random windowed inputs.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"text2mel_type (str) – The text2mel model type.\",\"text2mel_params (Dict *[*str,Any]) – Parameter dict for text2mel model.\",\"use_pqmf (bool) – Whether to use PQMF for multi-band vocoder.\",\"pqmf_params (Dict *[*str,Any]) – Parameter dict for PQMF module.\",\"vocoder_type (str) – The vocoder model type.\",\"vocoder_params (Dict *[*str,Any]) – Parameter dict for vocoder model.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"use_feat_match_loss (bool) – Whether to use feat match loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"use_mel_loss (bool) – Whether to use mel loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_text2mel (float) – Loss scaling coefficient for text2mel model loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, forward_generator: bool = True, **kwargs)\",\"Perform generator forward.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, **kwargs)\",\"Run inference.\",\"Parameters:text (Tensor) – Input text index tensor (T_text,).\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"feat_gan (Tensor): Generated feature tensor (T_text, C).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_speech\",\"Return whether or not speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"1853\":{\"h\":\"espnet2.gan_tts.vits.loss.KLDivergenceLoss\",\"t\":[\"<!-- _espnet2.gan_tts.vits.loss.KLDivergenceLoss -->\",\"class espnet2.gan_tts.vits.loss.KLDivergenceLoss\",\"Bases: Module\",\"KL divergence loss.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(z_p: Tensor, logs_q: Tensor, m_p: Tensor, logs_p: Tensor, z_mask: Tensor)\",\"Calculate KL divergence loss.\",\"Parameters:\",\"z_p (Tensor) – Flow hidden representation (B, H, T_feats).\",\"logs_q (Tensor) – Posterior encoder projected scale (B, H, T_feats).\",\"m_p (Tensor) – Expanded text encoder projected mean (B, H, T_feats).\",\"logs_p (Tensor) – Expanded text encoder projected scale (B, H, T_feats).\",\"z_mask (Tensor) – Mask tensor (B, 1, T_feats).\",\"Returns: KL divergence loss.\",\"Return type: Tensor\",\"training : bool\"]},\"1854\":{\"h\":\"espnet2.gan_tts.vits.loss.KLDivergenceLossWithoutFlow\",\"t\":[\"class espnet2.gan_tts.vits.loss.KLDivergenceLossWithoutFlow\",\"Bases: Module\",\"KL divergence loss without flow.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(m_q: Tensor, logs_q: Tensor, m_p: Tensor, logs_p: Tensor)\",\"Calculate KL divergence loss without flow.\",\"Parameters:\",\"m_q (Tensor) – Posterior encoder projected mean (B, H, T_feats).\",\"logs_q (Tensor) – Posterior encoder projected scale (B, H, T_feats).\",\"m_p (Tensor) – Expanded text encoder projected mean (B, H, T_feats).\",\"logs_p (Tensor) – Expanded text encoder projected scale (B, H, T_feats).\",\"training : bool\"]},\"1855\":{\"h\":\"espnet2.gan_tts.vits.flow.LogFlow\",\"t\":[\"<!-- _espnet2.gan_tts.vits.flow.LogFlow -->\",\"class espnet2.gan_tts.vits.flow.LogFlow\",\"Bases: Module\",\"Log flow module.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, x_mask: Tensor, inverse: bool = False, eps: float = 1e-05, **kwargs)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T).\",\"x_mask (Tensor) – Mask tensor (B, 1, T).\",\"inverse (bool) – Whether to inverse the flow.\",\"eps (float) – Epsilon for log.\",\"Returns: Output tensor (B, channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\",\"training : bool\"]},\"1856\":{\"h\":\"espnet2.gan_tts.melgan.melgan.MelGANDiscriminator\",\"t\":[\"class espnet2.gan_tts.melgan.melgan.MelGANDiscriminator(in_channels: int = 1, out_channels: int = 1, kernel_sizes: List[int] = [5, 3], channels: int = 16, max_downsample_channels: int = 1024, bias: bool = True, downsample_scales: List[int] = [4, 4, 4, 4], nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d', pad_params: Dict[str, Any] = {})\",\"Bases: Module\",\"MelGAN discriminator module.\",\"Initilize MelGANDiscriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_sizes (List *[*int]) – List of two kernel sizes. The prod will be used for the first conv layer, and the first and the second kernel sizes will be used for the last two layers. For example if kernel_sizes = [5, 3], the first layer kernel size will be 5 * 3 = 15, the last two layers’ kernel size will be 5 and 3, respectively.\",\"channels (int) – Initial number of channels for conv layer.\",\"max_downsample_channels (int) – Maximum number of channels for downsampling layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"downsample_scales (List *[*int]) – List of downsampling scales.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"pad (str) – Padding function module name before dilated convolution layer.\",\"pad_params (Dict *[*str,Any]) – Hyperparameters for padding function.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of output tensors of each layer.\",\"Return type: List[Tensor]\",\"training : bool\"]},\"1857\":{\"h\":\"espnet2.gan_tts.melgan.melgan.MelGANGenerator\",\"t\":[\"class espnet2.gan_tts.melgan.melgan.MelGANGenerator(in_channels: int = 80, out_channels: int = 1, kernel_size: int = 7, channels: int = 512, bias: bool = True, upsample_scales: List[int] = [8, 8, 2, 2], stack_kernel_size: int = 3, stacks: int = 3, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d', pad_params: Dict[str, Any] = {}, use_final_nonlinear_activation: bool = True, use_weight_norm: bool = True)\",\"Bases: Module\",\"MelGAN generator module.\",\"Initialize MelGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"channels (int) – Initial number of channels for conv layer.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"stack_kernel_size (int) – Kernel size of dilated conv layers in residual stack.\",\"stacks (int) – Number of stacks in a single residual stack.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"pad (str) – Padding function module name before dilated convolution layer.\",\"pad_params (Dict *[*str,Any]) – Hyperparameters for padding function.\",\"use_final_nonlinear_activation (torch.nn.Module) – Activation function for the final layer.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor)\",\"Calculate forward propagation.\",\"Parameters:c (Tensor) – Input tensor (B, channels, T).\",\"Returns: Output tensor (B, 1, T ** prod(upsample_scales)).\",\"Return type: Tensor\",\"inference(c: Tensor)\",\"Perform inference.\",\"Parameters:c (Tensor) – Input tensor (T, in_channels).\",\"Returns: Output tensor (T ** prod(upsample_scales), out_channels).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows official implementation manner. https://github.com/descriptinc/melgan-neurips/blob/master/mel2wav/modules.py\",\"training : bool\"]},\"1858\":{\"h\":\"espnet2.gan_tts.melgan.melgan.MelGANMultiScaleDiscriminator\",\"t\":[\"class espnet2.gan_tts.melgan.melgan.MelGANMultiScaleDiscriminator(in_channels: int = 1, out_channels: int = 1, scales: int = 3, downsample_pooling: str = 'AvgPool1d', downsample_pooling_params: Dict[str, Any] = {'count_include_pad': False, 'kernel_size': 4, 'padding': 1, 'stride': 2}, kernel_sizes: List[int] = [5, 3], channels: int = 16, max_downsample_channels: int = 1024, bias: bool = True, downsample_scales: List[int] = [4, 4, 4, 4], nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d', pad_params: Dict[str, Any] = {}, use_weight_norm: bool = True)\",\"Bases: Module\",\"MelGAN multi-scale discriminator module.\",\"Initilize MelGANMultiScaleDiscriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"scales (int) – Number of multi-scales.\",\"downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"downsample_pooling_params (Dict *[*str,Any]) – Parameters for the above pooling module.\",\"kernel_sizes (List *[*int]) – List of two kernel sizes. The sum will be used for the first conv layer, and the first and the second kernel sizes will be used for the last two layers.\",\"channels (int) – Initial number of channels for conv layer.\",\"max_downsample_channels (int) – Maximum number of channels for downsampling layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"downsample_scales (List *[*int]) – List of downsampling scales.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"pad (str) – Padding function module name before dilated convolution layer.\",\"pad_params (Dict *[*str,Any]) – Hyperparameters for padding function.\",\"use_weight_norm (bool) – Whether to use weight norm.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, which : consists of each layer output tensors.\",\"Return type: List[List[Tensor]]\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows official implementation manner. https://github.com/descriptinc/melgan-neurips/blob/master/mel2wav/modules.py\",\"training : bool\"]},\"1859\":{\"h\":\"espnet2.gan_tts.hifigan.loss.MelSpectrogramLoss\",\"t\":[\"class espnet2.gan_tts.hifigan.loss.MelSpectrogramLoss(fs: int = 22050, n_fft: int = 1024, hop_length: int = 256, win_length: int | None = None, window: str = 'hann', n_mels: int = 80, fmin: int | None = 0, fmax: int | None = None, center: bool = True, normalized: bool = False, onesided: bool = True, log_base: float | None = 10.0)\",\"Bases: Module\",\"Mel-spectrogram loss.\",\"Initialize Mel-spectrogram loss.\",\"Parameters:\",\"fs (int) – Sampling rate.\",\"n_fft (int) – FFT points.\",\"hop_length (int) – Hop length.\",\"win_length (Optional *[*int]) – Window length.\",\"window (str) – Window type.\",\"n_mels (int) – Number of Mel basis.\",\"fmin (Optional *[*int]) – Minimum frequency for Mel.\",\"fmax (Optional *[*int]) – Maximum frequency for Mel.\",\"center (bool) – Whether to use center window.\",\"normalized (bool) – Whether to use normalized one.\",\"onesided (bool) – Whether to use oneseded one.\",\"log_base (Optional *[*float]) – Log base value.\",\"forward(y_hat: Tensor, y: Tensor, spec: Tensor | None = None)\",\"Calculate Mel-spectrogram loss.\",\"Parameters:\",\"y_hat (Tensor) – Generated waveform tensor (B, 1, T).\",\"y (Tensor) – Groundtruth waveform tensor (B, 1, T).\",\"spec (Optional *[*Tensor]) – Groundtruth linear amplitude spectrum tensor (B, T, n_fft // 2 + 1). if provided, use it instead of groundtruth waveform.\",\"Returns: Mel-spectrogram loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"1860\":{\"h\":\"espnet2.gan_tts.melgan.pqmf.PQMF\",\"t\":[\"<!-- _espnet2.gan_tts.melgan.pqmf.PQMF -->\",\"class espnet2.gan_tts.melgan.pqmf.PQMF(subbands: int = 4, taps: int = 62, cutoff_ratio: float = 0.142, beta: float = 9.0)\",\"Bases: Module\",\"PQMF module.\",\"This module is based on Near-perfect-reconstruction pseudo-QMF banks.\",\"Initilize PQMF module.\",\"The cutoff_ratio and beta parameters are optimized for #subbands = 4. See dicussion in https://github.com/kan-bayashi/ParallelWaveGAN/issues/195.\",\"Parameters:\",\"subbands (int) – The number of subbands.\",\"taps (int) – The number of filter taps.\",\"cutoff_ratio (float) – Cut-off frequency ratio.\",\"beta (float) – Beta coefficient for kaiser window.\",\"analysis(x: Tensor)\",\"Analysis with PQMF.\",\"Parameters:x (Tensor) – Input tensor (B, 1, T).\",\"Returns: Output tensor (B, subbands, T // subbands).\",\"Return type: Tensor\",\"synthesis(x: Tensor)\",\"Synthesis with PQMF.\",\"Parameters:x (Tensor) – Input tensor (B, subbands, T // subbands).\",\"Returns: Output tensor (B, 1, T).\",\"Return type: Tensor\",\"training : bool\"]},\"1861\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.parallel_wavegan.ParallelWaveGANDiscriminator\",\"t\":[\"class espnet2.gan_tts.parallel_wavegan.parallel_wavegan.ParallelWaveGANDiscriminator(in_channels: int = 1, out_channels: int = 1, kernel_size: int = 3, layers: int = 10, conv_channels: int = 64, dilation_factor: int = 1, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, bias: bool = True, use_weight_norm: bool = True)\",\"Bases: Module\",\"Parallel WaveGAN Discriminator module.\",\"Initialize ParallelWaveGANDiscriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Number of output channels.\",\"layers (int) – Number of conv layers.\",\"conv_channels (int) – Number of chnn layers.\",\"dilation_factor (int) – Dilation factor. For example, if dilation_factor = 2, the dilation will be 2, 4, 8, …, and so on.\",\"nonlinear_activation (str) – Nonlinear function after each conv.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Nonlinear function parameters\",\"bias (bool) – Whether to use bias parameter in conv.\",\"use_weight_norm (bool) – If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: Output tensor (B, 1, T).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"training : bool\"]},\"1862\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.parallel_wavegan.ParallelWaveGANGenerator\",\"t\":[\"class espnet2.gan_tts.parallel_wavegan.parallel_wavegan.ParallelWaveGANGenerator(in_channels: int = 1, out_channels: int = 1, kernel_size: int = 3, layers: int = 30, stacks: int = 3, residual_channels: int = 64, gate_channels: int = 128, skip_channels: int = 64, aux_channels: int = 80, aux_context_window: int = 2, dropout_rate: float = 0.0, bias: bool = True, use_weight_norm: bool = True, upsample_conditional_features: bool = True, upsample_net: str = 'ConvInUpsampleNetwork', upsample_params: Dict[str, Any] = {'upsample_scales': [4, 4, 4, 4]})\",\"Bases: Module\",\"Parallel WaveGAN Generator module.\",\"Initialize ParallelWaveGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Kernel size of dilated convolution.\",\"layers (int) – Number of residual block layers.\",\"stacks (int) – Number of stacks i.e., dilation cycles.\",\"residual_channels (int) – Number of channels in residual conv.\",\"gate_channels (int) – Number of channels in gated conv.\",\"skip_channels (int) – Number of channels in skip conv.\",\"aux_channels (int) – Number of channels for auxiliary feature conv.\",\"aux_context_window (int) – Context window size for auxiliary feature.\",\"dropout_rate (float) – Dropout rate. 0.0 means no dropout applied.\",\"bias (bool) – Whether to use bias parameter in conv layer.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"upsample_conditional_features (bool) – Whether to use upsampling network.\",\"upsample_net (str) – Upsampling network architecture.\",\"upsample_params (Dict *[*str,Any]) – Upsampling network parameters.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor, z: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Local conditioning auxiliary features (B, C ,T_feats).\",\"z (Tensor) – Input noise signal (B, 1, T_wav).\",\"Returns: Output tensor (B, out_channels, T_wav)\",\"Return type: Tensor\",\"inference(c: Tensor, z: Tensor | None = None)\",\"Perform inference.\",\"Parameters:\",\"c (Tensor) – Local conditioning auxiliary features (T_feats ,C).\",\"z (Optional *[*Tensor]) – Input noise signal (T_wav, 1).\",\"Returns: Output tensor (T_wav, out_channels)\",\"Return type: Tensor\",\"property receptive_field_size\",\"Return receptive field size.\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"training : bool\"]},\"1863\":{\"h\":\"espnet2.gan_tts.vits.posterior_encoder.PosteriorEncoder\",\"t\":[\"class espnet2.gan_tts.vits.posterior_encoder.PosteriorEncoder(in_channels: int = 513, out_channels: int = 192, hidden_channels: int = 192, kernel_size: int = 5, layers: int = 16, stacks: int = 1, base_dilation: int = 1, global_channels: int = -1, dropout_rate: float = 0.0, bias: bool = True, use_weight_norm: bool = True)\",\"Bases: Module\",\"Posterior encoder module in VITS.\",\"This is a module of posterior encoder described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initilialize PosteriorEncoder module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size in WaveNet.\",\"layers (int) – Number of layers of WaveNet.\",\"stacks (int) – Number of repeat stacking of WaveNet.\",\"base_dilation (int) – Base dilation factor.\",\"global_channels (int) – Number of global conditioning channels.\",\"dropout_rate (float) – Dropout rate.\",\"bias (bool) – Whether to use bias parameters in conv.\",\"use_weight_norm (bool) – Whether to apply weight norm.\",\"forward(x: Tensor, x_lengths: Tensor, g: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T_feats).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Encoded hidden representation tensor (B, out_channels, T_feats). Tensor: Projected mean tensor (B, out_channels, T_feats). Tensor: Projected scale tensor (B, out_channels, T_feats). Tensor: Mask tensor for input tensor (B, 1, T_feats).\",\"Return type: Tensor\",\"training : bool\"]},\"1864\":{\"h\":\"espnet2.gan_tts.vits.residual_coupling.ResidualAffineCouplingBlock\",\"t\":[\"class espnet2.gan_tts.vits.residual_coupling.ResidualAffineCouplingBlock(in_channels: int = 192, hidden_channels: int = 192, flows: int = 4, kernel_size: int = 5, base_dilation: int = 1, layers: int = 4, global_channels: int = -1, dropout_rate: float = 0.0, use_weight_norm: bool = True, bias: bool = True, use_only_mean: bool = True)\",\"Bases: Module\",\"Residual affine coupling block module.\",\"This is a module of residual affine coupling block, which used as “Flow” in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initilize ResidualAffineCouplingBlock module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"flows (int) – Number of flows.\",\"kernel_size (int) – Kernel size for WaveNet.\",\"base_dilation (int) – Base dilation factor for WaveNet.\",\"layers (int) – Number of layers of WaveNet.\",\"stacks (int) – Number of stacks of WaveNet.\",\"global_channels (int) – Number of global channels.\",\"dropout_rate (float) – Dropout rate.\",\"use_weight_norm (bool) – Whether to use weight normalization in WaveNet.\",\"bias (bool) – Whether to use bias paramters in WaveNet.\",\"use_only_mean (bool) – Whether to estimate only mean.\",\"forward(x: Tensor, x_mask: Tensor, g: Tensor | None = None, inverse: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Output tensor (B, in_channels, T).\",\"Return type: Tensor\",\"training : bool\"]},\"1865\":{\"h\":\"espnet2.gan_tts.vits.residual_coupling.ResidualAffineCouplingLayer\",\"t\":[\"class espnet2.gan_tts.vits.residual_coupling.ResidualAffineCouplingLayer(in_channels: int = 192, hidden_channels: int = 192, kernel_size: int = 5, base_dilation: int = 1, layers: int = 5, stacks: int = 1, global_channels: int = -1, dropout_rate: float = 0.0, use_weight_norm: bool = True, bias: bool = True, use_only_mean: bool = True)\",\"Bases: Module\",\"Residual affine coupling layer.\",\"Initialzie ResidualAffineCouplingLayer module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size for WaveNet.\",\"base_dilation (int) – Base dilation factor for WaveNet.\",\"layers (int) – Number of layers of WaveNet.\",\"stacks (int) – Number of stacks of WaveNet.\",\"global_channels (int) – Number of global channels.\",\"dropout_rate (float) – Dropout rate.\",\"use_weight_norm (bool) – Whether to use weight normalization in WaveNet.\",\"bias (bool) – Whether to use bias paramters in WaveNet.\",\"use_only_mean (bool) – Whether to estimate only mean.\",\"forward(x: Tensor, x_mask: Tensor, g: Tensor | None = None, inverse: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Output tensor (B, in_channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\",\"training : bool\"]},\"1866\":{\"h\":\"espnet2.gan_tts.hifigan.residual_block.ResidualBlock\",\"t\":[\"class espnet2.gan_tts.hifigan.residual_block.ResidualBlock(kernel_size: int = 3, channels: int = 512, dilations: List[int] = [1, 3, 5], bias: bool = True, use_additional_convs: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1})\",\"Bases: Module\",\"Residual block module in HiFiGAN.\",\"Initialize ResidualBlock module.\",\"Parameters:\",\"kernel_size (int) – Kernel size of dilation convolution layer.\",\"channels (int) – Number of channels for convolution layer.\",\"dilations (List *[*int]) – List of dilation factors.\",\"use_additional_convs (bool) – Whether to use additional convolution layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor (B, channels, T).\",\"Returns: Output tensor (B, channels, T).\",\"Return type: Tensor\",\"training : bool\"]},\"1867\":{\"h\":\"espnet2.gan_tts.melgan.residual_stack.ResidualStack\",\"t\":[\"class espnet2.gan_tts.melgan.residual_stack.ResidualStack(kernel_size: int = 3, channels: int = 32, dilation: int = 1, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d', pad_params: Dict[str, Any] = {})\",\"Bases: Module\",\"Residual stack module introduced in MelGAN.\",\"Initialize ResidualStack module.\",\"Parameters:\",\"kernel_size (int) – Kernel size of dilation convolution layer.\",\"channels (int) – Number of channels of convolution layers.\",\"dilation (int) – Dilation factor.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"pad (str) – Padding function module name before dilated convolution layer.\",\"pad_params (Dict *[*str,Any]) – Hyperparameters for padding function.\",\"forward(c: Tensor)\",\"Calculate forward propagation.\",\"Parameters:c (Tensor) – Input tensor (B, channels, T).\",\"Returns: Output tensor (B, chennels, T).\",\"Return type: Tensor\",\"training : bool\"]},\"1868\":{\"h\":\"espnet2.gan_tts.vits.duration_predictor.StochasticDurationPredictor\",\"t\":[\"class espnet2.gan_tts.vits.duration_predictor.StochasticDurationPredictor(channels: int = 192, kernel_size: int = 3, dropout_rate: float = 0.5, flows: int = 4, dds_conv_layers: int = 3, global_channels: int = -1)\",\"Bases: Module\",\"Stochastic duration predictor module.\",\"This is a module of stochastic duration predictor described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initialize StochasticDurationPredictor module.\",\"Parameters:\",\"channels (int) – Number of channels.\",\"kernel_size (int) – Kernel size.\",\"dropout_rate (float) – Dropout rate.\",\"flows (int) – Number of flows.\",\"dds_conv_layers (int) – Number of conv layers in DDS conv.\",\"global_channels (int) – Number of global conditioning channels.\",\"forward(x: Tensor, x_mask: Tensor, w: Tensor | None = None, g: Tensor | None = None, inverse: bool = False, noise_scale: float = 1.0)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T_text).\",\"x_mask (Tensor) – Mask tensor (B, 1, T_text).\",\"w (Optional *[*Tensor]) – Duration tensor (B, 1, T_text).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, channels, 1)\",\"inverse (bool) – Whether to inverse the flow.\",\"noise_scale (float) – Noise scale value.\",\"Returns: If not inverse, negative log-likelihood (NLL) tensor (B,). : If inverse, log-duration tensor (B, 1, T_text).\",\"Return type: Tensor\",\"training : bool\"]},\"1869\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.upsample.Stretch2d\",\"t\":[\"class espnet2.gan_tts.parallel_wavegan.upsample.Stretch2d(x_scale: int, y_scale: int, mode: str = 'nearest')\",\"Bases: Module\",\"Stretch2d module.\",\"Initialize Stretch2d module.\",\"Parameters:\",\"x_scale (int) – X scaling factor (Time axis in spectrogram).\",\"y_scale (int) – Y scaling factor (Frequency axis in spectrogram).\",\"mode (str) – Interpolation mode.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor (B, C, F, T).\",\"Returns: Interpolated tensor (B, C, F * y_scale, T * x_scale),\",\"Return type: Tensor\",\"training : bool\"]},\"1870\":{\"h\":\"espnet2.gan_tts.style_melgan.style_melgan.StyleMelGANDiscriminator\",\"t\":[\"class espnet2.gan_tts.style_melgan.style_melgan.StyleMelGANDiscriminator(repeats: int = 2, window_sizes: List[int] = [512, 1024, 2048, 4096], pqmf_params: List[List[int]] = [[1, None, None, None], [2, 62, 0.267, 9.0], [4, 62, 0.142, 9.0], [8, 62, 0.07949, 9.0]], discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 16, 'downsample_scales': [4, 4, 4, 1], 'kernel_sizes': [5, 3], 'max_downsample_channels': 512, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.2}, 'out_channels': 1, 'pad': 'ReflectionPad1d', 'pad_params': {}}, use_weight_norm: bool = True)\",\"Bases: Module\",\"Style MelGAN disciminator module.\",\"Initilize StyleMelGANDiscriminator module.\",\"Parameters:\",\"repeats (int) – Number of repititons to apply RWD.\",\"window_sizes (List *[*int]) – List of random window sizes.\",\"pqmf_params (List *[*List *[*int]]) – List of list of Parameters for PQMF modules\",\"discriminator_params (Dict *[*str,Any]) – Parameters for base discriminator module.\",\"use_weight_nom (bool) – Whether to apply weight normalization.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor (B, 1, T).\",\"Returns: List of discriminator outputs, #items in the list will be : equal to repeats * #discriminators.\",\"Return type: List\",\"reset_parameters()\",\"Reset parameters.\",\"training : bool\"]},\"1871\":{\"h\":\"espnet2.gan_tts.style_melgan.style_melgan.StyleMelGANGenerator\",\"t\":[\"class espnet2.gan_tts.style_melgan.style_melgan.StyleMelGANGenerator(in_channels: int = 128, aux_channels: int = 80, channels: int = 64, out_channels: int = 1, kernel_size: int = 9, dilation: int = 2, bias: bool = True, noise_upsample_scales: List[int] = [11, 2, 2, 2], noise_upsample_activation: str = 'LeakyReLU', noise_upsample_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, upsample_scales: List[int] = [2, 2, 2, 2, 2, 2, 2, 2, 1], upsample_mode: str = 'nearest', gated_function: str = 'softmax', use_weight_norm: bool = True)\",\"Bases: Module\",\"Style MelGAN generator module.\",\"Initilize StyleMelGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input noise channels.\",\"aux_channels (int) – Number of auxiliary input channels.\",\"channels (int) – Number of channels for conv layer.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Kernel size of conv layers.\",\"dilation (int) – Dilation factor for conv layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"noise_upsample_scales (List *[*int]) – List of noise upsampling scales.\",\"noise_upsample_activation (str) – Activation function module name for noise upsampling.\",\"noise_upsample_activation_params (Dict *[*str,Any]) – Hyperparameters for the above activation function.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"upsample_mode (str) – Upsampling mode in TADE layer.\",\"gated_function (str) – Gated function used in TADEResBlock (“softmax” or “sigmoid”).\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor, z: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Auxiliary input tensor (B, channels, T).\",\"z (Tensor) – Input noise tensor (B, in_channels, 1).\",\"Returns: Output tensor (B, out_channels, T ** prod(upsample_scales)).\",\"Return type: Tensor\",\"inference(c: Tensor)\",\"Perform inference.\",\"Parameters:c (Tensor) – Input tensor (T, in_channels).\",\"Returns: Output tensor (T ** prod(upsample_scales), out_channels).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"training : bool\"]},\"1872\":{\"h\":\"espnet2.gan_tts.style_melgan.tade_res_block.TADELayer\",\"t\":[\"class espnet2.gan_tts.style_melgan.tade_res_block.TADELayer(in_channels: int = 64, aux_channels: int = 80, kernel_size: int = 9, bias: bool = True, upsample_factor: int = 2, upsample_mode: str = 'nearest')\",\"Bases: Module\",\"TADE Layer module.\",\"Initilize TADELayer module.\",\"Parameters:\",\"in_channels (int) – Number of input channles.\",\"aux_channels (int) – Number of auxirialy channles.\",\"kernel_size (int) – Kernel size.\",\"bias (bool) – Whether to use bias parameter in conv.\",\"upsample_factor (int) – Upsample factor.\",\"upsample_mode (str) – Upsample mode.\",\"forward(x: Tensor, c: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"c (Tensor) – Auxiliary input tensor (B, aux_channels, T’).\",\"Returns: Output tensor (B, in_channels, T * in_upsample_factor). Tensor: Upsampled aux tensor (B, in_channels, T * aux_upsample_factor).\",\"Return type: Tensor\",\"training : bool\"]},\"1873\":{\"h\":\"espnet2.gan_tts.style_melgan.tade_res_block.TADEResBlock\",\"t\":[\"class espnet2.gan_tts.style_melgan.tade_res_block.TADEResBlock(in_channels: int = 64, aux_channels: int = 80, kernel_size: int = 9, dilation: int = 2, bias: bool = True, upsample_factor: int = 2, upsample_mode: str = 'nearest', gated_function: str = 'softmax')\",\"Bases: Module\",\"TADEResBlock module.\",\"Initialize TADEResBlock module.\",\"Parameters:\",\"in_channels (int) – Number of input channles.\",\"aux_channels (int) – Number of auxirialy channles.\",\"kernel_size (int) – Kernel size.\",\"bias (bool) – Whether to use bias parameter in conv.\",\"upsample_factor (int) – Upsample factor.\",\"upsample_mode (str) – Upsample mode.\",\"gated_function (str) – Gated function type (softmax of sigmoid).\",\"forward(x: Tensor, c: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"c (Tensor) – Auxiliary input tensor (B, aux_channels, T’).\",\"Returns: Output tensor (B, in_channels, T * in_upsample_factor). Tensor: Upsampled auxirialy tensor (B, in_channels, T * in_upsample_factor).\",\"Return type: Tensor\",\"training : bool\"]},\"1874\":{\"h\":\"espnet2.gan_tts.vits.text_encoder.TextEncoder\",\"t\":[\"class espnet2.gan_tts.vits.text_encoder.TextEncoder(vocabs: int, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0)\",\"Bases: Module\",\"Text encoder module in VITS.\",\"This is a module of text encoder described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Instead of the relative positional Transformer, we use conformer architecture as the encoder module, which contains additional convolution layers.\",\"Initialize TextEncoder module.\",\"Parameters:\",\"vocabs (int) – Vocabulary size.\",\"attention_dim (int) – Attention dimension.\",\"attention_heads (int) – Number of attention heads.\",\"linear_units (int) – Number of linear units of positionwise layers.\",\"blocks (int) – Number of encoder blocks.\",\"positionwise_layer_type (str) – Positionwise layer type.\",\"positionwise_conv_kernel_size (int) – Positionwise layer’s kernel size.\",\"positional_encoding_layer_type (str) – Positional encoding layer type.\",\"self_attention_layer_type (str) – Self-attention layer type.\",\"activation_type (str) – Activation function type.\",\"normalize_before (bool) – Whether to apply LayerNorm before attention.\",\"use_macaron_style (bool) – Whether to use macaron style components.\",\"use_conformer_conv (bool) – Whether to use conformer conv layers.\",\"conformer_kernel_size (int) – Conformer’s conv kernel size.\",\"dropout_rate (float) – Dropout rate.\",\"positional_dropout_rate (float) – Dropout rate for positional encoding.\",\"attention_dropout_rate (float) – Dropout rate for attention.\",\"forward(x: Tensor, x_lengths: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input index tensor (B, T_text).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"Returns: Encoded hidden representation (B, attention_dim, T_text). Tensor: Projected mean tensor (B, attention_dim, T_text). Tensor: Projected scale tensor (B, attention_dim, T_text). Tensor: Mask tensor for input tensor (B, 1, T_text).\",\"Return type: Tensor\",\"training : bool\"]},\"1875\":{\"h\":\"espnet2.gan_tts.vits.flow.Transpose\",\"t\":[\"<!-- _espnet2.gan_tts.vits.flow.Transpose -->\",\"class espnet2.gan_tts.vits.flow.Transpose(dim1: int, dim2: int)\",\"Bases: Module\",\"Transpose module for torch.nn.Sequential().\",\"Initialize Transpose module.\",\"forward(x: Tensor)\",\"Transpose.\",\"training : bool\"]},\"1876\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.upsample.UpsampleNetwork\",\"t\":[\"class espnet2.gan_tts.parallel_wavegan.upsample.UpsampleNetwork(upsample_scales: List[int], nonlinear_activation: str | None = None, nonlinear_activation_params: Dict[str, Any] = {}, interpolate_mode: str = 'nearest', freq_axis_kernel_size: int = 1)\",\"Bases: Module\",\"Upsampling network module.\",\"Initialize UpsampleNetwork module.\",\"Parameters:\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"nonlinear_activation (Optional *[*str]) – Activation function name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Arguments for the specified activation function.\",\"interpolate_mode (str) – Interpolation mode.\",\"freq_axis_kernel_size (int) – Kernel size in the direction of frequency axis.\",\"forward(c: Tensor)\",\"Calculate forward propagation.\",\"Parameters:c – Input tensor (B, C, T_feats).\",\"Returns: Upsampled tensor (B, C, T_wav).\",\"Return type: Tensor\",\"training : bool\"]},\"1877\":{\"h\":\"espnet2.gan_tts.vits.vits.VITS\",\"t\":[\"<!-- _espnet2.gan_tts.vits.vits.VITS -->\",\"class espnet2.gan_tts.vits.vits.VITS(idim: int, odim: int, sampling_rate: int = 22050, generator_type: str = 'vits_generator', generator_params: Dict[str, Any] = {'decoder_channels': 512, 'decoder_kernel_size': 7, 'decoder_resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'decoder_resblock_kernel_sizes': [3, 7, 11], 'decoder_upsample_kernel_sizes': [16, 16, 4, 4], 'decoder_upsample_scales': [8, 8, 2, 2], 'flow_base_dilation': 1, 'flow_dropout_rate': 0.0, 'flow_flows': 4, 'flow_kernel_size': 5, 'flow_layers': 4, 'global_channels': -1, 'hidden_channels': 192, 'langs': None, 'posterior_encoder_base_dilation': 1, 'posterior_encoder_dropout_rate': 0.0, 'posterior_encoder_kernel_size': 5, 'posterior_encoder_layers': 16, 'posterior_encoder_stacks': 1, 'segment_size': 32, 'spk_embed_dim': None, 'spks': None, 'stochastic_duration_predictor_dds_conv_layers': 3, 'stochastic_duration_predictor_dropout_rate': 0.5, 'stochastic_duration_predictor_flows': 4, 'stochastic_duration_predictor_kernel_size': 3, 'text_encoder_activation_type': 'swish', 'text_encoder_attention_dropout_rate': 0.0, 'text_encoder_attention_heads': 2, 'text_encoder_blocks': 6, 'text_encoder_conformer_kernel_size': 7, 'text_encoder_dropout_rate': 0.1, 'text_encoder_ffn_expand': 4, 'text_encoder_normalize_before': True, 'text_encoder_positional_dropout_rate': 0.0, 'text_encoder_positional_encoding_layer_type': 'rel_pos', 'text_encoder_positionwise_conv_kernel_size': 1, 'text_encoder_positionwise_layer_type': 'conv1d', 'text_encoder_self_attention_layer_type': 'rel_selfattn', 'use_conformer_conv_in_text_encoder': True, 'use_macaron_style_in_text_encoder': True, 'use_only_mean_in_flow': True, 'use_weight_norm_in_decoder': True, 'use_weight_norm_in_flow': True, 'use_weight_norm_in_posterior_encoder': True}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_adv: float = 1.0, lambda_mel: float = 45.0, lambda_feat_match: float = 2.0, lambda_dur: float = 1.0, lambda_kl: float = 1.0, cache_generator_outputs: bool = True, plot_pred_mos: bool = False, mos_pred_tool: str = 'utmos')\",\"Bases: AbsGANTTS\",\"VITS module (generator + discriminator).\",\"This is a module of VITS described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initialize VITS module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since VITS is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"generator_type (str) – Generator type.\",\"generator_params (Dict *[*str,Any]) – Parameter dict for generator.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel spectrogram loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_dur (float) – Loss scaling coefficient for duration loss.\",\"lambda_kl (float) – Loss scaling coefficient for KL divergence loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"plot_pred_mos (bool) – Whether to plot predicted MOS during the training.\",\"mos_pred_tool (str) – MOS prediction tool name.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True)\",\"Perform generator forward.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, feats: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, durations: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False)\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (T_text,).\",\"feats (Tensor) – Feature tensor (T_feats, aux_channels).\",\"sids (Tensor) – Speaker index tensor (1,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (spk_embed_dim,).\",\"lids (Tensor) – Language index tensor (1,).\",\"durations (Tensor) – Ground-truth duration tensor (T_text,).\",\"noise_scale (float) – Noise scale value for flow.\",\"noise_scale_dur (float) – Noise scale value for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated speech.\",\"max_len (Optional *[*int]) – Maximum length.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"att_w (Tensor): Monotonic attention weight tensor (T_feats, T_text).\",\"duration (Tensor): Predicted duration tensor (T_text,).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_speech\",\"Return whether or not speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"1878\":{\"h\":\"espnet2.gan_tts.vits.generator.VITSGenerator\",\"t\":[\"class espnet2.gan_tts.vits.generator.VITSGenerator(vocabs: int, aux_channels: int = 513, hidden_channels: int = 192, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, global_channels: int = -1, segment_size: int = 32, text_encoder_attention_heads: int = 2, text_encoder_ffn_expand: int = 4, text_encoder_blocks: int = 6, text_encoder_positionwise_layer_type: str = 'conv1d', text_encoder_positionwise_conv_kernel_size: int = 1, text_encoder_positional_encoding_layer_type: str = 'rel_pos', text_encoder_self_attention_layer_type: str = 'rel_selfattn', text_encoder_activation_type: str = 'swish', text_encoder_normalize_before: bool = True, text_encoder_dropout_rate: float = 0.1, text_encoder_positional_dropout_rate: float = 0.0, text_encoder_attention_dropout_rate: float = 0.0, text_encoder_conformer_kernel_size: int = 7, use_macaron_style_in_text_encoder: bool = True, use_conformer_conv_in_text_encoder: bool = True, decoder_kernel_size: int = 7, decoder_channels: int = 512, decoder_upsample_scales: List[int] = [8, 8, 2, 2], decoder_upsample_kernel_sizes: List[int] = [16, 16, 4, 4], decoder_resblock_kernel_sizes: List[int] = [3, 7, 11], decoder_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], use_weight_norm_in_decoder: bool = True, posterior_encoder_kernel_size: int = 5, posterior_encoder_layers: int = 16, posterior_encoder_stacks: int = 1, posterior_encoder_base_dilation: int = 1, posterior_encoder_dropout_rate: float = 0.0, use_weight_norm_in_posterior_encoder: bool = True, flow_flows: int = 4, flow_kernel_size: int = 5, flow_base_dilation: int = 1, flow_layers: int = 4, flow_dropout_rate: float = 0.0, use_weight_norm_in_flow: bool = True, use_only_mean_in_flow: bool = True, stochastic_duration_predictor_kernel_size: int = 3, stochastic_duration_predictor_dropout_rate: float = 0.5, stochastic_duration_predictor_flows: int = 4, stochastic_duration_predictor_dds_conv_layers: int = 3)\",\"Bases: Module\",\"Generator module in VITS.\",\"This is a module of VITS described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"As text encoder, we use conformer architecture instead of the relative positional Transformer, which contains additional convolution layers.\",\"Initialize VITS generator module.\",\"Parameters:\",\"vocabs (int) – Input vocabulary size.\",\"aux_channels (int) – Number of acoustic feature channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"global_channels (int) – Number of global conditioning channels.\",\"segment_size (int) – Segment size for decoder.\",\"text_encoder_attention_heads (int) – Number of heads in conformer block of text encoder.\",\"text_encoder_ffn_expand (int) – Expansion ratio of FFN in conformer block of text encoder.\",\"text_encoder_blocks (int) – Number of conformer blocks in text encoder.\",\"text_encoder_positionwise_layer_type (str) – Position-wise layer type in conformer block of text encoder.\",\"text_encoder_positionwise_conv_kernel_size (int) – Position-wise convolution kernel size in conformer block of text encoder. Only used when the above layer type is conv1d or conv1d-linear.\",\"text_encoder_positional_encoding_layer_type (str) – Positional encoding layer type in conformer block of text encoder.\",\"text_encoder_self_attention_layer_type (str) – Self-attention layer type in conformer block of text encoder.\",\"text_encoder_activation_type (str) – Activation function type in conformer block of text encoder.\",\"text_encoder_normalize_before (bool) – Whether to apply layer norm before self-attention in conformer block of text encoder.\",\"text_encoder_dropout_rate (float) – Dropout rate in conformer block of text encoder.\",\"text_encoder_positional_dropout_rate (float) – Dropout rate for positional encoding in conformer block of text encoder.\",\"text_encoder_attention_dropout_rate (float) – Dropout rate for attention in conformer block of text encoder.\",\"text_encoder_conformer_kernel_size (int) – Conformer conv kernel size. It will be used when only use_conformer_conv_in_text_encoder = True.\",\"use_macaron_style_in_text_encoder (bool) – Whether to use macaron style FFN in conformer block of text encoder.\",\"use_conformer_conv_in_text_encoder (bool) – Whether to use covolution in conformer block of text encoder.\",\"decoder_kernel_size (int) – Decoder kernel size.\",\"decoder_channels (int) – Number of decoder initial channels.\",\"decoder_upsample_scales (List *[*int]) – List of upsampling scales in decoder.\",\"decoder_upsample_kernel_sizes (List *[*int]) – List of kernel size for upsampling layers in decoder.\",\"decoder_resblock_kernel_sizes (List *[*int]) – List of kernel size for resblocks in decoder.\",\"decoder_resblock_dilations (List *[*List *[*int]]) – List of list of dilations for resblocks in decoder.\",\"use_weight_norm_in_decoder (bool) – Whether to apply weight normalization in decoder.\",\"posterior_encoder_kernel_size (int) – Posterior encoder kernel size.\",\"posterior_encoder_layers (int) – Number of layers of posterior encoder.\",\"posterior_encoder_stacks (int) – Number of stacks of posterior encoder.\",\"posterior_encoder_base_dilation (int) – Base dilation of posterior encoder.\",\"posterior_encoder_dropout_rate (float) – Dropout rate for posterior encoder.\",\"use_weight_norm_in_posterior_encoder (bool) – Whether to apply weight normalization in posterior encoder.\",\"flow_flows (int) – Number of flows in flow.\",\"flow_kernel_size (int) – Kernel size in flow.\",\"flow_base_dilation (int) – Base dilation in flow.\",\"flow_layers (int) – Number of layers in flow.\",\"flow_dropout_rate (float) – Dropout rate in flow\",\"use_weight_norm_in_flow (bool) – Whether to apply weight normalization in flow.\",\"use_only_mean_in_flow (bool) – Whether to use only mean in flow.\",\"stochastic_duration_predictor_kernel_size (int) – Kernel size in stochastic duration predictor.\",\"stochastic_duration_predictor_dropout_rate (float) – Dropout rate in stochastic duration predictor.\",\"stochastic_duration_predictor_flows (int) – Number of flows in stochastic duration predictor.\",\"stochastic_duration_predictor_dds_conv_layers (int) – Number of DDS conv layers in stochastic duration predictor.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, aux_channels, T_feats).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"Returns: Waveform tensor (B, 1, segment_size * upsample_factor). Tensor: Duration negative log-likelihood (NLL) tensor (B,). Tensor: Monotonic attention weight tensor (B, 1, T_feats, T_text). Tensor: Segments start index tensor (B,). Tensor: Text mask tensor (B, 1, T_text). Tensor: Feature mask tensor (B, 1, T_feats). tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]: \",\"Tensor: Posterior encoder hidden representation (B, H, T_feats).\",\"Tensor: Flow hidden representation (B, H, T_feats).\",\"Tensor: Expanded text encoder projected mean (B, H, T_feats).\",\"Tensor: Expanded text encoder projected scale (B, H, T_feats).\",\"Tensor: Posterior encoder projected mean (B, H, T_feats).\",\"Tensor: Posterior encoder projected scale (B, H, T_feats).\",\"Return type: Tensor\",\"inference(text: Tensor, text_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, dur: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False)\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (B, T_text,).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, aux_channels, T_feats,).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"dur (Optional *[*Tensor]) – Ground-truth duration (B, T_text,). If provided, skip the prediction of durations (i.e., teacher forcing).\",\"noise_scale (float) – Noise scale parameter for flow.\",\"noise_scale_dur (float) – Noise scale parameter for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated speech.\",\"max_len (Optional *[*int]) – Maximum length of acoustic feature sequence.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Generated waveform tensor (B, T_wav). Tensor: Monotonic attention weight tensor (B, T_feats, T_text). Tensor: Duration tensor (B, T_text).\",\"Return type: Tensor\",\"training : bool\"]},\"1879\":{\"h\":\"espnet2.gan_tts.jets.loss.VarianceLoss\",\"t\":[\"<!-- _espnet2.gan_tts.jets.loss.VarianceLoss -->\",\"class espnet2.gan_tts.jets.loss.VarianceLoss(use_masking: bool = True, use_weighted_masking: bool = False)\",\"Bases: Module\",\"Initialize JETS variance loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(d_outs: Tensor, ds: Tensor, p_outs: Tensor, ps: Tensor, e_outs: Tensor, es: Tensor, ilens: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of pitch predictor (B, T_text, 1).\",\"ps (Tensor) – Batch of target token-averaged pitch (B, T_text, 1).\",\"e_outs (Tensor) – Batch of outputs of energy predictor (B, T_text, 1).\",\"es (Tensor) – Batch of target token-averaged energy (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"Returns: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: Energy predictor loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"1880\":{\"h\":\"espnet2.gan_tts.wavenet.wavenet.WaveNet\",\"t\":[\"<!-- _espnet2.gan_tts.wavenet.wavenet.WaveNet -->\",\"class espnet2.gan_tts.wavenet.wavenet.WaveNet(in_channels: int = 1, out_channels: int = 1, kernel_size: int = 3, layers: int = 30, stacks: int = 3, base_dilation: int = 2, residual_channels: int = 64, aux_channels: int = -1, gate_channels: int = 128, skip_channels: int = 64, global_channels: int = -1, dropout_rate: float = 0.0, bias: bool = True, use_weight_norm: bool = True, use_first_conv: bool = False, use_last_conv: bool = False, scale_residual: bool = False, scale_skip_connect: bool = False)\",\"Bases: Module\",\"WaveNet with global conditioning.\",\"Initialize WaveNet module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Kernel size of dilated convolution.\",\"layers (int) – Number of residual block layers.\",\"stacks (int) – Number of stacks i.e., dilation cycles.\",\"base_dilation (int) – Base dilation factor.\",\"residual_channels (int) – Number of channels in residual conv.\",\"gate_channels (int) – Number of channels in gated conv.\",\"skip_channels (int) – Number of channels in skip conv.\",\"aux_channels (int) – Number of channels for local conditioning feature.\",\"global_channels (int) – Number of channels for global conditioning feature.\",\"dropout_rate (float) – Dropout rate. 0.0 means no dropout applied.\",\"bias (bool) – Whether to use bias parameter in conv layer.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"use_first_conv (bool) – Whether to use the first conv layers.\",\"use_last_conv (bool) – Whether to use the last conv layers.\",\"scale_residual (bool) – Whether to scale the residual outputs.\",\"scale_skip_connect (bool) – Whether to scale the skip connection outputs.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor, x_mask: Tensor | None = None, c: Tensor | None = None, g: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input noise signal (B, 1, T) if use_first_conv else (B, residual_channels, T).\",\"x_mask (Optional *[*Tensor]) – Mask tensor (B, 1, T).\",\"c (Optional *[*Tensor]) – Local conditioning features (B, aux_channels, T).\",\"g (Optional *[*Tensor]) – Global conditioning features (B, global_channels, 1).\",\"Returns: Output tensor (B, out_channels, T) if use_last_conv else : (B, residual_channels, T).\",\"Return type: Tensor\",\"property receptive_field_size : int\",\"Return receptive field size.\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"training : bool\"]},\"1881\":{\"h\":\"espnet2.gan_tts.jets.alignments.average_by_duration\",\"t\":[\"espnet2.gan_tts.jets.alignments.average_by_duration(ds, xs, text_lengths, feats_lengths)\",\"Average frame-level features into token-level according to durations\",\"Parameters:\",\"ds (Tensor) – Batched token duration (B, T_text).\",\"xs (Tensor) – Batched feature sequences to be averaged (B, T_feats).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"Returns: Batched feature averaged according to the token duration (B, T_text).\",\"Return type: Tensor\"]},\"1882\":{\"h\":\"espnet2.gan_tts.vits.monotonic_align.setup.build_ext\"},\"1883\":{\"h\":\"espnet2.gan_tts.melgan.pqmf.design_prototype_filter\",\"t\":[\"espnet2.gan_tts.melgan.pqmf.design_prototype_filter(taps: int = 62, cutoff_ratio: float = 0.142, beta: float = 9.0)\",\"Design prototype filter for PQMF.\",\"This method is based on A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks.\",\"Parameters:\",\"taps (int) – The number of filter taps.\",\"cutoff_ratio (float) – Cut-off frequency ratio.\",\"beta (float) – Beta coefficient for kaiser window.\",\"Returns: Impluse response of prototype filter (taps + 1,).\",\"Return type: ndarray\"]},\"1884\":{\"h\":\"espnet2.gan_tts.utils.get_random_segments.get_random_segments\",\"t\":[\"espnet2.gan_tts.utils.get_random_segments.get_random_segments(x: Tensor, x_lengths: Tensor, segment_size: int)\",\"Get random segments.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, C, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"segment_size (int) – Segment size.\",\"Returns: Segmented tensor (B, C, segment_size). Tensor: Start index tensor (B,).\",\"Return type: Tensor\"]},\"1885\":{\"h\":\"espnet2.gan_tts.utils.get_random_segments.get_segments\",\"t\":[\"espnet2.gan_tts.utils.get_random_segments.get_segments(x: Tensor, start_idxs: Tensor, segment_size: int)\",\"Get segments.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, C, T).\",\"start_idxs (Tensor) – Start index tensor (B,).\",\"segment_size (int) – Segment size.\",\"Returns: Segmented tensor (B, C, segment_size).\",\"Return type: Tensor\"]},\"1886\":{\"h\":\"espnet2.gan_tts.vits.transform.piecewise_rational_quadratic_transform\",\"t\":[\"espnet2.gan_tts.vits.transform.piecewise_rational_quadratic_transform(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse=False, tails=None, tail_bound=1.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)\"]},\"1887\":{\"h\":\"espnet2.gan_tts.vits.transform.rational_quadratic_spline\",\"t\":[\"espnet2.gan_tts.vits.transform.rational_quadratic_spline(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse=False, left=0.0, right=1.0, bottom=0.0, top=1.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)\"]},\"1888\":{\"h\":\"espnet2.gan_tts.vits.transform.unconstrained_rational_quadratic_spline\",\"t\":[\"espnet2.gan_tts.vits.transform.unconstrained_rational_quadratic_spline(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse=False, tails='linear', tail_bound=1.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)\"]},\"1889\":{\"h\":\"espnet2.gan_tts.jets.alignments.viterbi_decode\",\"t\":[\"espnet2.gan_tts.jets.alignments.viterbi_decode(log_p_attn, text_lengths, feats_lengths)\",\"Extract duration from an attention probability matrix\",\"Parameters:\",\"log_p_attn (Tensor) – Batched log probability of attention matrix (B, T_feats, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats_legnths (Tensor) – Feature length tensor (B,).\",\"Returns: Batched token duration extracted from log_p_attn (B, T_text). Tensor: Binarization loss tensor ().\",\"Return type: Tensor\"]},\"1890\":{\"h\":\"espnet2.hubert.hubert_loss.HubertPretrainLoss\",\"t\":[\"class espnet2.hubert.hubert_loss.HubertPretrainLoss(pred_masked_weight: float = 1.0, pred_nomask_weight: float = 0.0, loss_weights: float = 10.0)\",\"Bases: Module\",\"Hubert criterion module.\",\"Parameters:\",\"pred_masked_weight – weight for predictive loss for masked frames\",\"pred_nomask_weight – weight for predictive loss for unmasked frames\",\"loss_weights – weights for additional loss terms (not first one)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(model, enc_outputs, reduce=True)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1891\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1892\":{\"h\":\"espnet2.hubert.espnet_model.HubertPretrainModel\",\"t\":[\"class espnet2.hubert.espnet_model.HubertPretrainModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = False, report_wer: bool = False, sym_space: str = '<space>', sym_blank: str = '<blank>', pred_masked_weight: float = 1.0, pred_nomask_weight: float = 0.0, loss_weights: float = 0.0)\",\"Bases: AbsESPnetModel\",\"Hubert Pretrain model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"compute_correct(logits)\",\"encode(speech: Tensor, speech_lengths: Tensor, y_pad: Tensor, y_pad_length: Tensor)\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"y_pad – (Batch, Length, …)\",\"y_pad_length – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"Frontend + Encoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\",\"training : bool\"]},\"1893\":{\"h\":\"espnet2.hubert.espnet_model.TorchAudioHubertPretrainModel\",\"t\":[\"class espnet2.hubert.espnet_model.TorchAudioHubertPretrainModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, ignore_id: int = -1)\",\"Bases: AbsESPnetModel\",\"TorchAudio Hubert Pretrain model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor, y_pad: Tensor, y_pad_length: Tensor)\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"y_pad – (Batch, Length, …)\",\"y_pad_length – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs)\",\"Frontend + Encoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\",\"training : bool\"]},\"1894\":{\"h\":\"espnet2.iterators.abs_iter_factory.AbsIterFactory\",\"t\":[\"class espnet2.iterators.abs_iter_factory.AbsIterFactory\",\"Bases: ABC\",\"abstract build_iter(epoch: int, shuffle: bool | None = None)\"]},\"1895\":{\"h\":\"espnet2.iterators.category_iter_factory.CategoryIterFactory\",\"t\":[\"class espnet2.iterators.category_iter_factory.CategoryIterFactory(dataset, batches: AbsSampler | Sequence[Sequence[Any]], num_iters_per_epoch: int | None = None, seed: int = 0, sampler_args: dict | None = None, shuffle: bool = False, num_workers: int = 0, collate_fn=None, pin_memory: bool = False)\",\"Bases: AbsIterFactory\",\"Build iterator for each epoch.\",\"This class simply creates pytorch DataLoader except for the following points:\",\"The random seed is decided according to the number of epochs. This feature\",\"guarantees reproducibility when resuming from middle of training process.\",\"Enable to restrict the number of samples for one epoch. This features controls the interval number between training and evaluation.\",\"build_iter(epoch: int, shuffle: bool | None = None)\"]},\"1896\":{\"h\":\"espnet2.iterators.chunk_iter_factory.ChunkIterFactory\",\"t\":[\"class espnet2.iterators.chunk_iter_factory.ChunkIterFactory(dataset, batch_size: int, batches: AbsSampler | Sequence[Sequence[Any]], chunk_length: int | str, chunk_shift_ratio: float = 0.5, num_cache_chunks: int = 1024, num_samples_per_epoch: int | None = None, seed: int = 0, shuffle: bool = False, num_workers: int = 0, collate_fn=None, pin_memory: bool = False, excluded_key_prefixes: List[str] | None = None, discard_short_samples: bool = True, default_fs: int | None = None, chunk_max_abs_length: int | None = None)\",\"Bases: AbsIterFactory\",\"Creates chunks from a sequence\"]},\"1897\":{\"h\":\"Examples\",\"t\":[\">>> batches = [[\\\"id1\\\"], [\\\"id2\\\"], ...] >>> batch_size = 128 >>> chunk_length = 1000 >>> iter_factory = ChunkIterFactory(dataset, batches, batch_size, chunk_length) >>> it = iter_factory.build_iter(epoch) >>> for ids, batch in it: ... ...\",\"The number of mini-batches are varied in each epochs and we can’t get the number in advance because IterFactory doesn’t be given to the length information.\",\"Since the first reason, “num_iters_per_epoch” can’t be implemented for this iterator. Instead of it, “num_samples_per_epoch” is implemented.\",\"build_iter(epoch: int, shuffle: bool | None = None)\",\"prepare_for_collate(id_list, batches)\"]},\"1898\":{\"h\":\"espnet2.iterators.multiple_iter_factory.MultipleIterFactory\",\"t\":[\"class espnet2.iterators.multiple_iter_factory.MultipleIterFactory(build_funcs: Collection[Callable[[], AbsIterFactory]], seed: int = 0, shuffle: bool = False)\",\"Bases: AbsIterFactory\",\"build_iter(epoch: int, shuffle: bool | None = None)\"]},\"1899\":{\"h\":\"espnet2.iterators.category_iter_factory.RawSampler\",\"t\":[\"class espnet2.iterators.category_iter_factory.RawSampler(batches)\",\"Bases: AbsSampler\",\"generate(seed)\"]},\"1900\":{\"h\":\"espnet2.iterators.sequence_iter_factory.SequenceIterFactory\",\"t\":[\"class espnet2.iterators.sequence_iter_factory.SequenceIterFactory(dataset, batches: AbsSampler | Sequence[Sequence[Any]], num_iters_per_epoch: int | None = None, seed: int = 0, shuffle: bool = False, shuffle_within_batch: bool = False, num_workers: int = 0, collate_fn=None, pin_memory: bool = False)\",\"Bases: AbsIterFactory\",\"Build iterator for each epoch.\",\"This class simply creates pytorch DataLoader except for the following points:\",\"The random seed is decided according to the number of epochs. This feature\",\"guarantees reproducibility when resuming from middle of training process.\",\"Enable to restrict the number of samples for one epoch. This features controls the interval number between training and evaluation.\",\"build_iter(epoch: int, shuffle: bool | None = None)\"]},\"1901\":{\"h\":\"espnet2.iterators.category_iter_factory.worker_init_fn\",\"t\":[\"espnet2.iterators.category_iter_factory.worker_init_fn(worker_id, base_seed=0)\",\"Set random seed for each worker in DataLoader.\"]},\"1902\":{\"h\":\"espnet2.layers.abs_normalize.AbsNormalize\",\"t\":[\"<!-- _espnet2.layers.abs_normalize.AbsNormalize -->\",\"class espnet2.layers.abs_normalize.AbsNormalize\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1903\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1904\":{\"h\":\"espnet2.layers.sinc_conv.BarkScale\",\"t\":[\"<!-- _espnet2.layers.sinc_conv.BarkScale -->\",\"class espnet2.layers.sinc_conv.BarkScale\",\"Bases: object\",\"Bark frequency scale.\",\"Has wider bandwidths at lower frequencies, see: Critical bandwidth: BARK Zwicker and Terhardt, 1980\",\"classmethod bank(channels: int, fs: float)\",\"Obtain initialization values for the Bark scale.\",\"Parameters:\",\"channels – Number of channels.\",\"fs – Sample rate.\",\"Returns: Filter start frequencíes. torch.Tensor: Filter stop frequencíes.\",\"Return type: torch.Tensor\",\"static convert(f)\",\"Convert Hz to Bark.\",\"static invert(x)\",\"Convert Bark to Hz.\"]},\"1905\":{\"h\":\"espnet2.layers.augmentation.DataAugmentation\",\"t\":[\"class espnet2.layers.augmentation.DataAugmentation(effects: List[Tuple[float, List[Tuple[float, str, Dict]]] | Tuple[float, str, Dict]], apply_n: Tuple[int, int] = [1, 1])\",\"Bases: object\",\"A series of data augmentation effects that can be applied to a given waveform.\",\"Note: Currently we only support single-channel waveforms.\",\"Parameters:\",\"effects (list) –\",\"a list of effects to be applied to the waveform. .. rubric:: Example\",\"[ : [0.1, “lowpass”, {“cutoff_freq”: 1000, “Q”: 0.707}], [0.1, “highpass”, {“cutoff_freq”: 3000, “Q”: 0.707}], [0.1, “equalization”, {“center_freq”: 1000, “gain”: 0, “Q”: 0.707}], [\",\"0.1, [\",\"[0.3, “speed_perturb”, {“factor”: 0.9}], [0.3, “speed_perturb”, {“factor”: 1.1}], <br/> ] <br/> ],\",\"]\",\"Description: : - The above list defines a series of data augmentation effects that will be randomly sampled to apply to a given waveform.\",\"The data structure of each element can be either type1=Tuple[float, str, Dict] or type2=Tuple[float, type1].\",\"In type1, the three values are the weight of sampling this effect, the name (key) of the effect, and the keyword arguments for the effect.\",\"In type2, the first value is the weight of sampling this effect. The second value is a list of type1 elements which are similarly defined as above.\",\"Note that he effects defined in each type2 data are mutually exclusive (i.e., only one of them can be applied each time). This can be useful when you want to avoid applying some specific effects at the same time.\",\"apply_n (list) – range of the number of effects to be applied to the waveform.\"]},\"1906\":{\"h\":\"espnet2.layers.global_mvn.GlobalMVN\",\"t\":[\"<!-- _espnet2.layers.global_mvn.GlobalMVN -->\",\"class espnet2.layers.global_mvn.GlobalMVN(stats_file: Path | str, norm_means: bool = True, norm_vars: bool = True, eps: float = 1e-20)\",\"Bases: AbsNormalize, InversibleInterface\",\"Apply global mean and variance normalization\",\"TODO(kamo): Make this class portable somehow\",\"Parameters:\",\"stats_file – npy file\",\"norm_means – Apply mean normalization\",\"norm_vars – Apply var normalization\",\"eps –\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x: Tensor, ilens: Tensor | None = None)\",\"Forward function\",\"Parameters:\",\"x – (B, L, …)\",\"ilens – (B,)\",\"inverse(x: Tensor, ilens: Tensor | None = None)\",\"training : bool\"]},\"1907\":{\"h\":\"espnet2.layers.houlsby_adapter_layer.Houlsby_Adapter\",\"t\":[\"class espnet2.layers.houlsby_adapter_layer.Houlsby_Adapter(input_size: int, bottleneck: int)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1908\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1909\":{\"h\":\"espnet2.layers.inversible_interface.InversibleInterface\",\"t\":[\"class espnet2.layers.inversible_interface.InversibleInterface\",\"Bases: ABC\",\"abstract inverse(input: Tensor, input_lengths: Tensor | None = None)\"]},\"1910\":{\"h\":\"espnet2.layers.label_aggregation.LabelAggregate\",\"t\":[\"class espnet2.layers.label_aggregation.LabelAggregate(win_length: int = 512, hop_length: int = 128, center: bool = True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(input: Tensor, ilens: Tensor | None = None)\",\"LabelAggregate forward function.\",\"Parameters:\",\"input – (Batch, Nsamples, Label_dim)\",\"ilens – (Batch)\",\"Returns: (Batch, Frames, Label_dim)\",\"Return type: output\",\"training : bool\"]},\"1911\":{\"h\":\"espnet2.layers.sinc_conv.LogCompression\",\"t\":[\"<!-- _espnet2.layers.sinc_conv.LogCompression -->\",\"class espnet2.layers.sinc_conv.LogCompression\",\"Bases: Module\",\"Log Compression Activation.\",\"Activation function log(abs(x) + 1).\",\"Initialize.\",\"forward(x: Tensor)\",\"Forward.\",\"Applies the Log Compression function elementwise on tensor x.\",\"training : bool\"]},\"1912\":{\"h\":\"espnet2.layers.log_mel.LogMel\",\"t\":[\"<!-- _espnet2.layers.log_mel.LogMel -->\",\"class espnet2.layers.log_mel.LogMel(fs: int = 16000, n_fft: int = 512, n_mels: int = 80, fmin: float | None = None, fmax: float | None = None, htk: bool = False, log_base: float | None = None)\",\"Bases: Module\",\"Convert STFT to fbank feats\",\"The arguments is same as librosa.filters.mel\",\"Parameters:\",\"fs – number > 0 [scalar] sampling rate of the incoming signal\",\"n_fft – int > 0 [scalar] number of FFT components\",\"n_mels – int > 0 [scalar] number of Mel bands to generate\",\"fmin – float >= 0 [scalar] lowest frequency (in Hz)\",\"fmax – float >= 0 [scalar] highest frequency (in Hz). If None, use fmax = fs / 2.0\",\"htk – use HTK formula instead of Slaney\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(feat: Tensor, ilens: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1913\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1914\":{\"h\":\"espnet2.layers.mask_along_axis.MaskAlongAxis\",\"t\":[\"class espnet2.layers.mask_along_axis.MaskAlongAxis(mask_width_range: int | Sequence[int] = (0, 30), num_mask: int = 2, dim: int | str = 'time', replace_with_zero: bool = True)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(spec: Tensor, spec_lengths: Tensor | None = None)\",\"Forward function.\",\"Parameters:spec – (Batch, Length, Freq)\",\"training : bool\"]},\"1915\":{\"h\":\"espnet2.layers.mask_along_axis.MaskAlongAxisVariableMaxWidth\",\"t\":[\"class espnet2.layers.mask_along_axis.MaskAlongAxisVariableMaxWidth(mask_width_ratio_range: float | Sequence[float] = (0.0, 0.05), num_mask: int = 2, dim: int | str = 'time', replace_with_zero: bool = True)\",\"Bases: Module\",\"Mask input spec along a specified axis with variable maximum width.\",\"Formula: : max_width = max_width_ratio * seq_len\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(spec: Tensor, spec_lengths: Tensor | None = None)\",\"Forward function.\",\"Parameters:spec – (Batch, Length, Freq)\",\"training : bool\"]},\"1916\":{\"h\":\"espnet2.layers.sinc_conv.MelScale\",\"t\":[\"<!-- _espnet2.layers.sinc_conv.MelScale -->\",\"class espnet2.layers.sinc_conv.MelScale\",\"Bases: object\",\"Mel frequency scale.\",\"classmethod bank(channels: int, fs: float)\",\"Obtain initialization values for the mel scale.\",\"Parameters:\",\"channels – Number of channels.\",\"fs – Sample rate.\",\"Returns: Filter start frequencíes. torch.Tensor: Filter stop frequencies.\",\"Return type: torch.Tensor\",\"static convert(f)\",\"Convert Hz to mel.\",\"static invert(x)\",\"Convert mel to Hz.\"]},\"1917\":{\"h\":\"espnet2.layers.sinc_conv.SincConv\",\"t\":[\"<!-- _espnet2.layers.sinc_conv.SincConv -->\",\"class espnet2.layers.sinc_conv.SincConv(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, window_func: str = 'hamming', scale_type: str = 'mel', fs: int | float = 16000)\",\"Bases: Module\",\"Sinc Convolution.\",\"This module performs a convolution using Sinc filters in time domain as kernel. Sinc filters function as band passes in spectral domain. The filtering is done as a convolution in time domain, and no transformation to spectral domain is necessary.\",\"This implementation of the Sinc convolution is heavily inspired by Ravanelli et al. https://github.com/mravanelli/SincNet, and adapted for the ESpnet toolkit. Combine Sinc convolutions with a log compression activation function, as in: https://arxiv.org/abs/2010.07597\",\"Notes: Currently, the same filters are applied to all input channels. The windowing function is applied on the kernel to obtained a smoother filter, and not on the input values, which is different to traditional ASR.\",\"Initialize Sinc convolutions.\",\"Parameters:\",\"in_channels – Number of input channels.\",\"out_channels – Number of output channels.\",\"kernel_size – Sinc filter kernel size (needs to be an odd number).\",\"stride – See torch.nn.functional.conv1d.\",\"padding – See torch.nn.functional.conv1d.\",\"dilation – See torch.nn.functional.conv1d.\",\"window_func – Window function on the filter, one of [“hamming”, “none”].\",\"fs (str,int,float) – Sample rate of the input data\",\"forward(xs: Tensor)\",\"Sinc convolution forward function.\",\"Parameters:xs – Batch in form of torch.Tensor (B, C_in, D_in).\",\"Returns: Batch in form of torch.Tensor (B, C_out, D_out).\",\"Return type: xs\",\"get_odim(idim: int)\",\"Obtain the output dimension of the filter.\",\"static hamming_window(x: Tensor)\",\"Hamming Windowing function.\",\"init_filters()\",\"Initialize filters with filterbank values.\",\"static none_window(x: Tensor)\",\"Identity-like windowing function.\",\"static sinc(x: Tensor)\",\"Sinc function.\",\"training : bool\"]},\"1918\":{\"h\":\"espnet2.layers.stft.Stft\",\"t\":[\"<!-- _espnet2.layers.stft.Stft -->\",\"class espnet2.layers.stft.Stft(n_fft: int = 512, win_length: int | None = None, hop_length: int = 128, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: Module, InversibleInterface\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(input: Tensor, ilens: Tensor | None = None)\",\"STFT forward function.\",\"Parameters:\",\"input – (Batch, Nsamples) or (Batch, Nsample, Channels)\",\"ilens – (Batch)\",\"Returns: (Batch, Frames, Freq, 2) or (Batch, Frames, Channels, Freq, 2)\",\"Return type: output\",\"inverse(input: Tensor | ComplexTensor, ilens: Tensor | None = None)\",\"Inverse STFT.\",\"Parameters:\",\"input – Tensor(batch, T, F, 2) or ComplexTensor(batch, T, F)\",\"ilens – (batch,)\",\"Returns: (batch, samples) ilens: (batch,)\",\"Return type: wavs\",\"training : bool\"]},\"1919\":{\"h\":\"espnet2.layers.time_warp.TimeWarp\",\"t\":[\"<!-- _espnet2.layers.time_warp.TimeWarp -->\",\"class espnet2.layers.time_warp.TimeWarp(window: int = 80, mode: str = 'bicubic')\",\"Bases: Module\",\"Time warping using torch.interpolate.\",\"Parameters:\",\"window – time warp parameter\",\"mode – Interpolate mode\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x: Tensor, x_lengths: Tensor | None = None)\",\"Forward function.\",\"Parameters:\",\"x – (Batch, Time, Freq)\",\"x_lengths – (Batch,)\",\"training : bool\"]},\"1920\":{\"h\":\"espnet2.layers.utterance_mvn.UtteranceMVN\",\"t\":[\"<!-- _espnet2.layers.utterance_mvn.UtteranceMVN -->\",\"class espnet2.layers.utterance_mvn.UtteranceMVN(norm_means: bool = True, norm_vars: bool = False, eps: float = 1e-20)\",\"Bases: AbsNormalize\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x: Tensor, ilens: Tensor | None = None)\",\"Forward function\",\"Parameters:\",\"x – (B, L, …)\",\"ilens – (B,)\",\"training : bool\"]},\"1921\":{\"h\":\"espnet2.layers.augmentation.bandpass_filtering\",\"t\":[\"espnet2.layers.augmentation.bandpass_filtering(waveform, sample_rate: int, center_freq: int = 3000, Q: float = 0.707, const_skirt_gain: bool = False)\",\"Bandpass filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"center_freq_freq (int) – filter’s center_freq frequency\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"const_skirt_gain (bool) – If True, uses a constant skirt gain (peak gain = Q). If False, uses a constant 0dB peak gain.\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1922\":{\"h\":\"espnet2.layers.augmentation.bandreject_filtering\",\"t\":[\"espnet2.layers.augmentation.bandreject_filtering(waveform, sample_rate: int, center_freq: int = 3000, Q: float = 0.707)\",\"Two-pole band-reject filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"center_freq_freq (int) – filter’s center_freq frequency\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1923\":{\"h\":\"espnet2.layers.augmentation.bandwidth_limitation\",\"t\":[\"espnet2.layers.augmentation.bandwidth_limitation(waveform, sample_rate: int, res_type='random')\",\"Apply the bandwidth limitation distortion to the input signal.\",\"Parameters:\",\"waveform (np.ndarray) – a single speech sample (…, Time)\",\"sample_rate (int) – input sampling rate in Hz\",\"fs_new (int) – effective sampling rate in Hz\",\"res_type (str) – resampling method\",\"Returns: bandwidth-limited speech sample (…, Time)\",\"Return type: ret (np.ndarray)\"]},\"1924\":{\"h\":\"espnet2.layers.create_adapter_utils.check_target_module_exists\",\"t\":[\"espnet2.layers.create_adapter_utils.check_target_module_exists(key: str, target_modules: List[str])\",\"Check if the target_modules matchs the given key.\"]},\"1925\":{\"h\":\"espnet2.layers.augmentation.clipping\",\"t\":[\"<!-- _espnet2.layers.augmentation.clipping -->\",\"espnet2.layers.augmentation.clipping(waveform, sample_rate: int, min_quantile: float = 0.0, max_quantile: float = 0.9)\",\"Apply the clipping distortion to the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz (not used)\",\"min_quantile (float) – lower bound on the total percent of samples to be clipped\",\"max_quantile (float) – upper bound on the total percent of samples to be clipped\",\"Returns: clipped signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1926\":{\"h\":\"espnet2.layers.augmentation.codecs\",\"t\":[\"<!-- _espnet2.layers.augmentation.codecs -->\",\"espnet2.layers.augmentation.codecs(waveform, sample_rate: int, format: str, compression: float | None = None, encoding: str | None = None, bits_per_sample: int | None = None)\",\"Apply the specified codecs to the input signal.\",\"Warning: Wait until torchaudio 2.1 for this function to work.\"]},\"1927\":{\"h\":\"NOTE\",\"t\":[\"This function only supports CPU backend.\",\"The GSM codec can be used to emulate phone line channel effects.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"format (str) – file format. Valid values are “wav”, “mp3”, “ogg”, “vorbis”, “amr-nb”, “amb”, “flac”, “sph”, “gsm”, and “htk”.\",\"compression (floatorNone,optional) –\",\"used for formats other than WAV\",\"For more details see torchaudio.backend.sox_io_backend.save().\",\"encoding (strorNone,optional) – change the encoding for the supported formats Valid values are “PCM_S” (signed integer Linear PCM), “PCM_U” (unsigned integer Linear PCM), “PCM_F” (floating point PCM), “ULAW” (mu-law), and “ALAW” (a-law). For more details see torchaudio.backend.sox_io_backend.save().\",\"bits_per_sample (intorNone,optional) – change the bit depth for the supported formats For more details see torchaudio.backend.sox_io_backend.save().\",\"Returns: compressed signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1928\":{\"h\":\"espnet2.layers.augmentation.contrast\",\"t\":[\"<!-- _espnet2.layers.augmentation.contrast -->\",\"espnet2.layers.augmentation.contrast(waveform, sample_rate: int = 16000, enhancement_amount: float = 75.0)\",\"Apply contrast effect to the input signal to make it sound louder.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz (not used)\",\"enhancement_amount (float) – controls the amount of the enhancement Allowed range of values for enhancement_amount : 0-100 Note that enhancement_amount = 0 still gives a significant contrast enhancement.\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1929\":{\"h\":\"espnet2.layers.augmentation.corrupt_phase\",\"t\":[\"<!-- _espnet2.layers.augmentation.corrupt_phase -->\",\"espnet2.layers.augmentation.corrupt_phase(waveform, sample_rate, scale: float = 0.5, n_fft: float = 0.032, win_length: float | None = None, hop_length: float = 0.008, window: str | None = 'hann')\",\"Adding random noise to the phase of input waveform.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"scale (float) – scale factor for the phase noise\",\"n_fft (float) – length of FFT (in second)\",\"win_length (floatorNone) – The window length (in second) used for STFT If None, it is treated as equal to n_fft\",\"hop_length (float) – The hop size (in second) used for STFT\",\"window (strorNone) – The windowing function applied to the signal after padding with zeros\",\"Returns: phase-corrupted signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1930\":{\"h\":\"espnet2.layers.create_adapter.create_adapter\",\"t\":[\"espnet2.layers.create_adapter.create_adapter(model: Module, adapter: str, adapter_conf: dict)\",\"Create adapter for the base model.\",\"Parameters:\",\"model (torch.nn.Module) – Base model to be adapted.\",\"adapter_type (str) – Name of adapter\",\"adapter_conf (dict) – Configuration for the adapter e.g. {“rank”: 8, “alpha”: 8, …} for lora\"]},\"1931\":{\"h\":\"espnet2.layers.create_adapter_fn.create_houlsby_adapter\",\"t\":[\"espnet2.layers.create_adapter_fn.create_houlsby_adapter(model: Module, bottleneck: int = 32, target_layers: List[int] = [])\"]},\"1932\":{\"h\":\"espnet2.layers.create_adapter_fn.create_lora_adapter\",\"t\":[\"espnet2.layers.create_adapter_fn.create_lora_adapter(model: Module, rank: int = 8, alpha: int = 8, dropout_rate: float = 0.0, target_modules: List[str] = ['query'], bias_type: str | None = 'none')\",\"Create LoRA adapter for the base model.\",\"See: https://arxiv.org/pdf/2106.09685.pdf\",\"Parameters:\",\"model (torch.nn.Module) – Base model to be adapted.\",\"rank (int) – Rank of LoRA matrices. Defaults to 8.\",\"alpha (int) – Constant number for LoRA scaling. Defaults to 8.\",\"dropout_rate (float) – Dropout probability for LoRA layers. Defaults to 0.0.\",\"target_modules (List *[*str]) – List of module(s) to apply LoRA adaptation. e.g. [“query”, “key”, “value”] for all layers, while [“encoder.encoders.blocks.0.attn.key”] for a specific layer.\",\"bias_type (str) – Bias training type for LoRA adaptaion, can be one of [“none”, “all”, “lora_only”]. “none” means not training any bias vectors; “all” means training all bias vectors, include LayerNorm biases; “lora_only” means only training bias vectors in LoRA adapted modules.\"]},\"1933\":{\"h\":\"espnet2.layers.create_adapter_fn.create_new_houlsby_module\",\"t\":[\"espnet2.layers.create_adapter_fn.create_new_houlsby_module(target_module: Module, bottleneck: int)\",\"Create a new houlsby adapter module for the given target module.\",\"Currently, only support: Wav2Vec2EncoderLayerStableLayerNorm & TransformerSentenceEncoderLayer\"]},\"1934\":{\"h\":\"espnet2.layers.create_adapter_fn.create_new_lora_module\",\"t\":[\"espnet2.layers.create_adapter_fn.create_new_lora_module(target_module: Module, rank: int, alpha: int, dropout_rate: float)\",\"Create a new lora module for the given target module.\"]},\"1935\":{\"h\":\"espnet2.layers.augmentation.deemphasis\",\"t\":[\"<!-- _espnet2.layers.augmentation.deemphasis -->\",\"espnet2.layers.augmentation.deemphasis(waveform, sample_rate: int, coeff: float = 0.97)\",\"De-emphasize a waveform along the time dimension.\",\"y[i] = x[i] + coeff * y[i - 1]\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz (not used)\",\"coeff (float) – de-emphasis coefficient. Typically between 0.0 and 1.0.\",\"Returns: de-emphasized signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1936\":{\"h\":\"espnet2.layers.augmentation.equalization_filtering\",\"t\":[\"espnet2.layers.augmentation.equalization_filtering(waveform, sample_rate: int, center_freq: int = 1000, gain: float = 0.0, Q: float = 0.707)\",\"Equalization filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"center_freq (int) – filter’s center frequency\",\"gain (floatortorch.Tensor) – desired gain at the boost (or attenuation) in dB\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1937\":{\"h\":\"espnet2.layers.create_adapter_utils.get_submodules\",\"t\":[\"espnet2.layers.create_adapter_utils.get_submodules(model: Module, key: str)\",\"Return the submodules of the given key.\"]},\"1938\":{\"h\":\"espnet2.layers.augmentation.highpass_filtering\",\"t\":[\"espnet2.layers.augmentation.highpass_filtering(waveform, sample_rate: int, cutoff_freq: int = 3000, Q: float = 0.707)\",\"Highpass filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"cutoff_freq (int) – filter cutoff frequency\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1939\":{\"h\":\"espnet2.layers.augmentation.lowpass_filtering\",\"t\":[\"espnet2.layers.augmentation.lowpass_filtering(waveform, sample_rate: int, cutoff_freq: int = 1000, Q: float = 0.707)\",\"Lowpass filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"cutoff_freq (int) – filter cutoff frequency\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1940\":{\"h\":\"espnet2.layers.mask_along_axis.mask_along_axis\",\"t\":[\"espnet2.layers.mask_along_axis.mask_along_axis(spec: Tensor, spec_lengths: Tensor, mask_width_range: Sequence[int] = (0, 30), dim: int = 1, num_mask: int = 2, replace_with_zero: bool = True)\",\"Apply mask along the specified direction.\",\"Parameters:\",\"spec – (Batch, Length, Freq)\",\"spec_lengths – (Length): Not using lengths in this implementation\",\"mask_width_range – Select the width randomly between this range\"]},\"1941\":{\"h\":\"espnet2.layers.augmentation.pitch_shift\",\"t\":[\"<!-- _espnet2.layers.augmentation.pitch_shift -->\",\"espnet2.layers.augmentation.pitch_shift(waveform, sample_rate: int, n_steps: int, bins_per_octave: int = 12, n_fft: float = 0.032, win_length: float | None = None, hop_length: float = 0.008, window: str | None = 'hann')\",\"Shift the pitch of a waveform by n_steps steps.\",\"Note: this function is slow.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"n_steps (int) – the (fractional) steps to shift the pitch -4 for shifting pitch down by 4/bins_per_octave octaves 4 for shifting pitch up by 4/bins_per_octave octaves\",\"bins_per_octave (int) – number of steps per octave\",\"n_fft (float) – length of FFT (in second)\",\"win_length (floatorNone) – The window length (in second) used for STFT If None, it is treated as equal to n_fft\",\"hop_length (float) – The hop size (in second) used for STFT\",\"window (strorNone) – The windowing function applied to the signal after padding with zeros\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1942\":{\"h\":\"espnet2.layers.augmentation.polarity_inverse\",\"t\":[\"espnet2.layers.augmentation.polarity_inverse(waveform, sample_rate)\"]},\"1943\":{\"h\":\"espnet2.layers.augmentation.preemphasis\",\"t\":[\"<!-- _espnet2.layers.augmentation.preemphasis -->\",\"espnet2.layers.augmentation.preemphasis(waveform, sample_rate: int, coeff: float = 0.97)\",\"Pre-emphasize a waveform along the time dimension.\",\"y[i] = x[i] - coeff * x[i - 1]\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz (not used)\",\"coeff (float) – pre-emphasis coefficient. Typically between 0.0 and 1.0.\",\"Returns: pre-emphasized signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1944\":{\"h\":\"espnet2.layers.create_adapter_utils.replace_module\",\"t\":[\"espnet2.layers.create_adapter_utils.replace_module(parent_module: Module, child_name: str, old_module: Module, new_module: Module)\",\"Replace the target module with the new module.\"]},\"1945\":{\"h\":\"espnet2.layers.augmentation.reverse\",\"t\":[\"<!-- _espnet2.layers.augmentation.reverse -->\",\"espnet2.layers.augmentation.reverse(waveform, sample_rate)\"]},\"1946\":{\"h\":\"espnet2.layers.augmentation.speed_perturb\",\"t\":[\"<!-- _espnet2.layers.augmentation.speed_perturb -->\",\"espnet2.layers.augmentation.speed_perturb(waveform, sample_rate: int, factor: float)\",\"Speed perturbation which also changes the pitch.\",\"Note: This function should be used with caution as it changes the signal duration.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"factor (float) – speed factor (e.g., 0.9 for 90% speed)\",\"lengths (torch.Tensor) – lengths of the input signals\",\"Returns: perturbed signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1947\":{\"h\":\"espnet2.layers.augmentation.time_stretch\",\"t\":[\"<!-- _espnet2.layers.augmentation.time_stretch -->\",\"espnet2.layers.augmentation.time_stretch(waveform, sample_rate: int, factor: float, n_fft: float = 0.032, win_length: float | None = None, hop_length: float = 0.008, window: str | None = 'hann')\",\"Time scaling (speed up in time without modifying pitch) via phase vocoder.\",\"Note: This function should be used with caution as it changes the signal duration.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"factor (float) – speed-up factor (e.g., 0.9 for 90% speed and 1.3 for 130% speed)\",\"n_fft (float) – length of FFT (in second)\",\"win_length (floatorNone) – The window length (in second) used for STFT If None, it is treated as equal to n_fft\",\"hop_length (float) – The hop size (in second) used for STFT\",\"window (strorNone) – The windowing function applied to the signal after padding with zeros\",\"Returns: perturbed signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1948\":{\"h\":\"espnet2.layers.time_warp.time_warp\",\"t\":[\"<!-- _espnet2.layers.time_warp.time_warp -->\",\"espnet2.layers.time_warp.time_warp(x: Tensor, window: int = 80, mode: str = 'bicubic')\",\"Time warping using torch.interpolate.\",\"Parameters:\",\"x – (Batch, Time, Freq)\",\"window – time warp parameter\",\"mode – Interpolate mode\"]},\"1949\":{\"h\":\"espnet2.layers.utterance_mvn.utterance_mvn\",\"t\":[\"<!-- _espnet2.layers.utterance_mvn.utterance_mvn -->\",\"espnet2.layers.utterance_mvn.utterance_mvn(x: Tensor, ilens: Tensor | None = None, norm_means: bool = True, norm_vars: bool = False, eps: float = 1e-20)\",\"Apply utterance mean and variance normalization\",\"Parameters:\",\"x – (B, T, D), assumed zero padded\",\"ilens – (B,)\",\"norm_means –\",\"norm_vars –\",\"eps –\"]},\"1950\":{\"h\":\"espnet2.layers.augmentation.weighted_sample_without_replacement\",\"t\":[\"espnet2.layers.augmentation.weighted_sample_without_replacement(population, weights, k, rng=<module 'random' from '/usr/lib/python3.8/random.py'>)\"]},\"1951\":{\"h\":\"espnet2.lm.abs_model.AbsLM\",\"t\":[\"<!-- _espnet2.lm.abs_model.AbsLM -->\",\"class espnet2.lm.abs_model.AbsLM\",\"Bases: Module, BatchScorerInterface, ABC\",\"The abstract LM class\",\"To share the loss calculation way among different models, We uses delegate pattern here: The instance of this class should be passed to “LanguageModel”\",\">>> from espnet2.lm.abs_model import AbsLM >>> lm = AbsLM() >>> model = LanguageESPnetModel(lm=lm)\",\"This “model” is one of mediator objects for “Task” class.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, hidden: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1952\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1953\":{\"h\":\"espnet2.lm.espnet_model.ESPnetLanguageModel\",\"t\":[\"class espnet2.lm.espnet_model.ESPnetLanguageModel(lm: AbsLM, vocab_size: int, ignore_id: int = 0)\",\"Bases: AbsESPnetModel\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batchify_nll(text: Tensor, text_lengths: Tensor, batch_size: int = 100)\",\"Compute negative log likelihood(nll) from transformer language model\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param text: (Batch, Length) :param text_lengths: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase\",\"collect_feats(text: Tensor, text_lengths: Tensor, **kwargs)\",\"forward(text: Tensor, text_lengths: Tensor, **kwargs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1954\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"nll(text: Tensor, text_lengths: Tensor, max_length: int | None = None)\",\"Compute negative log likelihood(nll)\",\"Normally, this function is called in batchify_nll. :param text: (Batch, Length) :param text_lengths: (Batch,) :param max_lengths: int\",\"training : bool\"]},\"1955\":{\"h\":\"espnet2.lm.espnet_model_multitask.ESPnetMultitaskLanguageModel\",\"t\":[\"class espnet2.lm.espnet_model_multitask.ESPnetMultitaskLanguageModel(lm: AbsLM, vocab_size: int, token_list: Tuple[str, ...] | List[str], ignore_id: int = 0, lsm_weight: float = 0.0, length_normalized_loss: bool = False, sos_syms: List[str] = ['<generatetext>', '<generatespeech>'], eos_sym: str = '<sos/eos>')\",\"Bases: AbsESPnetModel\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batchify_nll(text: Tensor, text_lengths: Tensor, batch_size: int = 100)\",\"Compute negative log likelihood(nll) from transformer language model\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param text: (Batch, Length) :param text_lengths: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase\",\"collect_feats(text: Tensor, text_lengths: Tensor, **kwargs)\",\"forward(text: Tensor, text_lengths: Tensor, **kwargs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1956\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"nll(text: Tensor, text_lengths: Tensor, max_length: int | None = None)\",\"Compute negative log likelihood (nll)\",\"NOTE(yifan): We only use nll to calculate perplexity, : so there is no condition in each sentence.\",\"Normally, this function is called in batchify_nll. :param text: (Batch, Length) :param text_lengths: (Batch,) :param max_lengths: int\",\"training : bool\"]},\"1957\":{\"h\":\"espnet2.lm.huggingface_pretrained_opt_lm.HuggingfaceOPTModel\",\"t\":[\"class espnet2.lm.huggingface_pretrained_opt_lm.HuggingfaceOPTModel(vocab_size: int, opt_name: str)\",\"Bases: AbsLM\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, vocab_size) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(input: Tensor, hidden: None)\",\"Compute LM loss value from buffer sequences.\",\"Parameters:\",\"input (torch.Tensor) – Input ids. (batch, len)\",\"hidden (torch.Tensor) – Target ids. (batch, len)\",\"reload_pretrained_parameters()\",\"score(y: Tensor, state: Any, x: Tensor)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (vocab_size) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"training : bool\"]},\"1958\":{\"h\":\"espnet2.lm.seq_rnn_lm.SequentialRNNLM\",\"t\":[\"<!-- _espnet2.lm.seq_rnn_lm.SequentialRNNLM -->\",\"class espnet2.lm.seq_rnn_lm.SequentialRNNLM(vocab_size: int, unit: int = 650, nhid: int | None = None, nlayers: int = 2, dropout_rate: float = 0.0, tie_weights: bool = False, rnn_type: str = 'lstm', ignore_id: int = 0)\",\"Bases: AbsLM\",\"Sequential RNNLM.\",\"SEE ALSO\",\"https://github.com/pytorch/examples/blob/4581968193699de14b56527296262dd76ab43557/word_language_model/model.py\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: Tensor, xs: Tensor)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(input: Tensor, hidden: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1959\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"score(y: Tensor, state: Tensor | Tuple[Tensor, Tensor], x: Tensor)\",\"Score new token.\",\"Parameters:\",\"y – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x – 2D encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (n_vocab) and next state for ys\",\"training : bool\",\"zero_state()\",\"Initialize LM state filled with zero values.\"]},\"1960\":{\"h\":\"espnet2.lm.transformer_lm.TransformerLM\",\"t\":[\"<!-- _espnet2.lm.transformer_lm.TransformerLM -->\",\"class espnet2.lm.transformer_lm.TransformerLM(vocab_size: int, pos_enc: str | None = None, embed_unit: int = 128, att_unit: int = 256, head: int = 2, unit: int = 1024, layer: int = 4, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1)\",\"Bases: AbsLM\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, vocab_size) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(input: Tensor, hidden: None)\",\"Compute LM loss value from buffer sequences.\",\"Parameters:\",\"input (torch.Tensor) – Input ids. (batch, len)\",\"hidden (torch.Tensor) – Target ids. (batch, len)\",\"score(y: Tensor, state: Any, x: Tensor)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (vocab_size) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"training : bool\"]},\"1961\":{\"h\":\"espnet2.main_funcs.pack_funcs.Archiver\",\"t\":[\"<!-- _espnet2.main_funcs.pack_funcs.Archiver -->\",\"class espnet2.main_funcs.pack_funcs.Archiver(file, mode='r')\",\"Bases: object\",\"add(filename, arcname=None, recursive: bool = True)\",\"addfile(info, fileobj)\",\"close()\",\"extract(info, path=None)\",\"extractfile(info, mode='r')\",\"generate_info(name, size)\",\"Generate TarInfo using system information\",\"get_name_from_info(info)\"]},\"1962\":{\"h\":\"espnet2.main_funcs.average_nbest_models.average_nbest_models\",\"t\":[\"espnet2.main_funcs.average_nbest_models.average_nbest_models(output_dir: Path, reporter: Reporter, best_model_criterion: Sequence[Sequence[str]], nbest: Collection[int] | int, suffix: str | None = None)\",\"Generate averaged model from n-best models\",\"Parameters:\",\"output_dir – The directory contains the model file for each epoch\",\"reporter – Reporter instance\",\"best_model_criterion – Give criterions to decide the best model. e.g. [(“valid”, “loss”, “min”), (“train”, “acc”, “max”)]\",\"nbest – Number of best model files to be averaged\",\"suffix – A suffix added to the averaged model file name\"]},\"1963\":{\"h\":\"espnet2.main_funcs.calculate_all_attentions.calculate_all_attentions\",\"t\":[\"espnet2.main_funcs.calculate_all_attentions.calculate_all_attentions(model: AbsESPnetModel, batch: Dict[str, Tensor])\",\"Derive the outputs from the all attention layers\",\"Parameters:\",\"model –\",\"batch – same as forward\",\"Returns: A dict of a list of tensor. key_names x batch x (D1, D2, …)\",\"Return type: return_dict\"]},\"1964\":{\"h\":\"espnet2.main_funcs.collect_stats.collect_stats\",\"t\":[\"espnet2.main_funcs.collect_stats.collect_stats(model: AbsESPnetModel | None, train_iter: Iterable[Tuple[List[str], Dict[str, Tensor]]], valid_iter: Iterable[Tuple[List[str], Dict[str, Tensor]]], output_dir: Path, ngpu: int | None, log_interval: int | None, write_collected_feats: bool)\",\"Perform on collect_stats mode.\",\"Running for deriving the shape information from data and gathering statistics. This method is used before executing train().\"]},\"1965\":{\"h\":\"espnet2.main_funcs.pack_funcs.find_path_and_change_it_recursive\",\"t\":[\"espnet2.main_funcs.pack_funcs.find_path_and_change_it_recursive(value, src: str, tgt: str)\"]},\"1966\":{\"h\":\"espnet2.main_funcs.pack_funcs.get_dict_from_cache\",\"t\":[\"espnet2.main_funcs.pack_funcs.get_dict_from_cache(meta: Path | str)\"]},\"1967\":{\"h\":\"espnet2.main_funcs.pack_funcs.pack\",\"t\":[\"<!-- _espnet2.main_funcs.pack_funcs.pack -->\",\"espnet2.main_funcs.pack_funcs.pack(files: Dict[str, str | Path], yaml_files: Dict[str, str | Path], outpath: str | Path, option: Iterable[str | Path] = ())\"]},\"1968\":{\"h\":\"espnet2.main_funcs.pack_funcs.unpack\",\"t\":[\"<!-- _espnet2.main_funcs.pack_funcs.unpack -->\",\"espnet2.main_funcs.pack_funcs.unpack(input_archive: Path | str, outpath: Path | str, use_cache: bool = True)\",\"Scan all files in the archive file and return as a dict of files.\"]},\"1969\":{\"h\":\"Examples\",\"t\":[\"tarfile: : model.pth some1.file some2.file\",\">>> unpack(\\\"tarfile\\\", \\\"out\\\") {'asr_model_file': 'out/model.pth'}\"]},\"1970\":{\"h\":\"espnet2.mt.espnet_model.ESPnetMTModel\",\"t\":[\"<!-- _espnet2.mt.espnet_model.ESPnetMTModel -->\",\"class espnet2.mt.espnet_model.ESPnetMTModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder, src_vocab_size: int = 0, src_token_list: Tuple[str, ...] | List[str] = [], ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_bleu: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', extract_feats_in_collect_stats: bool = True, share_decoder_input_output_embed: bool = False, share_encoder_decoder_input_embed: bool = False)\",\"Bases: AbsESPnetModel\",\"Encoder-Decoder model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(text: Tensor, text_lengths: Tensor, src_text: Tensor, src_text_lengths: Tensor, **kwargs)\",\"encode(src_text: Tensor, src_text_lengths: Tensor)\",\"Frontend + Encoder. Note that this method is used by mt_inference.py\",\"Parameters:\",\"src_text – (Batch, Length, …)\",\"src_text_lengths – (Batch, )\",\"forward(text: Tensor, text_lengths: Tensor, src_text: Tensor, src_text_lengths: Tensor, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"src_text – (Batch, length)\",\"src_text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\",\"training : bool\"]},\"1971\":{\"h\":\"espnet2.mt.frontend.embedding.Embedding\",\"t\":[\"<!-- _espnet2.mt.frontend.embedding.Embedding -->\",\"class espnet2.mt.frontend.embedding.Embedding(input_size: int = 400, embed_dim: int = 400, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, positional_dropout_rate: float = 0.1)\",\"Bases: AbsFrontend\",\"Embedding Frontend for text based inputs.\",\"Initialize.\",\"Parameters:\",\"input_size – Number of input tokens.\",\"embed_dim – Embedding Size.\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Apply a sliding window on the input.\",\"Parameters:\",\"input – Input (B, T) or (B, T,D), with D.\",\"input_lengths – Input lengths within batch.\",\"Returns: Output with dimensions (B, T, D). Tensor: Output lengths within batch.\",\"Return type: Tensor\",\"output_size()\",\"Return output length of feature dimension D, i.e. the embedding dim.\",\"training : bool\"]},\"1972\":{\"h\":\"espnet2.optimizers.sgd.SGD\",\"t\":[\"<!-- _espnet2.optimizers.sgd.SGD -->\",\"class espnet2.optimizers.sgd.SGD(params, lr: float = 0.1, momentum: float = 0.0, dampening: float = 0.0, weight_decay: float = 0.0, nesterov: bool = False)\",\"Bases: SGD\",\"Thin inheritance of torch.optim.SGD to bind the required arguments, ‘lr’\",\"Note that the arguments of the optimizer invoked by AbsTask.main() must have default value except for ‘param’.\",\"I can’t understand why only SGD.lr doesn’t have the default value.\"]},\"1973\":{\"h\":\"espnet2.optimizers.optim_groups.add_optimizer_hooks\",\"t\":[\"espnet2.optimizers.optim_groups.add_optimizer_hooks(model, bias_weight_decay=False, normalization_weight_decay=False)\",\"Set zero weight decay for some params\",\"Set weight_decay=0.0 for parameters in model.no_weight_decay, for parameters with attribute _no_weight_decayTrue, for bias parameters if bias_weight_decayFalse, for normalization parameters if normalization_weight_decay==False\",\"See: https://discuss.pytorch.org/t/weight-decay-only-for-weights-of-nn-linear-and-nn-conv/114348 # noqa\"]},\"1974\":{\"h\":\"espnet2.optimizers.optim_groups.configure_optimizer\",\"t\":[\"espnet2.optimizers.optim_groups.configure_optimizer(model, optim_class, optim_conf, weight_decay_conf)\"]},\"1975\":{\"h\":\"espnet2.s2t.espnet_model.ESPnetS2TModel\",\"t\":[\"<!-- _espnet2.s2t.espnet_model.ESPnetS2TModel -->\",\"class espnet2.s2t.espnet_model.ESPnetS2TModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder | None, ctc: CTC, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', sym_sos: str = '<sos>', sym_eos: str = '<eos>', sym_sop: str = '<sop>', sym_na: str = '<na>', extract_feats_in_collect_stats: bool = True)\",\"Bases: AbsESPnetModel\",\"CTC-attention hybrid Encoder-Decoder model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, text_prev: Tensor, text_prev_lengths: Tensor, text_ctc: Tensor, text_ctc_lengths: Tensor, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor)\",\"Frontend + Encoder. Note that this method is used by s2t_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, text_prev: Tensor, text_prev_lengths: Tensor, text_ctc: Tensor, text_ctc_lengths: Tensor, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"text_prev – (Batch, Length)\",\"text_prev_lengths – (Batch,)\",\"text_ctc – (Batch, Length)\",\"text_ctc_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\",\"training : bool\"]},\"1976\":{\"h\":\"espnet2.s2st.aux_attention.abs_aux_attention.AbsS2STAuxAttention\",\"t\":[\"class espnet2.s2st.aux_attention.abs_aux_attention.AbsS2STAuxAttention\",\"Bases: Module, ABC\",\"Base class for all S2ST auxiliary attention modules.\",\"Refer to https://arxiv.org/abs/2107.08661\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward()\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1977\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\",\"training : bool\"]},\"1978\":{\"h\":\"espnet2.s2st.losses.abs_loss.AbsS2STLoss\",\"t\":[\"<!-- _espnet2.s2st.losses.abs_loss.AbsS2STLoss -->\",\"class espnet2.s2st.losses.abs_loss.AbsS2STLoss\",\"Bases: Module, ABC\",\"Base class for all S2ST loss modules.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward()\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1979\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\",\"training : bool\"]},\"1980\":{\"h\":\"espnet2.s2st.synthesizer.abs_synthesizer.AbsSynthesizer\",\"t\":[\"class espnet2.s2st.synthesizer.abs_synthesizer.AbsSynthesizer\",\"Bases: Module, ABC\",\"TTS abstract class.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input_states: Tensor, input_states_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, **kwargs)\",\"Calculate outputs and return the loss tensor.\",\"abstract inference(input_states: Tensor, **kwargs)\",\"Return predicted output as a dict.\",\"property require_raw_speech\",\"Return whether or not raw_speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"1981\":{\"h\":\"espnet2.s2st.tgt_feats_extract.abs_tgt_feats_extract.AbsTgtFeatsExtract\",\"t\":[\"class espnet2.s2st.tgt_feats_extract.abs_tgt_feats_extract.AbsTgtFeatsExtract\",\"Bases: AbsFeatsExtract, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1982\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract get_parameters()\",\"abstract output_size()\",\"abstract spectrogram()\",\"training : bool\"]},\"1983\":{\"h\":\"espnet2.s2st.synthesizer.translatotron2.DurationPredictor\",\"t\":[\"class espnet2.s2st.synthesizer.translatotron2.DurationPredictor(cfg)\",\"Bases: Module\",\"Non-Attentive Tacotron (NAT) Duration Predictor module.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(encoder_outputs, input_lengths=None)\",\"Forward Duration Predictor\",\"Parameters:\",\"encoder_outputs – [batch_size, hidden_length, encoder_lstm_dim]\",\"input_lengths – [batch_size, hidden_length]\",\"Returns: [batch_size, hidden_length]\",\"training : bool\"]},\"1984\":{\"h\":\"espnet2.s2st.espnet_model.ESPnetS2STModel\",\"t\":[\"<!-- _espnet2.s2st.espnet_model.ESPnetS2STModel -->\",\"class espnet2.s2st.espnet_model.ESPnetS2STModel(s2st_type: str, frontend: AbsFrontend | None, tgt_feats_extract: AbsTgtFeatsExtract | None, specaug: AbsSpecAug | None, src_normalize: AbsNormalize | None, tgt_normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, asr_decoder: AbsDecoder | None, st_decoder: AbsDecoder | None, aux_attention: AbsS2STAuxAttention | None, unit_encoder: AbsEncoder | None, synthesizer: AbsSynthesizer | None, asr_ctc: CTC | None, st_ctc: CTC | None, losses: Dict[str, AbsS2STLoss], tgt_vocab_size: int | None, tgt_token_list: Tuple[str, ...] | List[str] | None, src_vocab_size: int | None, src_token_list: Tuple[str, ...] | List[str] | None, unit_vocab_size: int | None, unit_token_list: Tuple[str, ...] | List[str] | None, ignore_id: int = -1, report_cer: bool = True, report_wer: bool = True, report_bleu: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', extract_feats_in_collect_stats: bool = True)\",\"Bases: AbsESPnetModel\",\"ESPnet speech-to-speech translation model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(src_speech: Tensor, src_speech_lengths: Tensor, tgt_speech: Tensor, tgt_speech_lengths: Tensor, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor, return_all_hs: bool = False, **kwargs)\",\"Frontend + Encoder. Note that this method is used by st_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forward(src_speech: Tensor, src_speech_lengths: Tensor, tgt_speech: Tensor, tgt_speech_lengths: Tensor, tgt_text: Tensor | None = None, tgt_text_lengths: Tensor | None = None, src_text: Tensor | None = None, src_text_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1985\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"inference(src_speech: Tensor, src_speech_lengths: Tensor | None = None, tgt_speech: Tensor | None = None, tgt_speech_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_att_constraint: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False)\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"1986\":{\"h\":\"espnet2.s2st.synthesizer.translatotron2.GaussianUpsampling\",\"t\":[\"class espnet2.s2st.synthesizer.translatotron2.GaussianUpsampling\",\"Bases: Module\",\"Gaussian Upsample.\",\"Non-attention Tacotron: : - https://arxiv.org/abs/2010.04301\",\"this source code is implemenation of the ExpressiveTacotron from BridgetteSong : - https://github.com/BridgetteSong/ExpressiveTacotron/\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(encoder_outputs, durations, vars, input_lengths=None)\",\"Gaussian upsampling.\",\"Parameters:\",\"encoder_outputs – encoder outputs [batch_size, hidden_length, dim]\",\"durations – phoneme durations [batch_size, hidden_length]\",\"vars – phoneme attended ranges [batch_size, hidden_length]\",\"input_lengths – [batch_size]\",\"Returns: upsampled encoder_output : [batch_size, frame_length, dim]\",\"Return type: encoder_upsampling_outputs\",\"get_mask_from_lengths(lengths, max_len=None)\",\"training : bool\"]},\"1987\":{\"h\":\"espnet2.s2st.tgt_feats_extract.linear_spectrogram.LinearSpectrogram\",\"t\":[\"class espnet2.s2st.tgt_feats_extract.linear_spectrogram.LinearSpectrogram(n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: AbsTgtFeatsExtract\",\"Linear amplitude spectrogram.\",\"Stft -> amplitude-spec\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1988\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"Return the parameters required by Vocoder.\",\"output_size()\",\"spectrogram()\",\"training : bool\"]},\"1989\":{\"h\":\"espnet2.s2st.tgt_feats_extract.log_mel_fbank.LogMelFbank\",\"t\":[\"class espnet2.s2st.tgt_feats_extract.log_mel_fbank.LogMelFbank(fs: int | str = 16000, n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True, n_mels: int = 80, fmin: int | None = 80, fmax: int | None = 7600, htk: bool = False, log_base: float | None = 10.0)\",\"Bases: AbsTgtFeatsExtract\",\"Conventional frontend structure for TTS.\",\"Stft -> amplitude-spec -> Log-Mel-Fbank\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1990\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"Return the parameters required by Vocoder\",\"output_size()\",\"spectrogram()\",\"training : bool\"]},\"1991\":{\"h\":\"espnet2.s2st.tgt_feats_extract.log_spectrogram.LogSpectrogram\",\"t\":[\"class espnet2.s2st.tgt_feats_extract.log_spectrogram.LogSpectrogram(n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: AbsTgtFeatsExtract\",\"Conventional frontend structure for ASR\",\"Stft -> log-amplitude-spec\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1992\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"Return the parameters required by Vocoder\",\"output_size()\",\"spectrogram()\",\"training : bool\"]},\"1993\":{\"h\":\"espnet2.s2st.aux_attention.multihead.MultiHeadAttention\",\"t\":[\"class espnet2.s2st.aux_attention.multihead.MultiHeadAttention(n_head: int = 4, n_feat: int = 512, dropout_rate: float = 0.0)\",\"Bases: AbsS2STAuxAttention\",\"Multihead Attention for S2ST.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(query: Tensor, key: Tensor, value: Tensor, mask: Tensor)\",\"Forward.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"training : bool\"]},\"1994\":{\"h\":\"espnet2.s2st.synthesizer.translatotron2.Prenet\",\"t\":[\"class espnet2.s2st.synthesizer.translatotron2.Prenet(idim, units=128, num_layers=2, dropout=0.5)\",\"Bases: Module\",\"Non-Attentive Tacotron (NAT) Prenet.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1995\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1996\":{\"h\":\"espnet2.s2st.losses.attention_loss.S2STAttentionLoss\",\"t\":[\"class espnet2.s2st.losses.attention_loss.S2STAttentionLoss(vocab_size: int, padding_idx: int = -1, weight: float = 1.0, smoothing: float = 0.0, normalize_length: str2bool = False, criterion: Module = KLDivLoss())\",\"Bases: AbsS2STLoss\",\"attention-based label smoothing loss for S2ST.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(dense_y: Tensor, token_y: Tensor)\",\"Forward.\",\"Args:\",\"training : bool\"]},\"1997\":{\"h\":\"espnet2.s2st.losses.ctc_loss.S2STCTCLoss\",\"t\":[\"<!-- _espnet2.s2st.losses.ctc_loss.S2STCTCLoss -->\",\"class espnet2.s2st.losses.ctc_loss.S2STCTCLoss(weight: float = 1.0)\",\"Bases: AbsS2STLoss\",\"CTC-based loss for S2ST.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward()\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1998\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"1999\":{\"h\":\"espnet2.s2st.losses.guided_attention_loss.S2STGuidedAttentionLoss\",\"t\":[\"class espnet2.s2st.losses.guided_attention_loss.S2STGuidedAttentionLoss(weight: float = 1.0, sigma: float = 0.4, alpha: float = 1.0)\",\"Bases: AbsS2STLoss\",\"Tacotron-based loss for S2ST.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(att_ws: Tensor, ilens: Tensor, olens_in: Tensor)\",\"Forward.\",\"Args:\",\"Returns: guided attention loss\",\"Return type: Tensor\",\"training : bool\"]},\"2000\":{\"h\":\"espnet2.s2st.losses.tacotron_loss.S2STTacotron2Loss\",\"t\":[\"class espnet2.s2st.losses.tacotron_loss.S2STTacotron2Loss(weight: float = 1.0, loss_type: str = 'L1+L2', use_masking: str2bool = True, use_weighted_masking: str2bool = False, bce_pos_weight: float = 20.0)\",\"Bases: AbsS2STLoss\",\"Tacotron-based loss for S2ST.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(after_outs: Tensor, before_outs: Tensor, logits: Tensor, ys: Tensor, labels: Tensor, olens: Tensor)\",\"Forward.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"logits (Tensor) – Batch of stop logits (B, Lmax).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"labels (LongTensor) – Batch of the sequences of stop token labels (B, Lmax).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Mean square error loss value. Tensor: Binary cross entropy loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"2001\":{\"h\":\"espnet2.s2st.synthesizer.discrete_synthesizer.TransformerDiscreteSynthesizer\",\"t\":[\"class espnet2.s2st.synthesizer.discrete_synthesizer.TransformerDiscreteSynthesizer(odim: int, idim: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, layer_drop_rate: float = 0.0, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat')\",\"Bases: AbsSynthesizer, BatchScorerInterface\",\"Discrete unit Synthesizer related modules for speech-to-speech translation.\",\"This is a module of discrete unit prediction network in discrete-unit described in Direct speech-to-speech translation with discrete units, which converts the sequence of hidden states into the sequence of discrete unit (from SSLs).\",\"Transfomer decoder for discrete unit module.\",\"Parameters:\",\"vocab_size – output dim\",\"encoder_output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"self_attention_dropout_rate – dropout rate for attention\",\"input_layer – input layer type\",\"use_output_layer – whether to use output layer\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor)\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(enc_outputs: Tensor, enc_outputs_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, return_hs: bool = False, return_all_hs: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"enc_outputs (LongTensor) – Batch of padded character ids (B, T, idim).\",\"enc_outputs_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"Returns: hs hlens\",\"forward_one_step(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, cache: List[Tensor] | None = None, **kwargs)\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (batch, maxlen_out)\",\"tgt_mask – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"inference()\",\"Return predicted output as a dict.\",\"score(ys, state, x)\",\"Score.\",\"training : bool\"]},\"2002\":{\"h\":\"espnet2.s2st.synthesizer.translatotron.Translatotron\",\"t\":[\"class espnet2.s2st.synthesizer.translatotron.Translatotron(idim: int, odim: int, embed_dim: int = 512, atype: str = 'multihead', adim: int = 512, aheads: int = 4, aconv_chans: int = 32, aconv_filts: int = 15, cumulate_att_w: bool = True, dlayers: int = 4, dunits: int = 1024, prenet_layers: int = 2, prenet_units: int = 32, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, output_activation: str | None = None, use_batch_norm: bool = True, use_concate: bool = True, use_residual: bool = False, reduction_factor: int = 2, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat', dropout_rate: float = 0.5, zoneout_rate: float = 0.1)\",\"Bases: AbsSynthesizer\",\"TTranslatotron Synthesizer related modules for speech-to-speech translation.\",\"This is a module of Spectrogram prediction network in Translatotron described in Direct speech-to-speech translation with a sequence-to-sequence model, which converts the sequence of hidden states into the sequence of Mel-filterbanks.\",\"Initialize Tacotron2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim – (int) Dimension of the outputs.\",\"adim (int) – Number of dimension of mlp in attention.\",\"atype (str) – type of attention\",\"aconv_chans (int) – Number of attention conv filter channels.\",\"aconv_filts (int) – Number of attention conv filter size.\",\"embed_dim (int) – Dimension of the token embedding.\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"prenet_layers (int) – Number of prenet layers.\",\"prenet_units (int) – Number of prenet units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"output_activation (str) – Name of activation function for outputs.\",\"cumulate_att_w (bool) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"use_concate (bool) – Whether to concat enc outputs w/ dec lstm outputs.\",\"reduction_factor (int) – Reduction factor.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"dropout_rate (float) – Dropout rate.\",\"zoneout_rate (float) – Zoneout rate.\",\"forward(enc_outputs: Tensor, enc_outputs_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"enc_outputs (LongTensor) – Batch of padded character ids (B, T, idim).\",\"enc_outputs_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"Returns: after_outs (TODO(jiatong) add full comment) before_outs (TODO(jiatong) add full comments) logits att_ws ys stop_labels olens\",\"inference(enc_outputs: Tensor, feats: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_att_constraint: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"enc_outputs (LongTensor) – Input sequence of characters (N, idim).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, odim).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"threshold (float) – Threshold in inference.\",\"minlenratio (float) – Minimum length ratio in inference.\",\"maxlenratio (float) – Maximum length ratio in inference.\",\"use_att_constraint (bool) – Whether to apply attention constraint.\",\"backward_window (int) – Backward window in attention constraint.\",\"forward_window (int) – Forward window in attention constraint.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"prob (Tensor): Output sequence of stop probabilities (T_feats,).\",\"att_w (Tensor): Attention weights (T_feats, T).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2003\":{\"h\":\"espnet2.s2st.synthesizer.translatotron2.Translatotron2\",\"t\":[\"class espnet2.s2st.synthesizer.translatotron2.Translatotron2(idim: int, odim: int, synthesizer_type: str = 'rnn', layers: int = 2, units: int = 1024, prenet_layers: int = 2, prenet_units: int = 128, prenet_dropout_rate: float = 0.5, postnet_layers: int = 5, postnet_chans: int = 512, postnet_dropout_rate: float = 0.5, adim: int = 384, aheads: int = 4, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_type: str = 'rnn', duration_predictor_units: int = 128, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsSynthesizer\",\"Translatotron2 module.\",\"This is a module of the synthesizer in Translatotron2 described in\",\"`Translatotron 2: High-quality direct speech-to-speech translation with voice preservation`_\",\".\",\"<!-- _`Translatotron 2: -->\",\"High-quality direct speech-to-speech translation with voice preservation`: : https://arxiv.org/pdf/2107.08661v5.pdf\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"training : bool\"]},\"2004\":{\"h\":\"espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer\",\"t\":[\"class espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, layer_drop_rate: float = 0.0, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat')\",\"Bases: AbsSynthesizer\",\"UnitY Synthesizer related modules for speech-to-speech translation.\",\"This is a module of discrete unit prediction network in discrete-unit described in\",\"`Direct speech-to-speech translation with discrete units`_\",\", which converts the sequence of hidden states into the sequence of discrete unit (from SSLs).\",\"Transfomer decoder for discrete unit module.\",\"Parameters:\",\"vocab_size – output dim\",\"encoder_output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"self_attention_dropout_rate – dropout rate for attention\",\"input_layer – input layer type\",\"use_output_layer – whether to use output layer\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"forward(enc_outputs: Tensor, enc_outputs_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, return_last_hidden: bool = False, return_all_hiddens: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"enc_outputs (LongTensor) – Batch of padded character ids (B, T, idim).\",\"enc_outputs_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"Returns: hs hlens\",\"training : bool\"]},\"2005\":{\"h\":\"espnet2.samplers.abs_sampler.AbsSampler\",\"t\":[\"<!-- _espnet2.samplers.abs_sampler.AbsSampler -->\",\"class espnet2.samplers.abs_sampler.AbsSampler(data_source: Sized | None)\",\"Bases: Sampler, ABC\",\"generate(seed)\"]},\"2006\":{\"h\":\"espnet2.samplers.category_balanced_sampler.CategoryBalancedSampler\",\"t\":[\"class espnet2.samplers.category_balanced_sampler.CategoryBalancedSampler(batch_size: int, min_batch_size: int = 1, drop_last: bool = False, category2utt_file: str | None = None, epoch: int = 1, **kwargs)\",\"Bases: AbsSampler\"]},\"2007\":{\"h\":\"espnet2.samplers.folded_batch_sampler.FoldedBatchSampler\",\"t\":[\"class espnet2.samplers.folded_batch_sampler.FoldedBatchSampler(batch_size: int, shape_files: Tuple[str, ...] | List[str], fold_lengths: Sequence[int], min_batch_size: int = 1, sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False, utt2category_file: str | None = None)\",\"Bases: AbsSampler\"]},\"2008\":{\"h\":\"espnet2.samplers.length_batch_sampler.LengthBatchSampler\",\"t\":[\"class espnet2.samplers.length_batch_sampler.LengthBatchSampler(batch_bins: int, shape_files: Tuple[str, ...] | List[str], min_batch_size: int = 1, sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False, padding: bool = True)\",\"Bases: AbsSampler\"]},\"2009\":{\"h\":\"espnet2.samplers.num_elements_batch_sampler.NumElementsBatchSampler\",\"t\":[\"class espnet2.samplers.num_elements_batch_sampler.NumElementsBatchSampler(batch_bins: int, shape_files: Tuple[str, ...] | List[str], min_batch_size: int = 1, sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False, padding: bool = True)\",\"Bases: AbsSampler\"]},\"2010\":{\"h\":\"espnet2.samplers.sorted_batch_sampler.SortedBatchSampler\",\"t\":[\"class espnet2.samplers.sorted_batch_sampler.SortedBatchSampler(batch_size: int, shape_file: str, sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False)\",\"Bases: AbsSampler\",\"BatchSampler with sorted samples by length.\",\"Parameters:\",\"batch_size –\",\"shape_file –\",\"sort_in_batch – ‘descending’, ‘ascending’ or None.\",\"sort_batch –\"]},\"2011\":{\"h\":\"espnet2.samplers.unsorted_batch_sampler.UnsortedBatchSampler\",\"t\":[\"class espnet2.samplers.unsorted_batch_sampler.UnsortedBatchSampler(batch_size: int, key_file: str, drop_last: bool = False, utt2category_file: str | None = None)\",\"Bases: AbsSampler\",\"BatchSampler with constant batch-size.\",\"Any sorting is not done in this class, so no length information is required, This class is convenient for decoding mode, or not seq2seq learning e.g. classification.\",\"Parameters:\",\"batch_size –\",\"key_file –\"]},\"2012\":{\"h\":\"espnet2.samplers.build_batch_sampler.build_batch_sampler\",\"t\":[\"espnet2.samplers.build_batch_sampler.build_batch_sampler(type: str, batch_size: int, batch_bins: int, shape_files: Tuple[str, ...] | List[str], sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False, min_batch_size: int = 1, fold_lengths: Sequence[int] = (), padding: bool = True, utt2category_file: str | None = None)\",\"Helper function to instantiate BatchSampler.\",\"Parameters:\",\"type – mini-batch type. “unsorted”, “sorted”, “folded”, “numel”, “length”, or “catbel”\",\"batch_size – The mini-batch size. Used for “unsorted”, “sorted”, “folded”, “catbel” mode\",\"batch_bins – Used for “numel” model\",\"shape_files – Text files describing the length and dimension of each features. e.g. uttA 1330,80\",\"sort_in_batch –\",\"sort_batch –\",\"drop_last –\",\"min_batch_size – Used for “numel” or “folded” mode\",\"fold_lengths – Used for “folded” mode\",\"padding – Whether sequences are input as a padded tensor or not. used for “numel” mode\"]},\"2013\":{\"h\":\"espnet2.samplers.category_balanced_sampler.round_down\",\"t\":[\"espnet2.samplers.category_balanced_sampler.round_down(num, divisor)\"]},\"2014\":{\"h\":\"espnet2.schedulers.abs_scheduler.AbsBatchStepScheduler\",\"t\":[\"class espnet2.schedulers.abs_scheduler.AbsBatchStepScheduler\",\"Bases: AbsScheduler\",\"abstract load_state_dict(state)\",\"abstract state_dict()\",\"abstract step(epoch: int | None = None)\"]},\"2015\":{\"h\":\"espnet2.schedulers.abs_scheduler.AbsEpochStepScheduler\",\"t\":[\"class espnet2.schedulers.abs_scheduler.AbsEpochStepScheduler\",\"Bases: AbsScheduler\",\"abstract load_state_dict(state)\",\"abstract state_dict()\",\"abstract step(epoch: int | None = None)\"]},\"2016\":{\"h\":\"espnet2.schedulers.abs_scheduler.AbsScheduler\",\"t\":[\"class espnet2.schedulers.abs_scheduler.AbsScheduler\",\"Bases: ABC\",\"abstract load_state_dict(state)\",\"abstract state_dict()\",\"abstract step(epoch: int | None = None)\"]},\"2017\":{\"h\":\"espnet2.schedulers.abs_scheduler.AbsValEpochStepScheduler\",\"t\":[\"class espnet2.schedulers.abs_scheduler.AbsValEpochStepScheduler\",\"Bases: AbsEpochStepScheduler\",\"abstract load_state_dict(state)\",\"abstract state_dict()\",\"abstract step(val, epoch: int | None = None)\"]},\"2018\":{\"h\":\"espnet2.schedulers.cosine_anneal_warmup_restart.CosineAnnealingWarmupRestarts\",\"t\":[\"class espnet2.schedulers.cosine_anneal_warmup_restart.CosineAnnealingWarmupRestarts(optimizer: Optimizer, first_cycle_steps: int, cycle_mult: float = 1.0, max_lr: float = 0.1, min_lr: float = 0.001, warmup_steps: int = 0, gamma: float = 1.0, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"Cosine Annealing Warmup Restart.\",\"optimizer (Optimizer): Wrapped optimizer. first_cycle_steps (int): First cycle step size. cycle_mult(float): Cycle steps magnification. Default: -1. max_lr(float): First cycle’s max learning rate. Default: 0.1. min_lr(float): Min learning rate. Default: 0.001. warmup_steps(int): Linear warmup step size. Default: 0. gamma(float): Decrease rate of max learning rate by cycle. Default: 1. last_epoch (int): The index of last epoch. Default: -1.\",\"get_lr()\",\"init_lr()\",\"step(epoch=None)\"]},\"2019\":{\"h\":\"espnet2.schedulers.noam_lr.NoamLR\",\"t\":[\"<!-- _espnet2.schedulers.noam_lr.NoamLR -->\",\"class espnet2.schedulers.noam_lr.NoamLR(optimizer: Optimizer, model_size: int | float = 320, warmup_steps: int | float = 25000, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"The LR scheduler proposed by Noam\",\"Ref: : “Attention Is All You Need”, https://arxiv.org/pdf/1706.03762.pdf\",\"FIXME(kamo): PyTorch doesn’t provide _LRScheduler as public class, : thus the behaviour isn’t guaranteed at forward PyTorch version.\",\"NOTE(kamo): The “model_size” in original implementation is derived from : the model, but in this implementation, this parameter is a constant value. You need to change it if the model is changed.\",\"get_lr()\",\"lr_for_WarmupLR(lr: float)\"]},\"2020\":{\"h\":\"espnet2.schedulers.piecewise_linear_warmup_lr.PiecewiseLinearWarmupLR\",\"t\":[\"class espnet2.schedulers.piecewise_linear_warmup_lr.PiecewiseLinearWarmupLR(optimizer: Optimizer, warmup_steps_list: List[float | int] = [0, 25000], warmup_lr_list: List[float] = [0.0, 0.001], last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"The PiecewiseLinearWarmupLR scheduler\",\"This scheduler is similar to WarmupLR Scheduler except that the warmup stage is piecewise linear.\",\"get_lr()\"]},\"2021\":{\"h\":\"espnet2.schedulers.warmup_lr.WarmupLR\",\"t\":[\"<!-- _espnet2.schedulers.warmup_lr.WarmupLR -->\",\"class espnet2.schedulers.warmup_lr.WarmupLR(optimizer: Optimizer, warmup_steps: int | float = 25000, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"The WarmupLR scheduler\",\"This scheduler is almost same as NoamLR Scheduler except for following difference:\",\"NoamLR: : lr = optimizer.lr * model_size ** -0.5 : * min(step ** -0.5, step * warmup_step ** -1.5)\",\"WarmupLR: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5)\",\"Note that the maximum lr equals to optimizer.lr in this scheduler.\",\"get_lr()\"]},\"2022\":{\"h\":\"espnet2.schedulers.warmup_reducelronplateau.WarmupReduceLROnPlateau\",\"t\":[\"class espnet2.schedulers.warmup_reducelronplateau.WarmupReduceLROnPlateau(optimizer: Optimizer, warmup_steps: int | float = 25000, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\",\"Bases: AbsBatchStepScheduler, AbsValEpochStepScheduler\",\"The WarmupReduceLROnPlateau scheduler.\",\"This scheduler is the combination of WarmupLR and ReduceLROnPlateau:\",\"WarmupLR: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5)\",\"WarmupReduceLROnPlateau: : if step <= warmup_step: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5) <br/> else: : lr = ( : optimizer.lr * factor if no improvement for a ‘patience’ number of epochs else optimizer.lr <br/> )\",\"Note that the maximum lr equals to optimizer.lr in this scheduler.\",\"property in_cooldown\",\"is_better(a, best)\",\"load_state_dict(state_dict)\",\"state_dict()\",\"step(metrics=None, epoch=None)\"]},\"2023\":{\"h\":\"espnet2.schedulers.warmup_step_lr.WarmupStepLR\",\"t\":[\"class espnet2.schedulers.warmup_step_lr.WarmupStepLR(optimizer: Optimizer, warmup_steps: int | float = 25000, steps_per_epoch: int = 10000, step_size: int = 1, gamma: float = 0.1, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"The WarmupStepLR scheduler.\",\"This scheduler is the combination of WarmupLR and StepLR:\",\"WarmupLR: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5)\",\"WarmupStepLR: : if step <= warmup_step: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5) <br/> else: : lr = optimizer.lr * (gamma ** (epoch//step_size))\",\"Note that the maximum lr equals to optimizer.lr in this scheduler.\",\"get_lr()\"]},\"2024\":{\"h\":\"espnet2.slu.postdecoder.abs_postdecoder.AbsPostDecoder\",\"t\":[\"class espnet2.slu.postdecoder.abs_postdecoder.AbsPostDecoder\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract convert_examples_to_features(data: list, max_seq_length: int, output_size: int)\",\"abstract forward(transcript_input_ids: LongTensor, transcript_attention_mask: LongTensor, transcript_token_type_ids: LongTensor, transcript_position_ids: LongTensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2025\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size()\",\"training : bool\"]},\"2026\":{\"h\":\"espnet2.slu.postencoder.conformer_postencoder.ConformerPostEncoder\",\"t\":[\"class espnet2.slu.postencoder.conformer_postencoder.ConformerPostEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str = 'linear', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1)\",\"Bases: AbsPostEncoder\",\"Hugging Face Transformers PostEncoder.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Forward.\",\"output_size()\",\"Get the output size.\",\"training : bool\"]},\"2027\":{\"h\":\"espnet2.slu.espnet_model.ESPnetSLUModel\",\"t\":[\"<!-- _espnet2.slu.espnet_model.ESPnetSLUModel -->\",\"class espnet2.slu.espnet_model.ESPnetSLUModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder, ctc: CTC, joint_network: Module | None, postdecoder: AbsPostDecoder | None = None, deliberationencoder: AbsPostEncoder | None = None, transcript_token_list: Tuple[str, ...] | List[str] | None = None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', extract_feats_in_collect_stats: bool = True, two_pass: bool = False, pre_postencoder_norm: bool = False)\",\"Bases: ESPnetASRModel\",\"CTC-attention hybrid Encoder-Decoder model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, transcript: Tensor | None = None, transcript_lengths: Tensor | None = None, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor, transcript_pad: Tensor | None = None, transcript_pad_lens: Tensor | None = None)\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, transcript: Tensor | None = None, transcript_lengths: Tensor | None = None, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\",\"training : bool\"]},\"2028\":{\"h\":\"espnet2.slu.postdecoder.hugging_face_transformers_postdecoder.HuggingFaceTransformersPostDecoder\",\"t\":[\"class espnet2.slu.postdecoder.hugging_face_transformers_postdecoder.HuggingFaceTransformersPostDecoder(model_name_or_path: str, output_size=256)\",\"Bases: AbsPostDecoder\",\"Hugging Face Transformers PostEncoder.\",\"Initialize the module.\",\"convert_examples_to_features(data, max_seq_length)\",\"forward(transcript_input_ids: LongTensor, transcript_attention_mask: LongTensor, transcript_token_type_ids: LongTensor, transcript_position_ids: LongTensor)\",\"Forward.\",\"output_size()\",\"Get the output size.\",\"training : bool\"]},\"2029\":{\"h\":\"espnet2.slu.postencoder.transformer_postencoder.TransformerPostEncoder\",\"t\":[\"class espnet2.slu.postencoder.transformer_postencoder.TransformerPostEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'linear', pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, padding_idx: int = -1)\",\"Bases: AbsPostEncoder\",\"Transformer encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"input_layer – input layer type\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type – linear of conv1d\",\"positionwise_conv_kernel_size – kernel size of positionwise conv1d layer\",\"padding_idx – padding_idx for input_layer=embed\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size()\",\"training : bool\"]},\"2030\":{\"h\":\"espnet2.spk.loss.aamsoftmax.AAMSoftmax\",\"t\":[\"<!-- _espnet2.spk.loss.aamsoftmax.AAMSoftmax -->\",\"class espnet2.spk.loss.aamsoftmax.AAMSoftmax(nout, nclasses, margin=0.3, scale=15, easy_margin=False, **kwargs)\",\"Bases: AbsLoss\",\"Additive angular margin softmax.\",\"Paper: Deng, Jiankang, et al. “Arcface: Additive angular margin loss for deep face recognition.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\",\"Parameters:\",\"nout – dimensionality of speaker embedding\",\"nclases – number of speakers in the training set\",\"margin – margin value of AAMSoftmax\",\"scale – scale value of AAMSoftmax\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, label=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2031\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2032\":{\"h\":\"espnet2.spk.layers.rawnet_block.AFMS\",\"t\":[\"<!-- _espnet2.spk.layers.rawnet_block.AFMS -->\",\"class espnet2.spk.layers.rawnet_block.AFMS(nb_dim: int)\",\"Bases: Module\",\"Alpha-Feature map scaling, added to the output of each residual block[1,2].\",\"Reference: [1] RawNet2 : https://www.isca-speech.org/archive/Interspeech_2020/pdfs/1011.pdf [2] AMFS : https://www.koreascience.or.kr/article/JAKO202029757857763.page\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2033\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2034\":{\"h\":\"espnet2.spk.loss.abs_loss.AbsLoss\",\"t\":[\"<!-- _espnet2.spk.loss.abs_loss.AbsLoss -->\",\"class espnet2.spk.loss.abs_loss.AbsLoss(nout: int, **kwargs)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(x: Tensor, label=None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2035\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2036\":{\"h\":\"espnet2.spk.pooling.abs_pooling.AbsPooling\",\"t\":[\"<!-- _espnet2.spk.pooling.abs_pooling.AbsPooling -->\",\"class espnet2.spk.pooling.abs_pooling.AbsPooling\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2037\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size()\",\"training : bool\"]},\"2038\":{\"h\":\"espnet2.spk.projector.abs_projector.AbsProjector\",\"t\":[\"class espnet2.spk.projector.abs_projector.AbsProjector\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(utt_embd: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2039\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size()\",\"training : bool\"]},\"2040\":{\"h\":\"espnet2.spk.loss.aamsoftmax_subcenter_intertopk.ArcMarginProduct_intertopk_subcenter\",\"t\":[\"class espnet2.spk.loss.aamsoftmax_subcenter_intertopk.ArcMarginProduct_intertopk_subcenter(nout, nclasses, scale=32.0, margin=0.2, easy_margin=False, K=3, mp=0.06, k_top=5, do_lm=False)\",\"Bases: AbsLoss\",\"Implement of large margin arc distance with intertopk and subcenter:\",\"Reference: : MULTI-QUERY MULTI-HEAD ATTENTION POOLING AND INTER-TOPK PENALTY FOR SPEAKER VERIFICATION. https://arxiv.org/pdf/2110.05042.pdf Sub-center ArcFace: Boosting Face Recognition by Large-Scale Noisy Web Faces. https://ibug.doc.ic.ac.uk/media/uploads/documents/eccv_1445.pdf\",\"Parameters:\",\"in_features – size of each input sample\",\"out_features – size of each output sample\",\"scale – norm of input feature\",\"margin – margin\",\"cos (theta + margin) –\",\"K – number of sub-centers\",\"k_top – number of hard samples\",\"mp – margin penalty of hard samples\",\"do_lm – whether do large margin finetune\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, label)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2041\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\",\"update(margin=0.2)\"]},\"2042\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.Bottle2neck\",\"t\":[\"class espnet2.spk.encoder.ska_tdnn_encoder.Bottle2neck(inplanes, planes, kernel_size=None, kernel_sizes=[5, 7], dilation=None, scale=8, group=1)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2043\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2044\":{\"h\":\"espnet2.spk.pooling.chn_attn_stat_pooling.ChnAttnStatPooling\",\"t\":[\"class espnet2.spk.pooling.chn_attn_stat_pooling.ChnAttnStatPooling(input_size: int = 1536)\",\"Bases: AbsPooling\",\"Aggregates frame-level features to single utterance-level feature.\",\"Proposed in B.Desplanques et al., “ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification”\",\"Parameters:input_size – dimensionality of the input frame-level embeddings. Determined by encoder hyperparameter. For this pooling layer, the output dimensionality will be double of the input_size\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, task_tokens: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2045\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2046\":{\"h\":\"espnet2.spk.espnet_model.ESPnetSpeakerModel\",\"t\":[\"class espnet2.spk.espnet_model.ESPnetSpeakerModel(frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, encoder: AbsEncoder | None, pooling: AbsPooling | None, projector: AbsProjector | None, loss: AbsLoss | None)\",\"Bases: AbsESPnetModel\",\"Speaker embedding extraction model.\",\"Core model for diverse speaker-related tasks (e.g., verification, open-set identification, diarization)\",\"The model architecture comprises mainly ‘encoder’, ‘pooling’, and ‘projector’. In common speaker recognition field, the combination of three would be usually named as ‘speaker_encoder’ (or speaker embedding extractor). We splitted it into three for flexibility in future extensions:\",\"‘encoder’ : extract frame-level speaker embeddings.\",\"‘pooling’ : aggregate into single utterance-level embedding.\",\"‘projector’ : connected layer) to derive speaker embedding.\",\"Possibly, in the future, ‘pooling’ and/or ‘projector’ can be integrated as a ‘decoder’, depending on the extension for joint usage of different tasks (e.g., ASR, SE, target speaker extraction).\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"aggregate(frame_level_feats: Tensor)\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, spk_labels: Tensor | None = None, **kwargs)\",\"encode_frame(feats: Tensor)\",\"extract_feats(speech: Tensor, speech_lengths: Tensor)\",\"forward(speech: Tensor, spk_labels: Tensor | None = None, task_tokens: Tensor | None = None, extract_embd: bool = False, **kwargs)\",\"Feed-forward through encoder layers and aggregate into utterance-level\",\"feature.\",\"Parameters:\",\"speech – (Batch, samples)\",\"speech_lengths – (Batch,)\",\"extract_embd – a flag which doesn’t go through the classification head when set True\",\"spk_labels – (Batch, )\",\"phase (one-hot speaker labels used in the train) –\",\"task_tokens – (Batch, )\",\"trainings (task tokens used in caseoftoken-based) –\",\"project_spk_embd(utt_level_feat: Tensor)\",\"training : bool\"]},\"2047\":{\"h\":\"espnet2.spk.layers.ecapa_block.EcapaBlock\",\"t\":[\"<!-- _espnet2.spk.layers.ecapa_block.EcapaBlock -->\",\"class espnet2.spk.layers.ecapa_block.EcapaBlock(inplanes, planes, kernel_size=None, dilation=None, scale=8)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2048\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2049\":{\"h\":\"espnet2.spk.encoder.ecapa_tdnn_encoder.EcapaTdnnEncoder\",\"t\":[\"class espnet2.spk.encoder.ecapa_tdnn_encoder.EcapaTdnnEncoder(input_size: int, block: str = 'EcapaBlock', model_scale: int = 8, ndim: int = 1024, output_size: int = 1536, **kwargs)\",\"Bases: AbsEncoder\",\"ECAPA-TDNN encoder. Extracts frame-level ECAPA-TDNN embeddings from\",\"mel-filterbank energy or MFCC features. Paper: B Desplanques at el.,\",\"``\",\"ECAPA-TDNN: Emphasized Channel Attention,\",\"Propagation and Aggregation in TDNN Based Speaker Verification,’’ in Proc. INTERSPEECH, 2020.\",\"Parameters:\",\"input_size – input feature dimension.\",\"block – type of encoder block class to use.\",\"model_scale – scale value of the Res2Net architecture.\",\"ndim – dimensionality of the hidden representation.\",\"output_size – output embedding dimension.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"Returns: Output tensor (#batch, L, output_size).\",\"Return type: torch.Tensor\",\"output_size()\",\"training : bool\"]},\"2050\":{\"h\":\"espnet2.spk.encoder.identity_encoder.IdentityEncoder\",\"t\":[\"class espnet2.spk.encoder.identity_encoder.IdentityEncoder(input_size: int)\",\"Bases: AbsEncoder\",\"Identity encoder. Does nothing, just passes frontend feature to the pooling.\",\"Expected to be used for cases when frontend already has a good representation (e.g., SSL features).\",\"Parameters:input_size – input feature dimension.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2051\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2052\":{\"h\":\"espnet2.spk.pooling.mean_pooling.MeanPooling\",\"t\":[\"class espnet2.spk.pooling.mean_pooling.MeanPooling(input_size: int = 1536)\",\"Bases: AbsPooling\",\"Average frame-level features to a single utterance-level feature.\",\"Parameters:input_size – dimensionality of the input frame-level embeddings. Determined by encoder hyperparameter.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, task_tokens: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2053\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2054\":{\"h\":\"espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder\",\"t\":[\"class espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d2', normalize_before: bool = True, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, stochastic_depth_rate: float | List[float] = 0.0, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000, padding_idx: int | None = None)\",\"Bases: AbsEncoder\",\"Conformer encoder module for MFA-Conformer.\",\"Paper: Y. Zhang et al.,\",\"``\",\"Mfa-conformer: Multi-scale feature aggregation conformer for automatic speaker verification,’’ in Proc. INTERSPEECH, 2022.\",\"Parameters:\",\"input_size (int) – Input dimension.\",\"output_size (int) – Dimension of attention.\",\"attention_heads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"num_blocks (int) – The number of encoder blocks.\",\"dropout_rate (float) – Dropout rate.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"rel_pos_type (str) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"encoder_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"encoder_attn_layer_type (str) – Encoder attention layer type.\",\"activation_type (str) – Encoder activation function type.\",\"macaron_style (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_module (bool) – Whether to use convolution module.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"cnn_module_kernel (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"Returns: Output tensor (#batch, L, output_size).\",\"Return type: torch.Tensor\",\"output_size()\",\"training : bool\"]},\"2055\":{\"h\":\"espnet2.spk.encoder.rawnet3_encoder.RawNet3Encoder\",\"t\":[\"class espnet2.spk.encoder.rawnet3_encoder.RawNet3Encoder(input_size: int, block: str = 'Bottle2neck', model_scale: int = 8, ndim: int = 1024, output_size: int = 1536, **kwargs)\",\"Bases: AbsEncoder\",\"RawNet3 encoder. Extracts frame-level RawNet embeddings from raw waveform.\",\"paper: J. Jung et al., “Pushing the limits of raw waveform speaker : recognition”, in Proc. INTERSPEECH, 2022.\",\"Parameters:\",\"input_size – input feature dimension.\",\"block – type of encoder block class to use.\",\"model_scale – scale value of the Res2Net architecture.\",\"ndim – dimensionality of the hidden representation.\",\"output_size – ouptut embedding dimension.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2056\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2057\":{\"h\":\"espnet2.spk.projector.rawnet3_projector.RawNet3Projector\",\"t\":[\"class espnet2.spk.projector.rawnet3_projector.RawNet3Projector(input_size, output_size=192)\",\"Bases: AbsProjector\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2058\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2059\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.ResBlock\",\"t\":[\"class espnet2.spk.encoder.ska_tdnn_encoder.ResBlock(inplanes: int, planes: int, stride: int = 1, reduction: int = 8, skfwse_freq: int = 40, skcwse_channel: int = 128)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2060\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2061\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.SEModule\",\"t\":[\"class espnet2.spk.encoder.ska_tdnn_encoder.SEModule(channels, bottleneck=128)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2062\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2063\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.SKAttentionModule\",\"t\":[\"class espnet2.spk.encoder.ska_tdnn_encoder.SKAttentionModule(channel=128, reduction=4, L=16, num_kernels=2)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, convs)\",\"Forward function.\",\"Input: [B, C, T] Split: [K, B, C, T] Fues: [B, C, T] Attention weight: [B, C, 1] Output: [B, C, T]\",\"training : bool\"]},\"2064\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.SkaTdnnEncoder\",\"t\":[\"class espnet2.spk.encoder.ska_tdnn_encoder.SkaTdnnEncoder(input_size: int, block: str = 'Bottle2neck', ndim: int = 1024, model_scale: int = 8, skablock: str = 'ResBlock', ska_dim: int = 128, output_size: int = 1536, **kwargs)\",\"Bases: AbsEncoder\",\"SKA-TDNN encoder. Extracts frame-level SKA-TDNN embeddings from features.\",\"Paper: S. Mun, J. Jung et al., “Frequency and Multi-Scale Selective Kernel : Attention for Speaker Verification,’ in Proc. IEEE SLT 2022.\",\"Parameters:\",\"input_size – input feature dimension.\",\"block – type of encoder block class to use.\",\"model_scale – scale value of the Res2Net architecture.\",\"ndim – dimensionality of the hidden representation.\",\"output_size – ouptut embedding dimension.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2065\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2066\":{\"h\":\"espnet2.spk.projector.ska_tdnn_projector.SkaTdnnProjector\",\"t\":[\"class espnet2.spk.projector.ska_tdnn_projector.SkaTdnnProjector(input_size, output_size)\",\"Bases: AbsProjector\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2067\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2068\":{\"h\":\"espnet2.spk.pooling.stat_pooling.StatsPooling\",\"t\":[\"class espnet2.spk.pooling.stat_pooling.StatsPooling(input_size: int = 1536)\",\"Bases: AbsPooling\",\"Aggregates frame-level features to single utterance-level feature.\",\"Proposed in D. Snyder et al., “X-vectors: Robust dnn embeddings for speaker recognition”\",\"Parameters:input_size – dimensionality of the input frame-level embeddings. Determined by encoder hyperparameter. For this pooling layer, the output dimensionality will be double of the input_size\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, task_tokens: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2069\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2070\":{\"h\":\"espnet2.spk.encoder.xvector_encoder.XvectorEncoder\",\"t\":[\"class espnet2.spk.encoder.xvector_encoder.XvectorEncoder(input_size: int, ndim: int = 512, output_size: int = 1500, kernel_sizes: List = [5, 3, 3, 1, 1], paddings: List = [2, 1, 1, 0, 0], dilations: List = [1, 2, 3, 1, 1], **kwargs)\",\"Bases: AbsEncoder\",\"X-vector encoder. Extracts frame-level x-vector embeddings from features.\",\"Paper: D. Snyder et al., “X-vectors: Robust dnn embeddings for speaker recognition,” in Proc. IEEE ICASSP, 2018.\",\"Parameters:\",\"input_size – input feature dimension.\",\"ndim – dimensionality of the hidden representation.\",\"output_size – ouptut embedding dimension.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2071\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2072\":{\"h\":\"espnet2.spk.projector.xvector_projector.XvectorProjector\",\"t\":[\"class espnet2.spk.projector.xvector_projector.XvectorProjector(input_size, output_size)\",\"Bases: AbsProjector\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2073\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2074\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.cwSKAttention\",\"t\":[\"class espnet2.spk.encoder.ska_tdnn_encoder.cwSKAttention(freq=40, channel=128, kernels=[3, 5], receptive=[3, 5], dilations=[1, 1], reduction=8, groups=1, L=16)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward Function.\",\"Input: [B, C, F, T] Split: [K, B, C, F, T] Fuse: [B, C, F, T] Attention weight: [K, B, C, 1, 1] Output: [B, C, F, T]\",\"training : bool\"]},\"2075\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.fwSKAttention\",\"t\":[\"class espnet2.spk.encoder.ska_tdnn_encoder.fwSKAttention(freq=40, channel=128, kernels=[3, 5], receptive=[3, 5], dilations=[1, 1], reduction=8, groups=1, L=16)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward function.\",\"Input: [B, C, F, T] Split: [K, B, C, F, T] Fues: [B, C, F, T] Attention weight: [K, B, 1, F, 1] Output: [B, C, F, T]\",\"training : bool\"]},\"2076\":{\"h\":\"espnet2.st.espnet_model.ESPnetSTModel\",\"t\":[\"<!-- _espnet2.st.espnet_model.ESPnetSTModel -->\",\"class espnet2.st.espnet_model.ESPnetSTModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, hier_encoder: AbsEncoder | None, md_encoder: AbsEncoder | None, extra_mt_encoder: AbsEncoder | None, postencoder: AbsPostEncoder | None, decoder: AbsDecoder, extra_asr_decoder: AbsDecoder | None, extra_mt_decoder: AbsDecoder | None, ctc: CTC | None, st_ctc: CTC | None, st_joint_network: Module | None, src_vocab_size: int | None, src_token_list: Tuple[str, ...] | List[str] | None, asr_weight: float = 0.0, mt_weight: float = 0.0, mtlalpha: float = 0.0, st_mtlalpha: float = 0.0, ignore_id: int = -1, tgt_ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, report_bleu: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', tgt_sym_space: str = '<space>', tgt_sym_blank: str = '<blank>', extract_feats_in_collect_stats: bool = True, ctc_sample_rate: float = 0.0, tgt_sym_sos: str = '<sos/eos>', tgt_sym_eos: str = '<sos/eos>', lang_token_id: int = -1)\",\"Bases: AbsESPnetModel\",\"CTC-attention hybrid Encoder-Decoder model\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, src_text: Tensor | None = None, src_text_lengths: Tensor | None = None, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor, return_int_enc: bool = False)\",\"Frontend + Encoder. Note that this method is used by st_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, src_text: Tensor | None = None, src_text_lengths: Tensor | None = None, **kwargs)\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch,)\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"src_text – (Batch, length)\",\"src_text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\",\"training : bool\"]},\"2077\":{\"h\":\"espnet2.svs.abs_svs.AbsSVS\",\"t\":[\"<!-- _espnet2.svs.abs_svs.AbsSVS -->\",\"class espnet2.svs.abs_svs.AbsSVS\",\"Bases: Module, ABC\",\"SVS abstract class.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, **kwargs)\",\"Calculate outputs and return the loss tensor.\",\"abstract inference(text: Tensor, **kwargs)\",\"Return predicted output as a dict.\",\"property require_raw_singing\",\"Return whether or not raw_singing is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"2078\":{\"h\":\"espnet2.svs.singing_tacotron.decoder.Decoder\",\"t\":[\"class espnet2.svs.singing_tacotron.decoder.Decoder(idim, odim, att, dlayers=2, dunits=1024, prenet_layers=2, prenet_units=256, postnet_layers=5, postnet_chans=512, postnet_filts=5, output_activation_fn=None, cumulate_att_w=True, use_batch_norm=True, use_concate=True, dropout_rate=0.5, zoneout_rate=0.1, reduction_factor=1)\",\"Bases: Module\",\"Decoder module of Spectrogram prediction network.\",\"This is a module of decoder of Spectrogram prediction network in Singing Tacotron, which described in\",\"`https://arxiv.org/pdf/2202.07907v1.pdf`_\",\". The decoder generates the sequence of features from the sequence of the hidden states.\",\"Filter for End-to-end Singing Voice Synthesis`: : https://arxiv.org/pdf/2202.07907v1.pdf\",\"Initialize Singing Tacotron decoder module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"att (torch.nn.Module) – Instance of attention class.\",\"dlayers (int,optional) – The number of decoder lstm layers.\",\"dunits (int,optional) – The number of decoder lstm units.\",\"prenet_layers (int,optional) – The number of prenet layers.\",\"prenet_units (int,optional) – The number of prenet units.\",\"postnet_layers (int,optional) – The number of postnet layers.\",\"postnet_filts (int,optional) – The number of postnet filter size.\",\"postnet_chans (int,optional) – The number of postnet filter channels.\",\"output_activation_fn (torch.nn.Module,optional) – Activation function for outputs.\",\"cumulate_att_w (bool,optional) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool,optional) – Whether to use batch normalization.\",\"use_concate (bool,optional) – Whether to concatenate encoder embedding with decoder lstm outputs.\",\"dropout_rate (float,optional) – Dropout rate.\",\"zoneout_rate (float,optional) – Zoneout rate.\",\"reduction_factor (int,optional) – Reduction factor.\",\"forward(hs, hlens, trans_token, ys)\",\"Calculate forward propagation.\",\"Parameters:\",\"hs (Tensor) – Batch of the sequences of padded hidden states (B, Tmax, idim).\",\"hlens (LongTensor) – Batch of lengths of each input batch (B,).\",\"trans_token (Tensor) – Global transition token for duration (B x Tmax x 1)\",\"ys (Tensor) – Batch of the sequences of padded target features (B, Lmax, odim).\",\"Returns: Batch of output tensors after postnet (B, Lmax, odim). Tensor: Batch of output tensors before postnet (B, Lmax, odim). Tensor: Batch of logits of stop prediction (B, Lmax). Tensor: Batch of attention weights (B, Lmax, Tmax).\",\"Return type: Tensor\"]},\"2079\":{\"h\":\"NOTE\",\"t\":[\"This computation is performed in teacher-forcing manner.\",\"inference(h, trans_token, threshold=0.5, minlenratio=0.0, maxlenratio=30.0, use_att_constraint=False, use_dynamic_filter=True, backward_window=1, forward_window=3)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"h (Tensor) – Input sequence of encoder hidden states (T, C).\",\"trans_token (Tensor) – Global transition token for duration.\",\"threshold (float,optional) – Threshold to stop generation.\",\"minlenratio (float,optional) – Minimum length ratio. If set to 1.0 and the length of input is 10, the minimum length of outputs will be 10 * 1 = 10.\",\"minlenratio – Minimum length ratio. If set to 10 and the length of input is 10, the maximum length of outputs will be 10 * 10 = 100.\",\"use_att_constraint (bool) – Whether to apply attention constraint introduced in Deep Voice 3.\",\"use_dynamic_filter (bool) – Whether to apply dynamic filter introduced in\",\"`Singing Tacotron`_\",\".\",\"backward_window (int) – Backward window size in attention constraint.\",\"forward_window (int) – Forward window size in attention constraint.\",\"Returns: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T).\",\"Return type: Tensor\"]},\"2080\":{\"h\":\"NOTE\",\"t\":[\"This computation is performed in auto-regressive manner.\",\"training : bool\"]},\"2081\":{\"h\":\"espnet2.svs.singing_tacotron.encoder.Duration_Encoder\",\"t\":[\"class espnet2.svs.singing_tacotron.encoder.Duration_Encoder(idim, embed_dim=512, dropout_rate=0.5, padding_idx=0)\",\"Bases: Module\",\"Duration_Encoder module of Spectrogram prediction network.\",\"This is a module of encoder of Spectrogram prediction network in Singing-Tacotron, This is the encoder which converts the sequence of durations and tempo features into a transition token.\",\"END-TO-END SINGING VOICE SYNTHESIS`: : https://arxiv.org/abs/2202.07907\",\"Initialize Singing-Tacotron encoder module.\",\"Parameters:\",\"idim (int) –\",\"embed_dim (int,optional) –\",\"dropout_rate (float,optional) –\",\"forward(xs)\",\"Calculate forward propagation.\",\"Parameters:xs (Tensor) – Batch of the duration sequence.(B, Tmax, feature_len)\",\"Returns: Batch of the sequences of transition token (B, Tmax, 1). LongTensor: Batch of lengths of each sequence (B,)\",\"Return type: Tensor\",\"inference(x)\",\"Inference.\",\"training : bool\"]},\"2082\":{\"h\":\"espnet2.svs.espnet_model.ESPnetSVSModel\",\"t\":[\"<!-- _espnet2.svs.espnet_model.ESPnetSVSModel -->\",\"class espnet2.svs.espnet_model.ESPnetSVSModel(text_extract: AbsFeatsExtract | None, feats_extract: AbsFeatsExtract | None, score_feats_extract: AbsFeatsExtract | None, label_extract: AbsFeatsExtract | None, pitch_extract: AbsFeatsExtract | None, ying_extract: AbsFeatsExtract | None, duration_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, svs: AbsSVS)\",\"Bases: AbsESPnetModel\",\"ESPnet model for singing voice synthesis task.\",\"Initialize ESPnetSVSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, slur_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs)\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"---- ( ---- duration* is duration in time_shift) –\",\"duration_phn (Optional *[*Tensor]) – duration tensor (B, T_label).\",\"duration_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (B, T_phone).\",\"duration_ruled_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (B, T_syb).\",\"duration_syb_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"slur_lengths (Optional *[*Tensor]) – slur length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_wav). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, slur_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, flag_IsValid=False, **kwargs)\",\"Caclualte outputs and return the loss tensor.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (B, T_label).\",\"duration_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (B, T_phone).\",\"duration_ruled_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (B, T_syllable).\",\"duration_syb_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"slur_lengths (Optional *[*Tensor]) – slur length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_wav). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"kwargs – “utt_id” is among the input.\",\"Returns: Loss scalar tensor. Dict[str, float]: Statistics to be monitored. Tensor: Weight tensor to summarize losses.\",\"Return type: Tensor\",\"inference(text: Tensor, singing: Tensor | None = None, label: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, duration_phn: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_syb: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **decode_config)\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"singing (Tensor) – Singing waveform tensor (T_wav).\",\"label (Option *[*Tensor]) – Label tensor (T_label).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (T_l abel).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (T_label).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (T_phone).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (T_phone).\",\"slur (Optional *[*Tensor]) – slur tensor (T_phone).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"pitch (Optional *[*Tensor) – Pitch tensor (T_wav).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2083\":{\"h\":\"espnet2.svs.singing_tacotron.encoder.Encoder\",\"t\":[\"class espnet2.svs.singing_tacotron.encoder.Encoder(idim, input_layer='embed', embed_dim=512, elayers=1, eunits=512, econv_layers=3, econv_chans=512, econv_filts=5, use_batch_norm=True, use_residual=False, dropout_rate=0.5, padding_idx=0)\",\"Bases: Module\",\"Encoder module of Spectrogram prediction network.\",\"This is a module of encoder of Spectrogram prediction network in Singing Tacotron, which described in\",\"`Singing-Tacotron: Global Duration Control Attention and Dynamic Filter for End-to-end Singing Voice Synthesis`_\",\". This is the encoder which converts either a sequence of characters or acoustic features into the sequence of hidden states.\",\"Filter for End-to-end Singing Voice Synthesis`: : https://arxiv.org/abs/2202.07907\",\"Initialize Singing Tacotron encoder module.\",\"Parameters:\",\"idim (int) –\",\"input_layer (str) – Input layer type.\",\"embed_dim (int,optional) –\",\"elayers (int,optional) –\",\"eunits (int,optional) –\",\"econv_layers (int,optional) –\",\"econv_filts (int,optional) –\",\"econv_chans (int,optional) –\",\"use_batch_norm (bool,optional) –\",\"use_residual (bool,optional) –\",\"dropout_rate (float,optional) –\",\"forward(xs, ilens=None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of the padded sequence. Either character ids (B, Tmax) or acoustic feature (B, Tmax, idim * encoder_reduction_factor). Padded value should be 0.\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"Returns: Batch of the sequences of encoder states(B, Tmax, eunits). LongTensor: Batch of lengths of each sequence (B,)\",\"Return type: Tensor\",\"inference(x, ilens)\",\"Inference.\",\"Parameters:x (Tensor) – The sequeunce of character ids (T,) or acoustic feature (T, idim * encoder_reduction_factor).\",\"Returns: The sequences of encoder states(T, eunits).\",\"Return type: Tensor\",\"training : bool\"]},\"2084\":{\"h\":\"espnet2.svs.feats_extract.score_feats_extract.FrameScoreFeats\",\"t\":[\"class espnet2.svs.feats_extract.score_feats_extract.FrameScoreFeats(fs: int | str = 22050, n_fft: int = 1024, win_length: int = 512, hop_length: int = 128, window: str = 'hann', center: bool = True)\",\"Bases: AbsFeatsExtract\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(label: Tensor | None = None, label_lengths: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration: Tensor | None = None, duration_lengths: Tensor | None = None)\",\"FrameScoreFeats forward function.\",\"Parameters:\",\"label – (Batch, Nsamples)\",\"label_lengths – (Batch)\",\"midi – (Batch, Nsamples)\",\"midi_lengths – (Batch)\",\"duration – (Batch, Nsamples)\",\"duration_lengths – (Batch)\",\"Returns: (Batch, Frames)\",\"Return type: output\",\"get_parameters()\",\"label_aggregate(input: Tensor, input_lengths: Tensor | None = None)\",\"lage_aggregate function.\",\"Parameters:\",\"input – (Batch, Nsamples, Label_dim)\",\"input_lengths – (Batch)\",\"Returns: (Batch, Frames, Label_dim)\",\"Return type: output\",\"output_size()\",\"training : bool\"]},\"2085\":{\"h\":\"espnet2.svs.feats_extract.score_feats_extract.ListsToTensor\",\"t\":[\"espnet2.svs.feats_extract.score_feats_extract.ListsToTensor(xs)\"]},\"2086\":{\"h\":\"espnet2.svs.naive_rnn.naive_rnn.NaiveRNN\",\"t\":[\"<!-- _espnet2.svs.naive_rnn.naive_rnn.NaiveRNN -->\",\"class espnet2.svs.naive_rnn.naive_rnn.NaiveRNN(idim: int, odim: int, midi_dim: int = 129, embed_dim: int = 512, eprenet_conv_layers: int = 3, eprenet_conv_chans: int = 256, eprenet_conv_filts: int = 5, elayers: int = 3, eunits: int = 1024, ebidirectional: bool = True, midi_embed_integration_type: str = 'add', dlayers: int = 3, dunits: int = 1024, dbidirectional: bool = True, postnet_layers: int = 5, postnet_chans: int = 256, postnet_filts: int = 5, use_batch_norm: bool = True, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', eprenet_dropout_rate: float = 0.5, edropout_rate: float = 0.1, ddropout_rate: float = 0.1, postnet_dropout_rate: float = 0.5, init_type: str = 'xavier_uniform', use_masking: bool = False, use_weighted_masking: bool = False, loss_type: str = 'L1')\",\"Bases: AbsSVS\",\"NaiveRNN-SVS module.\",\"This is an implementation of naive RNN for singing voice synthesis The features are processed directly over time-domain from music score and predict the singing voice features\",\"Initialize NaiveRNN module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim (int) – Dimension of the outputs.\",\"midi_dim (int) – Dimension of the midi inputs.\",\"embed_dim (int) – Dimension of the token embedding.\",\"eprenet_conv_layers (int) – Number of prenet conv layers.\",\"eprenet_conv_filts (int) – Number of prenet conv filter size.\",\"eprenet_conv_chans (int) – Number of prenet conv filter channels.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"ebidirectional (bool) – If bidirectional in encoder.\",\"midi_embed_integration_type (str) – how to integrate midi information, (“add” or “cat”).\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"dbidirectional (bool) – if bidirectional in decoder.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"reduction_factor (int) – Reduction factor.\",\"related ( # extra embedding) –\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"eprenet_dropout_rate (float) – Prenet dropout rate.\",\"edropout_rate (float) – Encoder dropout rate.\",\"ddropout_rate (float) – Decoder dropout rate.\",\"postnet_dropout_rate (float) – Postnet dropout_rate.\",\"init_type (str) – How to initialize transformer parameters.\",\"use_masking (bool) – Whether to mask padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"loss_type (str) – Loss function type (“L1”, “L2”, or “L1+L2”).\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, flag_IsValid=False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"GS Fix: : arguements from forward func. V.S. <br/>\",\"**\",\"<br/> batch from espnet_model.py label == durations ｜ phone sequence melody -> pitch sequence\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, use_teacher_forcing: Tensor = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (Tmax).\",\"feats (Tensor) – Batch of padded target features (Lmax, odim).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score”; value (LongTensor): Batch of padded duration (Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (1).\",\"Returns: Output dict including the following items: * feat_gen (Tensor): Output sequence of features (T_feats, odim).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2087\":{\"h\":\"espnet2.svs.naive_rnn.naive_rnn_dp.NaiveRNNDP\",\"t\":[\"class espnet2.svs.naive_rnn.naive_rnn_dp.NaiveRNNDP(idim: int, odim: int, midi_dim: int = 129, embed_dim: int = 512, duration_dim: int = 500, eprenet_conv_layers: int = 3, eprenet_conv_chans: int = 256, eprenet_conv_filts: int = 5, elayers: int = 3, eunits: int = 1024, ebidirectional: bool = True, midi_embed_integration_type: str = 'add', dlayers: int = 3, dunits: int = 1024, dbidirectional: bool = True, postnet_layers: int = 5, postnet_chans: int = 256, postnet_filts: int = 5, use_batch_norm: bool = True, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', eprenet_dropout_rate: float = 0.5, edropout_rate: float = 0.1, ddropout_rate: float = 0.1, postnet_dropout_rate: float = 0.5, init_type: str = 'xavier_uniform', use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsSVS\",\"NaiveRNNDP-SVS module.\",\"This is an implementation of naive RNN with duration prediction for singing voice synthesis The features are processed directly over time-domain from music score and predict the singing voice features\",\"Initialize NaiveRNNDP module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim (int) – Dimension of the outputs.\",\"midi_dim (int) – Dimension of the midi inputs.\",\"embed_dim (int) – Dimension of the token embedding.\",\"eprenet_conv_layers (int) – Number of prenet conv layers.\",\"eprenet_conv_filts (int) – Number of prenet conv filter size.\",\"eprenet_conv_chans (int) – Number of prenet conv filter channels.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"ebidirectional (bool) – If bidirectional in encoder.\",\"midi_embed_integration_type (str) – how to integrate midi information, (“add” or “cat”).\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"dbidirectional (bool) – if bidirectional in decoder.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"reduction_factor (int) – Reduction factor.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"related ( # extra embedding) –\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"eprenet_dropout_rate (float) – Prenet dropout rate.\",\"edropout_rate (float) – Encoder dropout rate.\",\"ddropout_rate (float) – Decoder dropout rate.\",\"postnet_dropout_rate (float) – Postnet dropout_rate.\",\"init_type (str) – How to initialize transformer parameters.\",\"use_masking (bool) – Whether to mask padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, flag_IsValid=False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_length (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"GS Fix: : arguements from forward func. V.S. <br/>\",\"**\",\"<br/> batch from espnet_model.py label == durations | phone sequence melody -> pitch sequence\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, use_teacher_forcing: Tensor = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (Tmax).\",\"feats (Tensor) – Batch of padded target features (Lmax, odim).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (1).\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2088\":{\"h\":\"espnet2.svs.naive_rnn.naive_rnn.NaiveRNNLoss\",\"t\":[\"class espnet2.svs.naive_rnn.naive_rnn.NaiveRNNLoss(use_masking=True, use_weighted_masking=False)\",\"Bases: Module\",\"Loss function module for Tacotron2.\",\"Initialize Tactoron2 loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(after_outs, before_outs, ys, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Mean square error loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"2089\":{\"h\":\"espnet2.svs.feats_extract.score_feats_extract.SyllableScoreFeats\",\"t\":[\"class espnet2.svs.feats_extract.score_feats_extract.SyllableScoreFeats(fs: int | str = 22050, n_fft: int = 1024, win_length: int = 512, hop_length: int = 128, window: str = 'hann', center: bool = True)\",\"Bases: AbsFeatsExtract\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(label: Tensor | None = None, label_lengths: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration: Tensor | None = None, duration_lengths: Tensor | None = None)\",\"SyllableScoreFeats forward function.\",\"Parameters:\",\"label – (Batch, Nsamples)\",\"label_lengths – (Batch)\",\"midi – (Batch, Nsamples)\",\"midi_lengths – (Batch)\",\"duration – (Batch, Nsamples)\",\"duration_lengths – (Batch)\",\"Returns: (Batch, Frames)\",\"Return type: output\",\"get_parameters()\",\"get_segments(label: Tensor | None = None, label_lengths: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration: Tensor | None = None, duration_lengths: Tensor | None = None)\",\"output_size()\",\"training : bool\"]},\"2090\":{\"h\":\"espnet2.svs.xiaoice.XiaoiceSing.XiaoiceSing\",\"t\":[\"class espnet2.svs.xiaoice.XiaoiceSing.XiaoiceSing(idim: int, odim: int, midi_dim: int = 129, duration_dim: int = 500, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, loss_function: str = 'XiaoiceSing2', loss_type: str = 'L1', lambda_mel: float = 1, lambda_dur: float = 0.1, lambda_pitch: float = 0.01, lambda_vuv: float = 0.01)\",\"Bases: AbsSVS\",\"XiaoiceSing module for Singing Voice Synthesis.\",\"This is a module of XiaoiceSing. A high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling. It follows the main architecture of FastSpeech while proposing some singing-specific design:\",\"Add features from musical score (e.g.note pitch and length)\",\"Add a residual connection in F0 prediction to attenuate off-key issues\",\"The duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement (syllable loss)\",\"Initialize XiaoiceSing module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim (int) – Dimension of the outputs.\",\"midi_dim (int) – Dimension of the midi inputs.\",\"duration_dim (int) – Dimension of the duration inputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"loss_function (str) – Loss functions (“FastSpeech1” or “XiaoiceSing2”)\",\"loss_type (str) – Mel loss type (“L1” (MAE), “L2” (MSE) or “L1+L2”)\",\"lambda_mel (float) – Loss scaling coefficient for Mel loss.\",\"lambda_dur (float) – Loss scaling coefficient for duration loss.\",\"lambda_pitch (float) – Loss scaling coefficient for pitch loss.\",\"lambda_vuv (float) – Loss scaling coefficient for VUV loss.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, flag_IsValid=False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_length (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, use_teacher_forcing: Tensor = False, joint_training: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, idim).\",\"durations (Optional *[*LongTensor]) – Groundtruth of duration (T_text + 1,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"alpha (float) – Alpha to control the speed.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2091\":{\"h\":\"espnet2.svs.xiaoice.loss.XiaoiceSing2Loss\",\"t\":[\"<!-- _espnet2.svs.xiaoice.loss.XiaoiceSing2Loss -->\",\"class espnet2.svs.xiaoice.loss.XiaoiceSing2Loss(use_masking: bool = True, use_weighted_masking: bool = False)\",\"Bases: Module\",\"Loss function module for FastSpeech2.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, p_outs: Tensor, v_outs: Tensor, ys: Tensor, ds: Tensor, ps: Tensor, vs: Tensor, ilens: Tensor, olens: Tensor, loss_type: str = 'L1')\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, T_feats, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, T_feats, odim).\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of log_f0 (B, T_text, 1).\",\"v_outs (Tensor) – Batch of outputs of VUV (B, T_text, 1).\",\"ys (Tensor) – Batch of target features (B, T_feats, odim).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"ps (Tensor) – Batch of target log_f0 (B, T_text, 1).\",\"vs (Tensor) – Batch of target VUV (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"loss_type (str) – Mel loss type (“L1” (MAE), “L2” (MSE) or “L1+L2”)\",\"Returns: Mel loss value. Tensor: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: VUV predictor loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"2092\":{\"h\":\"espnet2.svs.singing_tacotron.decoder.decoder_init\",\"t\":[\"espnet2.svs.singing_tacotron.decoder.decoder_init(m)\",\"Initialize decoder parameters.\"]},\"2093\":{\"h\":\"espnet2.svs.singing_tacotron.encoder.encoder_init\",\"t\":[\"espnet2.svs.singing_tacotron.encoder.encoder_init(m)\",\"Initialize encoder parameters.\"]},\"2094\":{\"h\":\"espnet2.svs.feats_extract.score_feats_extract.expand_to_frame\",\"t\":[\"espnet2.svs.feats_extract.score_feats_extract.expand_to_frame(expand_len, len_size, label, midi, duration)\"]},\"2095\":{\"h\":\"espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron\",\"t\":[\"class espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron(idim: int, odim: int, midi_dim: int = 129, duration_dim: int = 500, embed_dim: int = 512, elayers: int = 1, eunits: int = 512, econv_layers: int = 3, econv_chans: int = 512, econv_filts: int = 5, atype: str = 'GDCA', adim: int = 512, aconv_chans: int = 32, aconv_filts: int = 15, cumulate_att_w: bool = True, dlayers: int = 2, dunits: int = 1024, prenet_layers: int = 2, prenet_units: int = 256, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, output_activation: str | None = None, use_batch_norm: bool = True, use_concate: bool = True, use_residual: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, dropout_rate: float = 0.5, zoneout_rate: float = 0.1, use_masking: bool = True, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1', use_guided_attn_loss: bool = True, guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)\",\"Bases: AbsSVS\",\"singing_Tacotron module for end-to-end singing-voice-synthesis.\",\"This is a module of Spectrogram prediction network in Singing Tacotron described in\",\"`Singing-Tacotron: Global Duration Control Attention and Dynamic Filter for End-to-end Singing Voice Synthesis`_\",\", which learn accurate alignment information automatically.\",\"Filter for End-to-end Singing Voice Synthesis`: : https://arxiv.org/pdf/2202.07907v1.pdf\",\"Initialize Singing Tacotron module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim – (int) Dimension of the outputs.\",\"embed_dim (int) – Dimension of the token embedding.\",\"elayers (int) – Number of encoder blstm layers.\",\"eunits (int) – Number of encoder blstm units.\",\"econv_layers (int) – Number of encoder conv layers.\",\"econv_filts (int) – Number of encoder conv filter size.\",\"econv_chans (int) – Number of encoder conv filter channels.\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"prenet_layers (int) – Number of prenet layers.\",\"prenet_units (int) – Number of prenet units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"output_activation (str) – Name of activation function for outputs.\",\"adim (int) – Number of dimension of mlp in attention.\",\"aconv_chans (int) – Number of attention conv filter channels.\",\"aconv_filts (int) – Number of attention conv filter size.\",\"cumulate_att_w (bool) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"use_concate (bool) – Whether to concat enc outputs w/ dec lstm outputs.\",\"reduction_factor (int) – Reduction factor.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – Number of GST embeddings.\",\"gst_heads (int) – Number of heads in GST multihead attention.\",\"gst_conv_layers (int) – Number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – Number of GRU layers in GST.\",\"gst_gru_units (int) – Number of GRU units in GST.\",\"dropout_rate (float) – Dropout rate.\",\"zoneout_rate (float) – Zoneout rate.\",\"use_masking (bool) – Whether to mask padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float) – Weight of positive sample of stop token (only for use_masking=True).\",\"loss_type (str) – Loss function type (“L1”, “L2”, or “L1+L2”).\",\"use_guided_attn_loss (bool) – Whether to use guided attention loss.\",\"guided_attn_loss_sigma (float) – Sigma in guided attention loss.\",\"guided_attn_loss_lambda (float) – Lambda in guided attention loss.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, ying: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, flag_IsValid=False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) –\",\"Batch of the lengths of each target (B,). : label (Optional[Dict]): key is “lab” or “score”;\",\"value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_length (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 30.0, use_att_constraint: bool = False, use_dynamic_filter: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, idim).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"threshold (float) – Threshold in inference.\",\"minlenratio (float) – Minimum length ratio in inference.\",\"maxlenratio (float) – Maximum length ratio in inference.\",\"use_att_constraint (bool) – Whether to apply attention constraint.\",\"use_dynamic_filter (bool) – Whether to apply dynamic filter.\",\"backward_window (int) – Backward window in attention constraint or dynamic filter.\",\"forward_window (int) – Forward window in attention constraint or dynamic filter.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"prob (Tensor): Output sequence of stop probabilities (T_feats,).\",\"att_w (Tensor): Attention weights (T_feats, T).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2096\":{\"h\":\"espnet2.tasks.asr.ASRTask\",\"t\":[\"<!-- _espnet2.tasks.asr.ASRTask -->\",\"class espnet2.tasks.asr.ASRTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2097\":{\"h\":\"espnet2.tasks.asr_transducer.ASRTransducerTask\",\"t\":[\"class espnet2.tasks.asr_transducer.ASRTransducerTask\",\"Bases: AbsTask\",\"ASR Transducer Task definition.\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"Add Transducer task arguments.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"parser – Transducer arguments parser.\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Build collate function.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"args – Task arguments.\",\"train – Training mode.\",\"Returns: Callable collate function.\",\"classmethod build_model(args: Namespace)\",\"Required data depending on task mode.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"args – Task arguments.\",\"Returns: ASR Transducer model.\",\"Return type: model\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"Build pre-processing function.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"args – Task arguments.\",\"train – Training mode.\",\"Returns: Callable pre-processing function.\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Optional data depending on task mode.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"train – Training mode.\",\"inference – Inference mode.\",\"Returns: Optional task data.\",\"Return type: retval\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Required data depending on task mode.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"train – Training mode.\",\"inference – Inference mode.\",\"Returns: Required task data.\",\"Return type: retval\",\"trainer\",\"alias of Trainer\"]},\"2098\":{\"h\":\"espnet2.tasks.asvspoof.ASVSpoofTask\",\"t\":[\"<!-- _espnet2.tasks.asvspoof.ASVSpoofTask -->\",\"class espnet2.tasks.asvspoof.ASVSpoofTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2099\":{\"h\":\"espnet2.tasks.abs_task.AbsTask\",\"t\":[\"<!-- _espnet2.tasks.abs_task.AbsTask -->\",\"class espnet2.tasks.abs_task.AbsTask\",\"Bases: ABC\",\"abstract classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_category_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str)\",\"classmethod build_chunk_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str)\",\"abstract classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_iter_factory(args: Namespace, distributed_option: DistributedOption, mode: str, kwargs: dict | None = None)\",\"Build a factory object of mini-batch iterator.\",\"This object is invoked at every epochs to build the iterator for each epoch as following:\",\">>> iter_factory = cls.build_iter_factory(...) >>> for epoch in range(1, max_epoch): ... for keys, batch in iter_fatory.build_iter(epoch): ... model(**batch)\",\"The mini-batches for each epochs are fully controlled by this class. Note that the random seed used for shuffling is decided as “seed + epoch” and the generated mini-batches can be reproduces when resuming.\",\"Note that the definition of “epoch” doesn’t always indicate to run out of the whole training corpus. “–num_iters_per_epoch” option restricts the number of iterations for each epoch and the rest of samples for the originally epoch are left for the next epoch. e.g. If The number of mini-batches equals to 4, the following two are same:\",\"1 epoch without “–num_iters_per_epoch”\",\"4 epoch with “–num_iters_per_epoch” == 1\",\"classmethod build_iter_options(args: Namespace, distributed_option: DistributedOption, mode: str)\",\"abstract classmethod build_model(args: Namespace)\",\"classmethod build_model_from_file(config_file: Path | str | None = None, model_file: Path | str | None = None, device: str = 'cpu')\",\"Build model from the files.\",\"This method is used for inference or fine-tuning.\",\"Parameters:\",\"config_file – The yaml file saved when training.\",\"model_file – The model file saved when training.\",\"device – Device type, “cpu”, “cuda”, or “cuda:N”.\",\"classmethod build_multiple_iter_factory(args: Namespace, distributed_option: DistributedOption, mode: str)\",\"classmethod build_optimizers(args: Namespace, model: Module)\",\"abstract classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"classmethod build_sequence_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str)\",\"classmethod build_streaming_iterator(data_path_and_name_and_type, preprocess_fn, collate_fn, key_file: str | None = None, batch_size: int = 1, dtype: str = <class 'numpy.float32'>, num_workers: int = 1, allow_variable_data_keys: bool = False, ngpu: int = 0, inference: bool = False, mode: str | None = None)\",\"Build DataLoader using iterable dataset\",\"classmethod build_task_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str)\",\"Build task specific iterator factory\",\"Example\",\">>> class YourTask(AbsTask): ... @classmethod ... def add_task_arguments(cls, parser: argparse.ArgumentParser): ... parser.set_defaults(iterator_type=\\\"task\\\") ... ... @classmethod ... def build_task_iter_factory( ... cls, ... args: argparse.Namespace, ... iter_options: IteratorOptions, ... mode: str, ... ): ... return FooIterFactory(...) ... ... @classmethod ... def build_iter_options( .... args: argparse.Namespace, ... distributed_option: DistributedOption, ... mode: str ... ): ... # if you need to customize options object\",\"classmethod check_required_command_args(args: Namespace)\",\"classmethod check_task_requirements(dataset: AbsDataset | IterableESPnetDataset, allow_variable_data_keys: bool, train: bool, inference: bool = False)\",\"Check if the dataset satisfy the requirement of current Task\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= []\",\"classmethod exclude_opts()\",\"The options not to be shown by –print_config\",\"classmethod get_default_config()\",\"Return the configuration as dict.\",\"This method is used by print_config()\",\"classmethod get_parser()\",\"classmethod main(args: Namespace | None = None, cmd: Sequence[str] | None = None)\",\"classmethod main_worker(args: Namespace)\",\"num_optimizers : int= 1\",\"abstract classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod print_config(file=<_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>)\",\"abstract classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2100\":{\"h\":\"espnet2.tasks.diar.DiarizationTask\",\"t\":[\"<!-- _espnet2.tasks.diar.DiarizationTask -->\",\"class espnet2.tasks.diar.DiarizationTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2101\":{\"h\":\"espnet2.tasks.enh_s2t.EnhS2TTask\",\"t\":[\"<!-- _espnet2.tasks.enh_s2t.EnhS2TTask -->\",\"class espnet2.tasks.enh_s2t.EnhS2TTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2102\":{\"h\":\"espnet2.tasks.enh.EnhancementTask\",\"t\":[\"<!-- _espnet2.tasks.enh.EnhancementTask -->\",\"class espnet2.tasks.enh.EnhancementTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_iter_factory(args: Namespace, distributed_option: DistributedOption, mode: str, kwargs: dict | None = None)\",\"Build a factory object of mini-batch iterator.\",\"This object is invoked at every epochs to build the iterator for each epoch as following:\",\">>> iter_factory = cls.build_iter_factory(...) >>> for epoch in range(1, max_epoch): ... for keys, batch in iter_fatory.build_iter(epoch): ... model(**batch)\",\"The mini-batches for each epochs are fully controlled by this class. Note that the random seed used for shuffling is decided as “seed + epoch” and the generated mini-batches can be reproduces when resuming.\",\"Note that the definition of “epoch” doesn’t always indicate to run out of the whole training corpus. “–num_iters_per_epoch” option restricts the number of iterations for each epoch and the rest of samples for the originally epoch are left for the next epoch. e.g. If The number of mini-batches equals to 4, the following two are same:\",\"1 epoch without “–num_iters_per_epoch”\",\"4 epoch with “–num_iters_per_epoch” == 1\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2103\":{\"h\":\"espnet2.tasks.gan_svs.GANSVSTask\",\"t\":[\"<!-- _espnet2.tasks.gan_svs.GANSVSTask -->\",\"class espnet2.tasks.gan_svs.GANSVSTask\",\"Bases: AbsTask\",\"GAN-based Singing-voice-synthesis task.\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_optimizers(args: Namespace, model: ESPnetGANSVSModel)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 2\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of GANTrainer\"]},\"2104\":{\"h\":\"espnet2.tasks.gan_tts.GANTTSTask\",\"t\":[\"<!-- _espnet2.tasks.gan_tts.GANTTSTask -->\",\"class espnet2.tasks.gan_tts.GANTTSTask\",\"Bases: AbsTask\",\"GAN-based text-to-speech task.\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_optimizers(args: Namespace, model: ESPnetGANTTSModel)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 2\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of GANTrainer\"]},\"2105\":{\"h\":\"espnet2.tasks.hubert.HubertTask\",\"t\":[\"<!-- _espnet2.tasks.hubert.HubertTask -->\",\"class espnet2.tasks.hubert.HubertTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2106\":{\"h\":\"espnet2.tasks.abs_task.IteratorOptions\",\"t\":[\"<!-- _espnet2.tasks.abs_task.IteratorOptions -->\",\"class espnet2.tasks.abs_task.IteratorOptions(preprocess_fn: <built-in function callable>, collate_fn: <built-in function callable>, data_path_and_name_and_type: list, shape_files: list, batch_size: int, batch_bins: int, batch_type: str, max_cache_size: float, max_cache_fd: int, allow_multi_rates: bool, distributed: bool, num_batches: Union[int, NoneType], num_iters_per_epoch: Union[int, NoneType], train: bool)\",\"Bases: object\",\"allow_multi_rates : bool\",\"batch_bins : int\",\"batch_size : int\",\"batch_type : str\",\"collate_fn : callable\",\"data_path_and_name_and_type : list\",\"distributed : bool\",\"max_cache_fd : int\",\"max_cache_size : float\",\"num_batches : int | None\",\"num_iters_per_epoch : int | None\",\"preprocess_fn : callable\",\"shape_files : list\",\"train : bool\"]},\"2107\":{\"h\":\"espnet2.tasks.lm.LMTask\",\"t\":[\"<!-- _espnet2.tasks.lm.LMTask -->\",\"class espnet2.tasks.lm.LMTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2108\":{\"h\":\"espnet2.tasks.mt.MTTask\",\"t\":[\"<!-- _espnet2.tasks.mt.MTTask -->\",\"class espnet2.tasks.mt.MTTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2109\":{\"h\":\"espnet2.tasks.s2st.S2STTask\",\"t\":[\"<!-- _espnet2.tasks.s2st.S2STTask -->\",\"class espnet2.tasks.s2st.S2STTask\",\"Bases: STTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"classmethod build_vocoder_from_file(vocoder_config_file: Path | str | None = None, vocoder_file: Path | str | None = None, model: ESPnetS2STModel | None = None, device: str = 'cpu')\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2110\":{\"h\":\"espnet2.tasks.s2t.S2TTask\",\"t\":[\"<!-- _espnet2.tasks.s2t.S2TTask -->\",\"class espnet2.tasks.s2t.S2TTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2111\":{\"h\":\"espnet2.tasks.slu.SLUTask\",\"t\":[\"<!-- _espnet2.tasks.slu.SLUTask -->\",\"class espnet2.tasks.slu.SLUTask\",\"Bases: ASRTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2112\":{\"h\":\"espnet2.tasks.st.STTask\",\"t\":[\"<!-- _espnet2.tasks.st.STTask -->\",\"class espnet2.tasks.st.STTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2113\":{\"h\":\"espnet2.tasks.svs.SVSTask\",\"t\":[\"<!-- _espnet2.tasks.svs.SVSTask -->\",\"class espnet2.tasks.svs.SVSTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"classmethod build_vocoder_from_file(vocoder_config_file: Path | str | None = None, vocoder_file: Path | str | None = None, model: ESPnetSVSModel | None = None, device: str = 'cpu')\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2114\":{\"h\":\"espnet2.tasks.spk.SpeakerTask\",\"t\":[\"<!-- _espnet2.tasks.spk.SpeakerTask -->\",\"class espnet2.tasks.spk.SpeakerTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of SpkTrainer\"]},\"2115\":{\"h\":\"espnet2.tasks.tts2.TTS2Task\",\"t\":[\"<!-- _espnet2.tasks.tts2.TTS2Task -->\",\"class espnet2.tasks.tts2.TTS2Task\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"classmethod build_vocoder_from_file(vocoder_config_file: Path | str | None = None, vocoder_file: Path | str | None = None, model: ESPnetTTS2Model | None = None, device: str = 'cpu')\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2116\":{\"h\":\"espnet2.tasks.tts.TTSTask\",\"t\":[\"<!-- _espnet2.tasks.tts.TTSTask -->\",\"class espnet2.tasks.tts.TTSTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"classmethod build_vocoder_from_file(vocoder_config_file: Path | str | None = None, vocoder_file: Path | str | None = None, model: ESPnetTTSModel | None = None, device: str = 'cpu')\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2117\":{\"h\":\"espnet2.tasks.enh_tse.TargetSpeakerExtractionTask\",\"t\":[\"class espnet2.tasks.enh_tse.TargetSpeakerExtractionTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2118\":{\"h\":\"espnet2.tasks.uasr.UASRTask\",\"t\":[\"<!-- _espnet2.tasks.uasr.UASRTask -->\",\"class espnet2.tasks.uasr.UASRTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool)\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_optimizers(args: Namespace, model: ESPnetUASRModel)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool)\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 2\",\"classmethod optional_data_names(train: bool = True, inference: bool = False)\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False)\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of UASRTrainer\"]},\"2119\":{\"h\":\"espnet2.text.abs_tokenizer.AbsTokenizer\",\"t\":[\"<!-- _espnet2.text.abs_tokenizer.AbsTokenizer -->\",\"class espnet2.text.abs_tokenizer.AbsTokenizer\",\"Bases: ABC\",\"abstract text2tokens(line: str)\",\"abstract tokens2text(tokens: Iterable[str])\"]},\"2120\":{\"h\":\"espnet2.text.char_tokenizer.CharTokenizer\",\"t\":[\"<!-- _espnet2.text.char_tokenizer.CharTokenizer -->\",\"class espnet2.text.char_tokenizer.CharTokenizer(non_linguistic_symbols: Path | str | Iterable[str] | None = None, space_symbol: str = '<space>', remove_non_linguistic_symbols: bool = False, nonsplit_symbols: Iterable[str] | None = None)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str)\",\"tokens2text(tokens: Iterable[str])\"]},\"2121\":{\"h\":\"espnet2.text.phoneme_tokenizer.G2p_en\",\"t\":[\"<!-- _espnet2.text.phoneme_tokenizer.G2p_en -->\",\"class espnet2.text.phoneme_tokenizer.G2p_en(no_space: bool = False)\",\"Bases: object\",\"On behalf of g2p_en.G2p.\",\"g2p_en.G2p isn’t pickalable and it can’t be copied to the other processes via multiprocessing module. As a workaround, g2p_en.G2p is instantiated upon calling this class.\"]},\"2122\":{\"h\":\"espnet2.text.phoneme_tokenizer.G2pk\",\"t\":[\"<!-- _espnet2.text.phoneme_tokenizer.G2pk -->\",\"class espnet2.text.phoneme_tokenizer.G2pk(descritive=False, group_vowels=False, to_syl=False, no_space=False, explicit_space=False, space_symbol='<space>')\",\"Bases: object\",\"On behalf of g2pk.G2p.\",\"g2pk.G2p isn’t pickalable and it can’t be copied to the other processes via multiprocessing module. As a workaround, g2pk.G2p is instantiated upon calling this class.\"]},\"2123\":{\"h\":\"espnet2.text.hugging_face_token_id_converter.HuggingFaceTokenIDConverter\",\"t\":[\"class espnet2.text.hugging_face_token_id_converter.HuggingFaceTokenIDConverter(model_name_or_path: str)\",\"Bases: object\",\"get_num_vocabulary_size()\",\"ids2tokens(integers: ndarray | Iterable[int])\",\"tokens2ids(tokens: Iterable[str])\"]},\"2124\":{\"h\":\"espnet2.text.hugging_face_tokenizer.HuggingFaceTokenizer\",\"t\":[\"class espnet2.text.hugging_face_tokenizer.HuggingFaceTokenizer(model: Path | str)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str)\",\"tokens2text(tokens: Iterable[str])\"]},\"2125\":{\"h\":\"espnet2.text.phoneme_tokenizer.IsG2p\",\"t\":[\"<!-- _espnet2.text.phoneme_tokenizer.IsG2p -->\",\"class espnet2.text.phoneme_tokenizer.IsG2p(dialect: str = 'standard', syllabify: bool = True, word_sep: str = ',', use_dict: bool = True)\",\"Bases: object\",\"Minimal wrapper for https://github.com/grammatek/ice-g2p\",\"The g2p module uses a Bi-LSTM model along with a pronunciation dictionary to generate phonemization Unfortunately does not support multi-thread phonemization as of yet\"]},\"2126\":{\"h\":\"espnet2.text.phoneme_tokenizer.Jaso\",\"t\":[\"<!-- _espnet2.text.phoneme_tokenizer.Jaso -->\",\"class espnet2.text.phoneme_tokenizer.Jaso(space_symbol=' ', no_space=False)\",\"Bases: object\",\"JAMO_LEADS = 'ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑᄒ'\",\"JAMO_TAILS = 'ᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ'\",\"JAMO_VOWELS = 'ᅡᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵ'\",\"PUNC = \\\"!'(),-.:;?\\\"\",\"SPACE = ' '\",\"VALID_CHARS = \\\"ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑ하ᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ!'(),-.:;? \\\"\"]},\"2127\":{\"h\":\"espnet2.text.korean_cleaner.KoreanCleaner\",\"t\":[\"<!-- _espnet2.text.korean_cleaner.KoreanCleaner -->\",\"class espnet2.text.korean_cleaner.KoreanCleaner\",\"Bases: object\",\"classmethod normalize_text(text)\"]},\"2128\":{\"h\":\"espnet2.text.whisper_token_id_converter.OpenAIWhisperTokenIDConverter\",\"t\":[\"class espnet2.text.whisper_token_id_converter.OpenAIWhisperTokenIDConverter(model_type: str, language: str | None = 'en', task: str = 'transcribe', added_tokens_txt: str | None = None, sot: bool = False, speaker_change_symbol: str = '<sc>')\",\"Bases: object\",\"get_num_vocabulary_size()\",\"ids2tokens(integers: ndarray | Iterable[int])\",\"tokens2ids(tokens: Iterable[str])\"]},\"2129\":{\"h\":\"espnet2.text.whisper_tokenizer.OpenAIWhisperTokenizer\",\"t\":[\"class espnet2.text.whisper_tokenizer.OpenAIWhisperTokenizer(model_type: str, language: str = 'en', task: str = 'transcribe', sot: bool = False, speaker_change_symbol: str = '<sc>', added_tokens_txt: str | None = None)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str)\",\"tokens2text(tokens: Iterable[str])\"]},\"2130\":{\"h\":\"espnet2.text.phoneme_tokenizer.PhonemeTokenizer\",\"t\":[\"class espnet2.text.phoneme_tokenizer.PhonemeTokenizer(g2p_type: None | str, non_linguistic_symbols: Path | str | Iterable[str] | None = None, space_symbol: str = '<space>', remove_non_linguistic_symbols: bool = False)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str)\",\"text2tokens_svs(syllable: str)\",\"tokens2text(tokens: Iterable[str])\"]},\"2131\":{\"h\":\"espnet2.text.phoneme_tokenizer.Phonemizer\",\"t\":[\"<!-- _espnet2.text.phoneme_tokenizer.Phonemizer -->\",\"class espnet2.text.phoneme_tokenizer.Phonemizer(backend, word_separator: str | None = None, syllable_separator: str | None = None, phone_separator: str | None = ' ', strip=False, split_by_single_token: bool = False, **phonemizer_kwargs)\",\"Bases: object\",\"Phonemizer module for various languages.\",\"This is wrapper module of https://github.com/bootphon/phonemizer. You can define various g2p modules by specifying options for phonemizer.\",\"See available options: : https://github.com/bootphon/phonemizer/blob/master/phonemizer/phonemize.py#L32\"]},\"2132\":{\"h\":\"espnet2.text.sentencepiece_tokenizer.SentencepiecesTokenizer\",\"t\":[\"class espnet2.text.sentencepiece_tokenizer.SentencepiecesTokenizer(model: Path | str, encode_kwargs: Dict = {})\",\"Bases: AbsTokenizer\",\"text2tokens(line: str)\",\"tokens2text(tokens: Iterable[str])\"]},\"2133\":{\"h\":\"espnet2.text.cleaner.TextCleaner\",\"t\":[\"<!-- _espnet2.text.cleaner.TextCleaner -->\",\"class espnet2.text.cleaner.TextCleaner(cleaner_types: Collection[str] | None = None)\",\"Bases: object\",\"Text cleaner.\"]},\"2134\":{\"h\":\"Examples\",\"t\":[\">>> cleaner = TextCleaner(\\\"tacotron\\\") >>> cleaner(\\\"(Hello-World); & jr. & dr.\\\") 'HELLO WORLD, AND JUNIOR AND DOCTOR'\"]},\"2135\":{\"h\":\"espnet2.text.token_id_converter.TokenIDConverter\",\"t\":[\"class espnet2.text.token_id_converter.TokenIDConverter(token_list: Path | str | Iterable[str], unk_symbol: str = '<unk>')\",\"Bases: object\",\"get_num_vocabulary_size()\",\"ids2tokens(integers: ndarray | Iterable[int])\",\"tokens2ids(tokens: Iterable[str])\"]},\"2136\":{\"h\":\"espnet2.text.word_tokenizer.WordTokenizer\",\"t\":[\"<!-- _espnet2.text.word_tokenizer.WordTokenizer -->\",\"class espnet2.text.word_tokenizer.WordTokenizer(delimiter: str | None = None, non_linguistic_symbols: Path | str | Iterable[str] | None = None, remove_non_linguistic_symbols: bool = False)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str)\",\"tokens2text(tokens: Iterable[str])\"]},\"2137\":{\"h\":\"espnet2.text.build_tokenizer.build_tokenizer\",\"t\":[\"espnet2.text.build_tokenizer.build_tokenizer(token_type: str, bpemodel: Path | str | Iterable[str] | None = None, non_linguistic_symbols: Path | str | Iterable[str] | None = None, remove_non_linguistic_symbols: bool = False, space_symbol: str = '<space>', delimiter: str | None = None, g2p_type: str | None = None, nonsplit_symbol: Iterable[str] | None = None, encode_kwargs: Dict | None = None, whisper_language: str | None = None, whisper_task: str | None = None, sot_asr: bool = False)\",\"A helper function to instantiate Tokenizer\"]},\"2138\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p\",\"t\":[\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p(text)\"]},\"2139\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_accent\",\"t\":[\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_accent(text)\"]},\"2140\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_accent_with_pause\",\"t\":[\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_accent_with_pause(text)\"]},\"2141\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_kana\",\"t\":[\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_kana(text)\"]},\"2142\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_prosody\",\"t\":[\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_prosody(text: str, drop_unvoiced_vowels: bool = True)\",\"Extract phoneme + prosoody symbol sequence from input full-context labels.\",\"The algorithm is based on Prosodic features control by symbols as input of sequence-to-sequence acoustic modeling for neural TTS with some r9y9’s tweaks.\",\"Parameters:\",\"text (str) – Input text.\",\"drop_unvoiced_vowels (bool) – whether to drop unvoiced vowels.\",\"Returns: List of phoneme + prosody symbols.\",\"Return type: List[str]\"]},\"2143\":{\"h\":\"Examples\",\"t\":[\">>> from espnet2.text.phoneme_tokenizer import pyopenjtalk_g2p_prosody >>> pyopenjtalk_g2p_prosody(\\\"こんにちは。\\\") ['^', 'k', 'o', '[', 'N', 'n', 'i', 'ch', 'i', 'w', 'a', '$']\"]},\"2144\":{\"h\":\"espnet2.text.phoneme_tokenizer.pypinyin_g2p\",\"t\":[\"espnet2.text.phoneme_tokenizer.pypinyin_g2p(text)\"]},\"2145\":{\"h\":\"espnet2.text.phoneme_tokenizer.pypinyin_g2p_phone\",\"t\":[\"espnet2.text.phoneme_tokenizer.pypinyin_g2p_phone(text)\"]},\"2146\":{\"h\":\"espnet2.text.phoneme_tokenizer.pypinyin_g2p_phone_without_prosody\",\"t\":[\"espnet2.text.phoneme_tokenizer.pypinyin_g2p_phone_without_prosody(text)\"]},\"2147\":{\"h\":\"espnet2.text.phoneme_tokenizer.split_by_space\",\"t\":[\"espnet2.text.phoneme_tokenizer.split_by_space(text)\"]},\"2148\":{\"h\":\"espnet2.torch_utils.forward_adaptor.ForwardAdaptor\",\"t\":[\"class espnet2.torch_utils.forward_adaptor.ForwardAdaptor(module: Module, name: str)\",\"Bases: Module\",\"Wrapped module to parallelize specified method\",\"torch.nn.DataParallel parallelizes only “forward()” and, maybe, the method having the other name can’t be applied except for wrapping the module just like this class.\"]},\"2149\":{\"h\":\"Examples\",\"t\":[\">>> class A(torch.nn.Module): ... def foo(self, x): ... ... >>> model = A() >>> model = ForwardAdaptor(model, \\\"foo\\\") >>> model = torch.nn.DataParallel(model, device_ids=[0, 1]) >>> x = torch.randn(2, 10) >>> model(x)\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(*args, **kwargs)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2150\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2151\":{\"h\":\"espnet2.torch_utils.add_gradient_noise.add_gradient_noise\",\"t\":[\"espnet2.torch_utils.add_gradient_noise.add_gradient_noise(model: Module, iteration: int, duration: float = 100, eta: float = 1.0, scale_factor: float = 0.55)\",\"Adds noise from a standard normal distribution to the gradients.\",\"The standard deviation (sigma) is controlled by the three hyper-parameters below. sigma goes to zero (no noise) with more iterations.\",\"Parameters:\",\"model – Model.\",\"iteration – Number of iterations.\",\"duration – {100, 1000}: Number of durations to control the interval of the sigma change.\",\"eta – {0.01, 0.3, 1.0}: The magnitude of sigma.\",\"scale_factor – {0.55}: The scale of sigma.\"]},\"2152\":{\"h\":\"espnet2.torch_utils.load_pretrained_model.filter_state_dict\",\"t\":[\"espnet2.torch_utils.load_pretrained_model.filter_state_dict(dst_state: Dict[str, float | Tensor], src_state: Dict[str, float | Tensor])\",\"Filter name, size mismatch instances between dicts.\",\"Parameters:\",\"dst_state – reference state dict for filtering\",\"src_state – target state dict for filtering\"]},\"2153\":{\"h\":\"espnet2.torch_utils.device_funcs.force_gatherable\",\"t\":[\"espnet2.torch_utils.device_funcs.force_gatherable(data, device)\",\"Change object to gatherable in torch.nn.DataParallel recursively\",\"The difference from to_device() is changing to torch.Tensor if float or int value is found.\",\"The restriction to the returned value in DataParallel: : The object must be\",\"torch.cuda.Tensor\",\"1 or more dimension. 0-dimension-tensor sends warning. or a list, tuple, dict.\"]},\"2154\":{\"h\":\"espnet2.torch_utils.model_summary.get_human_readable_count\",\"t\":[\"espnet2.torch_utils.model_summary.get_human_readable_count(number: int)\",\"Return human_readable_count\",\"Originated from: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/memory.py\",\"Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively. .. rubric:: Examples\",\">>> get_human_readable_count(123) '123 ' >>> get_human_readable_count(1234) # (one thousand) '1 K' >>> get_human_readable_count(2e6) # (two million) '2 M' >>> get_human_readable_count(3e9) # (three billion) '3 B' >>> get_human_readable_count(4e12) # (four trillion) '4 T' >>> get_human_readable_count(5e15) # (more than trillion) '5,000 T'\",\"Parameters:number – a positive integer number\",\"Returns: A string formatted according to the pattern described above.\"]},\"2155\":{\"h\":\"espnet2.torch_utils.get_layer_from_string.get_layer\",\"t\":[\"espnet2.torch_utils.get_layer_from_string.get_layer(l_name, library=<module 'torch.nn' from '/hdd/doc/vuepress/espnet_page/tools/venv/lib/python3.8/site-packages/torch/nn/_init_.py'>)\",\"Return layer object handler from library e.g. from torch.nn\",\"E.g. if l_name==”elu”, returns torch.nn.ELU.\",\"Parameters:\",\"l_name (string) – Case insensitive name for layer in library (e.g. .’elu’).\",\"library (module) – Name of library/module where to search for object handler\",\"\\\"torch.nn\\\". (with l_name e.g.) –\",\"Returns: handler for the requested layer e.g. (torch.nn.ELU)\",\"Return type: layer_handler (object)\"]},\"2156\":{\"h\":\"espnet2.torch_utils.initialize.initialize\",\"t\":[\"<!-- _espnet2.torch_utils.initialize.initialize -->\",\"espnet2.torch_utils.initialize.initialize(model: Module, init: str)\",\"Initialize weights of a neural network module.\",\"Parameters are initialized using the given method or distribution.\",\"Custom initialization routines can be implemented into submodules as function espnet_initialization_fn within the custom module.\",\"Parameters:\",\"model – Target.\",\"init – Method of initialization.\"]},\"2157\":{\"h\":\"espnet2.torch_utils.load_pretrained_model.load_pretrained_model\",\"t\":[\"espnet2.torch_utils.load_pretrained_model.load_pretrained_model(init_param: str, model: Module, ignore_init_mismatch: bool, map_location: str = 'cpu')\",\"Load a model state and set it to the model.\",\"Parameters:init_param – <file_path>:<src_key>:<dst_key>:<exclude_Keys>\"]},\"2158\":{\"h\":\"Examples\",\"t\":[\">>> load_pretrained_model(\\\"somewhere/model.pth\\\", model) >>> load_pretrained_model(\\\"somewhere/model.pth:decoder:decoder\\\", model) >>> load_pretrained_model(\\\"somewhere/model.pth:decoder:decoder:\\\", model) >>> load_pretrained_model( ... \\\"somewhere/model.pth:decoder:decoder:decoder.embed\\\", model ... ) >>> load_pretrained_model(\\\"somewhere/decoder.pth::decoder\\\", model)\"]},\"2159\":{\"h\":\"espnet2.torch_utils.model_summary.model_summary\",\"t\":[\"espnet2.torch_utils.model_summary.model_summary(model: Module)\"]},\"2160\":{\"h\":\"espnet2.torch_utils.pytorch_version.pytorch_cudnn_version\",\"t\":[\"espnet2.torch_utils.pytorch_version.pytorch_cudnn_version()\"]},\"2161\":{\"h\":\"espnet2.torch_utils.recursive_op.recursive_average\",\"t\":[\"espnet2.torch_utils.recursive_op.recursive_average(obj, weight: Tensor, distributed: bool = False)\"]},\"2162\":{\"h\":\"espnet2.torch_utils.recursive_op.recursive_divide\",\"t\":[\"espnet2.torch_utils.recursive_op.recursive_divide(a, b: Tensor)\"]},\"2163\":{\"h\":\"espnet2.torch_utils.recursive_op.recursive_sum\",\"t\":[\"espnet2.torch_utils.recursive_op.recursive_sum(obj, weight: Tensor, distributed: bool = False)\"]},\"2164\":{\"h\":\"espnet2.torch_utils.set_all_random_seed.set_all_random_seed\",\"t\":[\"espnet2.torch_utils.set_all_random_seed.set_all_random_seed(seed: int)\"]},\"2165\":{\"h\":\"espnet2.torch_utils.model_summary.to_bytes\",\"t\":[\"<!-- _espnet2.torch_utils.model_summary.to_bytes -->\",\"espnet2.torch_utils.model_summary.to_bytes(dtype)\"]},\"2166\":{\"h\":\"espnet2.torch_utils.device_funcs.to_device\",\"t\":[\"<!-- _espnet2.torch_utils.device_funcs.to_device -->\",\"espnet2.torch_utils.device_funcs.to_device(data, device=None, dtype=None, non_blocking=False, copy=False)\",\"Change the device of object recursively\"]},\"2167\":{\"h\":\"espnet2.train.dataset.AbsDataset\",\"t\":[\"<!-- _espnet2.train.dataset.AbsDataset -->\",\"class espnet2.train.dataset.AbsDataset(*args, **kwds)\",\"Bases: Dataset, ABC\",\"abstract has_name(name)\",\"abstract names()\"]},\"2168\":{\"h\":\"espnet2.train.abs_espnet_model.AbsESPnetModel\",\"t\":[\"class espnet2.train.abs_espnet_model.AbsESPnetModel\",\"Bases: Module, ABC\",\"The common abstract class among each tasks\",\"“ESPnetModel” is referred to a class which inherits torch.nn.Module, and makes the dnn-models forward as its member field, a.k.a delegate pattern, and defines “loss”, “stats”, and “weight” for the task.\",\"If you intend to implement new task in ESPNet, the model must inherit this class. In other words, the “mediator” objects between our training system and the your task class are just only these three values, loss, stats, and weight.\",\"Example\",\">>> from espnet2.tasks.abs_task import AbsTask >>> class YourESPnetModel(AbsESPnetModel): ... def forward(self, input, input_lengths): ... ... ... return loss, stats, weight >>> class YourTask(AbsTask): ... @classmethod ... def build_model(cls, args: argparse.Namespace) -> YourESPnetModel:\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract collect_feats(**batch: Tensor)\",\"abstract forward(**batch: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2169\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2170\":{\"h\":\"espnet2.train.abs_gan_espnet_model.AbsGANESPnetModel\",\"t\":[\"class espnet2.train.abs_gan_espnet_model.AbsGANESPnetModel\",\"Bases: AbsESPnetModel, Module, ABC\",\"The common abstract class among each GAN-based task.\",\"“ESPnetModel” is referred to a class which inherits torch.nn.Module, and makes the dnn-models “forward” as its member field, a.k.a delegate pattern. And “forward” must accept the argument “forward_generator” and Return the dict of “loss”, “stats”, “weight”, and “optim_idx”. “optim_idx” for generator must be 0 and that for discriminator must be 1.\",\"Example\",\">>> from espnet2.tasks.abs_task import AbsTask >>> class YourESPnetModel(AbsGANESPnetModel): ... def forward(self, input, input_lengths, forward_generator=True): ... ... ... if forward_generator: ... # return loss for the generator ... # optim idx 0 indicates generator optimizer ... return dict(loss=loss, stats=stats, weight=weight, optim_idx=0) ... else: ... # return loss for the discriminator ... # optim idx 1 indicates discriminator optimizer ... return dict(loss=loss, stats=stats, weight=weight, optim_idx=1) >>> class YourTask(AbsTask): ... @classmethod ... def build_model(cls, args: argparse.Namespace) -> YourESPnetModel:\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract collect_feats(**batch: Tensor)\",\"abstract forward(forward_generator: bool = True, **batch: Tensor)\",\"Return the generator loss or the discrimiantor loss.\",\"This method must have an argument “forward_generator” to switch the generator loss calculation and the discrimiantor loss calculation. If forward_generator is true, return the generator loss with optim_idx 0. If forward_generator is false, return the discrimiantor loss with optim_idx 1.\",\"Parameters:forward_generator (bool) – Whether to return the generator loss or the discrimiantor loss. This must have the default value.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"training : bool\"]},\"2171\":{\"h\":\"espnet2.train.preprocessor.AbsPreprocessor\",\"t\":[\"<!-- _espnet2.train.preprocessor.AbsPreprocessor -->\",\"class espnet2.train.preprocessor.AbsPreprocessor(train: bool)\",\"Bases: ABC\"]},\"2172\":{\"h\":\"espnet2.train.dataset.AdapterForLabelScpReader\",\"t\":[\"class espnet2.train.dataset.AdapterForLabelScpReader(loader)\",\"Bases: Mapping\",\"keys()\"]},\"2173\":{\"h\":\"espnet2.train.dataset.AdapterForSingingScoreScpReader\",\"t\":[\"class espnet2.train.dataset.AdapterForSingingScoreScpReader(loader)\",\"Bases: Mapping\",\"keys()\"]},\"2174\":{\"h\":\"espnet2.train.dataset.AdapterForSoundScpReader\",\"t\":[\"class espnet2.train.dataset.AdapterForSoundScpReader(loader, dtype=None, allow_multi_rates=False)\",\"Bases: Mapping\",\"keys()\"]},\"2175\":{\"h\":\"espnet2.train.reporter.Average\",\"t\":[\"<!-- _espnet2.train.reporter.Average -->\",\"class espnet2.train.reporter.Average(value: float | int | complex | torch.Tensor | numpy.ndarray)\",\"Bases: ReportedValue\",\"value : float | int | complex | Tensor | ndarray\"]},\"2176\":{\"h\":\"espnet2.train.class_choices.ClassChoices\",\"t\":[\"<!-- _espnet2.train.class_choices.ClassChoices -->\",\"class espnet2.train.class_choices.ClassChoices(name: str, classes: Mapping[str, Type], type_check: Type | None = None, default: str | None = None, optional: bool = False)\",\"Bases: object\",\"Helper class to manage the options for variable objects and its configuration.\",\"Example:\",\">>> class A: ... def __init__(self, foo=3): pass >>> class B: ... def __init__(self, bar=\\\"aaaa\\\"): pass >>> choices = ClassChoices(\\\"var\\\", dict(a=A, b=B), default=\\\"a\\\") >>> import argparse >>> parser = argparse.ArgumentParser() >>> choices.add_arguments(parser) >>> args = parser.parse_args([\\\"--var\\\", \\\"a\\\", \\\"--var_conf\\\", \\\"foo=4\\\") >>> args.var a >>> args.var_conf {\\\"foo\\\": 4} >>> class_obj = choices.get_class(args.var) >>> a_object = class_obj(**args.var_conf)\",\"add_arguments(parser)\",\"choices()\",\"get_class(name: str | None)\"]},\"2177\":{\"h\":\"espnet2.train.collate_fn.CommonCollateFn\",\"t\":[\"<!-- _espnet2.train.collate_fn.CommonCollateFn -->\",\"class espnet2.train.collate_fn.CommonCollateFn(float_pad_value: float | int = 0.0, int_pad_value: int = -32768, not_sequence: Collection[str] = ())\",\"Bases: object\",\"Functor class of common_collate_fn()\"]},\"2178\":{\"h\":\"espnet2.train.preprocessor.CommonPreprocessor\",\"t\":[\"class espnet2.train.preprocessor.CommonPreprocessor(train: bool, use_lang_prompt: bool = False, use_nlp_prompt: bool = False, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, aux_task_names: Collection[str] | None = None, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: str = 'text', fs: int = 0, nonsplit_symbol: Iterable[str] | None = None, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, whisper_language: str | None = None, whisper_task: str | None = None)\",\"Bases: AbsPreprocessor\"]},\"2179\":{\"h\":\"espnet2.train.preprocessor.CommonPreprocessor_multi\",\"t\":[\"class espnet2.train.preprocessor.CommonPreprocessor_multi(train: bool, use_lang_prompt: bool = False, use_nlp_prompt: bool = False, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, aux_task_names: Collection[str] | None = None, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: List[str] = ['text'], fs: int = 0, speaker_change_symbol: Iterable[str] | None = None, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, whisper_language: str | None = None, whisper_task: str | None = None)\",\"Bases: CommonPreprocessor\"]},\"2180\":{\"h\":\"espnet2.train.distributed_utils.DistributedOption\",\"t\":[\"class espnet2.train.distributed_utils.DistributedOption(distributed: bool = False, dist_backend: str = 'nccl', dist_init_method: str = 'env://', dist_world_size: int | NoneType = None, dist_rank: int | NoneType = None, local_rank: int | NoneType = None, ngpu: int = 0, dist_master_addr: str | NoneType = None, dist_master_port: int | NoneType = None, dist_launcher: str | NoneType = None, multiprocessing_distributed: bool = True)\",\"Bases: object\",\"dist_backend : str= 'nccl'\",\"dist_init_method : str= 'env://'\",\"dist_launcher : str | None= None\",\"dist_master_addr : str | None= None\",\"dist_master_port : int | None= None\",\"dist_rank : int | None= None\",\"dist_world_size : int | None= None\",\"distributed : bool= False\",\"init_options()\",\"init_torch_distributed()\",\"local_rank : int | None= None\",\"multiprocessing_distributed : bool= True\",\"ngpu : int= 0\"]},\"2181\":{\"h\":\"espnet2.train.preprocessor.DynamicMixingPreprocessor\",\"t\":[\"class espnet2.train.preprocessor.DynamicMixingPreprocessor(train: bool, source_scp: str | None = None, ref_num: int = 2, dynamic_mixing_gain_db: float = 0.0, speech_name: str = 'speech_mix', speech_ref_name_prefix: str = 'speech_ref', mixture_source_name: str | None = None, utt2spk: str | None = None, categories: List | None = None)\",\"Bases: AbsPreprocessor\"]},\"2182\":{\"h\":\"espnet2.train.dataset.ESPnetDataset\",\"t\":[\"<!-- _espnet2.train.dataset.ESPnetDataset -->\",\"class espnet2.train.dataset.ESPnetDataset(path_name_type_list: Collection[Tuple[str, str, str]], preprocess: Callable[[str, Dict[str, ndarray]], Dict[str, ndarray]] | None = None, float_dtype: str = 'float32', int_dtype: str = 'long', max_cache_size: float | int | str = 0.0, max_cache_fd: int = 0, allow_multi_rates: bool = False)\",\"Bases: AbsDataset\",\"Pytorch Dataset class for ESPNet.\"]},\"2183\":{\"h\":\"Examples\",\"t\":[\">>> dataset = ESPnetDataset([('wav.scp', 'input', 'sound'), ... ('token_int', 'output', 'text_int')], ... ) ... uttid, data = dataset['uttid'] {'input': per_utt_array, 'output': per_utt_array}\",\"has_name(name)\",\"names()\"]},\"2184\":{\"h\":\"espnet2.train.preprocessor.EnhPreprocessor\",\"t\":[\"<!-- _espnet2.train.preprocessor.EnhPreprocessor -->\",\"class espnet2.train.preprocessor.EnhPreprocessor(train: bool, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech_mix', speech_ref_name_prefix: str = 'speech_ref', noise_ref_name_prefix: str = 'noise_ref', dereverb_ref_name_prefix: str = 'dereverb_ref', use_reverberant_ref: bool = False, num_spk: int = 1, num_noise_type: int = 1, sample_rate: int = 8000, force_single_channel: bool = False, channel_reordering: bool = False, categories: List | None = None, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, speech_segment: int | None = None, avoid_allzero_segment: bool = True, flexible_numspk: bool = False)\",\"Bases: CommonPreprocessor\",\"Preprocessor for Speech Enhancement (Enh) task.\"]},\"2185\":{\"h\":\"espnet2.train.gan_trainer.GANTrainer\",\"t\":[\"<!-- _espnet2.train.gan_trainer.GANTrainer -->\",\"class espnet2.train.gan_trainer.GANTrainer\",\"Bases: Trainer\",\"Trainer for GAN-based training.\",\"If you’d like to use this trainer, the model must inherit espnet.train.abs_gan_espnet_model.AbsGANESPnetModel.\",\"classmethod add_arguments(parser: ArgumentParser)\",\"Add additional arguments for GAN-trainer.\",\"classmethod build_options(args: Namespace)\",\"Build options consumed by train(), eval(), and plot_attention().\",\"classmethod train_one_epoch(model: Module, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], scaler: GradScaler | None, reporter: SubReporter, summary_writer, options: GANTrainerOptions, distributed_option: DistributedOption)\",\"Train one epoch.\",\"classmethod validate_one_epoch(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: GANTrainerOptions, distributed_option: DistributedOption)\",\"Validate one epoch.\"]},\"2186\":{\"h\":\"espnet2.train.gan_trainer.GANTrainerOptions\",\"t\":[\"class espnet2.train.gan_trainer.GANTrainerOptions(ngpu: int, resume: bool, use_amp: bool, train_dtype: str, grad_noise: bool, accum_grad: int, grad_clip: float, grad_clip_type: float, log_interval: int | None, no_forward_run: bool, use_matplotlib: bool, use_tensorboard: bool, use_wandb: bool, adapter: str, use_adapter: bool, save_strategy: str, output_dir: Path | str, max_epoch: int, seed: int, sharded_ddp: bool, patience: int | None, keep_nbest_models: int | List[int], nbest_averaging_interval: int, early_stopping_criterion: Sequence[str], best_model_criterion: Sequence[Sequence[str]], val_scheduler_criterion: Sequence[str], unused_parameters: bool, wandb_model_log_interval: int, create_graph_in_tensorboard: bool, generator_first: bool)\",\"Bases: TrainerOptions\",\"Trainer option dataclass for GANTrainer.\",\"generator_first : bool\"]},\"2187\":{\"h\":\"espnet2.train.dataset.H5FileWrapper\",\"t\":[\"<!-- _espnet2.train.dataset.H5FileWrapper -->\",\"class espnet2.train.dataset.H5FileWrapper(path: str)\",\"Bases: object\"]},\"2188\":{\"h\":\"espnet2.train.collate_fn.HuBERTCollateFn\",\"t\":[\"<!-- _espnet2.train.collate_fn.HuBERTCollateFn -->\",\"class espnet2.train.collate_fn.HuBERTCollateFn(float_pad_value: float | int = 0.0, int_pad_value: int = -32768, label_downsampling: int = 1, pad: bool = False, rand_crop: bool = True, crop_audio: bool = True, not_sequence: Collection[str] = (), window_size: float = 25, window_shift: float = 20, sample_rate: float = 16)\",\"Bases: CommonCollateFn\",\"Functor class of common_collate_fn()\"]},\"2189\":{\"h\":\"espnet2.train.iterable_dataset.IterableESPnetDataset\",\"t\":[\"class espnet2.train.iterable_dataset.IterableESPnetDataset(path_name_type_list: Collection[Tuple[str, str, str]], preprocess: Callable[[str, Dict[str, ndarray]], Dict[str, ndarray]] | None = None, float_dtype: str = 'float32', int_dtype: str = 'long', key_file: str | None = None)\",\"Bases: IterableDataset\",\"Pytorch Dataset class for ESPNet.\"]},\"2190\":{\"h\":\"Examples\",\"t\":[\">>> dataset = IterableESPnetDataset([('wav.scp', 'input', 'sound'), ... ('token_int', 'output', 'text_int')], ... ) >>> for uid, data in dataset: ... data {'input': per_utt_array, 'output': per_utt_array}\",\"has_name(name)\",\"names()\"]},\"2191\":{\"h\":\"espnet2.train.preprocessor.MutliTokenizerCommonPreprocessor\",\"t\":[\"class espnet2.train.preprocessor.MutliTokenizerCommonPreprocessor(train: bool, token_type: List[str] = [None], token_list: List[Path | str | Iterable[str]] = [None], bpemodel: List[Path | str | Iterable[str]] = [None], text_cleaner: Collection[str] | None = None, g2p_type: List[str] | str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: List[str] = ['text'], tokenizer_encode_conf: List[Dict] = [{}, {}], fs: int = 0, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, whisper_language: List[str] | None = None, whisper_task: str | None = None)\",\"Bases: CommonPreprocessor\"]},\"2192\":{\"h\":\"espnet2.train.reporter.ReportedValue\",\"t\":[\"<!-- _espnet2.train.reporter.ReportedValue -->\",\"class espnet2.train.reporter.ReportedValue\",\"Bases: object\"]},\"2193\":{\"h\":\"espnet2.train.reporter.Reporter\",\"t\":[\"<!-- _espnet2.train.reporter.Reporter -->\",\"class espnet2.train.reporter.Reporter(epoch: int = 0)\",\"Bases: object\",\"Reporter class.\",\"##\",\"Example s\",\">>> reporter = Reporter() >>> with reporter.observe('train') as sub_reporter: ... for batch in iterator: ... stats = dict(loss=0.2) ... sub_reporter.register(stats)\",\"check_early_stopping(patience: int, key1: str, key2: str, mode: str, epoch: int | None = None, logger=None)\",\"finish_epoch(sub_reporter: SubReporter)\",\"get_all_keys(epoch: int | None = None)\",\"get_best_epoch(key: str, key2: str, mode: str, nbest: int = 0)\",\"get_epoch()\",\"get_keys(epoch: int | None = None)\",\"Returns keys1 e.g. train,eval.\",\"get_keys2(key: str, epoch: int | None = None)\",\"Returns keys2 e.g. loss,acc.\",\"get_value(key: str, key2: str, epoch: int | None = None)\",\"has(key: str, key2: str, epoch: int | None = None)\",\"load_state_dict(state_dict: dict)\",\"log_message(epoch: int | None = None)\",\"matplotlib_plot(output_dir: str | Path)\",\"Plot stats using Matplotlib and save images.\",\"observe(key: str, epoch: int = None)\",\"set_epoch(epoch: int)\",\"sort_epochs(key: str, key2: str, mode: str)\",\"sort_epochs_and_values(key: str, key2: str, mode: str)\",\"Return the epoch which resulted the best value.\",\"Example\",\">>> val = reporter.sort_epochs_and_values('eval', 'loss', 'min') >>> e_1best, v_1best = val[0] >>> e_2best, v_2best = val[1]\",\"sort_values(key: str, key2: str, mode: str)\",\"start_epoch(key: str, epoch: int | None = None)\",\"state_dict()\",\"tensorboard_add_scalar(summary_writer, epoch: int | None = None, key1: str | None = None)\",\"wandb_log(epoch: int | None = None)\"]},\"2194\":{\"h\":\"espnet2.train.preprocessor.S2TPreprocessor\",\"t\":[\"<!-- _espnet2.train.preprocessor.S2TPreprocessor -->\",\"class espnet2.train.preprocessor.S2TPreprocessor(train: bool, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: str = 'text', text_prev_name: str = 'text_prev', text_ctc_name: str = 'text_ctc', fs: int = 16000, na_symbol: str = '<na>', speech_length: float = 30, speech_resolution: float = 0.02, speech_init_silence: float = 1.0, text_prev_apply_prob: float = 0.5, time_apply_prob: float = 0.5, notime_symbol: str = '<notimestamps>', first_time_symbol: str = '<0.00>', last_time_symbol: str = '<30.00>')\",\"Bases: CommonPreprocessor\"]},\"2195\":{\"h\":\"espnet2.train.preprocessor.SLUPreprocessor\",\"t\":[\"<!-- _espnet2.train.preprocessor.SLUPreprocessor -->\",\"class espnet2.train.preprocessor.SLUPreprocessor(train: bool, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, transcript_token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: str = 'text', fs: int = 0, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0)\",\"Bases: CommonPreprocessor\"]},\"2196\":{\"h\":\"espnet2.train.preprocessor.SVSPreprocessor\",\"t\":[\"<!-- _espnet2.train.preprocessor.SVSPreprocessor -->\",\"class espnet2.train.preprocessor.SVSPreprocessor(train: bool, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, singing_volume_normalize: float | None = None, singing_name: str = 'singing', text_name: str = 'text', label_name: str = 'label', midi_name: str = 'score', fs: int32 = 0, hop_length: int32 = 256, phn_seg: dict = {1: [1], 2: [0.25, 1], 3: [0.1, 0.5, 1], 4: [0.05, 0.1, 0.5, 1]})\",\"Bases: AbsPreprocessor\",\"Preprocessor for Sing Voice Sythesis (SVS) task.\"]},\"2197\":{\"h\":\"espnet2.train.preprocessor.SpkPreprocessor\",\"t\":[\"<!-- _espnet2.train.preprocessor.SpkPreprocessor -->\",\"class espnet2.train.preprocessor.SpkPreprocessor(train: bool, target_duration: float, spk2utt: str | None = None, sample_rate: int = 16000, num_eval: int = 10, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_info: List[Tuple[float, str, Tuple[int, int], Tuple[float, float]]] | None = None, noise_apply_prob: float = 1.0, short_noise_thres: float = 0.5)\",\"Bases: CommonPreprocessor\",\"Preprocessor for Speaker tasks.\",\"Parameters:\",\"train (bool) – Whether to use in training mode.\",\"spk2utt (str) – Path to the spk2utt file.\",\"target_duration (float) – Target duration in seconds.\",\"sample_rate (int) – Sampling rate.\",\"num_eval (int) – Number of utterances to be used for evaluation.\",\"rir_scp (str) – Path to the RIR scp file.\",\"rir_apply_prob (float) – Probability of applying RIR.\",\"noise_info (List *[*Tuple *[*float,str,Tuple *[*int,int],Tuple *[*float,float]]]) –\",\"List of tuples of noise information. Each tuple represents a noise type. Each tuple consists of (prob, noise_scp, num_to_mix, db_range).\",\"prob (float) is the probability of applying the noise type.\",\"noise_scp (str) is the path to the noise scp file.\",\"num_to_mix (Tuple[int, int]) is the range of the number of noises : to be mixed.\",\"db_range (Tuple[float, float]) is the range of noise levels in dB.\",\"noise_apply_prob (float) – Probability of applying noise.\",\"short_noise_thres (float) – Threshold of short noise.\"]},\"2198\":{\"h\":\"espnet2.train.spk_trainer.SpkTrainer\",\"t\":[\"<!-- _espnet2.train.spk_trainer.SpkTrainer -->\",\"class espnet2.train.spk_trainer.SpkTrainer\",\"Bases: Trainer\",\"Trainer designed for speaker recognition.\",\"Training will be done as closed set classification. Validation will be open set EER calculation.\",\"classmethod extract_embed(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: TrainerOptions, distributed_option: DistributedOption, output_dir: str, custom_bs: int, average: bool = False)\",\"classmethod validate_one_epoch(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: TrainerOptions, distributed_option: DistributedOption)\"]},\"2199\":{\"h\":\"espnet2.train.reporter.SubReporter\",\"t\":[\"<!-- _espnet2.train.reporter.SubReporter -->\",\"class espnet2.train.reporter.SubReporter(key: str, epoch: int, total_count: int)\",\"Bases: object\",\"This class is used in Reporter.\",\"See the docstring of Reporter for the usage.\",\"finished()\",\"get_epoch()\",\"get_total_count()\",\"Returns the number of iterations over all epochs.\",\"log_message(start: int | None = None, end: int | None = None)\",\"measure_iter_time(iterable, name: str)\",\"measure_time(name: str)\",\"next()\",\"Close up this step and reset state for the next step\",\"register(stats: Dict[str, float | int | complex | Tensor | ndarray | Dict[str, float | int | complex | Tensor | ndarray] | None], weight: float | int | complex | Tensor | ndarray | None = None)\",\"tensorboard_add_scalar(summary_writer, start: int | None = None)\",\"wandb_log(start: int | None = None)\"]},\"2200\":{\"h\":\"espnet2.train.preprocessor.TSEPreprocessor\",\"t\":[\"<!-- _espnet2.train.preprocessor.TSEPreprocessor -->\",\"class espnet2.train.preprocessor.TSEPreprocessor(train: bool, train_spk2enroll: str | None = None, enroll_segment: int | None = None, load_spk_embedding: bool = False, load_all_speakers: bool = False, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech_mix', speech_ref_name_prefix: str = 'speech_ref', noise_ref_name_prefix: str = 'noise_ref', dereverb_ref_name_prefix: str = 'dereverb_ref', use_reverberant_ref: bool = False, num_spk: int = 1, num_noise_type: int = 1, sample_rate: int = 8000, force_single_channel: bool = False, channel_reordering: bool = False, categories: List | None = None, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, speech_segment: int | None = None, avoid_allzero_segment: bool = True, flexible_numspk: bool = False)\",\"Bases: EnhPreprocessor\",\"Preprocessor for Target Speaker Extraction.\"]},\"2201\":{\"h\":\"espnet2.train.trainer.Trainer\",\"t\":[\"<!-- _espnet2.train.trainer.Trainer -->\",\"class espnet2.train.trainer.Trainer\",\"Bases: object\",\"Trainer having a optimizer.\",\"If you’d like to use multiple optimizers, then inherit this class and override the methods if necessary - at least “train_one_epoch()”\",\">>> class TwoOptimizerTrainer(Trainer): ... @classmethod ... def add_arguments(cls, parser): ... ... ... ... @classmethod ... def train_one_epoch(cls, model, optimizers, ...): ... loss1 = model.model1(...) ... loss1.backward() ... optimizers[0].step() ... ... loss2 = model.model2(...) ... loss2.backward() ... optimizers[1].step()\",\"classmethod add_arguments(parser: ArgumentParser)\",\"Reserved for future development of another Trainer\",\"classmethod build_options(args: Namespace)\",\"Build options consumed by train(), eval(), and plot_attention()\",\"classmethod plot_attention(model: Module, output_dir: Path | None, summary_writer, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], reporter: SubReporter, options: TrainerOptions)\",\"static resume(checkpoint: str | Path, model: Module, reporter: Reporter, optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], scaler: GradScaler | None, ngpu: int = 0, strict: bool = True)\",\"classmethod run(model: AbsESPnetModel, optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], train_iter_factory: AbsIterFactory, valid_iter_factory: AbsIterFactory, plot_attention_iter_factory: AbsIterFactory | None, trainer_options, distributed_option: DistributedOption)\",\"Perform training. This method performs the main process of training.\",\"classmethod train_one_epoch(model: Module, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], scaler: GradScaler | None, reporter: SubReporter, summary_writer, options: TrainerOptions, distributed_option: DistributedOption)\",\"classmethod validate_one_epoch(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: TrainerOptions, distributed_option: DistributedOption)\"]},\"2202\":{\"h\":\"espnet2.train.trainer.TrainerOptions\",\"t\":[\"<!-- _espnet2.train.trainer.TrainerOptions -->\",\"class espnet2.train.trainer.TrainerOptions(ngpu: int, resume: bool, use_amp: bool, train_dtype: str, grad_noise: bool, accum_grad: int, grad_clip: float, grad_clip_type: float, log_interval: int | NoneType, no_forward_run: bool, use_matplotlib: bool, use_tensorboard: bool, use_wandb: bool, adapter: str, use_adapter: bool, save_strategy: str, output_dir: pathlib.Path | str, max_epoch: int, seed: int, sharded_ddp: bool, patience: int | NoneType, keep_nbest_models: int | List[int], nbest_averaging_interval: int, early_stopping_criterion: Sequence[str], best_model_criterion: Sequence[Sequence[str]], val_scheduler_criterion: Sequence[str], unused_parameters: bool, wandb_model_log_interval: int, create_graph_in_tensorboard: bool)\",\"Bases: object\",\"accum_grad : int\",\"adapter : str\",\"best_model_criterion : Sequence[Sequence[str]]\",\"create_graph_in_tensorboard : bool\",\"early_stopping_criterion : Sequence[str]\",\"grad_clip : float\",\"grad_clip_type : float\",\"grad_noise : bool\",\"keep_nbest_models : int | List[int]\",\"log_interval : int | None\",\"max_epoch : int\",\"nbest_averaging_interval : int\",\"ngpu : int\",\"no_forward_run : bool\",\"output_dir : Path | str\",\"patience : int | None\",\"resume : bool\",\"save_strategy : str\",\"seed : int\",\"sharded_ddp : bool\",\"train_dtype : str\",\"unused_parameters : bool\",\"use_adapter : bool\",\"use_amp : bool\",\"use_matplotlib : bool\",\"use_tensorboard : bool\",\"use_wandb : bool\",\"val_scheduler_criterion : Sequence[str]\",\"wandb_model_log_interval : int\"]},\"2203\":{\"h\":\"espnet2.train.uasr_trainer.UASRTrainer\",\"t\":[\"<!-- _espnet2.train.uasr_trainer.UASRTrainer -->\",\"class espnet2.train.uasr_trainer.UASRTrainer\",\"Bases: Trainer\",\"Trainer for GAN-based UASR training.\",\"If you’d like to use this trainer, the model must inherit espnet.train.abs_gan_espnet_model.AbsGANESPnetModel.\",\"classmethod add_arguments(parser: ArgumentParser)\",\"Add additional arguments for GAN-trainer.\",\"classmethod build_options(args: Namespace)\",\"Build options consumed by train(), eval(), and plot_attention().\",\"classmethod train_one_epoch(model: Module, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], scaler: GradScaler | None, reporter: SubReporter, summary_writer, options: UASRTrainerOptions, distributed_option: DistributedOption)\",\"Train one epoch for UASR.\",\"classmethod validate_one_epoch(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: UASRTrainerOptions, distributed_option: DistributedOption)\",\"Validate one epoch.\"]},\"2204\":{\"h\":\"espnet2.train.uasr_trainer.UASRTrainerOptions\",\"t\":[\"class espnet2.train.uasr_trainer.UASRTrainerOptions(ngpu: int, resume: bool, use_amp: bool, train_dtype: str, grad_noise: bool, accum_grad: int, grad_clip: float, grad_clip_type: float, log_interval: int | None, no_forward_run: bool, use_matplotlib: bool, use_tensorboard: bool, use_wandb: bool, adapter: str, use_adapter: bool, save_strategy: str, output_dir: Path | str, max_epoch: int, seed: int, sharded_ddp: bool, patience: int | None, keep_nbest_models: int | List[int], nbest_averaging_interval: int, early_stopping_criterion: Sequence[str], best_model_criterion: Sequence[Sequence[str]], val_scheduler_criterion: Sequence[str], unused_parameters: bool, wandb_model_log_interval: int, create_graph_in_tensorboard: bool, generator_first: bool, max_num_warning: int)\",\"Bases: TrainerOptions\",\"Trainer option dataclass for UASRTrainer.\",\"generator_first : bool\",\"max_num_warning : int\"]},\"2205\":{\"h\":\"espnet2.train.reporter.WeightedAverage\",\"t\":[\"<!-- _espnet2.train.reporter.WeightedAverage -->\",\"class espnet2.train.reporter.WeightedAverage(value: Tuple[float | int | complex | torch.Tensor | numpy.ndarray, float | int | complex | torch.Tensor | numpy.ndarray], weight: float | int | complex | torch.Tensor | numpy.ndarray)\",\"Bases: ReportedValue\",\"value : Tuple[float | int | complex | Tensor | ndarray, float | int | complex | Tensor | ndarray]\",\"weight : float | int | complex | Tensor | ndarray\"]},\"2206\":{\"h\":\"espnet2.train.reporter.aggregate\",\"t\":[\"<!-- _espnet2.train.reporter.aggregate -->\",\"espnet2.train.reporter.aggregate(values: Sequence[ReportedValue])\"]},\"2207\":{\"h\":\"espnet2.train.preprocessor.any_allzero\",\"t\":[\"<!-- _espnet2.train.preprocessor.any_allzero -->\",\"espnet2.train.preprocessor.any_allzero(signal)\"]},\"2208\":{\"h\":\"espnet2.train.collate_fn.common_collate_fn\",\"t\":[\"<!-- _espnet2.train.collate_fn.common_collate_fn -->\",\"espnet2.train.collate_fn.common_collate_fn(data: Collection[Tuple[str, Dict[str, ndarray]]], float_pad_value: float | int = 0.0, int_pad_value: int = -32768, not_sequence: Collection[str] = ())\",\"Concatenate ndarray-list to an array and convert to torch.Tensor.\"]},\"2209\":{\"h\":\"Examples\",\"t\":[\">>> from espnet2.samplers.constant_batch_sampler import ConstantBatchSampler, >>> import espnet2.tasks.abs_task >>> from espnet2.train.dataset import ESPnetDataset >>> sampler = ConstantBatchSampler(...) >>> dataset = ESPnetDataset(...) >>> keys = next(iter(sampler) >>> batch = [dataset[key] for key in keys] >>> batch = common_collate_fn(batch) >>> model(**batch)\",\"Note that the dict-keys of batch are propagated from that of the dataset as they are.\"]},\"2210\":{\"h\":\"espnet2.train.preprocessor.detect_non_silence\",\"t\":[\"espnet2.train.preprocessor.detect_non_silence(x: ndarray, threshold: float = 0.01, frame_length: int = 1024, frame_shift: int = 512, window: str = 'boxcar')\",\"Power based voice activity detection.\",\"Parameters:x – (Channel, Time)\",\">>> x = np.random.randn(1000) >>> detect = detect_non_silence(x) >>> assert x.shape == detect.shape >>> assert detect.dtype == np.bool\"]},\"2211\":{\"h\":\"espnet2.train.preprocessor.framing\",\"t\":[\"<!-- _espnet2.train.preprocessor.framing -->\",\"espnet2.train.preprocessor.framing(x, frame_length: int = 512, frame_shift: int = 256, centered: bool = True, padded: bool = True)\"]},\"2212\":{\"h\":\"espnet2.train.distributed_utils.free_port\",\"t\":[\"<!-- _espnet2.train.distributed_utils.free_port -->\",\"espnet2.train.distributed_utils.free_port()\",\"Find free port using bind().\",\"There are some interval between finding this port and using it and the other process might catch the port by that time. Thus it is not guaranteed that the port is really empty.\"]},\"2213\":{\"h\":\"espnet2.train.distributed_utils.get_local_rank\",\"t\":[\"espnet2.train.distributed_utils.get_local_rank(prior=None, launcher: str | None = None)\"]},\"2214\":{\"h\":\"espnet2.train.distributed_utils.get_master_addr\",\"t\":[\"espnet2.train.distributed_utils.get_master_addr(prior=None, launcher: str | None = None)\"]},\"2215\":{\"h\":\"espnet2.train.distributed_utils.get_master_port\",\"t\":[\"espnet2.train.distributed_utils.get_master_port(prior=None)\"]},\"2216\":{\"h\":\"espnet2.train.distributed_utils.get_node_rank\",\"t\":[\"espnet2.train.distributed_utils.get_node_rank(prior=None, launcher: str | None = None)\",\"Get Node Rank.\",\"Use for “multiprocessing distributed” mode. The initial RANK equals to the Node id in this case and the real Rank is set as (nGPU * NodeID) + LOCAL_RANK in torch.distributed.\"]},\"2217\":{\"h\":\"espnet2.train.distributed_utils.get_num_nodes\",\"t\":[\"espnet2.train.distributed_utils.get_num_nodes(prior=None, launcher: str | None = None)\",\"Get the number of nodes.\",\"Use for “multiprocessing distributed” mode. RANK equals to the Node id in this case and the real Rank is set as (nGPU * NodeID) + LOCAL_RANK in torch.distributed.\"]},\"2218\":{\"h\":\"espnet2.train.distributed_utils.get_rank\",\"t\":[\"<!-- _espnet2.train.distributed_utils.get_rank -->\",\"espnet2.train.distributed_utils.get_rank(prior=None, launcher: str | None = None)\"]},\"2219\":{\"h\":\"espnet2.train.distributed_utils.get_world_size\",\"t\":[\"espnet2.train.distributed_utils.get_world_size(prior=None, launcher: str | None = None)\"]},\"2220\":{\"h\":\"espnet2.train.distributed_utils.is_in_slurm_job\",\"t\":[\"espnet2.train.distributed_utils.is_in_slurm_job()\"]},\"2221\":{\"h\":\"espnet2.train.distributed_utils.is_in_slurm_step\",\"t\":[\"espnet2.train.distributed_utils.is_in_slurm_step()\"]},\"2222\":{\"h\":\"espnet2.train.dataset.kaldi_loader\",\"t\":[\"<!-- _espnet2.train.dataset.kaldi_loader -->\",\"espnet2.train.dataset.kaldi_loader(path, float_dtype=None, max_cache_fd: int = 0, allow_multi_rates=False)\"]},\"2223\":{\"h\":\"espnet2.train.dataset.label_loader\",\"t\":[\"<!-- _espnet2.train.dataset.label_loader -->\",\"espnet2.train.dataset.label_loader(path)\"]},\"2224\":{\"h\":\"espnet2.train.iterable_dataset.load_kaldi\",\"t\":[\"<!-- _espnet2.train.iterable_dataset.load_kaldi -->\",\"espnet2.train.iterable_dataset.load_kaldi(input)\"]},\"2225\":{\"h\":\"espnet2.train.dataset.multi_columns_sound_loader\",\"t\":[\"espnet2.train.dataset.multi_columns_sound_loader(path, float_dtype=None, allow_multi_rates=False)\"]},\"2226\":{\"h\":\"espnet2.train.dataset.rand_int_loader\",\"t\":[\"<!-- _espnet2.train.dataset.rand_int_loader -->\",\"espnet2.train.dataset.rand_int_loader(filepath, loader_type)\"]},\"2227\":{\"h\":\"espnet2.train.distributed_utils.resolve_distributed_mode\",\"t\":[\"espnet2.train.distributed_utils.resolve_distributed_mode(args)\"]},\"2228\":{\"h\":\"espnet2.train.dataset.score_loader\",\"t\":[\"<!-- _espnet2.train.dataset.score_loader -->\",\"espnet2.train.dataset.score_loader(path)\"]},\"2229\":{\"h\":\"espnet2.train.dataset.sound_loader\",\"t\":[\"<!-- _espnet2.train.dataset.sound_loader -->\",\"espnet2.train.dataset.sound_loader(path, float_dtype=None, multi_columns=False, allow_multi_rates=False)\"]},\"2230\":{\"h\":\"espnet2.train.reporter.to_reported_value\",\"t\":[\"<!-- _espnet2.train.reporter.to_reported_value -->\",\"espnet2.train.reporter.to_reported_value(v: float | int | complex | Tensor | ndarray, weight: float | int | complex | Tensor | ndarray | None = None)\"]},\"2231\":{\"h\":\"espnet2.train.dataset.variable_columns_sound_loader\",\"t\":[\"espnet2.train.dataset.variable_columns_sound_loader(path, float_dtype=None, allow_multi_rates=False)\"]},\"2232\":{\"h\":\"espnet2.train.reporter.wandb_get_prefix\",\"t\":[\"<!-- _espnet2.train.reporter.wandb_get_prefix -->\",\"espnet2.train.reporter.wandb_get_prefix(key: str)\"]},\"2233\":{\"h\":\"espnet2.tts.feats_extract.abs_feats_extract.AbsFeatsExtract\",\"t\":[\"class espnet2.tts.feats_extract.abs_feats_extract.AbsFeatsExtract\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2234\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract get_parameters()\",\"abstract output_size()\",\"training : bool\"]},\"2235\":{\"h\":\"espnet2.tts.abs_tts.AbsTTS\",\"t\":[\"<!-- _espnet2.tts.abs_tts.AbsTTS -->\",\"class espnet2.tts.abs_tts.AbsTTS\",\"Bases: Module, ABC\",\"TTS abstract class.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, **kwargs)\",\"Calculate outputs and return the loss tensor.\",\"abstract inference(text: Tensor, **kwargs)\",\"Return predicted output as a dict.\",\"property require_raw_speech\",\"Return whether or not raw_speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"2236\":{\"h\":\"espnet2.tts.feats_extract.dio.Dio\",\"t\":[\"<!-- _espnet2.tts.feats_extract.dio.Dio -->\",\"class espnet2.tts.feats_extract.dio.Dio(fs: int | str = 22050, n_fft: int = 1024, hop_length: int = 256, f0min: int = 80, f0max: int = 400, use_token_averaged_f0: bool = True, use_continuous_f0: bool = True, use_log_f0: bool = True, reduction_factor: int_or_none | None = None)\",\"Bases: AbsFeatsExtract\",\"F0 estimation with dio + stonemask algorithm.\",\"This is f0 extractor based on dio + stonmask algorithm introduced in WORLD: a vocoder-based high-quality speech synthesis system for real-time applications.\"]},\"2237\":{\"h\":\"NOTE\",\"t\":[\"This module is based on NumPy implementation. Therefore, the computational graph is not connected.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None, feats_lengths: Tensor | None = None, durations: Tensor | None = None, durations_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2238\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"output_size()\",\"training : bool\"]},\"2239\":{\"h\":\"espnet2.tts.utils.duration_calculator.DurationCalculator\",\"t\":[\"class espnet2.tts.utils.duration_calculator.DurationCalculator\",\"Bases: Module\",\"Duration calculator module.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(att_ws: Tensor)\",\"Convert attention weight to durations.\",\"Parameters:att_ws (Tesnor) – Attention weight tensor (T_feats, T_text) or (#layers, #heads, T_feats, T_text).\",\"Returns: Duration of each input (T_text,). Tensor: Focus rate value.\",\"Return type: LongTensor\",\"training : bool\"]},\"2240\":{\"h\":\"espnet2.tts.espnet_model.ESPnetTTSModel\",\"t\":[\"<!-- _espnet2.tts.espnet_model.ESPnetTTSModel -->\",\"class espnet2.tts.espnet_model.ESPnetTTSModel(feats_extract: AbsFeatsExtract | None, pitch_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, tts: AbsTTS)\",\"Bases: AbsESPnetModel\",\"ESPnet model for text-to-speech task.\",\"Initialize ESPnetTTSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs)\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"durations_lengths (Optional *[*Tensor) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs)\",\"Caclualte outputs and return the loss tensor.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"duration (Optional *[*Tensor]) – Duration tensor.\",\"duration_lengths (Optional *[*Tensor]) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"kwargs – “utt_id” is among the input.\",\"Returns: Loss scalar tensor. Dict[str, float]: Statistics to be monitored. Tensor: Weight tensor to summarize losses.\",\"Return type: Tensor\",\"inference(text: Tensor, speech: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, durations: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, **decode_config)\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"speech (Tensor) – Speech waveform tensor (T_wav).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2241\":{\"h\":\"espnet2.tts.feats_extract.energy.Energy\",\"t\":[\"<!-- _espnet2.tts.feats_extract.energy.Energy -->\",\"class espnet2.tts.feats_extract.energy.Energy(fs: int | str = 22050, n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True, use_token_averaged_energy: bool = True, reduction_factor: int | None = None)\",\"Bases: AbsFeatsExtract\",\"Energy extractor.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None, feats_lengths: Tensor | None = None, durations: Tensor | None = None, durations_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2242\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"output_size()\",\"training : bool\"]},\"2243\":{\"h\":\"espnet2.tts.fastspeech.fastspeech.FastSpeech\",\"t\":[\"class espnet2.tts.fastspeech.fastspeech.FastSpeech(idim: int, odim: int, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, zero_triu: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsTTS\",\"FastSpeech module for end-to-end text-to-speech.\",\"This is a module of FastSpeech, feed-forward Transformer with duration predictor described in FastSpeech: Fast, Robust and Controllable Text to Speech, which does not require any auto-regressive processing during inference, resulting in fast decoding compared with auto-regressive Transformer.\",\"Initialize FastSpeech module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – The number of GST embeddings.\",\"gst_heads (int) – The number of heads in GST multihead attention.\",\"gst_conv_layers (int) – The number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – The number of GRU layers in GST.\",\"gst_gru_units (int) – The number of GRU units in GST.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, durations: Tensor, durations_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"durations (LongTensor) – Batch of padded durations (B, T_text + 1).\",\"durations_lengths (LongTensor) – Batch of duration lengths (B, T_text + 1).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, durations: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, alpha: float = 1.0, use_teacher_forcing: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, idim).\",\"durations (Optional *[*LongTensor]) – Groundtruth of duration (T_text + 1,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"alpha (float) – Alpha to control the speed.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing. If true, groundtruth of duration, pitch and energy will be used.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2244\":{\"h\":\"espnet2.tts.fastspeech2.fastspeech2.FastSpeech2\",\"t\":[\"class espnet2.tts.fastspeech2.fastspeech2.FastSpeech2(idim: int, odim: int, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsTTS\",\"FastSpeech2 module.\",\"This is a module of FastSpeech2 described in FastSpeech 2: Fast and High-Quality End-to-End Text to Speech. Instead of quantized pitch and energy, we use token-averaged value introduced in FastPitch: Parallel Text-to-speech with Pitch Prediction.\",\"Initialize FastSpeech2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"pitch_predictor_layers (int) – Number of pitch predictor layers.\",\"pitch_predictor_chans (int) – Number of pitch predictor channels.\",\"pitch_predictor_kernel_size (int) – Kernel size of pitch predictor.\",\"pitch_predictor_dropout_rate (float) – Dropout rate in pitch predictor.\",\"pitch_embed_kernel_size (float) – Kernel size of pitch embedding.\",\"pitch_embed_dropout_rate (float) – Dropout rate for pitch embedding.\",\"stop_gradient_from_pitch_predictor – Whether to stop gradient from pitch predictor to encoder.\",\"energy_predictor_layers (int) – Number of energy predictor layers.\",\"energy_predictor_chans (int) – Number of energy predictor channels.\",\"energy_predictor_kernel_size (int) – Kernel size of energy predictor.\",\"energy_predictor_dropout_rate (float) – Dropout rate in energy predictor.\",\"energy_embed_kernel_size (float) – Kernel size of energy embedding.\",\"energy_embed_dropout_rate (float) – Dropout rate for energy embedding.\",\"stop_gradient_from_energy_predictor – Whether to stop gradient from energy predictor to encoder.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – The number of GST embeddings.\",\"gst_heads (int) – The number of heads in GST multihead attention.\",\"gst_conv_layers (int) – The number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – The number of GRU layers in GST.\",\"gst_gru_units (int) – The number of GRU units in GST.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, durations: Tensor, durations_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded token ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"durations (LongTensor) – Batch of padded durations (B, T_text + 1).\",\"durations_lengths (LongTensor) – Batch of duration lengths (B, T_text + 1).\",\"pitch (Tensor) – Batch of padded token-averaged pitch (B, T_text + 1, 1).\",\"pitch_lengths (LongTensor) – Batch of pitch lengths (B, T_text + 1).\",\"energy (Tensor) – Batch of padded token-averaged energy (B, T_text + 1, 1).\",\"energy_lengths (LongTensor) – Batch of energy lengths (B, T_text + 1).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, durations: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, alpha: float = 1.0, use_teacher_forcing: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor) – Feature sequence to extract style (N, idim).\",\"durations (Optional *[*Tensor) – Groundtruth of duration (T_text + 1,).\",\"spembs (Optional *[*Tensor) – Speaker embedding vector (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"pitch (Optional *[*Tensor]) – Groundtruth of token-avg pitch (T_text + 1, 1).\",\"energy (Optional *[*Tensor]) – Groundtruth of token-avg energy (T_text + 1, 1).\",\"alpha (float) – Alpha to control the speed.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing. If true, groundtruth of duration, pitch and energy will be used.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"pitch (Tensor): Pitch sequence (T_text + 1,).\",\"energy (Tensor): Energy sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2245\":{\"h\":\"espnet2.tts.fastspeech2.loss.FastSpeech2Loss\",\"t\":[\"class espnet2.tts.fastspeech2.loss.FastSpeech2Loss(use_masking: bool = True, use_weighted_masking: bool = False)\",\"Bases: Module\",\"Loss function module for FastSpeech2.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, p_outs: Tensor, e_outs: Tensor, ys: Tensor, ds: Tensor, ps: Tensor, es: Tensor, ilens: Tensor, olens: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, T_feats, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, T_feats, odim).\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of pitch predictor (B, T_text, 1).\",\"e_outs (Tensor) – Batch of outputs of energy predictor (B, T_text, 1).\",\"ys (Tensor) – Batch of target features (B, T_feats, odim).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"ps (Tensor) – Batch of target token-averaged pitch (B, T_text, 1).\",\"es (Tensor) – Batch of target token-averaged energy (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: Energy predictor loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"2246\":{\"h\":\"espnet2.tts.feats_extract.linear_spectrogram.LinearSpectrogram\",\"t\":[\"class espnet2.tts.feats_extract.linear_spectrogram.LinearSpectrogram(n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: AbsFeatsExtract\",\"Linear amplitude spectrogram.\",\"Stft -> amplitude-spec\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2247\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"Return the parameters required by Vocoder.\",\"output_size()\",\"training : bool\"]},\"2248\":{\"h\":\"espnet2.tts.feats_extract.log_mel_fbank.LogMelFbank\",\"t\":[\"class espnet2.tts.feats_extract.log_mel_fbank.LogMelFbank(fs: int | str = 16000, n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True, n_mels: int = 80, fmin: int | None = 80, fmax: int | None = 7600, htk: bool = False, log_base: float | None = 10.0)\",\"Bases: AbsFeatsExtract\",\"Conventional frontend structure for TTS.\",\"Stft -> amplitude-spec -> Log-Mel-Fbank\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2249\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"Return the parameters required by Vocoder\",\"output_size()\",\"training : bool\"]},\"2250\":{\"h\":\"espnet2.tts.feats_extract.log_spectrogram.LogSpectrogram\",\"t\":[\"class espnet2.tts.feats_extract.log_spectrogram.LogSpectrogram(n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: AbsFeatsExtract\",\"Conventional frontend structure for ASR\",\"Stft -> log-amplitude-spec\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2251\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"Return the parameters required by Vocoder\",\"output_size()\",\"training : bool\"]},\"2252\":{\"h\":\"espnet2.tts.prodiff.denoiser.Mish\",\"t\":[\"<!-- _espnet2.tts.prodiff.denoiser.Mish -->\",\"class espnet2.tts.prodiff.denoiser.Mish\",\"Bases: Module\",\"Mish Activation Function.\",\"Introduced in\",\"`Mish: A Self Regularized Non-Monotonic Activation Function`_\",\".\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Input tensor.\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"2253\":{\"h\":\"espnet2.tts.gst.style_encoder.MultiHeadedAttention\",\"t\":[\"class espnet2.tts.gst.style_encoder.MultiHeadedAttention(q_dim, k_dim, v_dim, n_head, n_feat, dropout_rate=0.0)\",\"Bases: MultiHeadedAttention\",\"Multi head attention module with different input dimension.\",\"Initialize multi head attention module.\",\"training : bool\"]},\"2254\":{\"h\":\"espnet2.tts.utils.parallel_wavegan_pretrained_vocoder.ParallelWaveGANPretrainedVocoder\",\"t\":[\"class espnet2.tts.utils.parallel_wavegan_pretrained_vocoder.ParallelWaveGANPretrainedVocoder(model_file: Path | str, config_file: Path | str | None = None)\",\"Bases: Module\",\"Wrapper class to load the vocoder trained with parallel_wavegan repo.\",\"Initialize ParallelWaveGANPretrainedVocoder module.\",\"forward(feats: Tensor)\",\"Generate waveform with pretrained vocoder.\",\"Parameters:feats (Tensor) – Feature tensor (T_feats, #mels).\",\"Returns: Generated waveform tensor (T_wav).\",\"Return type: Tensor\",\"training : bool\"]},\"2255\":{\"h\":\"espnet2.tts.prodiff.prodiff.ProDiff\",\"t\":[\"<!-- _espnet2.tts.prodiff.prodiff.ProDiff -->\",\"class espnet2.tts.prodiff.prodiff.ProDiff(idim: int, odim: int, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, postnet_layers: int = 0, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, encoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'diffusion', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, denoiser_layers: int = 20, denoiser_channels: int = 256, diffusion_steps: int = 1000, diffusion_timescale: int = 1, diffusion_beta: float = 40.0, diffusion_scheduler: str = 'vpsde', diffusion_cycle_ln: int = 1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsTTS\",\"ProDiff module.\",\"This is a module of ProDiff described in ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech.\",\"Initialize ProDiff module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"pitch_predictor_layers (int) – Number of pitch predictor layers.\",\"pitch_predictor_chans (int) – Number of pitch predictor channels.\",\"pitch_predictor_kernel_size (int) – Kernel size of pitch predictor.\",\"pitch_predictor_dropout_rate (float) – Dropout rate in pitch predictor.\",\"pitch_embed_kernel_size (float) – Kernel size of pitch embedding.\",\"pitch_embed_dropout_rate (float) – Dropout rate for pitch embedding.\",\"stop_gradient_from_pitch_predictor – Whether to stop gradient from pitch predictor to encoder.\",\"energy_predictor_layers (int) – Number of energy predictor layers.\",\"energy_predictor_chans (int) – Number of energy predictor channels.\",\"energy_predictor_kernel_size (int) – Kernel size of energy predictor.\",\"energy_predictor_dropout_rate (float) – Dropout rate in energy predictor.\",\"energy_embed_kernel_size (float) – Kernel size of energy embedding.\",\"energy_embed_dropout_rate (float) – Dropout rate for energy embedding.\",\"stop_gradient_from_energy_predictor – Whether to stop gradient from energy predictor to encoder.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – The number of GST embeddings.\",\"gst_heads (int) – The number of heads in GST multihead attention.\",\"gst_conv_layers (int) – The number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – The number of GRU layers in GST.\",\"gst_gru_units (int) – The number of GRU units in GST.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, durations: Tensor, durations_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded token ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"durations (LongTensor) – Batch of padded durations (B, T_text + 1).\",\"durations_lengths (LongTensor) – Batch of duration lengths (B, T_text + 1).\",\"pitch (Tensor) – Batch of padded token-averaged pitch (B, T_text + 1, 1).\",\"pitch_lengths (LongTensor) – Batch of pitch lengths (B, T_text + 1).\",\"energy (Tensor) – Batch of padded token-averaged energy (B, T_text + 1, 1).\",\"energy_lengths (LongTensor) – Batch of energy lengths (B, T_text + 1).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, durations: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, alpha: float = 1.0, use_teacher_forcing: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor) – Feature sequence to extract style (N, idim).\",\"durations (Optional *[*Tensor) – Groundtruth of duration (T_text + 1,).\",\"spembs (Optional *[*Tensor) – Speaker embedding vector (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"pitch (Optional *[*Tensor]) – Groundtruth of token-avg pitch (T_text + 1, 1).\",\"energy (Optional *[*Tensor]) – Groundtruth of token-avg energy (T_text + 1, 1).\",\"alpha (float) – Alpha to control the speed.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing. If true, groundtruth of duration, pitch and energy will be used.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"pitch (Tensor): Pitch sequence (T_text + 1,).\",\"energy (Tensor): Energy sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2256\":{\"h\":\"espnet2.tts.prodiff.loss.ProDiffLoss\",\"t\":[\"<!-- _espnet2.tts.prodiff.loss.ProDiffLoss -->\",\"class espnet2.tts.prodiff.loss.ProDiffLoss(use_masking: bool = True, use_weighted_masking: bool = False)\",\"Bases: Module\",\"Loss function module for ProDiffLoss.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, p_outs: Tensor, e_outs: Tensor, ys: Tensor, ds: Tensor, ps: Tensor, es: Tensor, ilens: Tensor, olens: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, T_feats, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, T_feats, odim).\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of pitch predictor (B, T_text, 1).\",\"e_outs (Tensor) – Batch of outputs of energy predictor (B, T_text, 1).\",\"ys (Tensor) – Batch of target features (B, T_feats, odim).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"ps (Tensor) – Batch of target token-averaged pitch (B, T_text, 1).\",\"es (Tensor) – Batch of target token-averaged energy (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: Energy predictor loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"2257\":{\"h\":\"espnet2.tts.gst.style_encoder.ReferenceEncoder\",\"t\":[\"class espnet2.tts.gst.style_encoder.ReferenceEncoder(idim=80, conv_layers: int = 6, conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), conv_kernel_size: int = 3, conv_stride: int = 2, gru_layers: int = 1, gru_units: int = 128)\",\"Bases: Module\",\"Reference encoder module.\",\"This module is reference encoder introduced in Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\",\"Parameters:\",\"idim (int,optional) – Dimension of the input mel-spectrogram.\",\"conv_layers (int,optional) – The number of conv layers in the reference encoder.\",\"conv_chans_list – (Sequence[int], optional): List of the number of channels of conv layers in the referece encoder.\",\"conv_kernel_size (int,optional) – Kernel size of conv layers in the reference encoder.\",\"conv_stride (int,optional) – Stride size of conv layers in the reference encoder.\",\"gru_layers (int,optional) – The number of GRU layers in the reference encoder.\",\"gru_units (int,optional) – The number of GRU units in the reference encoder.\",\"Initilize reference encoder module.\",\"forward(speech: Tensor)\",\"Calculate forward propagation.\",\"Parameters:speech (Tensor) – Batch of padded target features (B, Lmax, idim).\",\"Returns: Reference embedding (B, gru_units)\",\"Return type: Tensor\",\"training : bool\"]},\"2258\":{\"h\":\"espnet2.tts.prodiff.denoiser.ResidualBlock\",\"t\":[\"<!-- _espnet2.tts.prodiff.denoiser.ResidualBlock -->\",\"class espnet2.tts.prodiff.denoiser.ResidualBlock(adim: int, channels: int, dilation: int)\",\"Bases: Module\",\"Residual Block for Diffusion Denoiser.\",\"Initialization.\",\"Parameters:\",\"adim (int) – Size of dimensions.\",\"channels (int) – Number of channels.\",\"dilation (int) – Size of dilations.\",\"forward(x: Tensor, condition: Tensor, step: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor.\",\"condition (torch.Tensor) – Conditioning tensor.\",\"step (torch.Tensor) – Number of diffusion step.\",\"Returns: Output tensor.\",\"Return type: Union[torch.Tensor, torch.Tensor]\",\"training : bool\"]},\"2259\":{\"h\":\"espnet2.tts.prodiff.loss.SSimLoss\",\"t\":[\"<!-- _espnet2.tts.prodiff.loss.SSimLoss -->\",\"class espnet2.tts.prodiff.loss.SSimLoss(bias: float = 6.0, window_size: int = 11, channels: int = 1, reduction: str = 'none')\",\"Bases: Module\",\"SSimLoss.\",\"This is an implementation of structural similarity (SSIM) loss. This code is modified from https://github.com/Po-Hsun-Su/pytorch-ssim.\",\"Initialization.\",\"Parameters:\",\"bias (float,optional) – value of the bias. Defaults to 6.0.\",\"window_size (int,optional) – Window size. Defaults to 11.\",\"channels (int,optional) – Number of channels. Defaults to 1.\",\"reduction (str,optional) – Type of reduction during the loss calculation. Defaults to “none”.\",\"forward(outputs: Tensor, target: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"outputs (torch.Tensor) – Batch of output sequences generated by the model (batch, time, mels).\",\"target (torch.Tensor) – Batch of sequences with true states (batch, time, mels).\",\"Returns: Loss scalar value.\",\"Return type: Tensor\",\"ssim(tensor1: Tensor, tensor2: Tensor)\",\"Calculate SSIM loss.\",\"Parameters:\",\"tensor1 (torch.Tensor) – Generated output.\",\"tensor2 (torch.Tensor) – Groundtruth output.\",\"Returns: Loss scalar value.\",\"Return type: Tensor\",\"training : bool\"]},\"2260\":{\"h\":\"espnet2.tts.prodiff.denoiser.SpectogramDenoiser\",\"t\":[\"class espnet2.tts.prodiff.denoiser.SpectogramDenoiser(idim: int, adim: int = 256, layers: int = 20, channels: int = 256, cycle_length: int = 1, timesteps: int = 200, timescale: int = 1, max_beta: float = 40.0, scheduler: str = 'vpsde', dropout_rate: float = 0.05)\",\"Bases: Module\",\"Spectogram Denoiser.\",\"Ref: https://arxiv.org/pdf/2207.06389.pdf.\",\"Initialization.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"adim (int,optional) – Dimension of the hidden states. Defaults to 256.\",\"layers (int,optional) – Number of layers. Defaults to 20.\",\"channels (int,optional) – Number of channels of each layer. Defaults to 256.\",\"cycle_length (int,optional) – Cycle length of the diffusion. Defaults to 1.\",\"timesteps (int,optional) – Number of timesteps of the diffusion. Defaults to 200.\",\"timescale (int,optional) – Number of timescale. Defaults to 1.\",\"max_beta (float,optional) – Maximum beta value for schedueler. Defaults to 40.\",\"scheduler (str,optional) – Type of noise scheduler. Defaults to “vpsde”.\",\"dropout_rate (float,optional) – Dropout rate. Defaults to 0.05.\",\"diffusion(xs_ref: Tensor, steps: Tensor, noise: Tensor | None = None)\",\"Calculate diffusion process during training.\",\"Parameters:\",\"xs_ref (torch.Tensor) – Input tensor.\",\"steps (torch.Tensor) – Number of step.\",\"noise (Optional *[*torch.Tensor],optional) – Noise tensor. Defaults to None.\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\",\"forward(xs: Tensor, ys: Tensor | None = None, masks: Tensor | None = None, is_inference: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (torch.Tensor) – Phoneme-encoded tensor (#batch, time, dims)\",\"ys (Optional *[*torch.Tensor],optional) – Mel-based reference tensor (#batch, time, mels). Defaults to None.\",\"masks (Optional *[*torch.Tensor],optional) – Mask tensor (#batch, time). Defaults to None.\",\"Returns: Output tensor (#batch, time, dims).\",\"Return type: torch.Tensor\",\"forward_denoise(xs_noisy: Tensor, step: Tensor, condition: Tensor)\",\"Calculate forward for denoising diffusion.\",\"Parameters:\",\"xs_noisy (torch.Tensor) – Input tensor.\",\"step (torch.Tensor) – Number of step.\",\"condition (torch.Tensor) – Conditioning tensor.\",\"Returns: Denoised tensor.\",\"Return type: torch.Tensor\",\"inference(condition: Tensor)\",\"Calculate forward during inference.\",\"Parameters:condition (torch.Tensor) – Conditioning tensor (batch, time, dims).\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\",\"training : bool\"]},\"2261\":{\"h\":\"espnet2.tts.gst.style_encoder.StyleEncoder\",\"t\":[\"<!-- _espnet2.tts.gst.style_encoder.StyleEncoder -->\",\"class espnet2.tts.gst.style_encoder.StyleEncoder(idim: int = 80, gst_tokens: int = 10, gst_token_dim: int = 256, gst_heads: int = 4, conv_layers: int = 6, conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), conv_kernel_size: int = 3, conv_stride: int = 2, gru_layers: int = 1, gru_units: int = 128)\",\"Bases: Module\",\"Style encoder.\",\"This module is style encoder introduced in Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\",\"Parameters:\",\"idim (int,optional) – Dimension of the input mel-spectrogram.\",\"gst_tokens (int,optional) – The number of GST embeddings.\",\"gst_token_dim (int,optional) – Dimension of each GST embedding.\",\"gst_heads (int,optional) – The number of heads in GST multihead attention.\",\"conv_layers (int,optional) – The number of conv layers in the reference encoder.\",\"conv_chans_list – (Sequence[int], optional): List of the number of channels of conv layers in the referece encoder.\",\"conv_kernel_size (int,optional) – Kernel size of conv layers in the reference encoder.\",\"conv_stride (int,optional) – Stride size of conv layers in the reference encoder.\",\"gru_layers (int,optional) – The number of GRU layers in the reference encoder.\",\"gru_units (int,optional) – The number of GRU units in the reference encoder.\",\"Initilize global style encoder module.\",\"forward(speech: Tensor)\",\"Calculate forward propagation.\",\"Parameters:speech (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"Returns: Style token embeddings (B, token_dim).\",\"Return type: Tensor\",\"training : bool\"]},\"2262\":{\"h\":\"espnet2.tts.gst.style_encoder.StyleTokenLayer\",\"t\":[\"class espnet2.tts.gst.style_encoder.StyleTokenLayer(ref_embed_dim: int = 128, gst_tokens: int = 10, gst_token_dim: int = 256, gst_heads: int = 4, dropout_rate: float = 0.0)\",\"Bases: Module\",\"Style token layer module.\",\"This module is style token layer introduced in Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\",\"Parameters:\",\"ref_embed_dim (int,optional) – Dimension of the input reference embedding.\",\"gst_tokens (int,optional) – The number of GST embeddings.\",\"gst_token_dim (int,optional) – Dimension of each GST embedding.\",\"gst_heads (int,optional) – The number of heads in GST multihead attention.\",\"dropout_rate (float,optional) – Dropout rate in multi-head attention.\",\"Initilize style token layer module.\",\"forward(ref_embs: Tensor)\",\"Calculate forward propagation.\",\"Parameters:ref_embs (Tensor) – Reference embeddings (B, ref_embed_dim).\",\"Returns: Style token embeddings (B, gst_token_dim).\",\"Return type: Tensor\",\"training : bool\"]},\"2263\":{\"h\":\"espnet2.tts.tacotron2.tacotron2.Tacotron2\",\"t\":[\"<!-- _espnet2.tts.tacotron2.tacotron2.Tacotron2 -->\",\"class espnet2.tts.tacotron2.tacotron2.Tacotron2(idim: int, odim: int, embed_dim: int = 512, elayers: int = 1, eunits: int = 512, econv_layers: int = 3, econv_chans: int = 512, econv_filts: int = 5, atype: str = 'location', adim: int = 512, aconv_chans: int = 32, aconv_filts: int = 15, cumulate_att_w: bool = True, dlayers: int = 2, dunits: int = 1024, prenet_layers: int = 2, prenet_units: int = 256, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, output_activation: str | None = None, use_batch_norm: bool = True, use_concate: bool = True, use_residual: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, dropout_rate: float = 0.5, zoneout_rate: float = 0.1, use_masking: bool = True, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1+L2', use_guided_attn_loss: bool = True, guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)\",\"Bases: AbsTTS\",\"Tacotron2 module for end-to-end text-to-speech.\",\"This is a module of Spectrogram prediction network in Tacotron2 described in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions, which converts the sequence of characters into the sequence of Mel-filterbanks.\",\"Initialize Tacotron2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim – (int) Dimension of the outputs.\",\"embed_dim (int) – Dimension of the token embedding.\",\"elayers (int) – Number of encoder blstm layers.\",\"eunits (int) – Number of encoder blstm units.\",\"econv_layers (int) – Number of encoder conv layers.\",\"econv_filts (int) – Number of encoder conv filter size.\",\"econv_chans (int) – Number of encoder conv filter channels.\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"prenet_layers (int) – Number of prenet layers.\",\"prenet_units (int) – Number of prenet units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"output_activation (str) – Name of activation function for outputs.\",\"adim (int) – Number of dimension of mlp in attention.\",\"aconv_chans (int) – Number of attention conv filter channels.\",\"aconv_filts (int) – Number of attention conv filter size.\",\"cumulate_att_w (bool) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"use_concate (bool) – Whether to concat enc outputs w/ dec lstm outputs.\",\"reduction_factor (int) – Reduction factor.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – Number of GST embeddings.\",\"gst_heads (int) – Number of heads in GST multihead attention.\",\"gst_conv_layers (int) – Number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – Number of GRU layers in GST.\",\"gst_gru_units (int) – Number of GRU units in GST.\",\"dropout_rate (float) – Dropout rate.\",\"zoneout_rate (float) – Zoneout rate.\",\"use_masking (bool) – Whether to mask padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float) – Weight of positive sample of stop token (only for use_masking=True).\",\"loss_type (str) – Loss function type (“L1”, “L2”, or “L1+L2”).\",\"use_guided_attn_loss (bool) – Whether to use guided attention loss.\",\"guided_attn_loss_sigma (float) – Sigma in guided attention loss.\",\"guided_attn_loss_lambda (float) – Lambda in guided attention loss.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_att_constraint: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, idim).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"threshold (float) – Threshold in inference.\",\"minlenratio (float) – Minimum length ratio in inference.\",\"maxlenratio (float) – Maximum length ratio in inference.\",\"use_att_constraint (bool) – Whether to apply attention constraint.\",\"backward_window (int) – Backward window in attention constraint.\",\"forward_window (int) – Forward window in attention constraint.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"prob (Tensor): Output sequence of stop probabilities (T_feats,).\",\"att_w (Tensor): Attention weights (T_feats, T).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2264\":{\"h\":\"espnet2.tts.transformer.transformer.Transformer\",\"t\":[\"class espnet2.tts.transformer.transformer.Transformer(idim: int, odim: int, embed_dim: int = 512, eprenet_conv_layers: int = 3, eprenet_conv_chans: int = 256, eprenet_conv_filts: int = 5, dprenet_layers: int = 2, dprenet_units: int = 256, elayers: int = 6, eunits: int = 1024, adim: int = 512, aheads: int = 4, dlayers: int = 6, dunits: int = 1024, postnet_layers: int = 5, postnet_chans: int = 256, postnet_filts: int = 5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, transformer_enc_dec_attn_dropout_rate: float = 0.1, eprenet_dropout_rate: float = 0.5, dprenet_dropout_rate: float = 0.5, postnet_dropout_rate: float = 0.5, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1', use_guided_attn_loss: bool = True, num_heads_applied_guided_attn: int = 2, num_layers_applied_guided_attn: int = 2, modules_applied_guided_attn: Sequence[str] = 'encoder-decoder', guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)\",\"Bases: AbsTTS\",\"Transformer-TTS module.\",\"This is a module of text-to-speech Transformer described in Neural Speech Synthesis with Transformer Network, which convert the sequence of tokens into the sequence of Mel-filterbanks.\",\"Initialize Transformer module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"embed_dim (int) – Dimension of character embedding.\",\"eprenet_conv_layers (int) – Number of encoder prenet convolution layers.\",\"eprenet_conv_chans (int) – Number of encoder prenet convolution channels.\",\"eprenet_conv_filts (int) – Filter size of encoder prenet convolution.\",\"dprenet_layers (int) – Number of decoder prenet layers.\",\"dprenet_units (int) – Number of decoder prenet hidden units.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"adim (int) – Number of attention transformation dimensions.\",\"aheads (int) – Number of heads for multi head attention.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Filter size of postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"positionwise_layer_type (str) – Position-wise operation type.\",\"positionwise_conv_kernel_size (int) – Kernel size in position wise conv 1d.\",\"reduction_factor (int) – Reduction factor.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – Number of GST embeddings.\",\"gst_heads (int) – Number of heads in GST multihead attention.\",\"gst_conv_layers (int) – Number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – Number of GRU layers in GST.\",\"gst_gru_units (int) – Number of GRU units in GST.\",\"transformer_lr (float) – Initial value of learning rate.\",\"transformer_warmup_steps (int) – Optimizer warmup steps.\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"transformer_enc_dec_attn_dropout_rate (float) – Dropout rate in source attention module.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"eprenet_dropout_rate (float) – Dropout rate in encoder prenet.\",\"dprenet_dropout_rate (float) – Dropout rate in decoder prenet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float) – Positive sample weight in bce calculation (only for use_masking=true).\",\"loss_type (str) – How to calculate loss.\",\"use_guided_attn_loss (bool) – Whether to use guided attention loss.\",\"num_heads_applied_guided_attn (int) – Number of heads in each layer to apply guided attention loss.\",\"num_layers_applied_guided_attn (int) – Number of layers to apply guided attention loss.\",\"modules_applied_guided_attn (Sequence *[*str]) – List of module names to apply guided attention loss.\",\"guided_attn_loss_sigma (float) –\",\"guided_attn_loss_lambda (float) – Lambda in guided attention loss.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_teacher_forcing: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style embedding (T_feats’, idim).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"threshold (float) – Threshold in inference.\",\"minlenratio (float) – Minimum length ratio in inference.\",\"maxlenratio (float) – Maximum length ratio in inference.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"prob (Tensor): Output sequence of stop probabilities (T_feats,).\",\"att_w (Tensor): Source attn weight (#layers, #heads, T_feats, T_text).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2265\":{\"h\":\"espnet2.tts.fastspeech2.variance_predictor.VariancePredictor\",\"t\":[\"class espnet2.tts.fastspeech2.variance_predictor.VariancePredictor(idim: int, n_layers: int = 2, n_chans: int = 384, kernel_size: int = 3, bias: bool = True, dropout_rate: float = 0.5)\",\"Bases: Module\",\"Variance predictor module.\",\"This is a module of variacne predictor described in FastSpeech 2: Fast and High-Quality End-to-End Text to Speech.\",\"Initilize duration predictor module.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"n_layers (int) – Number of convolutional layers.\",\"n_chans (int) – Number of channels of convolutional layers.\",\"kernel_size (int) – Kernel size of convolutional layers.\",\"dropout_rate (float) – Dropout rate.\",\"forward(xs: Tensor, x_masks: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of input sequences (B, Tmax, idim).\",\"x_masks (ByteTensor) – Batch of masks indicating padded part (B, Tmax).\",\"Returns: Batch of predicted sequences (B, Tmax, 1).\",\"Return type: Tensor\",\"training : bool\"]},\"2266\":{\"h\":\"espnet2.tts.feats_extract.ying.Ying\",\"t\":[\"<!-- _espnet2.tts.feats_extract.ying.Ying -->\",\"class espnet2.tts.feats_extract.ying.Ying(fs: int = 22050, w_step: int = 256, W: int = 2048, tau_max: int = 2048, midi_start: int = -5, midi_end: int = 75, octave_range: int = 24, use_token_averaged_ying: bool = False)\",\"Bases: AbsFeatsExtract\",\"Extact Ying-based Features.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"crop_scope(x, yin_start, scope_shift)\",\"forward(input: Tensor, input_lengths: Tensor | None = None, feats_lengths: Tensor | None = None, durations: Tensor | None = None, durations_lengths: Tensor | None = None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2267\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters()\",\"midi_to_lag(m: int, octave_range: float = 12)\",\"converts midi-to-lag, eq. (4)\",\"Parameters:\",\"m – midi\",\"fs – sample_rate\",\"octave_range –\",\"Returns: time lag(tau, c(m)) calculated from midi, eq. (4)\",\"Return type: lag\",\"output_size()\",\"training : bool\",\"yingram(x: Tensor)\",\"calculates yingram from raw audio (multi segment)\",\"Parameters:\",\"x – raw audio, torch.Tensor of shape (t)\",\"W – yingram Window Size\",\"tau_max –\",\"fs – sampling rate\",\"w_step – yingram bin step size\",\"Returns: yingram. torch.Tensor of shape (80 x t’)\",\"Return type: yingram\",\"yingram_from_cmndf(cmndfs: Tensor)\",\"yingram calculator from cMNDFs.\",\"(cumulative Mean Normalized Difference Functions)\",\"Parameters:\",\"cmndfs – torch.Tensor calculated cumulative mean normalized difference function for details, see models/yin.py or eq. (1) and (2)\",\"ms – list of midi(int)\",\"fs – sampling rate\",\"Returns: calculated batch yingram\",\"Return type: y\"]},\"2268\":{\"h\":\"espnet2.tts.feats_extract.yin.cumulativeMeanNormalizedDifferenceFunction\",\"t\":[\"espnet2.tts.feats_extract.yin.cumulativeMeanNormalizedDifferenceFunction(df, N, eps=1e-08)\",\"Compute cumulative mean normalized difference function (CMND).\",\"This corresponds to equation (8) in [1]\",\"Parameters:\",\"df – Difference function\",\"N – length of data\",\"Returns: cumulative mean normalized difference function\",\"Return type: list\"]},\"2269\":{\"h\":\"espnet2.tts.feats_extract.yin.cumulativeMeanNormalizedDifferenceFunctionTorch\",\"t\":[\"espnet2.tts.feats_extract.yin.cumulativeMeanNormalizedDifferenceFunctionTorch(dfs: Tensor, N, eps=1e-08)\"]},\"2270\":{\"h\":\"espnet2.tts.feats_extract.yin.differenceFunction\",\"t\":[\"espnet2.tts.feats_extract.yin.differenceFunction(x, N, tau_max)\",\"Compute difference function of data x. This corresponds to equation (6) in [1]\",\"This solution is implemented directly with torch rfft.\",\"Parameters:\",\"x – audio data (Tensor)\",\"N – length of data\",\"tau_max – integration window size\",\"Returns: difference function\",\"Return type: list\"]},\"2271\":{\"h\":\"espnet2.tts.feats_extract.yin.differenceFunctionTorch\",\"t\":[\"espnet2.tts.feats_extract.yin.differenceFunctionTorch(xs: Tensor, N, tau_max)\",\"pytorch backend batch-wise differenceFunction\",\"has 1e-4 level error with input shape of (32, 22050*1.5) :param xs: :param N: :param tau_max:\",\"Returns:\"]},\"2272\":{\"h\":\"espnet2.tts.feats_extract.yin.differenceFunction_np\",\"t\":[\"espnet2.tts.feats_extract.yin.differenceFunction_np(x, N, tau_max)\",\"Compute difference function of data x. This corresponds to equation (6) in [1]\",\"This solution is implemented directly with Numpy fft.\",\"Parameters:\",\"x – audio data\",\"N – length of data\",\"tau_max – integration window size\",\"Returns: difference function\",\"Return type: list\"]},\"2273\":{\"h\":\"espnet2.tts.prodiff.loss.gaussian\",\"t\":[\"<!-- _espnet2.tts.prodiff.loss.gaussian -->\",\"espnet2.tts.prodiff.loss.gaussian(window_size: int, sigma: float)\",\"Gaussian Noise.\",\"Parameters:\",\"window_size (int) – Window size.\",\"sigma (float) – Noise sigma.\",\"Returns: Noise.\",\"Return type: torch.Tensor\"]},\"2274\":{\"h\":\"espnet2.tts.prodiff.denoiser.noise_scheduler\",\"t\":[\"espnet2.tts.prodiff.denoiser.noise_scheduler(sched_type: str, timesteps: int, min_beta: float = 0.0, max_beta: float = 0.01, s: float = 0.008)\",\"Noise Scheduler.\",\"Parameters:\",\"sched_type (str) – type of scheduler.\",\"timesteps (int) – numbern of time steps.\",\"min_beta (float,optional) – Minimum beta. Defaults to 0.0.\",\"max_beta (float,optional) – Maximum beta. Defaults to 0.01.\",\"s (float,optional) – Scheduler intersection. Defaults to 0.008.\",\"Returns: Noise.\",\"Return type: tensor\"]},\"2275\":{\"h\":\"espnet2.tts2.feats_extract.abs_feats_extract.AbsFeatsExtractDiscrete\",\"t\":[\"class espnet2.tts2.feats_extract.abs_feats_extract.AbsFeatsExtractDiscrete\",\"Bases: Module, ABC\",\"Parse the discrete token sequence into structured data format for predicting. E.g., (1) keep as sequence (2) resize as a matrix (3) multi-resolution …\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2276\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2277\":{\"h\":\"espnet2.tts2.abs_tts2.AbsTTS2\",\"t\":[\"<!-- _espnet2.tts2.abs_tts2.AbsTTS2 -->\",\"class espnet2.tts2.abs_tts2.AbsTTS2\",\"Bases: Module, ABC\",\"TTS2 (Discrete Unit-Based TTS) abstract class.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, **kwargs)\",\"Calculate outputs and return the loss tensor.\",\"abstract inference(text: Tensor, **kwargs)\",\"Return predicted output as a dict.\",\"property require_raw_speech\",\"Return whether or not raw_speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\",\"training : bool\"]},\"2278\":{\"h\":\"espnet2.tts2.espnet_model.ESPnetTTS2Model\",\"t\":[\"<!-- _espnet2.tts2.espnet_model.ESPnetTTS2Model -->\",\"class espnet2.tts2.espnet_model.ESPnetTTS2Model(discrete_feats_extract: AbsFeatsExtractDiscrete, pitch_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, tts: AbsTTS2)\",\"Bases: AbsESPnetModel\",\"ESPnet model for text-to-speech task.\",\"Initialize ESPnetTTSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, discrete_speech: Tensor, discrete_speech_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs)\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"discrete_speech (Tensor) – Discrete speech tensor (B, T_token).\",\"discrete_speech_lengths (Tensor) – Discrete speech length tensor (B,).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"durations_lengths (Optional *[*Tensor) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, discrete_speech: Tensor, discrete_speech_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs)\",\"Caclualte outputs and return the loss tensor.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"discrete_speech (Tensor) – Discrete speech tensor (B, T_token).\",\"discrete_speech_lengths (Tensor) – Discrete speech length tensor (B,).\",\"duration (Optional *[*Tensor]) – Duration tensor.\",\"duration_lengths (Optional *[*Tensor]) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"kwargs – “utt_id” is among the input.\",\"Returns: Loss scalar tensor. Dict[str, float]: Statistics to be monitored. Tensor: Weight tensor to summarize losses.\",\"Return type: Tensor\",\"inference(text: Tensor, speech: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, durations: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, **decode_config)\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"speech (Tensor) – Speech waveform tensor (T_wav).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2279\":{\"h\":\"espnet2.tts2.fastspeech2.fastspeech2_discrete.FastSpeech2Discrete\",\"t\":[\"class espnet2.tts2.fastspeech2.fastspeech2_discrete.FastSpeech2Discrete(idim: int, odim: int, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, ignore_id: int = 0)\",\"Bases: AbsTTS2\",\"FastSpeech2 module with discrete output.\",\"This is a module of discrete-output Fastspeech2: it uses the same Fastspeech2 architecture as tts1, but with discrete token as output.\",\"Initialize FastSpeech2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"pitch_predictor_layers (int) – Number of pitch predictor layers.\",\"pitch_predictor_chans (int) – Number of pitch predictor channels.\",\"pitch_predictor_kernel_size (int) – Kernel size of pitch predictor.\",\"pitch_predictor_dropout_rate (float) – Dropout rate in pitch predictor.\",\"pitch_embed_kernel_size (float) – Kernel size of pitch embedding.\",\"pitch_embed_dropout_rate (float) – Dropout rate for pitch embedding.\",\"stop_gradient_from_pitch_predictor – Whether to stop gradient from pitch predictor to encoder.\",\"energy_predictor_layers (int) – Number of energy predictor layers.\",\"energy_predictor_chans (int) – Number of energy predictor channels.\",\"energy_predictor_kernel_size (int) – Kernel size of energy predictor.\",\"energy_predictor_dropout_rate (float) – Dropout rate in energy predictor.\",\"energy_embed_kernel_size (float) – Kernel size of energy embedding.\",\"energy_embed_dropout_rate (float) – Dropout rate for energy embedding.\",\"stop_gradient_from_energy_predictor – Whether to stop gradient from energy predictor to encoder.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, discrete_feats: Tensor, discrete_feats_lengths: Tensor, durations: Tensor, durations_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False)\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded token ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"discrete_feats (Tensor) – Discrete speech tensor (B, T_token).\",\"discrete_feats_lengths (LongTensor) – Discrete speech length tensor (B,).\",\"durations (LongTensor) – Batch of padded durations (B, T_text + 1).\",\"durations_lengths (LongTensor) – Batch of duration lengths (B, T_text + 1).\",\"pitch (Tensor) – Batch of padded token-averaged pitch (B, T_text + 1, 1).\",\"pitch_lengths (LongTensor) – Batch of pitch lengths (B, T_text + 1).\",\"energy (Tensor) – Batch of padded token-averaged energy (B, T_text + 1, 1).\",\"energy_lengths (LongTensor) – Batch of energy lengths (B, T_text + 1).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, durations: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, alpha: float = 1.0, use_teacher_forcing: bool = False)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"durations (Optional *[*Tensor) – Groundtruth of duration (T_text + 1,).\",\"spembs (Optional *[*Tensor) – Speaker embedding vector (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"pitch (Optional *[*Tensor]) – Groundtruth of token-avg pitch (T_text + 1, 1).\",\"energy (Optional *[*Tensor]) – Groundtruth of token-avg energy (T_text + 1, 1).\",\"alpha (float) – Alpha to control the speed.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing. If true, groundtruth of duration, pitch and energy will be used.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"pitch (Tensor): Pitch sequence (T_text + 1,).\",\"energy (Tensor): Energy sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\",\"training : bool\"]},\"2280\":{\"h\":\"espnet2.tts2.fastspeech2.loss.FastSpeech2LossDiscrete\",\"t\":[\"class espnet2.tts2.fastspeech2.loss.FastSpeech2LossDiscrete(use_masking: bool = True, use_weighted_masking: bool = False, ignore_id: int = -1)\",\"Bases: Module\",\"Loss function module for FastSpeech2.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, p_outs: Tensor, e_outs: Tensor, ys: Tensor, ds: Tensor, ps: Tensor, es: Tensor, ilens: Tensor, olens: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, T_feats, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, T_feats, odim).\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of pitch predictor (B, T_text, 1).\",\"e_outs (Tensor) – Batch of outputs of energy predictor (B, T_text, 1).\",\"ys (Tensor) – Batch of target features in discrete space (B, T_feats).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"ps (Tensor) – Batch of target token-averaged pitch (B, T_text, 1).\",\"es (Tensor) – Batch of target token-averaged energy (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: CrossEntropy loss value. Tensor: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: Energy predictor loss value.\",\"Return type: Tensor\",\"training : bool\"]},\"2281\":{\"h\":\"espnet2.tts2.feats_extract.identity.IdentityFeatureExtract\",\"t\":[\"class espnet2.tts2.feats_extract.identity.IdentityFeatureExtract\",\"Bases: AbsFeatsExtractDiscrete\",\"Keep the input discrete sequence as-is\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2282\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2283\":{\"h\":\"espnet2.uasr.discriminator.abs_discriminator.AbsDiscriminator\",\"t\":[\"class espnet2.uasr.discriminator.abs_discriminator.AbsDiscriminator\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(xs_pad: Tensor, padding_mask: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2284\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2285\":{\"h\":\"espnet2.uasr.generator.abs_generator.AbsGenerator\",\"t\":[\"class espnet2.uasr.generator.abs_generator.AbsGenerator\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(xs_pad: Tensor, ilens: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2286\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size()\",\"training : bool\"]},\"2287\":{\"h\":\"espnet2.uasr.segmenter.abs_segmenter.AbsSegmenter\",\"t\":[\"class espnet2.uasr.segmenter.abs_segmenter.AbsSegmenter\",\"Bases: Module, ABC\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract logit_segment(xs_pad: Tensor, ilens: Tensor)\",\"abstract pre_segment(xs_pad: Tensor, ilens: Tensor)\",\"training : bool\"]},\"2288\":{\"h\":\"espnet2.uasr.loss.abs_loss.AbsUASRLoss\",\"t\":[\"<!-- _espnet2.uasr.loss.abs_loss.AbsUASRLoss -->\",\"class espnet2.uasr.loss.abs_loss.AbsUASRLoss\",\"Bases: Module, ABC\",\"Base class for all Diarization loss modules.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward()\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2289\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\",\"training : bool\"]},\"2290\":{\"h\":\"espnet2.uasr.discriminator.conv_discriminator.ConvDiscriminator\",\"t\":[\"class espnet2.uasr.discriminator.conv_discriminator.ConvDiscriminator(input_dim: int, cfg: Dict | None = None, conv_channels: int = 384, conv_kernel: int = 8, conv_dilation: int = 1, conv_depth: int = 2, linear_emb: str2bool = False, causal: str2bool = True, max_pool: str2bool = False, act_after_linear: str2bool = False, dropout: float = 0.0, spectral_norm: str2bool = False, weight_norm: str2bool = False)\",\"Bases: AbsDiscriminator\",\"convolutional discriminator for UASR.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, padding_mask: Tensor | None)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2291\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2292\":{\"h\":\"espnet2.uasr.generator.conv_generator.ConvGenerator\",\"t\":[\"class espnet2.uasr.generator.conv_generator.ConvGenerator(input_dim: int, output_dim: int, cfg: Dict | None = None, conv_kernel: int = 3, conv_dilation: int = 1, conv_stride: int = 9, pad: int = -1, bias: str2bool = False, dropout: float = 0.0, batch_norm: str2bool = True, batch_norm_weight: float = 30.0, residual: str2bool = True)\",\"Bases: AbsGenerator\",\"convolutional generator for UASR.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"bn_padded_data(feature: Tensor, padding_mask: Tensor)\",\"forward(feats: Tensor, text: Tensor | None, feats_padding_mask: Tensor)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2293\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\",\"training : bool\"]},\"2294\":{\"h\":\"espnet2.uasr.espnet_model.ESPnetUASRModel\",\"t\":[\"<!-- _espnet2.uasr.espnet_model.ESPnetUASRModel -->\",\"class espnet2.uasr.espnet_model.ESPnetUASRModel(frontend: AbsFrontend | None, segmenter: AbsSegmenter | None, generator: AbsGenerator, discriminator: AbsDiscriminator, losses: Dict[str, AbsUASRLoss], kenlm_path: str | None, token_list: list | None, max_epoch: int | None, vocab_size: int, cfg: Dict | None = None, pad: int = 1, sil_token: str = '<SIL>', sos_token: str = '<s>', eos_token: str = '</s>', skip_softmax: str2bool = False, use_gumbel: str2bool = False, use_hard_gumbel: str2bool = True, min_temperature: float = 0.1, max_temperature: float = 2.0, decay_temperature: float = 0.99995, use_collected_training_feats: str2bool = False)\",\"Bases: AbsESPnetModel\",\"Unsupervised ASR model.\",\"The source code is from FAIRSEQ: https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec/unsupervised\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor | None = None, text_lengths: Tensor | None = None, **kwargs)\",\"encode(speech: Tensor, speech_lengths: Tensor)\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor | None = None, text_lengths: Tensor | None = None, pseudo_labels: Tensor | None = None, pseudo_labels_lengths: Tensor | None = None, do_validation: str2bool | None = False, print_hyp: str2bool | None = False, **kwargs)\",\"Frontend + Segmenter + Generator + Discriminator + Calc Loss\",\"Args:\",\"get_optim_index()\",\"inference(speech: Tensor, speech_lengths: Tensor)\",\"is_discriminative_step()\",\"property number_updates\",\"training : bool\"]},\"2295\":{\"h\":\"espnet2.uasr.segmenter.join_segmenter.JoinSegmenter\",\"t\":[\"class espnet2.uasr.segmenter.join_segmenter.JoinSegmenter(cfg: Dict | None = None, subsample_rate: float = 0.25, mean_pool: str2bool = True, mean_join_pool: str2bool = False, remove_zeros: str2bool = False)\",\"Bases: AbsSegmenter\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"logit_segment(logits: Tensor, padding_mask: Tensor)\",\"pre_segment(xs_pad: Tensor, padding_mask: Tensor)\",\"training : bool\"]},\"2296\":{\"h\":\"espnet2.uasr.segmenter.random_segmenter.RandomSegmenter\",\"t\":[\"class espnet2.uasr.segmenter.random_segmenter.RandomSegmenter(subsample_rate: float = 0.25, mean_pool: str2bool = True, mean_join_pool: str2bool = False, remove_zeros: str2bool = False)\",\"Bases: AbsSegmenter\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"logit_segment(xs_pad: Tensor, padding_mask: Tensor)\",\"pre_segment(xs_pad: Tensor, padding_mask: Tensor)\",\"training : bool\"]},\"2297\":{\"h\":\"espnet2.uasr.discriminator.conv_discriminator.SamePad\",\"t\":[\"class espnet2.uasr.discriminator.conv_discriminator.SamePad(kernel_size, causal=False)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2298\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2299\":{\"h\":\"espnet2.uasr.generator.conv_generator.TransposeLast\",\"t\":[\"class espnet2.uasr.generator.conv_generator.TransposeLast(deconstruct_idx=None)\",\"Bases: Module\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Defines the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2300\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"training : bool\"]},\"2301\":{\"h\":\"espnet2.uasr.loss.discriminator_loss.UASRDiscriminatorLoss\",\"t\":[\"class espnet2.uasr.loss.discriminator_loss.UASRDiscriminatorLoss(weight: float = 1.0, smoothing: float = 0.0, smoothing_one_side: str2bool = False, reduction: str = 'sum')\",\"Bases: AbsUASRLoss\",\"discriminator loss for UASR.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(dense_y: Tensor, token_y: Tensor, is_discriminative_step: str2bool)\",\"Forward.\",\"Parameters:\",\"dense_y – predicted logits of generated samples\",\"token_y – predicted logits of real samples\",\"training : bool\"]},\"2302\":{\"h\":\"espnet2.uasr.loss.gradient_penalty.UASRGradientPenalty\",\"t\":[\"class espnet2.uasr.loss.gradient_penalty.UASRGradientPenalty(discriminator: AbsDiscriminator, weight: float = 1.0, probabilistic_grad_penalty_slicing: str2bool = False, reduction: str = 'sum')\",\"Bases: AbsUASRLoss\",\"gradient penalty for UASR.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(fake_sample: Tensor, real_sample: Tensor, is_training: str2bool, is_discrimininative_step: str2bool)\",\"Forward.\",\"Parameters:\",\"fake_sample – generated sample from generator\",\"real_sample – real sample\",\"is_training – whether is at training step\",\"is_discriminative_step – whether is training discriminator\",\"training : bool\"]},\"2303\":{\"h\":\"espnet2.uasr.loss.phoneme_diversity_loss.UASRPhonemeDiversityLoss\",\"t\":[\"class espnet2.uasr.loss.phoneme_diversity_loss.UASRPhonemeDiversityLoss(weight: float = 1.0)\",\"Bases: AbsUASRLoss\",\"phoneme diversity loss for UASR.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(dense_x: Tensor, sample_size: int, is_discriminative_step: str2bool)\",\"Forward.\",\"Parameters:\",\"dense_x – predicted logits of generated samples\",\"sample_size – batch size\",\"is_dicriminative_step – whether is training discriminator\",\"training : bool\"]},\"2304\":{\"h\":\"espnet2.uasr.loss.pseudo_label_loss.UASRPseudoLabelLoss\",\"t\":[\"class espnet2.uasr.loss.pseudo_label_loss.UASRPseudoLabelLoss(weight: float = 1.0, input_dim: int = 128, output_dim: int = 64, downsample_rate: int = 2, ignore_index: int = -1, reduction: str = 'none')\",\"Bases: AbsUASRLoss\",\"auxiliary pseudo label loss for UASR.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inter_x: Tensor, pseudo_labels: Tensor, is_discriminative_step: str2bool)\",\"Forward.\",\"Args:\",\"training : bool\"]},\"2305\":{\"h\":\"espnet2.uasr.loss.smoothness_penalty.UASRSmoothnessPenalty\",\"t\":[\"class espnet2.uasr.loss.smoothness_penalty.UASRSmoothnessPenalty(weight: float = 1.0, reduction: str = 'none')\",\"Bases: AbsUASRLoss\",\"smoothness penalty for UASR.\",\"Initializes internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(dense_logits: Tensor, dense_padding_mask: Tensor, sample_size: int, is_discriminative_step: bool)\",\"Forward.\",\"Parameters:\",\"dense_logits – output logits of generator\",\"dense_padding_mask – padding mask of logits\",\"sample_size – batch size\",\"is_discriminative_step – Whether is training discriminator\",\"training : bool\"]},\"2306\":{\"h\":\"espnetez.config.convert_none_to_None\",\"t\":[\"<!-- _espnetez.config.convert_none_to_None -->\",\"espnetez.config.convert_none_to_None(dic)\"]},\"2307\":{\"h\":\"espnetez.config.from_yaml\",\"t\":[\"<!-- _espnetez.config.from_yaml -->\",\"espnetez.config.from_yaml(task, path)\"]},\"2308\":{\"h\":\"espnetez.config.update_finetune_config\",\"t\":[\"<!-- _espnetez.config.update_finetune_config -->\",\"espnetez.config.update_finetune_config(task, pretrain_config, path)\"]},\"2309\":{\"h\":\"espnet2.utils.config_argparse.ArgumentParser\",\"t\":[\"class espnet2.utils.config_argparse.ArgumentParser(*args, **kwargs)\",\"Bases: ArgumentParser\",\"Simple implementation of ArgumentParser supporting config file\",\"This class is originated from https://github.com/bw2/ConfigArgParse, but this class is lack of some features that it has.\",\"Not supporting multiple config files\",\"Automatically adding “–config” as an option.\",\"Not supporting any formats other than yaml\",\"Not checking argument type\",\"parse_known_args(args=None, namespace=None)\"]},\"2310\":{\"h\":\"espnet2.utils.eer.ComputeErrorRates\",\"t\":[\"<!-- _espnet2.utils.eer.ComputeErrorRates -->\",\"espnet2.utils.eer.ComputeErrorRates(scores, labels)\"]},\"2311\":{\"h\":\"espnet2.utils.eer.ComputeMinDcf\",\"t\":[\"<!-- _espnet2.utils.eer.ComputeMinDcf -->\",\"espnet2.utils.eer.ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa)\"]},\"2312\":{\"h\":\"espnet2.utils.get_default_kwargs.Invalid\",\"t\":[\"<!-- _espnet2.utils.get_default_kwargs.Invalid -->\",\"class espnet2.utils.get_default_kwargs.Invalid\",\"Bases: object\",\"Marker object for not serializable-object\"]},\"2313\":{\"h\":\"espnet2.utils.nested_dict_action.NestedDictAction\",\"t\":[\"class espnet2.utils.nested_dict_action.NestedDictAction(option_strings, dest, nargs=None, default=None, choices=None, required=False, help=None, metavar=None)\",\"Bases: Action\",\"Action class to append items to dict object.\"]},\"2314\":{\"h\":\"Examples\",\"t\":[\">>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--conf', action=NestedDictAction, ... default={'a': 4}) >>> parser.parse_args(['--conf', 'a=3', '--conf', 'c=4']) Namespace(conf={'a': 3, 'c': 4}) >>> parser.parse_args(['--conf', 'c.d=4']) Namespace(conf={'a': 4, 'c': {'d': 4}}) >>> parser.parse_args(['--conf', 'c.d=4', '--conf', 'c=2']) Namespace(conf={'a': 4, 'c': 2}) >>> parser.parse_args(['--conf', '{d: 5, e: 9}']) Namespace(conf={'d': 5, 'e': 9})\"]},\"2315\":{\"h\":\"espnet2.utils.yaml_no_alias_safe_dump.NoAliasSafeDumper\",\"t\":[\"class espnet2.utils.yaml_no_alias_safe_dump.NoAliasSafeDumper(stream, default_style=None, default_flow_style=False, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None, encoding=None, explicit_start=None, explicit_end=None, version=None, tags=None, sort_keys=True)\",\"Bases: SafeDumper\",\"ignore_aliases(data)\"]},\"2316\":{\"h\":\"espnet2.utils.sized_dict.SizedDict\",\"t\":[\"<!-- _espnet2.utils.sized_dict.SizedDict -->\",\"class espnet2.utils.sized_dict.SizedDict(shared: bool = False, data: dict | None = None)\",\"Bases: MutableMapping\"]},\"2317\":{\"h\":\"espnet2.utils.griffin_lim.Spectrogram2Waveform\",\"t\":[\"class espnet2.utils.griffin_lim.Spectrogram2Waveform(n_fft: int, n_shift: int, fs: int | None = None, n_mels: int | None = None, win_length: int | None = None, window: str | None = 'hann', fmin: int | None = None, fmax: int | None = None, griffin_lim_iters: int | None = 8)\",\"Bases: object\",\"Spectrogram to waveform conversion module.\",\"Initialize module.\",\"Parameters:\",\"fs – Sampling frequency.\",\"n_fft – The number of FFT points.\",\"n_shift – Shift size in points.\",\"n_mels – The number of mel basis.\",\"win_length – Window length in points.\",\"window – Window function type.\",\"f_min – Minimum frequency to analyze.\",\"f_max – Maximum frequency to analyze.\",\"griffin_lim_iters – The number of iterations.\"]},\"2318\":{\"h\":\"espnet2.utils.build_dataclass.build_dataclass\",\"t\":[\"espnet2.utils.build_dataclass.build_dataclass(dataclass, args: Namespace)\",\"Helper function to build dataclass from ‘args’.\"]},\"2319\":{\"h\":\"espnet2.utils.types.float_or_none\",\"t\":[\"<!-- _espnet2.utils.types.float_or_none -->\",\"espnet2.utils.types.float_or_none(value: str)\",\"float_or_none.\"]},\"2320\":{\"h\":\"Examples\",\"t\":[\">>> import argparse >>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--foo', type=float_or_none) >>> parser.parse_args(['--foo', '4.5']) Namespace(foo=4.5) >>> parser.parse_args(['--foo', 'none']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'null']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'nil']) Namespace(foo=None)\"]},\"2321\":{\"h\":\"espnet2.utils.kwargs2args.func\",\"t\":[\"<!-- _espnet2.utils.kwargs2args.func -->\",\"espnet2.utils.kwargs2args.func(a: int, b, *, c, **kwargs)\"]},\"2322\":{\"h\":\"espnet2.utils.get_default_kwargs.get_default_kwargs\",\"t\":[\"espnet2.utils.get_default_kwargs.get_default_kwargs(func)\",\"Get the default values of the input function.\"]},\"2323\":{\"h\":\"Examples\",\"t\":[\">>> def func(a, b=3): pass >>> get_default_kwargs(func) {'b': 3}\"]},\"2324\":{\"h\":\"espnet2.utils.sized_dict.get_size\",\"t\":[\"<!-- _espnet2.utils.sized_dict.get_size -->\",\"espnet2.utils.sized_dict.get_size(obj, seen=None)\",\"Recursively finds size of objects\",\"Taken from https://github.com/bosswissam/pysize\"]},\"2325\":{\"h\":\"espnet2.utils.griffin_lim.griffin_lim\",\"t\":[\"<!-- _espnet2.utils.griffin_lim.griffin_lim -->\",\"espnet2.utils.griffin_lim.griffin_lim(spc: ndarray, n_fft: int, n_shift: int, win_length: int | None = None, window: str | None = 'hann', n_iter: int | None = 32)\",\"Convert linear spectrogram into waveform using Griffin-Lim.\",\"Parameters:\",\"spc – Linear spectrogram (T, n_fft // 2 + 1).\",\"n_fft – The number of FFT points.\",\"n_shift – Shift size in points.\",\"win_length – Window length in points.\",\"window – Window function type.\",\"n_iter – The number of iterations.\",\"Returns: Reconstructed waveform (N,).\"]},\"2326\":{\"h\":\"espnet2.utils.types.humanfriendly_parse_size_or_none\",\"t\":[\"espnet2.utils.types.humanfriendly_parse_size_or_none(value)\"]},\"2327\":{\"h\":\"espnet2.utils.types.int_or_none\",\"t\":[\"<!-- _espnet2.utils.types.int_or_none -->\",\"espnet2.utils.types.int_or_none(value: str)\",\"int_or_none.\"]},\"2328\":{\"h\":\"Examples\",\"t\":[\">>> import argparse >>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--foo', type=int_or_none) >>> parser.parse_args(['--foo', '456']) Namespace(foo=456) >>> parser.parse_args(['--foo', 'none']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'null']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'nil']) Namespace(foo=None)\"]},\"2329\":{\"h\":\"espnet2.utils.kwargs2args.kwargs2args\",\"t\":[\"<!-- _espnet2.utils.kwargs2args.kwargs2args -->\",\"espnet2.utils.kwargs2args.kwargs2args(func, kwargs)\"]},\"2330\":{\"h\":\"espnet2.utils.griffin_lim.logmel2linear\",\"t\":[\"<!-- _espnet2.utils.griffin_lim.logmel2linear -->\",\"espnet2.utils.griffin_lim.logmel2linear(lmspc: ndarray, fs: int, n_fft: int, n_mels: int, fmin: int | None = None, fmax: int | None = None)\",\"Convert log Mel filterbank to linear spectrogram.\",\"Parameters:\",\"lmspc – Log Mel filterbank (T, n_mels).\",\"fs – Sampling frequency.\",\"n_fft – The number of FFT points.\",\"n_mels – The number of mel basis.\",\"f_min – Minimum frequency to analyze.\",\"f_max – Maximum frequency to analyze.\",\"Returns: Linear spectrogram (T, n_fft // 2 + 1).\"]},\"2331\":{\"h\":\"espnet2.utils.types.remove_parenthesis\",\"t\":[\"<!-- _espnet2.utils.types.remove_parenthesis -->\",\"espnet2.utils.types.remove_parenthesis(value: str)\"]},\"2332\":{\"h\":\"espnet2.utils.types.remove_quotes\",\"t\":[\"<!-- _espnet2.utils.types.remove_quotes -->\",\"espnet2.utils.types.remove_quotes(value: str)\"]},\"2333\":{\"h\":\"espnet2.utils.types.str2bool\",\"t\":[\"<!-- _espnet2.utils.types.str2bool -->\",\"espnet2.utils.types.str2bool(value: str)\"]},\"2334\":{\"h\":\"espnet2.utils.types.str2pair_str\",\"t\":[\"<!-- _espnet2.utils.types.str2pair_str -->\",\"espnet2.utils.types.str2pair_str(value: str)\",\"str2pair_str.\"]},\"2335\":{\"h\":\"Examples\",\"t\":[\">>> import argparse >>> str2pair_str('abc,def ') ('abc', 'def') >>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--foo', type=str2pair_str) >>> parser.parse_args(['--foo', 'abc,def']) Namespace(foo=('abc', 'def'))\"]},\"2336\":{\"h\":\"espnet2.utils.types.str2triple_str\",\"t\":[\"<!-- _espnet2.utils.types.str2triple_str -->\",\"espnet2.utils.types.str2triple_str(value: str)\",\"str2triple_str.\"]},\"2337\":{\"h\":\"Examples\",\"t\":[\">>> str2triple_str('abc,def ,ghi') ('abc', 'def', 'ghi')\"]},\"2338\":{\"h\":\"espnet2.utils.types.str_or_int\",\"t\":[\"<!-- _espnet2.utils.types.str_or_int -->\",\"espnet2.utils.types.str_or_int(value: str)\"]},\"2339\":{\"h\":\"espnet2.utils.types.str_or_none\",\"t\":[\"<!-- _espnet2.utils.types.str_or_none -->\",\"espnet2.utils.types.str_or_none(value: str)\",\"str_or_none.\"]},\"2340\":{\"h\":\"Examples\",\"t\":[\">>> import argparse >>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--foo', type=str_or_none) >>> parser.parse_args(['--foo', 'aaa']) Namespace(foo='aaa') >>> parser.parse_args(['--foo', 'none']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'null']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'nil']) Namespace(foo=None)\"]},\"2341\":{\"h\":\"espnet2.utils.eer.tuneThresholdfromScore\",\"t\":[\"<!-- _espnet2.utils.eer.tuneThresholdfromScore -->\",\"espnet2.utils.eer.tuneThresholdfromScore(scores, labels, target_fa, target_fr=None)\"]},\"2342\":{\"h\":\"espnet2.utils.yaml_no_alias_safe_dump.yaml_no_alias_safe_dump\",\"t\":[\"espnet2.utils.yaml_no_alias_safe_dump.yaml_no_alias_safe_dump(data, stream=None, **kwargs)\",\"Safe-dump in yaml with no anchor/alias\"]},\"2343\":{\"h\":\"espnetez.data.dump.create_dump_file\",\"t\":[\"<!-- _espnetez.data.dump.create_dump_file -->\",\"espnetez.data.dump.create_dump_file(dump_dir: str | Path, dataset: Dict[str, Dict] | List[Dict], data_inputs: Dict[str, Dict])\",\"Create a dump file for a dataset.\",\"Parameters:\",\"dump_dir (str) – Output folder of the dump files.\",\"dataset (Union *[*Dict *[*str,Dict],List *[*Dict]]) – Dictionary of dataset.\",\"data_inputs (Dict *[*str,List *[*str,str]]) – data information for each input variables.\"]},\"2344\":{\"h\":\"espnetez.data.dump.join_dumps\",\"t\":[\"<!-- _espnetez.data.dump.join_dumps -->\",\"espnetez.data.dump.join_dumps(dump_paths: List[str], dump_prefix: List[str], output_dir: str | Path)\",\"Create a joined dump file from a list of dump paths.\",\"Parameters:\",\"dump_paths (List *[*str]) – List of paths for the dump directory.\",\"dump_prefix (List *[*str]) – List of prefixes for the dump files.\",\"output_dir (Union *[*str,Path]) – Output directory of the joined dump file.\"]},\"2345\":{\"h\":\"espnetez.dataloader.Dataloader\",\"t\":[\"<!-- _espnetez.dataloader.Dataloader -->\",\"class espnetez.dataloader.Dataloader(**kwargs)\",\"Bases: AbsIterFactory\",\"build_iter(epoch: int, shuffle: bool | None = None)\"]},\"2346\":{\"h\":\"espnetez.dataset.ESPnetEZDataset\",\"t\":[\"<!-- _espnetez.dataset.ESPnetEZDataset -->\",\"class espnetez.dataset.ESPnetEZDataset(dataset, data_info)\",\"Bases: AbsDataset\",\"has_name(name)\",\"names()\"]},\"2347\":{\"h\":\"espnetez.preprocess.sentencepiece.prepare_sentences\",\"t\":[\"espnetez.preprocess.sentencepiece.prepare_sentences(dump_text_paths: str | Path, output_path: str | Path, remove_characters: str = '')\",\"Create train.txt file for sentencepiece training from the given dump file.\",\"Parameters:\",\"dump_text_paths (Union *[*str,Path]) – Dump text file path.\",\"output_path (Union *[*str,Path]) – Output directory for train.txt file.\",\"remove_characters (str) – Characters to be removed from the text.\"]},\"2348\":{\"h\":\"espnetez.preprocess.tokenizer.tokenize\",\"t\":[\"<!-- _espnetez.preprocess.tokenizer.tokenize -->\",\"espnetez.preprocess.tokenizer.tokenize(input, output, write_vocabulary=True, blank='<blank>', oov='<unk>', sos_eos='<sos/eos>', **kwargs)\"]},\"2349\":{\"h\":\"espnetez.preprocess.sentencepiece.train_sentencepiece\",\"t\":[\"espnetez.preprocess.sentencepiece.train_sentencepiece(dump_text_path: str | Path, output_path: str | Path, vocab_size: int = 5000, character_coverage: float = 0.9995, model_type: str = 'bpe', user_defined_symbols: list = [])\",\"Main function to train sentencepiece model.\",\"Parameters:\",\"dump_text_path (Union *[*str,Path]) – Path to the train.txt file.\",\"output_path (Union *[*str,Path]) – Output directory to store sentencepiece model and vocaburary list.\",\"vocab_size (int,optional) – Vocaburary size. Defaults to 5000.\",\"character_coverage (float,optional) – Character coverage. Defaults to 0.9995.\",\"model_type (str,optional) – Model type of sentencepiece. Defaults to “bpe”.\",\"user_defined_symbols (list,optional) – User defined symbols.\"]},\"2350\":{\"h\":\"espnetez.trainer.Trainer\",\"t\":[\"<!-- _espnetez.trainer.Trainer -->\",\"class espnetez.trainer.Trainer(task, train_config, output_dir, stats_dir, data_info=None, train_dump_dir=None, valid_dump_dir=None, train_dataset=None, valid_dataset=None, train_dataloader=None, valid_dataloader=None, build_model_fn=None, **kwargs)\",\"Bases: object\",\"Generic trainer class for ESPnet training!\",\"collect_stats()\",\"train()\"]},\"2351\":{\"h\":\"espnetez.trainer.check_argument\",\"t\":[\"<!-- _espnetez.trainer.check_argument -->\",\"espnetez.trainer.check_argument(train_dump_dir, valid_dump_dir, train_dataset, valid_dataset, train_dataloader, valid_dataloader)\"]},\"2352\":{\"h\":\"espnetez.task.get_ez_task\",\"t\":[\"<!-- _espnetez.task.get_ez_task -->\",\"espnetez.task.get_ez_task(task_name: str, use_custom_dataset: bool = False)\"]},\"2353\":{\"h\":\"espnetez.task.get_ez_task_with_dataset\",\"t\":[\"<!-- _espnetez.task.get_ez_task_with_dataset -->\",\"espnetez.task.get_ez_task_with_dataset(task_name: str)\"]},\"2354\":{\"h\":\"CMU 11751/18781 2021: ESPnet Tutorial\",\"t\":[\"ESPnet is an end-to-end speech processing toolkit, initially focused on end-to-end speech recognition and end-to-end text-to-speech, but now extended to various other speech processing. ESPnet uses PyTorch as a main deep learning engine, and also follows Kaldi style data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments.\",\"This tutorial is based on the collection of espnet notebook demos https://github.com/espnet/notebook, espnet documentations in https://espnet.github.io/espnet/, and README.md in https://github.com/espnet/espnet\",\"Author: Shinji Watanabe (@sw005320)\"]},\"2355\":{\"h\":\"Useful links\",\"t\":[\"Installation https://espnet.github.io/espnet/installation.html\",\"Usage https://espnet.github.io/espnet/espnet2_tutorial.html\",\"ESPnet covers various speech applications and their pre-trained models.\",\"Please check a model shown in espnet_model_zoo\",\"We can play with a demo based on these pre-trained models.\",\"What we only need is to install espnet_model_zoo\",\"Note that this pip based installation does not include training and so on. The full installation is explained later.\",\"You can also find similar demos in HuggingFace Hub https://huggingface.co/espnet\",\"# It takes 1 minute. !pip install -q espnet_model_zoo\"]},\"2356\":{\"h\":\"Speech recognition demo\",\"t\":[\"Author: Jiatong Shi (@ftshijt)\"]},\"2357\":{\"h\":\"Model Selection\",\"t\":[\"Please select the model shown in espnet_model_zoo.\",\"They are stored in zenodo https://zenodo.org/communities/espnet or HuggingFace Hub https://huggingface.co/espnet\",\"In this demonstration, we will show English, Japanese, Spanish, Mandrain, and Multilingual ASR models, respectively\",\"#@title Choose English ASR model { run: \\\"auto\\\" } lang = 'en' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave' #@param [\\\"Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave\\\", \\\"kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave\\\"] {type:\\\"string\\\"}\",\"#@title Choose Japanese ASR model { run: \\\"auto\\\" } lang = 'ja' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'Shinji Watanabe/laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid.acc.ave' #@param [\\\"Shinji Watanabe/laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid.acc.ave\\\"] {type:\\\"string\\\"}\",\"#@title Choose Spanish ASR model { run: \\\"auto\\\" } lang = 'es' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'ftshijt/mls_asr_transformer_valid.acc.best' #@param [\\\"ftshijt/mls_asr_transformer_valid.acc.best\\\"] {type:\\\"string\\\"}\",\"#@title Choose Mandrain ASR model { run: \\\"auto\\\" } lang = 'zh' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'Emiru Tsunoo/aishell_asr_train_asr_streaming_transformer_raw_zh_char_sp_valid.acc.ave' #@param [\\\" Emiru Tsunoo/aishell_asr_train_asr_streaming_transformer_raw_zh_char_sp_valid.acc.ave\\\"] {type:\\\"string\\\"}\",\"#@title Choose Multilingual ASR model { run: \\\"auto\\\" } lang = 'multilingual' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'ftshijt/open_li52_asr_train_asr_raw_bpe7000_valid.acc.ave_10best' #@param [\\\" ftshijt/open_li52_asr_train_asr_raw_bpe7000_valid.acc.ave_10best\\\"] {type:\\\"string\\\"}\"]},\"2358\":{\"h\":\"Model Setup\",\"t\":[\"import time import torch import string from espnet_model_zoo.downloader import ModelDownloader from espnet2.bin.asr_inference import Speech2Text d = ModelDownloader() # It may takes a while to download and build models speech2text = Speech2Text( **d.download_and_unpack(tag), device=\\\"cuda\\\", minlenratio=0.0, maxlenratio=0.0, ctc_weight=0.3, beam_size=10, batch_size=0, nbest=1 ) def text_normalizer(text): text = text.upper() return text.translate(str.maketrans('', '', string.punctuation))\"]},\"2359\":{\"h\":\"Recognize our examples of pre-recorded samples\",\"t\":[\"!git clone https://github.com/ftshijt/ESPNet_asr_egs.git import pandas as pd import soundfile import librosa.display from IPython.display import display, Audio import matplotlib.pyplot as plt egs = pd.read_csv(\\\"ESPNet_asr_egs/egs.csv\\\") for index, row in egs.iterrows(): if row[\\\"lang\\\"] == lang or lang == \\\"multilingual\\\": speech, rate = soundfile.read(\\\"ESPNet_asr_egs/\\\" + row[\\\"path\\\"]) assert fs == int(row[\\\"sr\\\"]) nbests = speech2text(speech) text, *_ = nbests[0] print(f\\\"Input Speech: ESPNet_asr_egs/{row['path']}\\\") # let us listen to samples display(Audio(speech, rate=rate)) librosa.display.waveplot(speech, sr=rate) plt.show() print(f\\\"Reference text: {text_normalizer(row['text'])}\\\") print(f\\\"ASR hypothesis: {text_normalizer(text)}\\\") print(\\\"*\\\" * 50)\"]},\"2360\":{\"h\":\"Recognize your own live-recordings\",\"t\":[\"Record your own voice\",\"Recognize your voice with the ASR system\",\"# from https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be from IPython.display import Javascript from google.colab import output from base64 import b64decode RECORD = \\\"\\\"\\\" const sleep = time => new Promise(resolve => setTimeout(resolve, time)) const b2text = blob => new Promise(resolve => { const reader = new FileReader() reader.onloadend = e => resolve(e.srcElement.result) reader.readAsDataURL(blob) }) var record = time => new Promise(async resolve => { stream = await navigator.mediaDevices.getUserMedia({ audio: true }) recorder = new MediaRecorder(stream) chunks = [] recorder.ondataavailable = e => chunks.push(e.data) recorder.start() await sleep(time) recorder.onstop = async ()=>{ blob = new Blob(chunks) text = await b2text(blob) resolve(text) } recorder.stop() }) \\\"\\\"\\\" def record(sec, filename='audio.wav'): display(Javascript(RECORD)) s = output.eval_js('record(%d)' % (sec * 1000)) b = b64decode(s.split(',')[1]) with open(filename, 'wb+') as f: f.write(b) audio = 'audio.wav' second = 5 print(f\\\"Speak to your microphone {second} sec...\\\") record(second, audio) print(\\\"Done!\\\") import librosa import librosa.display speech, rate = librosa.load(audio, sr=16000) librosa.display.waveplot(speech, sr=rate) import matplotlib.pyplot as plt plt.show() import pysndfile pysndfile.sndio.write('audio_ds.wav', speech, rate=rate, format='wav', enc='pcm16') from IPython.display import display, Audio display(Audio(speech, rate=rate))\",\"nbests = speech2text(speech) text, *_ = nbests[0] print(f\\\"ASR hypothesis: {text_normalizer(text)}\\\")\"]},\"2361\":{\"h\":\"Speech synthesis demo\",\"t\":[\"This notebook provides a demonstration of the realtime E2E-TTS using ESPnet2-TTS and ParallelWaveGAN repo.\",\"ESPnet2-TTS: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1\",\"ParallelWaveGAN: https://github.com/kan-bayashi/ParallelWaveGAN\",\"Author: Tomoki Hayashi (@kan-bayashi)\"]},\"2362\":{\"h\":\"Installation\",\"t\":[\"# NOTE: pip shows imcompatible errors due to preinstalled libraries but you do not need to care # It takes 1 minute !pip install -q pyopenjtalk==0.1.5 parallel_wavegan==0.5.3\"]},\"2363\":{\"h\":\"Model Selection\",\"t\":[\"Please select model: English, Japanese, and Mandarin are supported.\",\"You can try end-to-end text2wav model & combination of text2mel and vocoder. If you use text2wav model, you do not need to use vocoder (automatically disabled).\",\"Text2wav models:\",\"VITS\",\"Text2mel models:\",\"Tacotron2\",\"Transformer-TTS\",\"(Conformer) FastSpeech\",\"(Conformer) FastSpeech2\",\"Vocoders:\",\"Parallel WaveGAN\",\"Multi-band MelGAN\",\"HiFiGAN\",\"Style MelGAN.\",\"The terms of use follow that of each corpus. We use the following corpora:\",\"ljspeech_*: LJSpeech dataset \",\"https://keithito.com/LJ-Speech-Dataset/\",\"jsut_*: JSUT corpus \",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"jvs_*: JVS corpus + JSUT corpus \",\"https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus\",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"tsukuyomi_*: つくよみちゃんコーパス + JSUT corpus \",\"https://tyc.rei-yumesaki.net/material/corpus/\",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"csmsc_*: Chinese Standard Mandarin Speech Corpus \",\"https://www.data-baker.com/open_source.html\",\"#@title Choose English model { run: \\\"auto\\\" } lang = 'English' tag = 'kan-bayashi/ljspeech_vits' #@param [\\\"kan-bayashi/ljspeech_tacotron2\\\", \\\"kan-bayashi/ljspeech_fastspeech\\\", \\\"kan-bayashi/ljspeech_fastspeech2\\\", \\\"kan-bayashi/ljspeech_conformer_fastspeech2\\\", \\\"kan-bayashi/ljspeech_vits\\\"] {type:\\\"string\\\"} vocoder_tag = \\\"none\\\" #@param [\\\"none\\\", \\\"parallel_wavegan/ljspeech_parallel_wavegan.v1\\\", \\\"parallel_wavegan/ljspeech_full_band_melgan.v2\\\", \\\"parallel_wavegan/ljspeech_multi_band_melgan.v2\\\", \\\"parallel_wavegan/ljspeech_hifigan.v1\\\", \\\"parallel_wavegan/ljspeech_style_melgan.v1\\\"] {type:\\\"string\\\"}\",\"#@title Choose Japanese model { run: \\\"auto\\\" } lang = 'Japanese' tag = 'kan-bayashi/jsut_full_band_vits_prosody' #@param [\\\"kan-bayashi/jsut_tacotron2\\\", \\\"kan-bayashi/jsut_transformer\\\", \\\"kan-bayashi/jsut_fastspeech\\\", \\\"kan-bayashi/jsut_fastspeech2\\\", \\\"kan-bayashi/jsut_conformer_fastspeech2\\\", \\\"kan-bayashi/jsut_conformer_fastspeech2_accent\\\", \\\"kan-bayashi/jsut_conformer_fastspeech2_accent_with_pause\\\", \\\"kan-bayashi/jsut_vits_accent_with_pause\\\", \\\"kan-bayashi/jsut_full_band_vits_accent_with_pause\\\", \\\"kan-bayashi/jsut_tacotron2_prosody\\\", \\\"kan-bayashi/jsut_transformer_prosody\\\", \\\"kan-bayashi/jsut_conformer_fastspeech2_tacotron2_prosody\\\", \\\"kan-bayashi/jsut_vits_prosody\\\", \\\"kan-bayashi/jsut_full_band_vits_prosody\\\", \\\"kan-bayashi/jvs_jvs010_vits_prosody\\\", \\\"kan-bayashi/tsukuyomi_full_band_vits_prosody\\\"] {type:\\\"string\\\"} vocoder_tag = 'none' #@param [\\\"none\\\", \\\"parallel_wavegan/jsut_parallel_wavegan.v1\\\", \\\"parallel_wavegan/jsut_multi_band_melgan.v2\\\", \\\"parallel_wavegan/jsut_style_melgan.v1\\\", \\\"parallel_wavegan/jsut_hifigan.v1\\\"] {type:\\\"string\\\"}\",\"#@title Choose Mandarin model { run: \\\"auto\\\" } lang = 'Mandarin' tag = 'kan-bayashi/csmsc_full_band_vits' #@param [\\\"kan-bayashi/csmsc_tacotron2\\\", \\\"kan-bayashi/csmsc_transformer\\\", \\\"kan-bayashi/csmsc_fastspeech\\\", \\\"kan-bayashi/csmsc_fastspeech2\\\", \\\"kan-bayashi/csmsc_conformer_fastspeech2\\\", \\\"kan-bayashi/csmsc_vits\\\", \\\"kan-bayashi/csmsc_full_band_vits\\\"] {type: \\\"string\\\"} vocoder_tag = \\\"none\\\" #@param [\\\"none\\\", \\\"parallel_wavegan/csmsc_parallel_wavegan.v1\\\", \\\"parallel_wavegan/csmsc_multi_band_melgan.v2\\\", \\\"parallel_wavegan/csmsc_hifigan.v1\\\", \\\"parallel_wavegan/csmsc_style_melgan.v1\\\"] {type:\\\"string\\\"}\"]},\"2364\":{\"h\":\"Model Setup\",\"t\":[\"from espnet2.bin.tts_inference import Text2Speech from espnet2.utils.types import str_or_none text2speech = Text2Speech.from_pretrained( model_tag=str_or_none(tag), vocoder_tag=str_or_none(vocoder_tag), device=\\\"cuda\\\", # Only for Tacotron 2 & Transformer threshold=0.5, # Only for Tacotron 2 minlenratio=0.0, maxlenratio=10.0, use_att_constraint=False, backward_window=1, forward_window=3, # Only for FastSpeech & FastSpeech2 & VITS speed_control_alpha=1.0, # Only for VITS noise_scale=0.667, noise_scale_dur=0.8, )\"]},\"2365\":{\"h\":\"Synthesis\",\"t\":[\"import time import torch # decide the input sentence by yourself print(f\\\"Input your favorite sentence in {lang}.\\\") x = input() # synthesis with torch.no_grad(): start = time.time() wav = text2speech(x)[\\\"wav\\\"] rtf = (time.time() - start) / (len(wav) / text2speech.fs) print(f\\\"RTF = {rtf:5f}\\\") # let us listen to generated samples from IPython.display import display, Audio display(Audio(wav.view(-1).cpu().numpy(), rate=text2speech.fs))\"]},\"2366\":{\"h\":\"Speech enhancement demo\",\"t\":[\"ESPnet2-SE: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/enh1\",\"Author: Chenda Li (@LiChenda), Wangyou Zhang (@Emrys365)\"]},\"2367\":{\"h\":\"Single-Channel Enhancement, the CHiME example\",\"t\":[\"# Download one utterance from real noisy speech of CHiME4 !gdown --id 1SmrN5NFSg6JuQSs2sfy3ehD8OIcqK6wS -O /content/M05_440C0213_PED_REAL.wav import os import soundfile from IPython.display import display, Audio mixwav_mc, sr = soundfile.read(\\\"/content/M05_440C0213_PED_REAL.wav\\\") # mixwav.shape: num_samples, num_channels mixwav_sc = mixwav_mc[:,4] display(Audio(mixwav_mc.T, rate=sr))\"]},\"2368\":{\"h\":\"Download and load the pretrained Conv-Tasnet\",\"t\":[\"!gdown --id 17DMWdw84wF3fz3t7ia1zssdzhkpVQGZm -O /content/chime_tasnet_singlechannel.zip !unzip /content/chime_tasnet_singlechannel.zip -d /content/enh_model_sc\",\"# Load the model # If you encounter error \\\"No module named 'espnet2'\\\", please re-run the 1st Cell. This might be a colab bug. import sys import soundfile from espnet2.bin.enh_inference import SeparateSpeech separate_speech = {} # For models downloaded from GoogleDrive, you can use the following script: enh_model_sc = SeparateSpeech( train_config=\\\"/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/config.yaml\\\", model_file=\\\"/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/5epoch.pth\\\", # for segment-wise process on long speech normalize_segment_scale=False, show_progressbar=True, ref_channel=4, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2369\":{\"h\":\"Enhance the single-channel real noisy speech in CHiME4\",\"t\":[\"# play the enhanced single-channel speech wave = enh_model_sc(mixwav_sc[None, ...], sr) print(\\\"Input real noisy speech\\\", flush=True) display(Audio(mixwav_sc, rate=sr)) print(\\\"Enhanced speech\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2370\":{\"h\":\"Speech Separation\"},\"2371\":{\"h\":\"Model Selection\",\"t\":[\"Please select model shown in espnet_model_zoo\",\"In this demonstration, we will show different speech separation models on wsj0_2mix.\",\"#@title Choose Speech Separation model { run: \\\"auto\\\" } fs = 8000 #@param {type:\\\"integer\\\"} tag = \\\"Chenda Li/wsj0_2mix_enh_train_enh_conv_tasnet_raw_valid.si_snr.ave\\\" #@param [\\\"Chenda Li/wsj0_2mix_enh_train_enh_conv_tasnet_raw_valid.si_snr.ave\\\", \\\"Chenda Li/wsj0_2mix_enh_train_enh_rnn_tf_raw_valid.si_snr.ave\\\", \\\"https://zenodo.org/record/4688000/files/enh_train_enh_dprnn_tasnet_raw_valid.si_snr.ave.zip\\\"]\",\"# For models uploaded to Zenodo, you can use the following python script instead: import sys import soundfile from espnet_model_zoo.downloader import ModelDownloader from espnet2.bin.enh_inference import SeparateSpeech d = ModelDownloader() cfg = d.download_and_unpack(tag) separate_speech = SeparateSpeech( train_config=cfg[\\\"train_config\\\"], model_file=cfg[\\\"model_file\\\"], # for segment-wise process on long speech segment_size=2.4, hop_size=0.8, normalize_segment_scale=False, show_progressbar=True, ref_channel=None, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2372\":{\"h\":\"Separate the example in wsj0_2mix testing set\",\"t\":[\"!gdown --id 1ZCUkd_Lb7pO2rpPr4FqYdtJBZ7JMiInx -O /content/447c020t_1.2106_422a0112_-1.2106.wav import os import soundfile from IPython.display import display, Audio mixwav, sr = soundfile.read(\\\"/content/447c020t_1.2106_422a0112_-1.2106.wav\\\") waves_wsj = separate_speech(mixwav[None, ...], fs=sr) print(\\\"Input mixture\\\", flush=True) display(Audio(mixwav, rate=sr)) print(f\\\"========= Separated speech with model {tag} =========\\\", flush=True) print(\\\"Separated spk1\\\", flush=True) display(Audio(waves_wsj[0].squeeze(), rate=sr)) print(\\\"Separated spk2\\\", flush=True) display(Audio(waves_wsj[1].squeeze(), rate=sr))\",\"This is a full installation method to perform data preprocess, training, inference, scoring, and so on. for various experiments.\",\"We prepare various ways of installations. We also prepare a docker image as well.\",\"See https://espnet.github.io/espnet/installation.html#step-2-installation-espnet for more details.\",\"Installation of required tools\",\"See https://espnet.github.io/espnet/installation.html#requirements for more details.\",\"# It takes ~10 seconds !sudo apt-get install cmake sox libsndfile1-dev\",\"Download espnet\",\"# It takes a few seconds !git clone --depth 5 https://github.com/espnet/espnet\",\"Setup Python environment based on anaconda\",\"There are several other installation methods, but we highly recommend the anaconda-based one.\",\"# It takes 30 seconds %cd /content/espnet/tools !./setup_anaconda.sh anaconda espnet 3.8\",\"Install espnet\",\"This includes the installation of PyTorch and other tools.\",\"We just specify CUDA_VERSION=10.2 for the latest PyTorch (1.9.0)\",\"# It may take ~8 minutes %cd /content/espnet/tools !make CUDA_VERSION=10.2\",\"Install other speech processing tools\",\"We install NIST SCTK toolkit for scoring\",\"Please manually install other tools if needed.\",\"%cd /content/espnet/tools !./installers/install_sctk.sh\",\"Check installation\",\"Please check whether torch, torch cuda, and espnet are correctly installed.\",\"If torch, torch cuda, and espnet are successfully installed, it would be no problem.\",\"[x] torch=1.9.0 [x] torch cuda=10.2 : [x] espnet=0.10.3a3\",\"%cd /content/espnet/tools !. ./activate_python.sh; python3 check_install.py\",\"ESPnet has a number of recipes (73 recipes on Sep. 16, 2021). Let's first check https://github.com/espnet/espnet/blob/master/egs2/README.md\",\"Please also check the general usage of the recipe in https://espnet.github.io/espnet/espnet2_tutorial.html#recipes-using-espnet2\",\"CMU AN4 recipe\",\"In this tutorial, we use the CMU an4 recipe. This is a small-scale speech recognition task mainly used for testing.\",\"First, move to the recipe directory\",\"%cd /content/espnet/egs2/an4/asr1 !ls\",\"egs2/an4/asr1/ - conf/ # Configuration files for training, inference, etc. - scripts/ # Bash utilities of espnet2 - pyscripts/ # Python utilities of espnet2 - steps/ # From Kaldi utilities - utils/ # From Kaldi utilities - db.sh # The directory path of each corpora - path.sh # Setup script for environment variables - cmd.sh # Configuration for your backend of job scheduler - run.sh # Entry point - asr.sh # Invoked by run.sh\",\"ESPnet is designed for various use cases (local machines or cluster machines) based on Kaldi tools. If you use it in the cluster machines, please also check https://kaldi-asr.org/doc/queue.html\",\"The main stages can be parallelized by various jobs.\",\"!cat run.sh\",\"run.sh can call asr.sh, which completes the entire speech recognition experiments, including data preparation, training, inference, and scoring. They are based on separate stages (totally 15 stages).\",\"Instead of executing the entire experiments by run.sh, the following example executes the experiment for each stage to understand the process in each stage.\"]},\"2373\":{\"h\":\"data preparation\",\"t\":[\"Stage 1: Data preparation for training, validation, and evaluation data\",\"Note that --stage &lt;N&gt; is to start the stage and --stop_stage &lt;N&gt; is to stop the stage. We also need to specify training, validation, and test data.\",\"# 30 seconds !./asr.sh --stage 1 --stop_stage 1 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"After this stage is finished, please check the data directory\",\"!ls data\",\"In this recipe, we use train_nodev as a training set, train_dev as a validation set (monitor the training progress by checking the validation score). We also use (reuse) test and train_dev sets for the final speech recognition evaluation.\",\"Let's check one of the training data directory:\",\"!ls -1 data/train_nodev/\",\"These are the speech and corresponding text and speaker information based on the Kaldi format. Please also check https://kaldi-asr.org/doc/data_prep.html\",\"spk2utt # Speaker information text # Transcription file utt2spk # Speaker information wav.scp # Audio file\",\"Stage 2: Speed perturbation (one of the data augmentation methods)\",\"We do not use speed perturbation for this demo. But you can turn it on by adding an argument --speed_perturb_factors \\\"0.9 1.0 1.1\\\" to the shell script\",\"!./asr.sh --stage 2 --stop_stage 2 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"Stage 3: Format wav.scp: data/ -> dump/raw\",\"We dump the data with specified format (flac in this case) for the efficient use of the data.\",\"Note that --nj &lt;N&gt; means the number of CPU jobs. Please set it appropriately by considering your CPU resources and disk access.\",\"# 30 seconds !./asr.sh --stage 3 --stop_stage 3 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --nj 4\",\"Stage 4: Remove long/short data: dump/raw/org -> dump/raw\",\"There are too long and too short audio data, which are harmful for our efficient training. Those data are removed from the list.\",\"!./asr.sh --stage 4 --stop_stage 4 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"Stage 5: Generate token_list from dump/raw/train_nodev/text using BPE.\",\"This is important for text processing. We make a dictionary based on the English character in this example. We use a sentencepiece toolkit developed by Google.\",\"!./asr.sh --stage 5 --stop_stage 5 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"Let's check the content of the dictionary. There are several special symbols, e.g.,\",\"&lt;blank&gt; used for CTC &lt;unk&gt; unknown symbols do not appear in the training data &lt;sos/eos&gt; start and end sentence symbols\",\"!cat data/token_list/bpe_unigram30/tokens.txt\"]},\"2374\":{\"h\":\"language modeling (skip in this tutorial)\",\"t\":[\"Stages 6--9: Stages related to language modeling.\",\"We skip the language modeling part in the recipe (stages 6 -- 9) in this tutorial.\"]},\"2375\":{\"h\":\"End-to-end ASR\",\"t\":[\"Stage 10: ASR collect stats: train_set=dump/raw/train_nodev, valid_set=dump/raw/train_dev\",\"We estimate the mean and variance of the data to normalize the data. We also collect the information of input and output lengths for the efficient mini batch creation.\",\"# 18 seconds !./asr.sh --stage 10 --stop_stage 10 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --nj 4\",\"Stage 11: ASR Training: train_set=dump/raw/train_nodev, valid_set=dump/raw/train_dev\",\"Main training loop.\",\"Please also monitor the following files\",\"log file /content/espnet/egs2/an4/asr1/exp/asr_train_raw_bpe30/train.log\",\"loss /content/espnet/egs2/an4/asr1/exp/asr_train_raw_bpe30/images/loss.png\",\"accuracy /content/espnet/egs2/an4/asr1/exp/asr_train_raw_bpe30/images/acc.png\",\"# It would take 20-30 min. !./asr.sh --stage 11 --stop_stage 11 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --ngpu 1\",\"Stage 12: Decoding: training_dir=exp/asr_train_raw_bpe30\",\"Note that we need to make --use_lm false since we skip the language model.\",\"inference_nj &lt;N&gt; specifies the number of inference jobs\",\"Let's monitor the log /content/espnet/egs2/an4/asr1/exp/asr_train_raw_bpe30/inference_asr_model_valid.acc.ave/train_dev/logdir/asr_inference.1.log\",\"# It would take ~10 minutes !./asr.sh --inference_nj 4 --stage 12 --stop_stage 12 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --use_lm false\",\"Stage 13: Scoring\",\"You can find word error rate (WER), character error rate (CER), etc. for each test set.\",\"!./asr.sh --stage 13 --stop_stage 13 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --use_lm false\",\"You can also check the break down of the word error rate in /content/espnet/egs2/an4/asr1/exp/asr_train_raw_bpe30/inference_asr_model_valid.acc.ave/train_dev/score_wer/result.txt\"]},\"2376\":{\"h\":\"How to change the training configs?\"},\"2377\":{\"h\":\"config file based\",\"t\":[\"All training options are changed by using a config file.\",\"Pleae check https://espnet.github.io/espnet/espnet2_training_option.html\",\"Let's first check config files prepared in the an4 recipe\",\"- LSTM-based E2E ASR /content/espnet/egs2/an4/asr1/conf/train_asr_rnn.yaml - Transformer based E2E ASR /content/espnet/egs2/an4/asr1/conf/train_asr_transformer.yaml\",\"You can run\",\"RNN\",\"./asr.sh --stage 10 \\\\ --train_set train_nodev \\\\ --valid_set train_dev \\\\ --test_sets \\\"train_dev test\\\" \\\\ --nj 4 \\\\ --inference_nj 4 \\\\ --use_lm false \\\\ ----asr_config conf/train_asr_rnn.yaml\",\"Transformer\",\"./asr.sh --stage 10 \\\\ --train_set train_nodev \\\\ --valid_set train_dev \\\\ --test_sets \\\"train_dev test\\\" \\\\ --nj 4 \\\\ --inference_nj 4 \\\\ --use_lm false \\\\ ----asr_config conf/train_asr_transformer.yaml\",\"You can also find various configs in espnet/egs2/*/asr1/conf/, including\",\"Conformer espnet/egs2/librispeech/asr1/conf/train_asr_confformer.yaml\",\"Wav2vec2.0 pre-trained model and fine-tuning https://github.com/espnet/espnet/blob/master/egs2/librispeech/asr1/conf/tuning/train_asr_conformer7_wav2vec2_960hr_large.yaml\",\"HuBERT pre-trained model and fine-tuning https://github.com/espnet/espnet/blob/master/egs2/librispeech/asr1/conf/tuning/train_asr_conformer7_hubert_960hr_large.yaml\"]},\"2378\":{\"h\":\"command line argument based\",\"t\":[\"You can also customize it by editing the file or passing the command line arguments, e.g.,\",\"./run.sh --stage 10 --asr_args \\\"--model_conf ctc_weight=0.3\\\"\",\"./run.sh --stage 10 --asr_args \\\"--optim_conf lr=0.1\\\"\",\"See https://espnet.github.io/espnet/espnet2_tutorial.html#change-the-configuration-for-training\"]},\"2379\":{\"h\":\"How to make a new recipe?\",\"t\":[\"Check https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE\"]},\"2380\":{\"h\":\"CMU 11492/11692 Spring 2023: Data preparation\",\"t\":[\"In this demonstration, we will show you the procedure to prepare the data for speech processing (ASR as an example).\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"ESPnet tutorial in Speech Recognition and Understanding (Fall 2021)\",\"Recitation in Multilingual NLP (Spring 2022)\",\"ESPnet tutorail in Speech Recognition and Understanding (Fall 2022)\",\"Author:\",\"Jiatong Shi (jiatongs@andrew.cmu.edu)\"]},\"2381\":{\"h\":\"Objectives\",\"t\":[\"After this demonstration, you are expected to know:\",\"Understand the Kaldi(ESPnet) data format\"]},\"2382\":{\"h\":\"Useful links\",\"t\":[\"Installation https://espnet.github.io/espnet/installation.html\",\"Kaldi Data format https://kaldi-asr.org/doc/data_prep.html\",\"ESPnet data format https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE#about-kaldi-style-data-directory\"]},\"2383\":{\"h\":\"Download ESPnet\",\"t\":[\"We use git clone to download the source code of ESPnet and then go to a specific commit.\",\"# It takes a few seconds !git clone --depth 5 https://github.com/espnet/espnet\"]},\"2384\":{\"h\":\"Setup Python environment based on anaconda\",\"t\":[\"There are several other installation methods, but we highly recommend the anaconda-based one. In this demonstration, we will only need to have the python environment (no need to install the full espnet). But installation of ESPnet main codebase will be necessary for for training/inference/scoring.\",\"# It takes 30 seconds %cd /content/espnet/tools !./setup_anaconda.sh anaconda espnet 3.9 !./installers/install_sph2pipe.sh !pip install typeguard==2.13.0\",\"We will also install some essential python libraries (these will be auto-matically downloaded during espnet installation. However, today, we won't go through that part, so we need to mannually install the packages.\",\"!pip install kaldiio soundfile tqdm librosa matplotlib IPython\",\"We will also need Kaldi for some essential scripts.\",\"!git clone https://github.com/kaldi-asr/kaldi.git\",\"ESPnet has a number of recipes (146 recipes on Jan. 23, 2023). One of the most important steps for those recipes is the preparation of the data. Constructing in different scenarios, spoken corpora need to be converted into a unified format. In ESPnet, we follow and adapt the Kaldi data format for various tasks.\",\"In this demonstration, we will focus on a specific recipe an4 as an example.\",\"Other materials:\",\"Kaldi format documentation can be found in https://kaldi-asr.org/doc/data_prep.html\",\"ESPnet data format is in https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE#about-kaldi-style-data-directory\",\"Please refer to https://github.com/espnet/espnet/blob/master/egs2/README.md for a complete list of recipes.\",\"Please also check the general usage of the recipe in https://espnet.github.io/espnet/espnet2_tutorial.html#recipes-using-espnet2\"]},\"2385\":{\"h\":\"Data preparation for AN4\",\"t\":[\"All the data preparation in ESPnet2 happens in egs2/recipe_name/task/local/data.sh where the task can be either asr1, enh1, tts1, etc.\",\"CMU AN4 recipe\",\"In this demonstration, we will use the CMU an4 recipe. This is a small-scale speech recognition task mainly used for testing.\",\"First, let's go to the recipe directory.\",\"%cd /content/espnet/egs2/an4/asr1 !ls\",\"egs2/an4/asr1/ - conf/ # Configuration files for training, inference, etc. - scripts/ # Bash utilities of espnet2 - pyscripts/ # Python utilities of espnet2 - steps/ # From Kaldi utilities - utils/ # From Kaldi utilities - local/ # Some local scripts for specific recipes (Data Preparation usually in `local/data.sh`) - db.sh # The directory path of each corpora - path.sh # Setup script for environment variables - cmd.sh # Configuration for your backend of job scheduler - run.sh # Entry point - asr.sh # Invoked by run.sh\",\"# a few seconds !./local/data.sh\",\"The orginal data usually in various format. AN4 has a quite straightforward format. You may dig into the folder an4 to see the raw format. After this preparation is finished, all the information will be in the data directory:\",\"!ls data\",\"In this recipe, we use train_nodev as a training set, train_dev as a validation set (monitor the training progress by checking the validation score). We also use test and train_dev sets for the final speech recognition evaluation.\",\"Let's check one of the training data directories:\",\"!ls -1 data/train_nodev/\",\"In short, the four files are:\",\"spk2utt # Speaker information text # Transcription file utt2spk # Speaker information wav.scp # Audio file\",\"The wav.scp is the most important file that holds the speech data. For each line of the wav.scp, there are generally two components WAV_ID and SPEECH_AUDIO for each line of the file. The WAV_ID is an identifier for the utterance, while the SPEECH_AUDIO holds the speech audio data.\",\"The audio data can be in various audio formats, such as wav, flac, sph, etc. We can also use pipe to normalize audio files with (e.g., sox, ffmpeg, sph2pipe). The following from an4 is an example using sph2pipe.\",\"!head -n 10 data/train_nodev/wav.scp\",\"The text is to hold the transription of the speech. Similar to wav.scp, for each line of text, there are UTT_ID and TRANSCRIPTION. Note that the UTT_ID in text and WAV_ID in wav.scp are not necessary the same. But for the simple case (e.g., the AN4), we regard them as the same. The example in AN4 is as:\",\"!head -n 10 data/train_nodev/text\",\"The spk2utt and utt2spk are mapping between utterances and speakers. The information is widely used in conventional hidden Markov model (HMM)-based ASR systems, but not that popular in end-to-end ASR systems nowadays. However, they are still very important for tasks such as speaker diarization and multi-speaker text-to-speech. The examples of AN4 is as follows:\",\"!head -n 10 data/train_nodev/spk2utt !echo \\\"--------------------------\\\" !head -n 10 data/train_nodev/utt2spk\"]},\"2386\":{\"h\":\"How to read file in pipe\",\"t\":[\"We can use kaldiio package to read audio files from wav.scp. The example is as follows:\",\"import soundfile import kaldiio import matplotlib.pyplot as plt from io import BytesIO from tqdm import tqdm import librosa.display import numpy as np import IPython.display as ipd import os os.environ['PATH'] = os.environ['PATH'] + \\\":/content/espnet/tools/sph2pipe\\\" wavscp = open(\\\"data/test/wav.scp\\\", \\\"r\\\") num_wav = 5 count = 1 for line in tqdm(wavscp): utt_id, wavpath = line.strip().split(None, 1) with kaldiio.open_like_kaldi(wavpath, \\\"rb\\\") as f: with BytesIO(f.read()) as g: wave, rate = soundfile.read(g, dtype=np.float32) print(\\\"audio: {}\\\".format(utt_id)) librosa.display.waveshow(wave, rate) plt.show() ipd.display(ipd.Audio(wave, rate=rate)) # load a NumPy array if count == num_wav: break count += 1\"]},\"2387\":{\"h\":\"Data preparation for TOTONAC\",\"t\":[\"CMU TOTONAC recipe\",\"In the second part of the demonstration, we will use the CMU totonac recipe. This is a small-scale ASR recipe, which is an endangered language in central Mexico. We will follow mostly the similar procedure as the showcase of AN4. For the start, the recipe directory of totonac is almost the same as an4.\",\"%cd /content/espnet/egs2/totonac/asr1 !ls\",\"Then we execute ./local/data.sh for the data preparation, which is the same as an4. The downloading takes a longer time (around 2-3 mins) for totonac as the speech is in higher-sampling rate and recorded in a conversational manner which include longer session rather than single utterances.\",\"!. ../../../tools/activate_python.sh && pip install soundfile # we need soundfile for necessary processing !./local/data.sh\",\"Let's first check the original data format of the totonac. To facilate the linguists working on the language, we use the ELAN format, which is special XML format. For preparation, we need to parse the format into the same Kaldi format as mentioned ahead. For more details, please check https://github.com/espnet/espnet/blob/master/egs2/totonac/asr1/local/data_prep.py\",\"!ls -l downloads/Conversaciones/Botany/Transcripciones/ELAN-para-traducir | head -n 5 !echo \\\"-----------------------------------------------\\\" !cat downloads/Conversaciones/Botany/Transcripciones/ELAN-para-traducir/Zongo_Botan_ESP400-SLC388_Convolvulaceae-Cuscuta-sp_2019-09-25-c_ed-2020-12-30.eaf\",\"Similar to AN4, we will have three sets for the experiments for totonac, including train, test and dev. However, within the set, we also have a segments file apart from the files mentioned above.\",\"For each line of segments, we will have four fields for each line, including UTT_ID, WAV_ID, \\\"start time\\\" and \\\"end time\\\". Note that when segments files are presented, the WAV_ID in wav.scp and UTT_ID in text, utt2spk and spk2utt are not the same anymore. And the segments is the file that keeps the relationship between WAV_ID and UTT_ID.\",\"!ls -l data !echo \\\"--------------------------\\\" !ls -l data/train !echo \\\"------------- wav.scp file -------------\\\" !head -n 10 data/train/wav.scp !echo \\\"------------- Segment file -------------\\\" !head -n 10 data/train/segments\",\"#Questions:\",\"Q1: The format itself is very general. But it cannot fit to all the tasks in speech processing. Could you list three tasks where the current format cannot be sufficient?\",\"Your Answers here\",\"Q2: For the three tasks you listed above, can you think of some modification or addition to the format to make it also working for the tasks?\",\"Your Answers here\",\"Q3: Briefly discuss the difference within the wav.scp between an4 and totonac\",\"Your Answers here\",\"(Note that for this assignment, you do not need to submit anything.)\"]},\"2388\":{\"h\":\"CMU 11492/11692 Spring 2023: ESPnet Tutorial2 (New task)\",\"t\":[\"ESPnet is a widely-used end-to-end speech processing toolkit. It has supported various speech processing tasks. ESPnet uses PyTorch as a main deep learning engine, and also follows Kaldi style data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"ESPnet tutorial in Speech Recognition and Understanding (Fall 2021)\",\"ESPnet tutorial in Speech Recognition and Understanding (Fall 2022)\",\"Recitation in Multilingual NLP (Spring 2022)\",\"ESPnet tutorial1 in Speech Recognition and Understanding (Fall 2022)\",\"Author: Jiatong Shi (jiatongs@andrew.cmu.edu)\",\"We would like to thank You (Neil) Zhang for kindly helping the hands-on tutorial and sharing his knowledge on the task.\"]},\"2389\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.\",\"This tutorial covers some advanced usage of ESPnet, which is the extension of the first tutorial.\"]},\"2390\":{\"h\":\"Useful links\",\"t\":[\"Installation https://espnet.github.io/espnet/installation.html\",\"Usage https://espnet.github.io/espnet/espnet2_tutorial.html\",\"Reference of task class in ESPnet https://espnet.github.io/espnet/espnet2_task.html\"]},\"2391\":{\"h\":\"Objectives\",\"t\":[\"After this tutorial, you are expected to know:\",\"How to add new task in ESPnet2\",\"How to add new models in ESPnet2\",\"How to create a new recipe (and template) of a new task from scratch\"]},\"2392\":{\"h\":\"Function to print date and time\",\"t\":[\"We first define a function to print the current date and time, which will be used in multiple places below.\",\"def print_date_and_time(): from datetime import datetime import pytz now = datetime.now(pytz.timezone(\\\"America/New_York\\\")) print(\\\"=\\\" * 60) print(f' Current date and time: {now.strftime(\\\"%m/%d/%Y %H:%M:%S\\\")}') print(\\\"=\\\" * 60) # example output print_date_and_time()\"]},\"2393\":{\"h\":\"Download ESPnet\",\"t\":[\"We use git clone to download the source code of ESPnet and then go to a specific commit.\",\"Important: In other versions of ESPnet, you may encounter errors related to imcompatible package versions (numba). Please use the same commit to avoid such issues.\",\"Note that we are using another branch espnet_tutorial_asvspoof instead of \\\"master\\\". You can also use your own fork to proceed the following sections if you want to use Github to save your code.\",\"# It takes a few seconds !git clone --depth 5 https://github.com/espnet/espnet %cd /content/espnet\"]},\"2394\":{\"h\":\"Setup Python environment based on anaconda + Install ESPnet\",\"t\":[\"# It takes 30 seconds %cd /content/espnet/tools !./setup_anaconda.sh anaconda espnet 3.9 # It may take 12 minutes %cd /content/espnet/tools !make TH_VERSION=1.12.1 CUDA_VERSION=11.6\",\"We have provide you most of the files needed for ASVSpoof recipe. So you do not need to add any additional files. However, noted that some of the files are not complete and need your completion to proceed. For a quick overview of the whole layout of the new task, please refer to https://github.com/espnet/espnet/compare/master...2022fall_new_task_tutorial\",\"As elaborated in the warming-up, we have shown that there are two core components for a new task in ESPnet: a task library and correponding recipe setups. For the following of the section, we will briefly show the overall layout of adding the ASVSpoof task in ESPnet. The listed files are almost the minimum requirements to add a new task in ESPnet.\",\"Task library for ASVSpoof\",\"Followings are a list of files adding to ESPnet for ASVSpoof (files in \\\"\\\" are ones that need modifications)\",\"- espnet2 - bin - asvspoof_train.py # Major entry point for asvspoof - \\\"asvspoof_inference.py\\\" (Checkpoint 4) # Inference scripts for asvspoof - asvspoof - decoder - __init__.py - abs_decoder.py # abstract class for decoder in ASVSpoof - \\\"linear_decoder.py\\\" (Checkpoint 3) # simple linear decoder for ASVSpoof - loss - __init__.py - abs_loss.py # abstract class for loss in ASVSpoof - binary_loss.py # naive binary class loss for ASVSpoof - am_softmax.py - \\\"oc_softmax.py\\\" - __init__.py - \\\"espnet_model.py\\\" - tasks - \\\"asvspoof.py\\\" (Checkpoint 2)\",\"To help you understand more, we would recommend you to check the layout of other tasks (e.g., ASR, TTS, ST, etc.) to understand how the codebase is functioning.\",\"Recipe for ASVSpoof\",\"Followings are a list of files adding to ESPnet for ASVSpoof (files in boldface are ones that need modifications)\",\"- egs2 - TEMPLATE - asvspoof1 - \\\"asvspoof.sh\\\" (Checkpoint 1) - others - espnet_tutorial - asvspoof11 - conf - \\\"asvspoof.sh” (Checkpoint 1) - local - \\\"data_prep.py\\\" - \\\"data.sh\\\" - \\\"run.sh\\\" (Checkpoint 5) - scripts - pyscripts - utils - steps - path.sh - db.sh - cmd.sh\",\"Noted that because of the symlink, the asvspoof.sh is essentially the same for checkpoint 1.\"]},\"2395\":{\"h\":\"ASVSpoof data preparation\",\"t\":[\"As discussed in the warm-up session, ASVSpoof aims to conduct a binary classfication. As the task layout is a bit different from the ASR task we touched on the first tutorial, so we need to use a different format to formulate the data. For here, to keep the simplicity, we stil use the exact same file as the first tutorial:\",\"wav.scp text utt2spk spk2utt\",\"But on the other hand, we change the format of text into\",\"utt_id1 0 utt_id2 1 utt_id3 0\",\"where 0 represents real speech and 1 stands for fake speech.\"]},\"2396\":{\"h\":\"Download dataset\",\"t\":[\"We first download the data from google drive. Noted that the data is a subset of the ASVSpoof2019 Challenge.\",\"# a few seconds %cd /content/espnet/egs2/espnet_tutorial/asvspoof1/ !pip install --upgrade --no-cache-dir gdown !gdown 1HRdjjmGXBTXOqOq9iijuXPCA4y_46OzP !unzip espnet_tutorial_asvspoof.zip\"]},\"2397\":{\"h\":\"Prepare data (Stage1 & Stage2)\",\"t\":[\"This time, we make the task template to be as simple as possible. The data preparation will be only two stages, including basic data preparation and wave format.\",\"# It may take around 6 minutes !./asvspoof.sh --stage 1 --stop_stage 2 --train_set train --valid_set dev --test_sets \\\"eval\\\"\"]},\"2398\":{\"h\":\"ASVSpoof collect stats (✅ Checkpoint 1 (0.5 point))\",\"t\":[\"Similar to the previous tutorial, we collect the statisitcs for the data.\",\"In the process, the data will be passed into a iterable loader. However, remember that the text file is no longer the format as the ASR recipe. Therefore, we will need to use another data loader to load the corresponding information.\",\"Fortunately, we have a wide range of data loaders for choices, which is listing in here. Please choose the correct file format and replace the [REPLACE_ME] token (in stage 3 and stage 4) in /content/espnet/egs2/espnet_tutorial/asvspoof1/asvspoof.sh\",\"After the replacement, you should be able to run the following blocks\",\"# It takes less than 2 minutes !./asvspoof.sh --stage 3 --stop_stage 3 --train_set train --valid_set dev --test_sets \\\"dev eval\\\" --asvspoof_config conf/checkpoint1_dummy.yaml # NOTE: Checkpoint 1 print_date_and_time()\"]},\"2399\":{\"h\":\"ASVSpoof Model\",\"t\":[\"In this section, we will define the ASVSpoof model and use the model to conduct the training of ASVSpoof task. For easier understanding, we first use an encoder to convert speech features into hidden representations and then use a decoder to conduct the classification.\"]},\"2400\":{\"h\":\"Encoder (✅ Checkpoint 2 (0.5 point))\",\"t\":[\"First, we are going to focus on the encoder part. There has been a long history over the discussion of the speech encoder in our community. Given the sequential perspective, people firstly investigated recurrent neural networks. More recently, we are focusing on conformer block, which is an extension to the transformer block. In the previous settings, we used a transformer block to collect stats. However, we would want to switch to conformer.\",\"Code-reusibility is one of the major benefits of using ESPnet as a toolkit for speech tasks. As ESPnet already support conformer block in ASR, it is easy to import into this new task.\",\"In ESPnet, adding modules that we already have can be as simple as two-line codes. Please add lines into /content/espnet/espnet2/tasks/asvspoof.py. We have marked TODO in the scripts for your convenience.\",\"# It takes less than 2 minutes !./asvspoof.sh --stage 3 --stop_stage 3 --train_set train --valid_set dev --test_sets \\\"dev eval\\\" --asvspoof_config conf/checkpoint2.yaml # NOTE: Checkpoint 2 print_date_and_time()\"]},\"2401\":{\"h\":\"Decoder (✅ Checkpoint 3 (0.5 point))\",\"t\":[\"In this stage, we will finally start the training. As the previous tutorial, we can use the Tensorboard to monitor the process.\",\"# Load the TensorBoard notebook extension %reload_ext tensorboard # Launch tensorboard before training %tensorboard --logdir /content/espnet/egs2/espnet_tutorial/asvspoof1/exp\",\"After we finished the encoder, we also need to create a decoder to conduct the prediciton. As the encoder will generate hidden representations, we want to have a simple decoder to conduct mean-pooling to all the hidden representation at the time-axis. There should be another linear layer to conclude the models into binary classification. Please fill the missing part in /content/espnet/espnet2/asvspoof/decoder/linear_decoder.py to finally start the training. For people who are not familiar with Pytorch, please refer the related resources for details.\",\"Related resources that could be helpful for this checkpoint:\",\"https://pytorch.org/docs/stable/generated/torch.mean.html\",\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\",\"https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\",\"!nvidia-smi # Training takes around 2 minutes !./asvspoof.sh --stage 4 --stop_stage 4 --train_set train --valid_set dev --test_sets \\\"dev eval\\\" --asvspoof_config conf/checkpoint2.yaml --inference_config conf/decode_asvspoof.yaml # NOTE: Checkpoint 3 print_date_and_time()\"]},\"2402\":{\"h\":\"Model Inference\"},\"2403\":{\"h\":\"(✅ Checkpoint 4 (0.5 point))\",\"t\":[\"As the training is finished, we expect to conduct ASVSpoof on the test set. To approach that, we first have to finish the inference codebase. For our task specifically, we need the log-probability of the prediction to compute equal error rate (EER). Therefore the output should be a float number for each utterance.\",\"Please fill the missing parts with TODOs in /content/espnet/espnet2/bin/asvspoof_inference.py\",\"!./asvspoof.sh --stage 5 --stop_stage 5 --train_set train --valid_set dev --test_sets \\\"eval\\\" --asvspoof_config conf/checkpoint2.yaml --inference_nj 1 --gpu_inference true # NOTE: Checkpoint 4 print_date_and_time()\"]},\"2404\":{\"h\":\"Scoring\"},\"2405\":{\"h\":\"(✅ Checkpoint 5 (0.5 point))\",\"t\":[\"We have prepred the scoring script for you. We can get the EER by the following code-block\",\"!./asvspoof.sh --stage 6 --stop_stage 6 --train_set train --valid_set dev --test_sets \\\"eval\\\" --asvspoof_config conf/checkpoint2.yaml !chmod +x scripts/utils/show_asvspoof_result.sh # NOTE: Checkpoint 5 print_date_and_time()\"]},\"2406\":{\"h\":\"CMU 11492/11692 Spring 2023: Speaker Recognition\",\"t\":[\"In this demonstration, we will show you the procedure to conduct speaker recognition with the ASR functions of ESPnet.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"Author:\",\"Jiatong Shi (jiatongs@andrew.cmu.edu)\"]},\"2407\":{\"h\":\"Objectives\",\"t\":[\"After this demonstration, you are expected to understand the main procedure of using ESPnet ASR functions for speaker recognition.\"]},\"2408\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.\"]},\"2409\":{\"h\":\"ESPnet installation\",\"t\":[\"We follow the ESPnet installation as the previous tutorials (takes around 15 minutes).\",\"!git clone --depth 5 -b 2023spring_speaker_recognition https://github.com/espnet/espnet %cd /content/espnet/tools !./setup_anaconda.sh anaconda espnet 3.9 # # It may take 12 minutes %cd /content/espnet/tools !make TH_VERSION=1.12.1 CUDA_VERSION=11.6 !. ./activate_python.sh && installers/install_speechbrain.sh !. ./activate_python.sh && installers/install_rawnet.sh !. ./activate_python.sh && pip install ipykernel\"]},\"2410\":{\"h\":\"Speaker Recognition\",\"t\":[\"Speaker recognition is a typical task that conduct utterance-level classification. Specifically, we will map an utterance into a pre-defined category. Recall that the ASR is doing a sequence-to-sequence task, so we can easily utilize ASR by using a 1-length sequence (i.e., class). Following this concept, we can start to implement the speaker recognition system! Noted that following the definition of the lecture, today, we will focus on speaker identification (close-set classification) instead of speaker verification.\"]},\"2411\":{\"h\":\"Dataset\",\"t\":[\"mini_librispeech is a tiny subset of librispeech dataset for development usage. Because of the free-license and cleaness of the data, librispeech has been one of the most widely used corpora in the speech community. For more details, please refer to its original paper. In this demonstration, we will use the train set of mini_librispeech to train and test a simple speaker recognition model.\",\"First of all, let's get into the directory to check the structure.\",\"%cd /content/espnet/egs2/mini_librispeech/sid1 !ls -l\"]},\"2412\":{\"h\":\"Data Preparation\",\"t\":[\"Similar to the previous tutorials, we will use the Kaldi-style format for the data preparation. The differences in this recipe is that we need to predict speaker ID instead of predicting transcription. Therefore, a straightforward process is to simply change the text into utt2spk.\",\"So final files after preparation should be:\",\"wav.scp text utt2spk spk2utt\",\"But on the other hand, we change the format of text into\",\"utt_id1 spk_id0 utt_id2 spk_id0 utt_id3 spk_id1\",\"where spk_id0 and spk_id1 refers to the speaker IDs\",\"!./run.sh --stage 1 --stop_stage 1\"]},\"2413\":{\"h\":\"Data Preprocessing\",\"t\":[\"For data preprocessing, we follow the similar way in previous tutorials/assignments.\",\"!./run.sh --stage 2 --stop_stage 5\"]},\"2414\":{\"h\":\"Question1 (✅ Checkpoint 1 (1 point))\",\"t\":[\"In previous tutorials, we usually use character as our modeling units. But for here, we use a speaker id, which is a sequence of character, representing one speaker. So, in our preprocessing, which tokenizer (e.g., char, bpe, phn, word) is actually used to achieve speaker prediction? Please also indicate your reason(s).\",\"To help you understand more, please check the documentation at https://espnet.github.io/espnet/search.html?q=tokenizer&check_keywords=yes&area=default\",\"(For question-based checkpoint: please directly answer it in the text box)\",\"[ANSWER HERE]\"]},\"2415\":{\"h\":\"Use Pre-trained speaker representation\",\"t\":[\"One feature in ESPnet is to adopt pre-trained speaker representation from other toolkits (including TDNN-based speaker embedding extraction from speechbrain and RawNet-based speaker embedding from RawNet. We can efficiently extract the speaker embedding with our supported scripts.\",\"The speaker embedding can be used for text-to-speech purpose to handle multi-speaker synthesis. In this demonstration, we directly use the extraction model for speaker recognition.\",\"!cat ./local/extract_xvector.sh\",\"!./local/extract_xvector.sh\",\"After calculating the xvectors, we also can analysis the embedding by t-SNE algorithm. The t-sne image is located at the extracted xvector folder\",\"from IPython.display import Image, display display(Image('dump/extracted/train/tsne.png'))\"]},\"2416\":{\"h\":\"Extract speaker embedding from SpeechBrain\",\"t\":[\"Similarly, we can also extract speaker embedding from speechbrain.\",\"!cat ./local/extract_xvector_speechbrain.sh !./local/extract_xvector_speechbrain.sh\",\"Similar to the speechbrain-based embedding, we can visualize the embeddings from RawNet with t-SNE plot.\",\"from IPython.display import Image, display display(Image('dump/extracted_speechbrain/train/tsne.png'))\"]},\"2417\":{\"h\":\"Training for speaker recognition\",\"t\":[\"First, let's use xvector trained from TDNN (speech-brain model) to conduct speaker recognition.\",\"!cat ./run_xvector_speechbrain.sh !./run_xvector_speechbrain.sh\"]},\"2418\":{\"h\":\"Question2 (✅ Checkpoint 2 (0.5 point))\",\"t\":[\"We still use the ASR scoring scheme for our evaluation because it is already sufficient. Please briefly discuss which metric can be used for evaluation of the accuracy/error rate of speaker recognition results.\",\"(For question-based checkpoint: please directly answer it in the text box)\",\"[ANSWER HERE]\",\"Then, let's use RawNet-based xvector to conduct speaker recognition\",\"!cat ./run_xvector.sh !./run_xvector.sh\"]},\"2419\":{\"h\":\"Question3 (✅ Checkpoint 3 (0.5 point))\",\"t\":[\"Clearly, we find some differences in the number between TDNN-based speaker embedding and RawNet-based speaker embedding. Could you briefly exaplin some possible reasons that why we could get such different results?\",\"References:\",\"RawNet\",\"Xvector (TDNN-based)\",\"(For question-based checkpoint: please directly answer it in the text box)\",\"[ANSWER HERE]\",\"We can also use ESPnet ASR model directly for speaker recognition purpose by predicting the target as speaker ID.\",\"!./run.sh --stage 10\"]},\"2420\":{\"h\":\"Question4 (✅ Checkpoint 4 (0.5 point))\",\"t\":[\"We could get reasonable performances with the ASR model. However, we could easily find that the training is much more time-consuming than those with speaker embeddings. Could you please explain why we have such differences?\",\"(For question-based checkpoint: please directly answer it in the text box)\",\"[ANSWER HERE]\"]},\"2421\":{\"h\":\"CMU 11751/18781 Fall 2022: ESPnet Tutorial\",\"t\":[\"ESPnet is a widely-used end-to-end speech processing toolkit. It has supported various speech processing tasks. ESPnet uses PyTorch as a main deep learning engine, and also follows Kaldi style data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"ESPnet tutorial in Speech Recognition and Understanding (Fall 2021)\",\"Recitation in Multilingual NLP (Spring 2022)\",\"Author: Siddhant Arora (siddhana@andrew.cmu.edu) This notebook was modified from the material made by Yifan Peng (yifanpen@andrew.cmu.edu)\"]},\"2422\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU, you may fail to connect to a GPU backend for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. There will also be some after-class excersices 📗 after the tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.\",\"This tutorial covers the basics of ESPnet, which will be the foundation of the next tutorial on Wednesday.\"]},\"2423\":{\"h\":\"Objectives\",\"t\":[\"After this tutorial, you are expected to know:\",\"How to run existing recipes (data prep, training, inference and scoring) in ESPnet2\",\"How to change the training and decoding configurations\",\"How to create a new recipe from scratch\",\"Where to find resources if you encounter an issue\"]},\"2424\":{\"h\":\"Useful links\",\"t\":[\"Installation https://espnet.github.io/espnet/installation.html\",\"Usage https://espnet.github.io/espnet/espnet2_tutorial.html\",\"This is a full installation method to perform data preprocessing, training, inference, scoring, and so on.\",\"We prepare various ways of installation. Please read https://espnet.github.io/espnet/installation.html#step-2-installation-espnet for more details.\"]},\"2425\":{\"h\":\"Function to print date and time\",\"t\":[\"We first define a function to print the current date and time, which will be used in multiple places below.\",\"def print_date_and_time(): from datetime import datetime import pytz now = datetime.now(pytz.timezone(\\\"America/New_York\\\")) print(\\\"=\\\" * 60) print(f' Current date and time: {now.strftime(\\\"%m/%d/%Y %H:%M:%S\\\")}') print(\\\"=\\\" * 60) # example output print_date_and_time()\"]},\"2426\":{\"h\":\"Check GPU type\",\"t\":[\"Let's check the GPU type of this allocated environment.\",\"!nvidia-smi\"]},\"2427\":{\"h\":\"Download ESPnet\",\"t\":[\"We use git clone to download the source code of ESPnet and then go to a specific commit.\",\"Important: In other versions of ESPnet, you may encounter errors related to imcompatible package versions (numba). Please use the same commit to avoid such issues.\",\"# It takes a few seconds !git clone --depth 5 https://github.com/espnet/espnet\"]},\"2428\":{\"h\":\"Setup Python environment based on anaconda\",\"t\":[\"There are several other installation methods, but we highly recommend the anaconda-based one.\",\"# It takes 30 seconds %cd /content/espnet/tools !./setup_anaconda.sh anaconda espnet 3.9\"]},\"2429\":{\"h\":\"Install ESPnet (same procedure as your first tutorial)\",\"t\":[\"This step installs PyTorch and other required tools.\",\"We specify CUDA_VERSION=11.6 for PyTorch 1.12.1. We also support many other versions. Please check https://github.com/espnet/espnet/blob/master/tools/installers/install_torch.sh for the detailed version list.\",\"# It may take 12 minutes %cd /content/espnet/tools !make TH_VERSION=1.12.1 CUDA_VERSION=11.6\",\"If other listed packages are necessary, install any of them using\",\". ./activation_python.sh && ./installers/install_xxx.sh\",\"We show two examples, although they are not used in this demo.\",\"# s3prl and fairseq are necessary if you want to use self-supervised pre-trained models # It takes 50s %cd /content/espnet/tools !. ./activate_python.sh && ./installers/install_s3prl.sh !. ./activate_python.sh && ./installers/install_fairseq.sh # install s3prl to use Wav2Vec2 / HuBERT model series\",\"ESPnet has a number of recipes (130 recipes on Sep. 11, 2022). Please refer to https://github.com/espnet/espnet/blob/master/egs2/README.md for a complete list.\",\"Please also check the general usage of the recipe in https://espnet.github.io/espnet/espnet2_tutorial.html#recipes-using-espnet2\",\"CMU AN4 recipe\",\"In this tutorial, we will use the CMU an4 recipe. This is a small-scale speech recognition task mainly used for testing.\",\"First, let's go to the recipe directory.\",\"%cd /content/espnet/egs2/an4/asr1 !ls\",\"egs2/an4/asr1/ - conf/ # Configuration files for training, inference, etc. - scripts/ # Bash utilities of espnet2 - pyscripts/ # Python utilities of espnet2 - steps/ # From Kaldi utilities - utils/ # From Kaldi utilities - db.sh # The directory path of each corpora - path.sh # Setup script for environment variables - cmd.sh # Configuration for your backend of job scheduler - run.sh # Entry point - asr.sh # Invoked by run.sh\",\"⭕ [SSL] Get the dump_hubert_feature.sh script and the training config ready.\",\"GitHub: https://github.com/simpleoier/ESPnet_SSL_ASR_tutorial_misc.git)\",\"!rm -r ESPnet_SSL_ASR_tutorial_misc !git clone https://github.com/simpleoier/ESPnet_SSL_ASR_tutorial_misc.git !cp ESPnet_SSL_ASR_tutorial_misc/dump_ssl_feature.sh ./local !cp ESPnet_SSL_ASR_tutorial_misc/dump_feats.py ./local !cp ESPnet_SSL_ASR_tutorial_misc/feats_loaders.py ./local !chmod +x local/dump_ssl_feature.sh !cp ESPnet_SSL_ASR_tutorial_misc/train_asr_demo_branchformer.yaml ./conf\",\"ESPnet is designed for various use cases (local machines or cluster machines) based on Kaldi tools. If you use it in the cluster machines, please also check https://kaldi-asr.org/doc/queue.html\",\"The main stages can be parallelized by various jobs.\",\"!cat run.sh !ls conf !ls local\",\"run.sh calls asr.sh, which completes the entire speech recognition experiments, including data preparation, training, inference, and scoring. They are separated into multiple stages (totally 16).\",\"Instead of executing the entire pipeline by run.sh, let's run it stage-by-stage to understand the process in each stage.\"]},\"2430\":{\"h\":\"Data preparation\",\"t\":[\"Stage 1: Data preparation: download raw data, split the entire set into train/dev/test, and prepare them in the Kaldi format\",\"Note that --stage &lt;N&gt; is to start from this stage and --stop_stage &lt;N&gt; is to stop after this stage. We also need to specify the train, dev and test sets.\",\"# a few seconds !./asr.sh --stage 1 --stop_stage 1 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"After this stage is finished, please check the newly created data directory:\",\"!ls data\",\"In this recipe, we use train_nodev as a training set, train_dev as a validation set (monitor the training progress by checking the validation score). We also use test and train_dev sets for the final speech recognition evaluation.\",\"Let's check one of the training data directories:\",\"!ls -1 data/train_nodev/\",\"These are the speech and corresponding text and speaker information in the Kaldi format. To understand their meanings, please check https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE#about-kaldi-style-data-directory.\",\"Please also check the official documentation of Kaldi: https://kaldi-asr.org/doc/data_prep.html\",\"spk2utt # Speaker information text # Transcription file utt2spk # Speaker information wav.scp # Audio file\",\"Stage 2: Speed perturbation (one of the data augmentation methods)\",\"We do not use speed perturbation for this demo. But you can turn it on by adding an argument --speed_perturb_factors \\\"0.9 1.0 1.1\\\" to the shell script.\",\"Note that we perform speed perturbation and save the augmented data in the disk before training. Another approach is to perform data augmentation during training, such as SpecAug.\",\"!./asr.sh --stage 2 --stop_stage 2 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"Stage 3: Format wav.scp: data/ -> dump/raw\",\"We dump the data with specified format (flac in this case) for the efficient use of the data.\",\"# ====== Recreating \\\"wav.scp\\\" ====== # Kaldi-wav.scp, which can describe the file path with unix-pipe, like \\\"cat /some/path |\\\", # shouldn't be used in training process. # \\\"format_wav_scp.sh\\\" dumps such pipe-style-wav to real audio file # and it can also change the audio-format and sampling rate. # If nothing is need, then format_wav_scp.sh does nothing: # i.e. the input file format and rate is same as the output.\",\"Note that --nj &lt;N&gt; means the number of CPU jobs. Please set it appropriately by considering your CPU resources and disk access.\",\"# 25 seconds !./asr.sh --stage 3 --stop_stage 3 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --nj 4\"]},\"2431\":{\"h\":\"⭕ [SSL] Stage 3.5: Extract SSL features\",\"t\":[\"We dump the SSL features of the data with specified format (kaldi mat in this case) for the efficient use of the data.\",\"First, we need to prepare the pretrained SSL models. In this colab, we use HuBERT models. We have three choices:\",\"HuBERT through FairSeq API; Model choices can be found from fairseq/hubert pretrained models\",\"Example usage: mkdir -p downloads/hubert_pretrained_models wget https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt -O ./downloads/hubert_pretrained_models/hubert_large_ll60k.pt Append the following arguments: --feature_type hubert --hubert_type fairseq --hubert_url \\\"https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt\\\" --hubert_dir_path \\\"./downloads/hubert_pretrained_models\\\" --layer 23\",\"HuBERT from ESPnet;\",\"Example usage: # Download model ./asr.sh --skip_data_prep true --skip_train true --skip_eval true --skip_upload true --download_model simpleoier/simpleoier_librispeech_hubert_iter1_train_ssl_torchaudiohubert_base_960h_pretrain_it1_raw --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" Append the following arguments: --feature_type hubert --hubert_type espnet --hubert_dir_path \\\"/content/espnet/tools/anaconda/envs/espnet/lib/python3.9/site-packages/espnet_model_zoo/models--simpleoier--simpleoier_librispeech_hubert_iter1_train_ssl_torchaudiohubert_base_960h_pretrain_it1_raw/snapshots/4256c702685249202f333348a87c13143985b90b/exp/hubert_iter1_train_ssl_torchaudiohubert_base_960h_pretrain_it1_raw/valid.loss.ave.pth\\\" --layer 12\",\"HuBERT through S3PRL API. S3prl also supports many other SSL models. Model choices can be found from s3prl_upstream_nameshere\",\"Append the following arguments: --feature_type s3prl --s3prl_upstream_name hubert_large_ll60k --layer 24\",\"Second, we extract the hubert features and copy the feats.scp into data dirs.\",\"# ====== Creating \\\"feats.scp\\\" ====== # Kaldi-feats.scp, which describe the file path (ark file) and offset,\",\"Note that --nj &lt;N&gt; means the number of CPU / GPU jobs. Please set it appropriately by considering your CPU resources and disk access. local/dump_ssl_feature.sh is the entry script.\"]},\"2432\":{\"h\":\"📗 Check the shape of dumped feature [1.0 pt]\",\"t\":[\"We will finally read the dumped feature and print the shape information to check if it is successful. The expected output is\",\"fkai-an311-b (155, 1024)\",\"# 5 min # 'dump_hubert_feature.sh' reads wave files from a common dir, so we symbolically link dump/raw/test in dump/raw/org !ln -s /content/espnet/egs2/an4/asr1/dump/raw/test /content/espnet/egs2/an4/asr1/dump/raw/org !rm -r ssl_feats/ # Fairseq HuBERT large example # !mkdir -p downloads/hubert_pretrained_models # !wget https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt -O ./downloads/hubert_pretrained_models/hubert_large_ll60k.pt # !local/dump_ssl_feature.sh --feat_dir ssl_feats --datadir dump/raw/org --train_set train_nodev --dev_set train_dev --test_sets \\\"test\\\" --use_gpu true --nj 1 --feature_type hubert --hubert_type fairseq --hubert_url \\\"https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt\\\" --hubert_dir_path \\\"./downloads/hubert_pretrained_models\\\" --layer 23 # S3PRL HuBERT large example !local/dump_ssl_feature.sh --feat_dir ssl_feats --datadir dump/raw/org --train_set train_nodev --dev_set train_dev --test_sets \\\"test\\\" --use_gpu true --nj 1 --feature_type s3prl --s3prl_upstream_name wavlm_large --layer 24 #!local/dump_ssl_feature.sh --feat_dir ssl_feats --datadir dump/raw/org --train_set train_nodev --dev_set train_dev --test_sets \\\"test\\\" --use_gpu true --nj 1 --feature_type s3prl --s3prl_upstream_name hubert_large_ll60k --layer 24 # copy the feats.scp to data/* !cp ssl_feats/s3prl/train_nodev/feats.scp data/train_nodev !cp ssl_feats/s3prl/train_dev/feats.scp data/train_dev !cp ssl_feats/s3prl/test/feats.scp data/test # Print the shape of dumped features. !/content/espnet/tools/anaconda/envs/espnet/bin/python3 -c \\\"import kaldiio; reader=kaldiio.ReadHelper('scp:data/train_nodev/feats.scp'); key, array = next(reader.generator); print(key, array.shape)\\\"\"]},\"2433\":{\"h\":\"⭕ [SSL] Stage 3: Format feats.scp: data/ -> dump/extracted\",\"t\":[\"Because we want to use extracted feature instead of raw wave, we need to run step 3 again**. It only construct a new dump/extracted folder, with some superficial commands.\",\"👀 From now on, --feats_type \\\"extracted\\\" will be added.\",\"# 25 seconds !./asr.sh --stage 3 --stop_stage 3 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --feats_type \\\"extracted\\\" --nj 4\",\"Stage 4: Remove long/short data: dump/extracted/org -> dump/raw\",\"Too long and too short audio data are harmful for efficient training. Those utterances are removed for training. But for inference and scoring, we still use the full data, which is important for fair comparison.\",\"!./asr.sh --stage 4 --stop_stage 4 --feats_type \\\"extracted\\\" --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"Stage 5: Generate token_list from dump/extracted/train_nodev/text using BPE.\",\"This is important for text processing. Here, we make a dictionary simply using the English characters. We use the sentencepiece toolkit developed by Google.\",\"!./asr.sh --stage 5 --stop_stage 5 --feats_type \\\"extracted\\\" --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\"]},\"2434\":{\"h\":\"Language modeling (skipped in this tutorial)\",\"t\":[\"Stages 6--9: Stages related to language modeling.\",\"We skip the language modeling part in the recipe (stages 6 -- 9) in this tutorial.\"]},\"2435\":{\"h\":\"How to change the configs?\",\"t\":[\"Let's revisit the configs, since this is probably the most important part to improve the performance.\"]},\"2436\":{\"h\":\"Config file based\",\"t\":[\"All training options are changed in the config file.\",\"Pleae check https://espnet.github.io/espnet/espnet2_training_option.html\",\"Let's first check config files prepared in the an4 recipe\",\"LSTM-based E2E ASR /content/espnet/egs2/an4/asr1/conf/train_asr_rnn.yaml\",\"Transformer based E2E ASR /content/espnet/egs2/an4/asr1/conf/train_asr_transformer.yaml\",\"You can run\",\"RNN\",\"./asr.sh --stage 10 \\\\ --feats_type \\\"extracted\\\" \\\\ --train_set train_nodev \\\\ --valid_set train_dev \\\\ --test_sets \\\"train_dev test\\\" \\\\ --nj 4 \\\\ --inference_nj 4 \\\\ --use_lm false \\\\ --asr_config conf/train_asr_rnn.yaml\",\"Transformer\",\"./asr.sh --stage 10 \\\\ --feats_type \\\"extracted\\\" \\\\ --train_set train_nodev \\\\ --valid_set train_dev \\\\ --test_sets \\\"train_dev test\\\" \\\\ --nj 4 \\\\ --inference_nj 4 \\\\ --use_lm false \\\\ --asr_config conf/train_asr_transformer.yaml\",\"You can also find various configs in other recipes espnet/egs2/*/asr1/conf/, including\",\"Conformer egs2/librispeech/asr1/conf/tuning/train_asr_conformer10_hop_length160.yaml\",\"Branchformer egs2/librispeech/asr1/conf/tuning/train_asr_branchformer_hop_length160_e18_linear3072.yaml\"]},\"2437\":{\"h\":\"Command line argument based\",\"t\":[\"You can also customize it by passing the command line arguments, e.g.,\",\"./run.sh --stage 10 --asr_args \\\"--model_conf ctc_weight=0.3\\\"\",\"./run.sh --stage 10 --asr_args \\\"--optim_conf lr=0.1\\\"\",\"This approach has a highest priority. Thus, the arguments passed in the command line will overwrite those defined in the config file. This is convenient if you only want to change a few arguments.\",\"Please refer to https://espnet.github.io/espnet/espnet2_tutorial.html#change-the-configuration-for-training for more details.\"]},\"2438\":{\"h\":\"📗 Exercise 1\",\"t\":[\"Run training, inference and scoring on AN4 using a new config. Here is an example config using Branchformer (Peng et al, ICML 2022).\"]},\"2439\":{\"h\":\"⭕ [SSL] Config modifications:\",\"t\":[\"Frontend is set to null.\",\"A preencoder is added to reduce input dimension.\",\"In the encoder, the subsampling is reduced to 2 (input_layer is conv2d2)\"]},\"2440\":{\"h\":\"⭕ [SSL] Normalization\",\"t\":[\"Gobal Mean normalization \",\"Compute the statistics (mean / var) on the full training set. This is done in stage 10. Both mean and var are considered.\",\"This is set by default in asr.sh by, specifically the argument --feats_normalize global_mvn.\",\"Utterance Mean normalization \",\"Compute the statistics (mean / var) on each single utterance. By default, ESPnet only normalize the mean.\",\"This can specified to asr.sh by --feats_normalize utt_mvn. Whatever the value is, as long as it is not global_mvn.\",\"No normalization \",\"Nothing is done in the feature.\",\"This can be specified by --feats_normalize null --asr_args \\\"--normalize null\\\"\",\"Similarly, we create a config file named train_asr_demo_branchformer.yaml and start training.\",\"batch_type: numel batch_bins: 4000000 accum_grad: 1 # gradient accumulation steps max_epoch: 40 patience: 10 init: xavier_uniform best_model_criterion: # criterion to save best models - - valid - acc - max keep_nbest_models: 10 # save nbest models and average these checkpoints use_amp: true # whether to use automatic mixed precision num_att_plot: 0 # do not save attention plots to save time in the demo num_workers: 2 # number of workers in dataloader frontend: null # Since extracted features are used, frontend is not used. preencoder: linear preencoder_conf: input_size: 1024 output_size: 128 encoder: branchformer encoder_conf: output_size: 256 use_attn: true attention_heads: 4 attention_layer_type: rel_selfattn pos_enc_layer_type: rel_pos rel_pos_type: latest use_cgmlp: true cgmlp_linear_units: 1024 cgmlp_conv_kernel: 31 use_linear_after_conv: false gate_activation: identity merge_method: concat cgmlp_weight: 0.5 # used only if merge_method is \\\"fixed_ave\\\" attn_branch_drop_rate: 0.0 # used only if merge_method is \\\"learned_ave\\\" num_blocks: 12 dropout_rate: 0.1 positional_dropout_rate: 0.1 attention_dropout_rate: 0.1 input_layer: conv2d2 stochastic_depth_rate: 0.0 decoder: transformer decoder_conf: attention_heads: 4 linear_units: 1024 num_blocks: 3 dropout_rate: 0.1 positional_dropout_rate: 0.1 self_attention_dropout_rate: 0.1 src_attention_dropout_rate: 0.1 model_conf: ctc_weight: 0.3 # joint CTC/attention training lsm_weight: 0.1 # label smoothing weight length_normalized_loss: false optim: adam optim_conf: lr: 0.0002 scheduler: warmuplr # linearly increase and exponentially decrease scheduler_conf: warmup_steps: 200\",\"My result is shown below:\",\"## exp/asr_train_asr_demo_branchformer_extracted_bpe30 ### WER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|773|95.9|2.6|1.6|0.0|4.1|16.9| |decode_asr_asr_model_valid.acc.ave/train_dev|100|591|92.0|5.9|2.0|0.2|8.1|28.0| ### CER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|2565|98.1|0.1|1.8|0.1|2.0|16.9| |decode_asr_asr_model_valid.acc.ave/train_dev|100|1915|95.5|0.7|3.8|0.2|4.7|28.0| ### TER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|2695|98.1|0.1|1.7|0.1|1.9|16.9| |decode_asr_asr_model_valid.acc.ave/train_dev|100|2015|95.7|0.7|3.6|0.1|4.5|28.0|\",\"# ~10 min # Run multiple stages !rm -r exp/asr_train_asr_demo_branchformer_extracted_bpe30 !./asr.sh --stage 10 --stop_stage 13 --feats_type \\\"extracted\\\" --feats_normalize utt_mvn --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --nj 4 --ngpu 1 --use_lm false --gpu_inference true --inference_nj 1 --asr_config conf/train_asr_demo_branchformer.yaml --inference_config conf/decode_asr.yaml\",\"# Load the TensorBoard notebook extension %load_ext tensorboard # Launch tensorboard before training %tensorboard --logdir /content/espnet/egs2/an4/asr1/exp/asr_train_asr_demo_branchformer_extracted_bpe30/tensorboard\",\"# NOTE: Exercise 1 Result 1 (HuBERT) !scripts/utils/show_asr_result.sh exp from IPython.display import Image, display display(Image('exp/asr_train_asr_demo_branchformer_extracted_bpe30/images/acc.png', width=400)) print_date_and_time()\",\"# NOTE: Exercise 1 Result 2 (WavLM) !scripts/utils/show_asr_result.sh exp from IPython.display import Image, display display(Image('exp/asr_train_asr_demo_branchformer_extracted_bpe30/images/acc.png', width=400)) print_date_and_time()\",\"# NOTE: Exercise 1 Result 3 (WavLM utt_mvn) !scripts/utils/show_asr_result.sh exp from IPython.display import Image, display display(Image('exp/asr_train_asr_demo_branchformer_extracted_bpe30/images/acc.png', width=400)) print_date_and_time()\"]},\"2441\":{\"h\":\"📗 Questions\",\"t\":[\"What is the difference between HuBERT and WavLM? [1 pt]\",\"WavLM is a newer model which uses masked speech denoising to create an embedding applicable to multiple downstream tasks, not just ASR.\",\"Get the ASR performance of one more SSL feature, WavLM, and show the results. [1 pt]\",\"Hint: change the s3prl_upstream_name to wavlm_large at stage 3.5 and run the following stages.\",\"# RESULTS ## Environments - date: `Sat Feb 25 03:26:54 UTC 2023` - python version: `3.9.16 (main, Jan 11 2023, 16:05:54) [GCC 11.2.0]` - espnet version: `espnet 202301` - pytorch version: `pytorch 1.12.1` - Git hash: `15a6dc1501b65211725a4fb514fcf5dd24f7ae95` - Commit date: `Thu Feb 23 22:04:23 2023 -0500` ## exp/asr_train_asr_demo_branchformer_extracted_bpe30 ### WER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|773|63.5|13.6|22.9|2.2|38.7|79.2| |decode_asr_asr_model_valid.acc.ave/train_dev|100|591|59.6|18.1|22.3|2.4|42.8|82.0| ### CER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|2565|80.6|2.6|16.8|1.4|20.8|79.2| |decode_asr_asr_model_valid.acc.ave/train_dev|100|1915|78.0|4.6|17.4|0.8|22.8|82.0| ### TER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|2695|81.6|2.4|16.0|1.3|19.8|79.2| |decode_asr_asr_model_valid.acc.ave/train_dev|100|2015|79.1|4.4|16.6|0.7|21.7|82.0| ============================================================ Current date and time: 02/24/2023 22:26:55 ============================================================\",\"Compare the performance between HuBERT, WavLM and MFCC features. Which is better? How much is it? Why do you think it is better in one sentence? [1 pt]\",\"It seems like HuBERT performed slightly better than WavLM, probably because HuBERT is more specifically focused on this ASR.\",\"Make a exploration of normalization mentioned in Stage 10 for either HuBRET or WavLM feature. Report the performance. [1 pt]\",\"Hint: you may change the number of epochs to get better performance.\"]},\"2442\":{\"h\":\"Environments\",\"t\":[\"date: Sat Feb 25 04:31:27 UTC 2023\",\"python version: 3.9.16 (main, Jan 11 2023, 16:05:54) [GCC 11.2.0]\",\"espnet version: espnet 202301\",\"pytorch version: pytorch 1.12.1\",\"Git hash: 15a6dc1501b65211725a4fb514fcf5dd24f7ae95\",\"Commit date: Thu Feb 23 22:04:23 2023 -0500\"]},\"2443\":{\"h\":\"exp/asr_train_asr_demo_branchformer_extracted_bpe30\"},\"2444\":{\"h\":\"WER\",\"t\":[\"dataset\",\"Snt\",\"Wrd\",\"Corr\",\"Sub\",\"Del\",\"Ins\",\"Err\",\"S.Err\",\"decode_asr_asr_model_valid.acc.ave/test\",\"130\",\"773\",\"63.5\",\"13.6\",\"22.9\",\"2.2\",\"38.7\",\"79.2\",\"decode_asr_asr_model_valid.acc.ave/train_dev\",\"100\",\"591\",\"59.6\",\"18.1\",\"22.3\",\"2.4\",\"42.8\",\"82.0\"]},\"2445\":{\"h\":\"CER\",\"t\":[\"dataset\",\"Snt\",\"Wrd\",\"Corr\",\"Sub\",\"Del\",\"Ins\",\"Err\",\"S.Err\",\"decode_asr_asr_model_valid.acc.ave/test\",\"130\",\"2565\",\"80.6\",\"2.6\",\"16.8\",\"1.4\",\"20.8\",\"79.2\",\"decode_asr_asr_model_valid.acc.ave/train_dev\",\"100\",\"1915\",\"78.0\",\"4.6\",\"17.4\",\"0.8\",\"22.8\",\"82.0\"]},\"2446\":{\"h\":\"TER\",\"t\":[\"dataset\",\"Snt\",\"Wrd\",\"Corr\",\"Sub\",\"Del\",\"Ins\",\"Err\",\"S.Err\",\"decode_asr_asr_model_valid.acc.ave/test\",\"130\",\"2695\",\"81.6\",\"2.4\",\"16.0\",\"1.3\",\"19.8\",\"79.2\",\"decode_asr_asr_model_valid.acc.ave/train_dev\",\"100\",\"2015\",\"79.1\",\"4.4\",\"16.6\",\"0.7\",\"21.7\",\"82.0\",\" ## Contribute to ESPnet Please follow https://github.com/espnet/espnet/blob/master/CONTRIBUTING.md to upload your pre-trained model to [Hugging Face](https://huggingface.co/espnet) and make a pull request in the [ESPnet repository](https://github.com/espnet/espnet/pulls).\"]},\"2447\":{\"h\":\"CMU 11492/11692 Spring 2023: Speech Translation\",\"t\":[\"In this demonstration, we will show you some demonstrations of speech translation systems in ESPnet.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"ESPnet-ST-v2 demo\",\"ESPnet-ST repo (WIP)\",\"Author:\",\"Jiatong Shi (jiatongs@andrew.cmu.edu)\"]},\"2448\":{\"h\":\"Objectives\",\"t\":[\"After this demonstration, you are expected to understand some latest advancements in speech translation.\"]},\"2449\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.\"]},\"2450\":{\"h\":\"ESPnet installation (Inference vesion)\",\"t\":[\"Different from previous assignment where we install the full version of ESPnet, we use a lightweight ESPnet package, which mainly designed for inference purpose. The installation with the light version can be much faster than a full installation. Noted that this is an active on-going work in ESPnet. The codebase is still in merging, so we will use a branch from our development fork for this assignment.\",\"!pip install typeguard==2.13.3 !git clone --depth 5 -b merge_s2st_st https://github.com/ftshijt/espnet.git !cd espnet && pip install . !pip install -q espnet_model_zoo\",\"We also have some other toolkits/packages needed for this assignment.\",\"!pip install --upgrade --no-cache-dir gdown !git clone --depth 1 https://github.com/kan-bayashi/ParallelWaveGAN.git !cd ParallelWaveGAN && pip install . !pip install pysndfile !pip install sacrebleu !pip install mosestokenizer !git clone https://github.com/facebookresearch/SimulEval.git !cd SimulEval && pip install -e .\"]},\"2451\":{\"h\":\"Speech Translation\",\"t\":[\"Speech translation is a typical task that translate speech in a language into text/speech in another language. In this tutorial, we will show you the some latest models (in ESPnet-ST-v2) in the field of speech translation and demonstrate using them in different scenarios, including\",\"offline speech-to-text translation\",\"simultaneous speech-to-text translation\",\"speech-to-speech translation\"]},\"2452\":{\"h\":\"Overview of the ESPnet-ST-v2\",\"t\":[\"ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models.\",\"picture\",\"In general, the toolkit is organizd in a pythonic way to support model training/inference, while we also provide recipes for data preparation, model training, and evaluation.\",\"pitcture\"]},\"2453\":{\"h\":\"1. Offline Speech-to-text Translation (ST)\"},\"2454\":{\"h\":\"1.1 Model download\",\"t\":[\"# Download pretrained st model !gdown 1Sn2rAZXVSm1hrCj5OIlq61EgbjKXNGdq !unzip -o st_train_st_ctc_md_conformer_asrinit_v3_noamp_batch50m_ctcsamp0.1_lr1e-3_raw_en_es_bpe_tc4000_sp_valid.acc.ave.zip\"]},\"2455\":{\"h\":\"1.2 Model Setup\",\"t\":[\"import time import torch import string from espnet2.bin.st_inference import Speech2Text lang=\\\"es\\\" fs = 16000 speech2text = Speech2Text( st_model_file=\\\"/content/exp/st_train_st_ctc_md_conformer_asrinit_v3_noamp_batch50m_ctcsamp0.1_lr1e-3_raw_en_es_bpe_tc4000_sp/valid.acc.ave_10best.pth\\\", st_train_config=\\\"/content/exp/st_train_st_ctc_md_conformer_asrinit_v3_noamp_batch50m_ctcsamp0.1_lr1e-3_raw_en_es_bpe_tc4000_sp/config.yaml\\\", beam_size=10, ctc_weight=0.3, asr_beam_size=10, asr_ctc_weight=0.3, device=\\\"cuda\\\", )\"]},\"2456\":{\"h\":\"1.3 Translate our example recordings\",\"t\":[\"!git clone https://github.com/ftshijt/ESPnet_st_egs.git\",\"import torch import pandas as pd import soundfile as sf import librosa.display from IPython.display import display, Audio import matplotlib.pyplot as plt from sacrebleu.metrics import BLEU bleu = BLEU() egs = pd.read_csv(\\\"ESPnet_st_egs/st/egs.csv\\\") for index, row in egs.iterrows(): if row[\\\"lang\\\"] == lang or lang == \\\"multilingual\\\": speech, rate = sf.read(\\\"ESPnet_st_egs/\\\" + row[\\\"path\\\"]) assert fs == int(row[\\\"sr\\\"]) text, _, _, _ = speech2text(speech)[0][0] display(Audio(speech, rate=fs)) librosa.display.waveplot(speech, sr=fs) plt.show() print(f\\\"Reference source text: {row['src_text']}\\\") print(f\\\"Translation results: {text}\\\") print(f\\\"Reference target text: {row['tgt_text']}\\\") print(f\\\"Sentence BLEU Score: {bleu.sentence_score(text, [row['tgt_text']])}\\\") print(\\\"*\\\" * 50)\"]},\"2457\":{\"h\":\"Task1 (✅ Checkpoint 1 (2 point))\",\"t\":[\"We have printout the sentence BLEU score of the model. Can you compute the corpus BLEU with examples in sacreBLEU based on the five utterances in the example?\",\"# CHECKPOINT1 refs = [[ \\\"Acabo de regresar de una comunidad que tiene el secreto de la supervivencia humana .\\\", \\\"En última instancia , avanzar , creo que tenemos que darle lugar al miedo .\\\", \\\"Cuando recién ingresaba a la universidad tuve mi primer clase de biología .\\\", \\\"Comparte sus experiencias con ellos .\\\", \\\"Cada vez que estén de vacaciones y alguien colapse , puede ser un pariente o alguien enfrente de Uds. , pueden encontrarlo .\\\", ]] # Please fill the translation results to here hyps = [ ] # Please compute the corpus bleu score\"]},\"2458\":{\"h\":\"1.4 Translate your own live-recordings\",\"t\":[\"Record your own voice\",\"Tralsate your vocie with the ST system\",\"# from https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be from IPython.display import Javascript from google.colab import output from base64 import b64decode RECORD = \\\"\\\"\\\" const sleep = time => new Promise(resolve => setTimeout(resolve, time)) const b2text = blob => new Promise(resolve => { const reader = new FileReader() reader.onloadend = e => resolve(e.srcElement.result) reader.readAsDataURL(blob) }) var record = time => new Promise(async resolve => { stream = await navigator.mediaDevices.getUserMedia({ audio: true }) recorder = new MediaRecorder(stream) chunks = [] recorder.ondataavailable = e => chunks.push(e.data) recorder.start() await sleep(time) recorder.onstop = async ()=>{ blob = new Blob(chunks) text = await b2text(blob) resolve(text) } recorder.stop() }) \\\"\\\"\\\" def record(sec, filename='audio.wav'): display(Javascript(RECORD)) s = output.eval_js('record(%d)' % (sec * 1000)) b = b64decode(s.split(',')[1]) with open(filename, 'wb+') as f: f.write(b) audio = 'audio.wav' second = 5 print(f\\\"Speak to your microphone {second} sec...\\\") record(second, audio) print(\\\"Done!\\\") import librosa import librosa.display speech, rate = librosa.load(audio, sr=16000) librosa.display.waveplot(speech, sr=rate) import matplotlib.pyplot as plt plt.show() import pysndfile pysndfile.sndio.write('audio_ds.wav', speech, rate=rate, format='wav', enc='pcm16') from IPython.display import display, Audio display(Audio(speech, rate=rate))\"]},\"2459\":{\"h\":\"Task2 (✅ Checkpoint 2 (1 point))\",\"t\":[\"Please follow the same procedure as previous examples and print out the translation results. (You can directly use the speech2text function)\",\"# [CHECKPOINT2] # Follow the same procedure as previous examples, print out the translation results\"]},\"2460\":{\"h\":\"2. Simultaneous Speech-to-text Translation (SST)\",\"t\":[\"# Download retrained sst model !gdown 1ekUeMvmaB3ZhAIY_KAb_we1zhIRFZhtu !unzip -o /content/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp_valid.acc.ave.zip\",\"import time import torch import string from espnet2.bin.st_inference_streaming import Speech2TextStreaming lang=\\\"es\\\" fs = 16000 speech2textstreaming = Speech2TextStreaming( st_model_file=\\\"/content/exp/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp/valid.acc.ave_10best.pth\\\", st_train_config=\\\"/content/exp/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp/config.yaml\\\", penalty=0.4, blank_penalty=0.5, beam_size=10, ctc_weight=0.5, incremental_decode=True, time_sync=True, device=\\\"cuda\\\", )\",\"import torch import pandas as pd import soundfile as sf import librosa.display from IPython.display import display, Audio import matplotlib.pyplot as plt from sacrebleu.metrics import BLEU bleu = BLEU() egs = pd.read_csv(\\\"ESPnet_st_egs/st/egs.csv\\\") for index, row in egs.iterrows(): if row[\\\"lang\\\"] == lang or lang == \\\"multilingual\\\": speech, rate = sf.read(\\\"ESPnet_st_egs/\\\" + row[\\\"path\\\"]) assert fs == int(row[\\\"sr\\\"]) text = speech2textstreaming(speech)[0][0] display(Audio(speech, rate=fs)) librosa.display.waveplot(speech, sr=fs) plt.show() print(f\\\"Reference source text: {row['src_text']}\\\") print(f\\\"Translation results: {text}\\\") print(f\\\"Reference target text: {row['tgt_text']}\\\") print(f\\\"Sentence BLEU Score: {bleu.sentence_score(text, [row['tgt_text']])}\\\") print(\\\"*\\\" * 50)\"]},\"2461\":{\"h\":\"Question3 (✅ Checkpoint 3 (1 point))\",\"t\":[\"How is the performance of the streaming model compared to the offline model? Could you provide some explanation on the performances differences?\",\"(For question-based checkpoint: please directly answer it in the text box)\",\"[YOUR ANSWER HERE]\",\"!simuleval --source /content/ESPnet_st_egs/st/wav.scp --target /content/ESPnet_st_egs/st/ref.detok.trn --agent /content/espnet/egs2/TEMPLATE/st1/pyscripts/utils/simuleval_agent.py --batch_size 1 --ngpu 0 --st_train_config /content/exp/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp/config.yaml --st_model_file exp/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp/valid.acc.ave_10best.pth --disable_repetition_detection false --beam_size 10 --sim_chunk_length 2048 --backend streaming --ctc_weight 0.5 --incremental_decode true --penalty 0.4 --blank_penalty 0.7 --time_sync true --latency-metrics LAAL AL AP DAL\"]},\"2462\":{\"h\":\"Question4 (✅ Checkpoint 4 (1 point))\",\"t\":[\"Despite from BLEU, we have LAACL, AL, AP, DAL for evaluation. AL (average lagging) is one of the most widely used metrics in recent works. Please use one sentence to describe what AL is.\",\"(For question-based checkpoint: please directly answer it in the text box)\",\"[YOUR ANSWER HERE]\"]},\"2463\":{\"h\":\"CMU 11492/11692 Spring 2023: Spoken Language Understanding\",\"t\":[\"In this demonstration, we will show you the procedure to conduct spoken language understanding in ESPnet.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"Author:\",\"Siddhant Arora (siddhana@andrew.cmu.edu)\"]},\"2464\":{\"h\":\"Objectives\",\"t\":[\"After this demonstration, you are expected to understand some latest advancements in spoken language understanding.\"]},\"2465\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.\"]},\"2466\":{\"h\":\"ESPnet installation\",\"t\":[\"We follow the ESPnet installation as the previous tutorials (takes around 15 minutes).\",\"! python -m pip install transformers !git clone https://github.com/espnet/espnet /espnet !pip install /espnet %pip install -q espnet_model_zoo %pip install fairseq@git+https://github.com//pytorch/fairseq.git@f2146bdc7abf293186de9449bfa2272775e39e1d#egg=fairseq\"]},\"2467\":{\"h\":\"Spoken Language Understanding\",\"t\":[\"Spoken Language Understanding (SLU) refers to the task of extracting semantic meaning or linguistic structure from spoken utterances. Some examples include recognizing the intent and their associated entities of a user’s command to take appropriate action, or even understanding the emotion behind a particular utterance, and engaging in conversations with a user by modeling the topic of a conversation. SLU is an essential component of many commercial applications like voice assistants, social bots, and intelligent home devices which have to map speech signals to executable commands every day.\",\"Conventional SLU systems employ a cascaded approach for sequence labeling, where an automatic speech recognition (ASR) system first recognizes the spoken words from the input audio and a natural language understanding (NLU) system then extracts the intent from the predicted text. These cascaded approaches can effectively utilize pretrained ASR and NLU systems. However, they suffer from error propagation as errors in the ASR transcripts can adversely affect downstream SLU performance. Consequently, in this demo, we focus on end-to-end (E2E) SLU systems. E2E SLU systems aim to predict intent directly from speech. These E2E SLU systems can avoid the cascading of errors but cannot directly utilize strong acoustic and semantic representations from pretrained ASR systems and language models.\",\"In this tutorial, we will show you some latest E2E SLU model architectures (in ESPnet-SLU) in the field of spoken language understanding, including\",\"E2E SLU (https://arxiv.org/abs/2111.14706)\",\"Two Pass E2E SLU (https://arxiv.org/abs/2207.06670)\"]},\"2468\":{\"h\":\"Overview of the ESPnet-SLU\",\"t\":[\"As ASR systems are getting better, there is an increasing interest in using the ASR output directly to do downstream Natural Language Processing (NLP) tasks. With the increase in SLU datasets and methodologies proposed, ESPnet-SLU is an open-source SLU toolkit built on an already existing open-source speech processing toolkit ESPnet. ESPnet-SLU standardize the pipelines involved in building an SLU model like data preparation, model training, and its evaluation. Having ESPnet-SLU would help users build systems for real world scenarios where many speech processing steps need to be applied before running the downstream task. ESPnet also provides an easy access to other speech technologies being developed like data augmentation, encoder sub-sampling, and speech-focused encoders like conformers. They also support many pretrained ASR and NLU systems that can be used as feature extractors in a SLU framework.\",\"We have shown a sample architecure of our E2E SLU Model in the figure below:\",\"picture\"]},\"2469\":{\"h\":\"1. E2E SLU\"},\"2470\":{\"h\":\"1.1 Download Sample Audio File\",\"t\":[\"!gdown --id 18ANT62ittt7Ai2E8bQRlvT0ZVXXsf1eE -O /content/audio_file.wav import os import soundfile from IPython.display import display, Audio mixwav_mc, sr = soundfile.read(\\\"/content/audio_file.wav\\\") display(Audio(mixwav_mc.T, rate=sr))\"]},\"2471\":{\"h\":\"Question1 (✅ Checkpoint 1 (1 points))\",\"t\":[\"Run inference on given audio using E2E SLU for intent classification\"]},\"2472\":{\"h\":\"1.2 Download and Load pretrained E2E SLU Model\",\"t\":[\"!git lfs clone https://huggingface.co/espnet/siddhana_slurp_new_asr_train_asr_conformer_raw_en_word_valid.acc.ave_10best /content/slurp_first_pass_model from espnet2.bin.asr_inference import Speech2Text speech2text_slurp = Speech2Text.from_pretrained( asr_train_config=\\\"/content/slurp_first_pass_model/exp/asr_train_asr_conformer_raw_en_word/config.yaml\\\", asr_model_file=\\\"/content/slurp_first_pass_model/exp/asr_train_asr_conformer_raw_en_word/valid.acc.ave_10best.pth\\\", nbest=1, )\",\"nbests_orig = speech2text_slurp(mixwav_mc) text, *_ = nbests_orig[0] def text_normalizer(sub_word_transcript): transcript = sub_word_transcript[0].replace(\\\"▁\\\", \\\"\\\") for sub_word in sub_word_transcript[1:]: if \\\"▁\\\" in sub_word: transcript = transcript + \\\" \\\" + sub_word.replace(\\\"▁\\\", \\\"\\\") else: transcript = transcript + sub_word return transcript intent_text=\\\"{scenario: \\\"+text.split()[0].split(\\\"_\\\")[0]+\\\", action: \\\"+\\\"_\\\".join(text.split()[0].split(\\\"_\\\")[1:])+\\\"}\\\" print(f\\\"INTENT: {intent_text}\\\") transcript=text_normalizer(text.split()[1:]) print(f\\\"ASR hypothesis: {transcript}\\\") print(f\\\"E2E SLU model fails to predict the correct action.\\\")\"]},\"2473\":{\"h\":\"2. Two Pass E2E SLU\",\"t\":[\"However, recent work has shown that E2E-SLU systems struggle to generalize to unique phrasing for the same intent, suggesting an opportunity for enhancing semantic modeling of existing SLU systems. A number of approaches have been proposed to learn semantic content directly from audio. These approaches aim to incorporate pretrained language models to improve semantic processing of SLU architectures. In this demo, we use the Two Pass E2E SLU model where the second pass model improves on the initial prediction by combining acoustic information from the entire speech and semantic information from ASR-hypothesis using a deliberation network.\",\"pitcture\"]},\"2474\":{\"h\":\"Question2 (✅ Checkpoint 2 (1 points))\",\"t\":[\"Run inference on given audio using 2 pass SLU\",\"!git lfs clone https://huggingface.co/espnet/slurp_slu_2pass /content/slurp_second_pass_model\",\"from espnet2.bin.slu_inference import Speech2Understand from transformers import AutoModel, AutoTokenizer speech2text_second_pass_slurp = Speech2Understand.from_pretrained( slu_train_config=\\\"/content/slurp_second_pass_model/exp/slu_train_asr_bert_conformer_deliberation_raw_en_word/config.yaml\\\", slu_model_file=\\\"/content/slurp_second_pass_model/exp/slu_train_asr_bert_conformer_deliberation_raw_en_word/valid.acc.ave_10best.pth\\\", nbest=1, )\",\"from espnet2.tasks.slu import SLUTask preprocess_fn=SLUTask.build_preprocess_fn( speech2text_second_pass_slurp.asr_train_args, False ) import numpy as np transcript = preprocess_fn.text_cleaner(transcript) tokens = preprocess_fn.transcript_tokenizer.text2tokens(transcript) text_ints = np.array(preprocess_fn.transcript_token_id_converter.tokens2ids(tokens), dtype=np.int64)\",\"import torch nbests = speech2text_second_pass_slurp(mixwav_mc,torch.tensor(text_ints)) text1, *_ = nbests[0] intent_text=\\\"{scenario: \\\"+text1.split()[0].split(\\\"_\\\")[0]+\\\", action: \\\"+\\\"_\\\".join(text1.split()[0].split(\\\"_\\\")[1:])+\\\"}\\\" print(f\\\"INTENT: {intent_text}\\\") transcript=text_normalizer(text1.split()[1:]) print(f\\\"ASR hypothesis: {transcript}\\\") print(f\\\"Second pass SLU model successfully recognizes the correct action.\\\")\"]},\"2475\":{\"h\":\"3. E2E SLU for Slot Filling\"},\"2476\":{\"h\":\"Question3 (✅ Checkpoint 3 (1 point))\",\"t\":[\"Run inference on given audio using E2E SLU for slot filling\",\"!gdown --id 1ezs8IPutLr-C0PXKb6pfOlb6XXFDXcPd -O /content/audio_slurp_entity_file.wav import os import soundfile from IPython.display import display, Audio mixwav_mc, sr = soundfile.read(\\\"/content/audio_slurp_entity_file.wav\\\") display(Audio(mixwav_mc.T, rate=sr))\",\"!git lfs clone https://huggingface.co/espnet/siddhana_slurp_entity_asr_train_asr_conformer_raw_en_word_valid.acc.ave_10best /content/slurp_entity_model from espnet2.bin.asr_inference import Speech2Text speech2text_slurp = Speech2Text.from_pretrained( asr_train_config=\\\"/content/slurp_entity_model/exp/asr_train_asr_conformer_raw_en_word/config.yaml\\\", asr_model_file=\\\"/content/slurp_entity_model/exp/asr_train_asr_conformer_raw_en_word/valid.acc.ave_10best.pth\\\", nbest=1, )\",\"nbests_orig = speech2text_slurp(mixwav_mc) text, *_ = nbests_orig[0]\",\"def entity_text_normalizer(sub_word_transcript_list): transcript_dict={} for sub_word_transcript_new in sub_word_transcript_list: sub_word_transcript=sub_word_transcript_new.split() # print(sub_word_transcript_list) # print(sub_word_transcript) transcript = sub_word_transcript[0].replace(\\\"▁\\\", \\\"\\\") for sub_word in sub_word_transcript[1:]: if \\\"▁\\\" in sub_word: transcript = transcript + \\\" \\\" + sub_word.replace(\\\"▁\\\", \\\"\\\") else: transcript = transcript + sub_word transcript_dict[transcript.split(\\\" FILL \\\")[0]]=transcript.split(\\\" FILL \\\")[1] return transcript_dict intent_text=\\\"{scenario: \\\"+text.split()[0].split(\\\"_\\\")[0]+\\\", action: \\\"+\\\"_\\\".join(text.split()[0].split(\\\"_\\\")[1:])+\\\"}\\\" # print(text) print(f\\\"INTENT: {intent_text}\\\") # print(\\\" \\\".join(text.split()[1:]).split(\\\"▁SEP\\\")[-1].split()) transcript=text_normalizer(\\\" \\\".join(text.split()[1:]).split(\\\"▁SEP\\\")[-1].split()) print(f\\\"ASR hypothesis: {transcript}\\\") entity_transcript=entity_text_normalizer(\\\" \\\".join(text.split()[1:]).split(\\\"▁SEP\\\")[1:-1]) print(f\\\"Slot dictionary: {entity_transcript}\\\")\"]},\"2477\":{\"h\":\"4. E2E SLU for Sentiment Analysis\"},\"2478\":{\"h\":\"Question4 (✅ Checkpoint 4 (1 point))\",\"t\":[\"Run inference on given audio using E2E SLU for sentiment analysis\",\"!gdown --id 1CZzmpMliwSzja9TdBV7wmidlGepZBEUi -O /content/audio_iemocap_file.wav import os import soundfile from IPython.display import display, Audio mixwav_mc, sr = soundfile.read(\\\"/content/audio_iemocap_file.wav\\\") display(Audio(mixwav_mc.T, rate=sr))\",\"!git lfs clone https://huggingface.co/espnet/YushiUeda_iemocap_sentiment_asr_train_asr_conformer /content/iemocap_model from espnet2.bin.asr_inference import Speech2Text speech2text_iemocap = Speech2Text.from_pretrained( asr_train_config=\\\"/content/iemocap_model/exp/asr_train_asr_conformer_raw_en_word/config.yaml\\\", asr_model_file=\\\"/content/iemocap_model/exp/asr_train_asr_conformer_raw_en_word/valid.acc.ave_10best.pth\\\", nbest=1, )\",\"nbests_orig = speech2text_iemocap(mixwav_mc) text, *_ = nbests_orig[0] sentiment_text=text.split()[0] print(f\\\"SENTIMENT: {sentiment_text}\\\")\"]},\"2479\":{\"h\":\"Question5 (✅ Checkpoint 5 (1 point))\",\"t\":[\"Discuss about potential advantages of integrating pre-trained LMs inside E2E SLU framework compared to using them in cascaded manner?\",\"[ANSWER HERE]\"]},\"2480\":{\"h\":\"CMU 11492/11692 Spring 2023: Speech Enhancement\",\"t\":[\"In this demonstration, we will show you some demonstrations of speech enhancement systems in ESPnet.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"ESPnet-SE repo\",\"Author:\",\"Siddhant Arora (siddhana@andrew.cmu.edu)\",\"The notebook is adapted from this Colab\"]},\"2481\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.You also need to submit the spectrogram and waveform of noisy and enhanced audio files to Gradescope.\",\"Tutorials on the Basic Usage\",\"Install\",\"Speech Enhancement with Pretrained Models\",\"We support various interfaces, e.g. Python API, HuggingFace API, portable speech enhancement scripts for other tasks, etc.\",\"2.1 Single-channel Enhancement (CHiME-4)\",\"2.2 Enhance Your Own Recordings\",\"2.3 Multi-channel Enhancement (CHiME-4)\",\"Speech Separation with Pretrained Models\",\"3.1 Model Selection\",\"3.2 Separate Speech Mixture\",\"Evaluate Separated Speech with the Pretrained ASR Model\",\"Tutorials on the Basic Usage\"]},\"2482\":{\"h\":\"Install\",\"t\":[\"Different from previous assignment where we install the full version of ESPnet, we use a lightweight ESPnet package, which mainly designed for inference purpose. The installation with the light version can be much faster than a full installation.\",\"import locale locale.getpreferredencoding = lambda: \\\"UTF-8\\\" %pip uninstall torch %pip install torch==1.13.0+cu117 torchvision==0.14.0+cu117 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117 %pip install -q git+https://github.com/espnet/espnet %pip install -q espnet_model_zoo\"]},\"2483\":{\"h\":\"Speech Enhancement with Pretrained Models\"},\"2484\":{\"h\":\"Single-Channel Enhancement, the CHiME example\"},\"2485\":{\"h\":\"Task1 (✅ Checkpoint 1 (1 point))\",\"t\":[\"Run inference of pretrained single-channel enhancement model.\",\"# Download one utterance from real noisy speech of CHiME4 !gdown --id 1SmrN5NFSg6JuQSs2sfy3ehD8OIcqK6wS -O /content/M05_440C0213_PED_REAL.wav import os import soundfile from IPython.display import display, Audio mixwav_mc, sr = soundfile.read(\\\"/content/M05_440C0213_PED_REAL.wav\\\") # mixwav.shape: num_samples, num_channels mixwav_sc = mixwav_mc[:,4] display(Audio(mixwav_mc.T, rate=sr))\"]},\"2486\":{\"h\":\"Download and load the pretrained Conv-Tasnet\",\"t\":[\"!gdown --id 17DMWdw84wF3fz3t7ia1zssdzhkpVQGZm -O /content/chime_tasnet_singlechannel.zip !unzip /content/chime_tasnet_singlechannel.zip -d /content/enh_model_sc\",\"# Load the model # If you encounter error \\\"No module named 'espnet2'\\\", please re-run the 1st Cell. This might be a colab bug. import sys import soundfile from espnet2.bin.enh_inference import SeparateSpeech separate_speech = {} # For models downloaded from GoogleDrive, you can use the following script: enh_model_sc = SeparateSpeech( train_config=\\\"/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/config.yaml\\\", model_file=\\\"/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/5epoch.pth\\\", # for segment-wise process on long speech normalize_segment_scale=False, show_progressbar=True, ref_channel=4, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2487\":{\"h\":\"Enhance the single-channel real noisy speech in CHiME4\",\"t\":[\"Please submit the screenshot of output of current block and the spectogram and waveform of noisy and enhanced speech file to Gradescope for Task 1.\",\"# play the enhanced single-channel speech wave = enh_model_sc(mixwav_sc[None, ...], sr) print(\\\"Input real noisy speech\\\", flush=True) display(Audio(mixwav_sc, rate=sr)) print(\\\"Enhanced speech\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2488\":{\"h\":\"Multi-Channel Enhancement\"},\"2489\":{\"h\":\"Download and load the pretrained mvdr neural beamformer.\"},\"2490\":{\"h\":\"Task2 (✅ Checkpoint 2 (1 point))\",\"t\":[\"Run inference of pretrained multi-channel enhancement model.\",\"# Download the pretained enhancement model !gdown --id 1FohDfBlOa7ipc9v2luY-QIFQ_GJ1iW_i -O /content/mvdr_beamformer_16k_se_raw_valid.zip !unzip /content/mvdr_beamformer_16k_se_raw_valid.zip -d /content/enh_model_mc\",\"# Load the model # If you encounter error \\\"No module named 'espnet2'\\\", please re-run the 1st Cell. This might be a colab bug. import sys import soundfile from espnet2.bin.enh_inference import SeparateSpeech separate_speech = {} # For models downloaded from GoogleDrive, you can use the following script: enh_model_mc = SeparateSpeech( train_config=\\\"/content/enh_model_mc/exp/enh_train_enh_beamformer_mvdr_raw/config.yaml\\\", model_file=\\\"/content/enh_model_mc/exp/enh_train_enh_beamformer_mvdr_raw/11epoch.pth\\\", # for segment-wise process on long speech normalize_segment_scale=False, show_progressbar=True, ref_channel=4, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2491\":{\"h\":\"Enhance the multi-channel real noisy speech in CHiME4\",\"t\":[\"Please submit the screenshot of output of current block and the spectrogram and waveform of noisy and enhanced speech file to Gradescope for Task 2.\",\"wave = enh_model_mc(mixwav_mc[None, ...], sr) print(\\\"Input real noisy speech\\\", flush=True) display(Audio(mixwav_mc.T, rate=sr)) print(\\\"Enhanced speech\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2492\":{\"h\":\"Portable speech enhancement scripts for other tasks\",\"t\":[\"For an ESPNet ASR or TTS dataset like below:\",\"data `-- et05_real_isolated_6ch_track |-- spk2utt |-- text |-- utt2spk |-- utt2uniq `-- wav.scp\",\"Run the following scripts to create an enhanced dataset:\",\"scripts/utils/enhance_dataset.sh \\\\ --spk_num 1 \\\\ --gpu_inference true \\\\ --inference_nj 4 \\\\ --fs 16k \\\\ --id_prefix \\\"\\\" \\\\ dump/raw/et05_real_isolated_6ch_track \\\\ data/et05_real_isolated_6ch_track_enh \\\\ exp/enh_train_enh_beamformer_mvdr_raw/valid.loss.best.pth\",\"The above script will generate a new directory data/et05_real_isolated_6ch_track_enh:\",\"data `-- et05_real_isolated_6ch_track_enh |-- spk2utt |-- text |-- utt2spk |-- utt2uniq |-- wav.scp `-- wavs/\",\"where wav.scp contains paths to the enhanced audios (stored in wavs/).\"]},\"2493\":{\"h\":\"Speech Separation\"},\"2494\":{\"h\":\"Model Selection\",\"t\":[\"In this demonstration, we will show different speech separation models on wsj0_2mix.\",\"The pretrained models can be download from a direct URL, or from zenodo and huggingface with the corresponding model ID.\",\"!gdown --id 1TasZxZSnbSPsk_Wf7ZDhBAigS6zN8G9G -O enh_train_enh_tfgridnet_tf_lr-patience3_patience5_raw_valid.loss.ave.zip !unzip enh_train_enh_tfgridnet_tf_lr-patience3_patience5_raw_valid.loss.ave.zip -d /content/enh_model_ss\",\"import sys import soundfile from espnet2.bin.enh_inference import SeparateSpeech # For models downloaded from GoogleDrive, you can use the following script: separate_speech = SeparateSpeech( train_config=\\\"/content/enh_model_ss/exp/enh_train_enh_tfgridnet_tf_lr-patience3_patience5_raw/config.yaml\\\", model_file=\\\"/content/enh_model_ss/exp/enh_train_enh_tfgridnet_tf_lr-patience3_patience5_raw/98epoch.pth\\\", # for segment-wise process on long speech segment_size=2.4, hop_size=0.8, normalize_segment_scale=False, show_progressbar=True, ref_channel=None, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2495\":{\"h\":\"Separate Speech Mixture\"},\"2496\":{\"h\":\"Separate the example in wsj0_2mix testing set\"},\"2497\":{\"h\":\"Task3 (✅ Checkpoint 3 (1 point))\",\"t\":[\"Run inference of pretrained speech seperation model based on TF-GRIDNET.\",\"Please submit the screenshot of output of current block and the spectrogram and waveform of mixed and seperated speech files to Gradescope for Task 3.\",\"!gdown --id 1ZCUkd_Lb7pO2rpPr4FqYdtJBZ7JMiInx -O /content/447c020t_1.2106_422a0112_-1.2106.wav import os import soundfile from IPython.display import display, Audio mixwav, sr = soundfile.read(\\\"447c020t_1.2106_422a0112_-1.2106.wav\\\") waves_wsj = separate_speech(mixwav[None, ...], fs=sr) print(\\\"Input mixture\\\", flush=True) display(Audio(mixwav, rate=sr)) print(f\\\"========= Separated speech with model =========\\\", flush=True) print(\\\"Separated spk1\\\", flush=True) display(Audio(waves_wsj[0].squeeze(), rate=sr)) print(\\\"Separated spk2\\\", flush=True) display(Audio(waves_wsj[1].squeeze(), rate=sr))\"]},\"2498\":{\"h\":\"Show spectrums of separated speech\",\"t\":[\"Show wavform and spectrogram of mixed and seperated speech.\",\"import matplotlib.pyplot as plt import torch from torch_complex.tensor import ComplexTensor from espnet.asr.asr_utils import plot_spectrogram from espnet2.layers.stft import Stft stft = Stft( n_fft=512, win_length=None, hop_length=128, window=\\\"hann\\\", ) ilens = torch.LongTensor([len(mixwav)]) # specs: (T, F) spec_mix = ComplexTensor( *torch.unbind( stft(torch.as_tensor(mixwav).unsqueeze(0), ilens)[0].squeeze(), dim=-1 ) ) spec_sep1 = ComplexTensor( *torch.unbind( stft(torch.as_tensor(waves_wsj[0]), ilens)[0].squeeze(), dim=-1 ) ) spec_sep2 = ComplexTensor( *torch.unbind( stft(torch.as_tensor(waves_wsj[1]), ilens)[0].squeeze(), dim=-1 ) ) samples = torch.linspace(0, len(mixwav) / sr, len(mixwav)) plt.figure(figsize=(24, 12)) plt.subplot(3, 2, 1) plt.title('Mixture Spectrogram') plot_spectrogram( plt, abs(spec_mix).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 2) plt.title('Mixture Wavform') plt.plot(samples, mixwav) plt.xlim(0, len(mixwav) / sr) plt.subplot(3, 2, 3) plt.title('Separated Spectrogram (spk1)') plot_spectrogram( plt, abs(spec_sep1).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 4) plt.title('Separated Wavform (spk1)') plt.plot(samples, waves_wsj[0].squeeze()) plt.xlim(0, len(mixwav) / sr) plt.subplot(3, 2, 5) plt.title('Separated Spectrogram (spk2)') plot_spectrogram( plt, abs(spec_sep2).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 6) plt.title('Separated Wavform (spk2)') plt.plot(samples, waves_wsj[1].squeeze()) plt.xlim(0, len(mixwav) / sr) plt.xlabel(\\\"Time (s)\\\") plt.show()\"]},\"2499\":{\"h\":\"Evaluate separated speech with pretrained ASR model\",\"t\":[\"The ground truths are:\",\"text_1: SOME CRITICS INCLUDING HIGH REAGAN ADMINISTRATION OFFICIALS ARE RAISING THE ALARM THAT THE FED'S POLICY IS TOO TIGHT AND COULD CAUSE A RECESSION NEXT YEAR\",\"text_2: THE UNITED STATES UNDERTOOK TO DEFEND WESTERN EUROPE AGAINST SOVIET ATTACK\",\"(This may take a while for the speech recognition.)\",\"%pip install -q https://github.com/kpu/kenlm/archive/master.zip # ASR needs kenlm\"]},\"2500\":{\"h\":\"Task4 (✅ Checkpoint 4 (1 point))\",\"t\":[\"Show inference of pre-trained ASR model on mixed and seperated speech.\",\"!gdown --id 1H7--jXTTwmwxzfO8LT5kjZyBjng-HxED -O asr_train_asr_transformer_raw_char_1gpu_valid.acc.ave.zip !unzip asr_train_asr_transformer_raw_char_1gpu_valid.acc.ave.zip -d /content/asr_model !ln -sf /content/asr_model/exp .\",\"Please submit the screenshot of ASR inference on Mix Speech and Separated Speech 1 and Separated Speech 2 files to Gradescope for Task 4.\",\"import espnet_model_zoo from espnet2.bin.asr_inference import Speech2Text # For models downloaded from GoogleDrive, you can use the following script: speech2text = Speech2Text( asr_train_config=\\\"/content/asr_model/exp/asr_train_asr_transformer_raw_char_1gpu/config.yaml\\\", asr_model_file=\\\"/content/asr_model/exp/asr_train_asr_transformer_raw_char_1gpu/valid.acc.ave_10best.pth\\\", device=\\\"cuda:0\\\" ) text_est = [None, None] text_est[0], *_ = speech2text(waves_wsj[0].squeeze())[0] text_est[1], *_ = speech2text(waves_wsj[1].squeeze())[0] text_m, *_ = speech2text(mixwav)[0] print(\\\"Mix Speech to Text: \\\", text_m) print(\\\"Separated Speech 1 to Text: \\\", text_est[0]) print(\\\"Separated Speech 2 to Text: \\\", text_est[1])\",\"import difflib from itertools import permutations import editdistance import numpy as np colors = dict( red=lambda text: f\\\"\\\\033[38;2;255;0;0m{text}\\\\033[0m\\\" if text else \\\"\\\", green=lambda text: f\\\"\\\\033[38;2;0;255;0m{text}\\\\033[0m\\\" if text else \\\"\\\", yellow=lambda text: f\\\"\\\\033[38;2;225;225;0m{text}\\\\033[0m\\\" if text else \\\"\\\", white=lambda text: f\\\"\\\\033[38;2;255;255;255m{text}\\\\033[0m\\\" if text else \\\"\\\", black=lambda text: f\\\"\\\\033[38;2;0;0;0m{text}\\\\033[0m\\\" if text else \\\"\\\", ) def diff_strings(ref, est): \\\"\\\"\\\"Reference: https://stackoverflow.com/a/64404008/7384873\\\"\\\"\\\" ref_str, est_str, err_str = [], [], [] matcher = difflib.SequenceMatcher(None, ref, est) for opcode, a0, a1, b0, b1 in matcher.get_opcodes(): if opcode == \\\"equal\\\": txt = ref[a0:a1] ref_str.append(txt) est_str.append(txt) err_str.append(\\\" \\\" * (a1 - a0)) elif opcode == \\\"insert\\\": ref_str.append(\\\"*\\\" * (b1 - b0)) est_str.append(colors[\\\"green\\\"](est[b0:b1])) err_str.append(colors[\\\"black\\\"](\\\"I\\\" * (b1 - b0))) elif opcode == \\\"delete\\\": ref_str.append(ref[a0:a1]) est_str.append(colors[\\\"red\\\"](\\\"*\\\" * (a1 - a0))) err_str.append(colors[\\\"black\\\"](\\\"D\\\" * (a1 - a0))) elif opcode == \\\"replace\\\": diff = a1 - a0 - b1 + b0 if diff >= 0: txt_ref = ref[a0:a1] txt_est = colors[\\\"yellow\\\"](est[b0:b1]) + colors[\\\"red\\\"](\\\"*\\\" * diff) txt_err = \\\"S\\\" * (b1 - b0) + \\\"D\\\" * diff elif diff < 0: txt_ref = ref[a0:a1] + \\\"*\\\" * -diff txt_est = colors[\\\"yellow\\\"](est[b0:b1]) + colors[\\\"green\\\"](\\\"*\\\" * -diff) txt_err = \\\"S\\\" * (b1 - b0) + \\\"I\\\" * -diff ref_str.append(txt_ref) est_str.append(txt_est) err_str.append(colors[\\\"black\\\"](txt_err)) return \\\"\\\".join(ref_str), \\\"\\\".join(est_str), \\\"\\\".join(err_str) text_ref = [ \\\"SOME CRITICS INCLUDING HIGH REAGAN ADMINISTRATION OFFICIALS ARE RAISING THE ALARM THAT THE FED'S POLICY IS TOO TIGHT AND COULD CAUSE A RECESSION NEXT YEAR\\\", \\\"THE UNITED STATES UNDERTOOK TO DEFEND WESTERN EUROPE AGAINST SOVIET ATTACK\\\", ] print(\\\"=====================\\\" , flush=True) perms = list(permutations(range(2))) string_edit = [ [ editdistance.eval(text_ref[m], text_est[n]) for m, n in enumerate(p) ] for p in perms ] dist = [sum(edist) for edist in string_edit] perm_idx = np.argmin(dist) perm = perms[perm_idx] for i, p in enumerate(perm): print(\\\"\\\\n--------------- Text %d ---------------\\\" % (i + 1), flush=True) ref, est, err = diff_strings(text_ref[i], text_est[p]) print(\\\"REF: \\\" + ref + \\\"\\\\n\\\" + \\\"HYP: \\\" + est + \\\"\\\\n\\\" + \\\"ERR: \\\" + err, flush=True) print(\\\"Edit Distance = {}\\\\n\\\".format(string_edit[perm_idx][i]), flush=True)\"]},\"2501\":{\"h\":\"Task5 (✅ Checkpoint 5 (1 point))\",\"t\":[\"Enhance your own pre-recordings. Your input speech can be recorded by yourself or you can also find it from other sources (e.g., youtube).\",\"Discuss whether input speech was clearly denoised, and if not, what would be a potential reason.\",\"[YOUR ANSWER HERE]\",\"Please submit the spectrogram and waveform of your input and enhanced speech to GradeScope for Task 5 along with the screenshot of your answer.\",\"from google.colab import files from IPython.display import display, Audio import soundfile fs = 16000 uploaded = files.upload() for file_name in uploaded.keys(): speech, rate = soundfile.read(file_name) assert rate == fs, \\\"mismatch in sampling rate\\\" wave = enh_model_sc(speech[None, ...], fs) print(f\\\"Your input speech {file_name}\\\", flush=True) display(Audio(speech, rate=fs)) print(f\\\"Enhanced speech for {file_name}\\\", flush=True) display(Audio(wave[0].squeeze(), rate=fs))\"]},\"2502\":{\"h\":\"CMU 11492/11692 Spring 2023: Text to Speech\",\"t\":[\"Open In Colab\",\"In this demonstration, we will show you some demonstrations of text to speech systems in ESPnet.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"Author:\",\"Siddhant Arora (siddhana@andrew.cmu.edu)\",\"The notebook is adapted from this Colab\"]},\"2503\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.\"]},\"2504\":{\"h\":\"Installation\",\"t\":[\"# NOTE: pip shows imcompatible errors due to preinstalled libraries but you do not need to care !pip install typeguard==2.13.3 !git clone --depth 5 -b spoken_dialog_demo https://github.com/siddhu001/espnet.git !cd espnet && pip install .\",\"!pip install parallel_wavegan==0.5.4\",\"!pip install pyopenjtalk==0.2 !pip install pypinyin==0.44.0 !pip install parallel_wavegan==0.5.4 !pip install gdown==4.4.0\",\"!pip install espnet_model_zoo\"]},\"2505\":{\"h\":\"Single speaker TTS model demo\"},\"2506\":{\"h\":\"TTS Model\",\"t\":[\"You can try end-to-end text2wav model & combination of text2mel and vocoder. If you use text2wav model, you do not need to use vocoder (automatically disabled).\",\"Text2wav models:\",\"VITS\",\"Text2mel models:\",\"Tacotron2\",\"Transformer-TTS\",\"(Conformer) FastSpeech\",\"(Conformer) FastSpeech2\",\"Vocoders:\",\"Griffin Lim\",\"Parallel WaveGAN\",\"Multi-band MelGAN\",\"HiFiGAN\",\"Style MelGAN.\",\"In this demo, we will only experiment with the English TTS model, but ESPnet-TTS supports multiple languages like Japanese and Mandarin.\",\"The terms of use follow that of each corpus. ESPnet-TTS use the following corpora:\",\"ljspeech_*: LJSpeech dataset \",\"https://keithito.com/LJ-Speech-Dataset/\",\"jsut_*: JSUT corpus \",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"jvs_*: JVS corpus + JSUT corpus \",\"https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus\",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"tsukuyomi_*: つくよみちゃんコーパス + JSUT corpus \",\"https://tyc.rei-yumesaki.net/material/corpus/\",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"csmsc_*: Chinese Standard Mandarin Speech Corpus \",\"https://www.data-baker.com/open_source.html\",\"#@title Download English model { run: \\\"auto\\\" } lang = 'English' tag = \\\"kan-bayashi/ljspeech_vits\\\" #@param [\\\"kan-bayashi/ljspeech_tacotron2\\\", \\\"kan-bayashi/ljspeech_fastspeech\\\", \\\"kan-bayashi/ljspeech_vits\\\"] vocoder_tag = \\\"none\\\" #@param [\\\"none\\\", \\\"parallel_wavegan/ljspeech_parallel_wavegan.v1\\\"]\",\"!gdown --id \\\"1PjT9FX13d7Mv6loCs-wv5R_v3QrmLixf&confirm=t\\\" -O /content/tts_model.zip !unzip /content/tts_model.zip -d /content/tts_model\"]},\"2507\":{\"h\":\"Model Setup\",\"t\":[\"from espnet2.bin.tts_inference import Text2Speech from espnet2.utils.types import str_or_none text2speech = Text2Speech.from_pretrained( train_config=\\\"/content/tts_model/exp/tts_train_vits_raw_phn_tacotron_g2p_en_no_space/config.yaml\\\", model_file=\\\"/content/tts_model/exp/tts_train_vits_raw_phn_tacotron_g2p_en_no_space/train.total_count.ave_10best.pth\\\", vocoder_tag=str_or_none(vocoder_tag), device=\\\"cuda\\\", # Only for Tacotron 2 & Transformer threshold=0.5, # Only for Tacotron 2 minlenratio=0.0, maxlenratio=10.0, use_att_constraint=False, backward_window=1, forward_window=3, # Only for FastSpeech & FastSpeech2 & VITS speed_control_alpha=1.0, # Only for VITS noise_scale=0.333, noise_scale_dur=0.333, )\"]},\"2508\":{\"h\":\"Synthesis (✅ Checkpoint 1 (2 point))\",\"t\":[\"Run inference of pretrained single-speaker TTS model. Please experiment with running TTS model on different utterances. Provide some examples of failure cases and plot spectrogram and waveform of the utterances for both successful and failure cases. (1 point)\",\"Please also discuss possible explanation of these failure cases. (1 point)\",\"import time import torch # decide the input sentence by yourself print(f\\\"Input your favorite sentence in {lang}.\\\") x = input() # synthesis with torch.no_grad(): start = time.time() wav = text2speech(x)[\\\"wav\\\"] rtf = (time.time() - start) / (len(wav) / text2speech.fs) print(f\\\"RTF = {rtf:5f}\\\") # let us listen to generated samples from IPython.display import display, Audio display(Audio(wav.view(-1).cpu().numpy(), rate=text2speech.fs))\"]},\"2509\":{\"h\":\"TTS Model selection\"},\"2510\":{\"h\":\"Question2 (✅ Checkpoint 2 (1 point))\",\"t\":[\"Please experiment with running different TTS models like Tacotron or FastSpeech. Please also experiment both with Griffin Lim and Parallel WaveGAN vocoder. Please discuss which is better and why.\",\"#@title Download English model { run: \\\"auto\\\" } lang = 'English' tag = \\\"kan-bayashi/ljspeech_tacotron2\\\" #@param [\\\"kan-bayashi/ljspeech_tacotron2\\\", \\\"kan-bayashi/ljspeech_fastspeech\\\", \\\"kan-bayashi/ljspeech_vits\\\"] vocoder_tag = \\\"none\\\" #@param [\\\"none\\\", \\\"parallel_wavegan/ljspeech_parallel_wavegan.v1\\\"] # when vocoder_tag is none, Griffin Lim algorithm is used\",\"!gdown --id \\\"1PXsSaulipN31HnQ8YWwsi9Ndb3B2My-J&confirm=t\\\" -O /content/tts_tacotron_model.zip !unzip /content/tts_tacotron_model.zip -d /content/tts_tacotron_model #For fastspeech model run the commented lines below #!gdown --id \\\"13Jek_NbI8Qai42v4GKYxx3-jXOun5m2-&confirm=t\\\" -O /content/tts_fastspeech_model.zip #!unzip /content/tts_fastspeech_model.zip -d /content/tts_fastspeech_model\",\"from espnet2.bin.tts_inference import Text2Speech from espnet2.utils.types import str_or_none !ln -sf /content/tts_tacotron_model/exp . text2speech = Text2Speech.from_pretrained( # model_tag=str_or_none(tag), train_config=\\\"/content/tts_tacotron_model/exp/tts_train_tacotron2_raw_phn_tacotron_g2p_en_no_space/config.yaml\\\", model_file=\\\"/content/tts_tacotron_model/exp/tts_train_tacotron2_raw_phn_tacotron_g2p_en_no_space/199epoch.pth\\\", vocoder_tag=str_or_none(vocoder_tag), device=\\\"cuda\\\", # Only for Tacotron 2 & Transformer threshold=0.5, # Only for Tacotron 2 minlenratio=0.0, maxlenratio=10.0, use_att_constraint=False, backward_window=1, forward_window=3, # Only for FastSpeech & FastSpeech2 & VITS speed_control_alpha=1.0, # Only for VITS noise_scale=0.333, noise_scale_dur=0.333, ) # For fastspeech model run the commented lines below # from espnet2.bin.tts_inference import Text2Speech # from espnet2.utils.types import str_or_none # !ln -sf /content/tts_fastspeech_model/exp . # text2speech = Text2Speech.from_pretrained( # # model_tag=str_or_none(tag), # train_config=\\\"/content/tts_fastspeech_model/exp/tts_train_fastspeech_raw_phn_tacotron_g2p_en_no_space/config.yaml\\\", # model_file=\\\"/content/tts_fastspeech_model/exp/tts_train_fastspeech_raw_phn_tacotron_g2p_en_no_space/1000epoch.pth\\\", # vocoder_tag=str_or_none(vocoder_tag), # device=\\\"cuda\\\", # # Only for Tacotron 2 & Transformer # threshold=0.5, # # Only for Tacotron 2 # minlenratio=0.0, # maxlenratio=10.0, # use_att_constraint=False, # backward_window=1, # forward_window=3, # # Only for FastSpeech & FastSpeech2 & VITS # speed_control_alpha=1.0, # # Only for VITS # noise_scale=0.333, # noise_scale_dur=0.333, # )\",\"import time import torch # decide the input sentence by yourself print(f\\\"Input your favorite sentence in {lang}.\\\") x = input() # synthesis with torch.no_grad(): start = time.time() wav = text2speech(x)[\\\"wav\\\"] rtf = (time.time() - start) / (len(wav) / text2speech.fs) print(f\\\"RTF = {rtf:5f}\\\") # let us listen to generated samples from IPython.display import display, Audio display(Audio(wav.view(-1).cpu().numpy(), rate=text2speech.fs))\"]},\"2511\":{\"h\":\"Multi-speaker Model Demo\"},\"2512\":{\"h\":\"Model Selection\",\"t\":[\"Now we provide only English multi-speaker pretrained model.\",\"The terms of use follow that of each corpus. We use the following corpora:\",\"libritts_*: LibriTTS corpus \",\"http://www.openslr.org/60\",\"vctk_*: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit \",\"http://www.udialogue.org/download/cstr-vctk-corpus.html\",\"#@title English multi-speaker pretrained model { run: \\\"auto\\\" } lang = 'English' tag = 'kan-bayashi/vctk_full_band_multi_spk_vits' #@param [\\\"kan-bayashi/vctk_gst_tacotron2\\\", \\\"kan-bayashi/vctk_gst_transformer\\\", \\\"kan-bayashi/vctk_xvector_tacotron2\\\", \\\"kan-bayashi/vctk_xvector_transformer\\\", \\\"kan-bayashi/vctk_xvector_conformer_fastspeech2\\\", \\\"kan-bayashi/vctk_gst+xvector_tacotron2\\\", \\\"kan-bayashi/vctk_gst+xvector_transformer\\\", \\\"kan-bayashi/vctk_gst+xvector_conformer_fastspeech2\\\", \\\"kan-bayashi/vctk_multi_spk_vits\\\", \\\"kan-bayashi/vctk_full_band_multi_spk_vits\\\", \\\"kan-bayashi/libritts_xvector_transformer\\\", \\\"kan-bayashi/libritts_xvector_conformer_fastspeech2\\\", \\\"kan-bayashi/libritts_gst+xvector_transformer\\\", \\\"kan-bayashi/libritts_gst+xvector_conformer_fastspeech2\\\", \\\"kan-bayashi/libritts_xvector_vits\\\"] {type:\\\"string\\\"} vocoder_tag = \\\"none\\\" #@param [\\\"none\\\", \\\"parallel_wavegan/vctk_parallel_wavegan.v1.long\\\", \\\"parallel_wavegan/vctk_multi_band_melgan.v2\\\", \\\"parallel_wavegan/vctk_style_melgan.v1\\\", \\\"parallel_wavegan/vctk_hifigan.v1\\\", \\\"parallel_wavegan/libritts_parallel_wavegan.v1.long\\\", \\\"parallel_wavegan/libritts_multi_band_melgan.v2\\\", \\\"parallel_wavegan/libritts_hifigan.v1\\\", \\\"parallel_wavegan/libritts_style_melgan.v1\\\"] {type:\\\"string\\\"}\",\"!gdown --id \\\"1fzyyjLvrT_jldw4lfOD1P8FK2MGoIZO_&confirm=t\\\" -O /content/tts_multi-speaker_model.zip !unzip /content/tts_multi-speaker_model.zip -d /content/tts_multi-speaker_model\"]},\"2513\":{\"h\":\"Model Setup\",\"t\":[\"from espnet2.bin.tts_inference import Text2Speech from espnet2.utils.types import str_or_none text2speech = Text2Speech.from_pretrained( train_config=\\\"/content/tts_multi-speaker_model/exp/tts_train_full_band_multi_spk_vits_raw_phn_tacotron_g2p_en_no_space/config.yaml\\\", model_file=\\\"/content/tts_multi-speaker_model/exp/tts_train_full_band_multi_spk_vits_raw_phn_tacotron_g2p_en_no_space/train.total_count.ave_10best.pth\\\", vocoder_tag=str_or_none(vocoder_tag), device=\\\"cuda\\\", # Only for Tacotron 2 & Transformer threshold=0.5, # Only for Tacotron 2 minlenratio=0.0, maxlenratio=10.0, use_att_constraint=False, backward_window=1, forward_window=3, # Only for FastSpeech & FastSpeech2 & VITS speed_control_alpha=1.0, # Only for VITS noise_scale=0.333, noise_scale_dur=0.333, )\"]},\"2514\":{\"h\":\"Speaker selection\",\"t\":[\"For multi-speaker model, we need to provide X-vector and/or the reference speech to decide the speaker characteristics. For X-vector, you can select the speaker from the dumped x-vectors. For the reference speech, you can use any speech but please make sure the sampling rate is matched.\",\"import glob import os import numpy as np import kaldiio # Get model directory path from espnet_model_zoo.downloader import ModelDownloader d = ModelDownloader() # model_dir = os.path.dirname(d.download_and_unpack(tag)[\\\"train_config\\\"]) # X-vector selection spembs = None if text2speech.use_spembs: xvector_ark = [p for p in glob.glob(f\\\"/content/tts_multi-speaker_model/dump/**/spk_xvector.ark\\\", recursive=True) if \\\"tr\\\" in p][0] xvectors = {k: v for k, v in kaldiio.load_ark(xvector_ark)} spks = list(xvectors.keys()) # randomly select speaker random_spk_idx = np.random.randint(0, len(spks)) spk = spks[random_spk_idx] spembs = xvectors[spk] print(f\\\"selected spk: {spk}\\\") # Speaker ID selection sids = None if text2speech.use_sids: spk2sid = glob.glob(f\\\"/content/tts_multi-speaker_model/dump/**/spk2sid\\\", recursive=True)[0] with open(spk2sid) as f: lines = [line.strip() for line in f.readlines()] sid2spk = {int(line.split()[1]): line.split()[0] for line in lines} # randomly select speaker sids = np.array(np.random.randint(1, len(sid2spk))) spk = sid2spk[int(sids)] print(f\\\"selected spk: {spk}\\\") # Reference speech selection for GST speech = None if text2speech.use_speech: # you can change here to load your own reference speech # e.g. # import soundfile as sf # speech, fs = sf.read(\\\"/path/to/reference.wav\\\") # speech = torch.from_numpy(speech).float() speech = torch.randn(50000,) * 0.01\"]},\"2515\":{\"h\":\"Synthesis(✅ Checkpoint3 (2 point))\",\"t\":[\"Run inference of pretrained multi-speaker TTS model on more than one speaker id. Plot spectrogram and waveform of the synthesized speech for these speaker ids.\",\"import time import torch # decide the input sentence by yourself print(f\\\"Input your favorite sentence in {lang}.\\\") x = input() # synthesis with torch.no_grad(): start = time.time() wav = text2speech(x, speech=speech, spembs=spembs, sids=sids)[\\\"wav\\\"] rtf = (time.time() - start) / (len(wav) / text2speech.fs) print(f\\\"RTF = {rtf:5f}\\\") # let us listen to generated samples from IPython.display import display, Audio display(Audio(wav.view(-1).cpu().numpy(), rate=text2speech.fs))\"]},\"2516\":{\"h\":\"ESPnet-S2ST realtime demonstration\",\"t\":[\"This notebook provides a demonstration of the realtime end-to-end speech translation using ESPnet-ST-v2.\",\"Authors: Jiatong Shi (@ftshijt)\"]},\"2517\":{\"h\":\"Setup Envrionments\",\"t\":[\"!pip install --upgrade --no-cache-dir gdown !git clone --depth 5 -b merge_s2st_st https://github.com/ftshijt/espnet.git !cd espnet && pip install . !git clone --depth 1 https://github.com/kan-bayashi/ParallelWaveGAN.git !cd ParallelWaveGAN && pip install . !pip install -q espnet_model_zoo !pip install pysndfile !pip install sacrebleu !pip install mosestokenizer !git clone https://github.com/facebookresearch/SimulEval.git !cd SimulEval && pip install -e . !pip install typeguard==2.13.3\"]},\"2518\":{\"h\":\"Offline Speech-to-speech translation (S2ST)\",\"t\":[\"In this demonstration, we show an example model that is trained with discrete-unit-based speech-to-speech translation model. Specifically, the model is trained on Spanish-to-English subset of the CVSS-C corpus. The source speech/transcription of CVSS is from commonvoice (read speech); the target transcription is from CoVOST2 (a speech-to-text translation corpus); and the final speech is synthesized by a single-speaker TTS.\"]},\"2519\":{\"h\":\"Model download\",\"t\":[\"# Download pretrained s2st model !gdown 1wNaEebJDDcgi8RZhniKKEMFV15ktcNRE !unzip -o /content/s2st_train_s2st_discrete_unit_raw_fbank_es_en_train.loss.best.zip # Download pretrained unit-based vocoder !gdown 1ezVM3YujTVZSytOeWtD0MsXdqw19AaXa !unzip -o /content/hubert6_500_unit_vocoder.zip\"]},\"2520\":{\"h\":\"Model Setup\",\"t\":[\"import time import torch import string from espnet2.bin.s2st_inference import Speech2Speech # temporary for a buggy checkpoint !cp /content/exp/s2st_stats_raw_es_en/train/src_feats_stats.npz /content/exp/s2st_stats_raw_es_en/train/tgt_feats_stats.npz lang = \\\"es\\\" fs = 16000 speech2speech = Speech2Speech( model_file=\\\"/content/exp/s2st_train_s2st_discrete_unit_raw_fbank_es_en/362epoch.pth\\\", train_config=\\\"/content/exp/s2st_train_s2st_discrete_unit_raw_fbank_es_en/config.yaml\\\", minlenratio=0.0, maxlenratio=4, beam_size=3, vocoder_file=\\\"/content/unit_pretrained_vocoder/checkpoint-50000steps.pkl\\\", device=\\\"cuda\\\", ) def text_normalizer(text): text = text.upper() return text.translate(str.maketrans('', '', string.punctuation))\",\"# Load ASR models for ASR-BLEU import time import torch import string from espnet_model_zoo.downloader import ModelDownloader from espnet2.bin.asr_inference import Speech2Text as asr tag = \\\"asapp/e_branchformer_librispeech\\\" d = ModelDownloader() # It may takes a while to download and build models asr_model = asr( **d.download_and_unpack(tag), device=\\\"cuda\\\", minlenratio=0.0, maxlenratio=0.0, ctc_weight=0.3, beam_size=10, batch_size=0, nbest=1 )\"]},\"2521\":{\"h\":\"Translate our example recordings\",\"t\":[\"!git clone https://github.com/ftshijt/ESPnet_st_egs.git\",\"import torch import pandas as pd import soundfile as sf import librosa.display from IPython.display import display, Audio import matplotlib.pyplot as plt from sacrebleu.metrics import BLEU bleu = BLEU(effective_order=True) egs = pd.read_csv(\\\"ESPnet_st_egs/s2st/egs.csv\\\") for index, row in egs.iterrows(): if row[\\\"lang\\\"] == lang or lang == \\\"multilingual\\\": speech, rate = sf.read(\\\"ESPnet_st_egs/\\\" + row[\\\"path\\\"]) assert fs == int(row[\\\"sr\\\"]) tensor_speech = torch.tensor(speech, dtype=torch.double).unsqueeze(0).float() length = tensor_speech.new_full([1], dtype=torch.long, fill_value=tensor_speech.size(1)) output_dict = speech2speech(tensor_speech, length) output_wav = output_dict[\\\"wav\\\"].cpu().numpy() sf.write( \\\"ESPnet_st_egs/{row['path']}.predict.wav\\\", output_wav, fs, \\\"PCM_16\\\", ) print(f\\\"Input Speech: ESPnet_st_egs/{row['path']}\\\") # let us listen to samples display(Audio(speech, rate=fs)) librosa.display.waveshow(speech, sr=fs) plt.show() print(f\\\"Reference source text: {text_normalizer(row['src_text'])}\\\") print(f\\\"Reference target text: {text_normalizer(row['tgt_text'])}\\\") print(f\\\"Output speech: ESPnet_st_egs/{row['path']}.predict.wav\\\") print(f\\\"Unit: {output_dict}\\\") display(Audio(output_wav, rate=fs)) librosa.display.waveshow(output_wav, sr=fs) plt.show() # ASR recognition to the output samples text, *_ = asr_model(output_wav)[0] print(text) print(f\\\"ASR hypothesis: {text_normalizer(text)}\\\") print(f\\\"ASR BLEU: {bleu.sentence_score(text_normalizer(text), [text_normalizer(row['tgt_text'])])}\\\") print(\\\"*\\\" * 50)\"]},\"2522\":{\"h\":\"Translate your own pre-recordings\",\"t\":[\"Upload your own pre-recorded recordings\",\"Translate your voice with the S2ST system\",\"from google.colab import files from IPython.display import display, Audio import soundfile import librosa.display import matplotlib.pyplot as plt uploaded = files.upload() for file_name in uploaded.keys(): speech, rate = soundfile.read(file_name) assert rate == fs, \\\"mismatch in sampling rate\\\" tensor_speech = torch.tensor(speech, dtype=torch.double).unsqueeze(0).float() length = tensor_speech.new_full([1], dtype=torch.long, fill_value=tensor_speech.size(1)) output_dict = speech2speech(tensor_speech, length) output_wav = output_dict[\\\"wav\\\"].cpu().numpy() print(f\\\"Input Speech: {file_name}\\\") display(Audio(speech, rate=rate)) librosa.display.waveshow(speech, sr=rate) plt.show() print(\\\"*\\\" * 50) print(f\\\"Output speech: predict.wav\\\") display(Audio(output_wav, rate=fs)) librosa.display.waveshow(output_wav, sr=fs) plt.show()\"]},\"2523\":{\"h\":\"Translate your own live-recordings\",\"t\":[\"Record your own voice (for people cannot speak Spanish, you may find some web sources and play that with your phone to simulate the real-time recording).\",\"Tralsate your vocie with the S2ST system\",\"# from https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be from IPython.display import Javascript from google.colab import output from base64 import b64decode RECORD = \\\"\\\"\\\" const sleep = time => new Promise(resolve => setTimeout(resolve, time)) const b2text = blob => new Promise(resolve => { const reader = new FileReader() reader.onloadend = e => resolve(e.srcElement.result) reader.readAsDataURL(blob) }) var record = time => new Promise(async resolve => { stream = await navigator.mediaDevices.getUserMedia({ audio: true }) recorder = new MediaRecorder(stream) chunks = [] recorder.ondataavailable = e => chunks.push(e.data) recorder.start() await sleep(time) recorder.onstop = async ()=>{ blob = new Blob(chunks) text = await b2text(blob) resolve(text) } recorder.stop() }) \\\"\\\"\\\" def record(sec, filename='audio.wav'): display(Javascript(RECORD)) s = output.eval_js('record(%d)' % (sec * 1000)) b = b64decode(s.split(',')[1]) with open(filename, 'wb+') as f: f.write(b) audio = 'audio.wav' second = 5 print(f\\\"Speak to your microphone {second} sec...\\\") record(second, audio) print(\\\"Done!\\\") import librosa import librosa.display speech, rate = librosa.load(audio, sr=16000) librosa.display.waveshow(speech, sr=rate) import matplotlib.pyplot as plt plt.show() import pysndfile pysndfile.sndio.write('audio_ds.wav', speech, rate=rate, format='wav', enc='pcm16') from IPython.display import display, Audio display(Audio(speech, rate=rate))\",\"tensor_speech = torch.tensor(speech, dtype=torch.double).unsqueeze(0).float() length = tensor_speech.new_full([1], dtype=torch.long, fill_value=tensor_speech.size(1)) output_dict = speech2speech(tensor_speech, length) output_wav = output_dict[\\\"wav\\\"].cpu().numpy() print(f\\\"Output speech: predict.wav\\\") display(Audio(output_wav, rate=fs)) librosa.display.waveshow(output_wav, sr=fs) plt.show()\"]},\"2524\":{\"h\":\"CMU 11751/18781 Fall 2022: ESPnet Tutorial2 (New task)\",\"t\":[\"ESPnet is a widely-used end-to-end speech processing toolkit. It has supported various speech processing tasks. ESPnet uses PyTorch as a main deep learning engine, and also follows Kaldi style data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"ESPnet tutorial in Speech Recognition and Understanding (Fall 2021)\",\"Recitation in Multilingual NLP (Spring 2022)\",\"ESPnet tutorial1 in Speech Recognition and Understanding (Fall 2022)\",\"Author: Jiatong Shi (jiatongs@andrew.cmu.edu)\",\"We would like to thank You (Neil) Zhang for kindly helping the hands-on tutorial and sharing his knowledge on the task.\"]},\"2525\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. There will also be some after-class excersices 📗 after the tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.\",\"This tutorial covers some advanced usage of ESPnet, which is the extension of the first tutorial.\"]},\"2526\":{\"h\":\"Useful links\",\"t\":[\"Installation https://espnet.github.io/espnet/installation.html\",\"Usage https://espnet.github.io/espnet/espnet2_tutorial.html\",\"Reference of task class in ESPnet https://espnet.github.io/espnet/espnet2_task.html\"]},\"2527\":{\"h\":\"Objectives\",\"t\":[\"After this tutorial, you are expected to know:\",\"How to add new task in ESPnet2\",\"How to add new models in ESPnet2\",\"How to create a new recipe (and template) of a new task from scratch\"]},\"2528\":{\"h\":\"Function to print date and time\",\"t\":[\"We first define a function to print the current date and time, which will be used in multiple places below.\",\"def print_date_and_time(): from datetime import datetime import pytz now = datetime.now(pytz.timezone(\\\"America/New_York\\\")) print(\\\"=\\\" * 60) print(f' Current date and time: {now.strftime(\\\"%m/%d/%Y %H:%M:%S\\\")}') print(\\\"=\\\" * 60) # example output print_date_and_time()\"]},\"2529\":{\"h\":\"Download ESPnet\",\"t\":[\"We use git clone to download the source code of ESPnet and then go to a specific commit.\",\"Important: In other versions of ESPnet, you may encounter errors related to imcompatible package versions (numba). Please use the same commit to avoid such issues.\",\"Note that we are using another branch espnet_tutorial_asvspoof instead of \\\"master\\\". You can also use your own fork to proceed the following sections if you want to use Github to save your code.\",\"# It takes a few seconds !git clone --depth 5 -b 2022fall_new_task_tutorial https://github.com/espnet/espnet # We use a specific commit just for reproducibility. %cd /content/espnet !git checkout 9cff98a78ceaa4d85843be0a50b369ec826b27f6\"]},\"2530\":{\"h\":\"Setup Python environment based on anaconda + Install ESPnet\",\"t\":[\"# It takes 30 seconds %cd /content/espnet/tools !./setup_anaconda.sh anaconda espnet 3.9 # It may take 12 minutes %cd /content/espnet/tools !make TH_VERSION=1.12.1 CUDA_VERSION=11.6\",\"We have provide you most of the files needed for ASVSpoof recipe. So you do not need to add any additional files. However, noted that some of the files are not complete and need your completion to proceed. For a quick overview of the whole layout of the new task, please refer to https://github.com/espnet/espnet/compare/master...2022fall_new_task_tutorial\",\"As elaborated in the warming-up, we have shown that there are two core components for a new task in ESPnet: a task library and correponding recipe setups. For the following of the section, we will briefly show the overall layout of adding the ASVSpoof task in ESPnet. The listed files are almost the minimum requirements to add a new task in ESPnet.\",\"Task library for ASVSpoof\",\"Followings are a list of files adding to ESPnet for ASVSpoof (files in \\\"\\\" are ones that need modifications)\",\"- espnet2 - bin - asvspoof_train.py # Major entry point for asvspoof - \\\"asvspoof_inference.py\\\" (Checkpoint 4) # Inference scripts for asvspoof - asvspoof - decoder - __init__.py - abs_decoder.py # abstract class for decoder in ASVSpoof - \\\"linear_decoder.py\\\" (Checkpoint 3) # simple linear decoder for ASVSpoof - loss - __init__.py - abs_loss.py # abstract class for loss in ASVSpoof - binary_loss.py # naive binary class loss for ASVSpoof - am_softmax.py - \\\"oc_softmax.py\\\" (Bouns) - __init__.py - \\\"espnet_model.py\\\" (Bouns) - tasks - \\\"asvspoof.py\\\" (Checkpoint 2)\",\"To help you understand more, we would recommend you to check the layout of other tasks (e.g., ASR, TTS, ST, etc.) to understand how the codebase is functioning.\",\"Recipe for ASVSpoof\",\"Followings are a list of files adding to ESPnet for ASVSpoof (files in boldface are ones that need modifications)\",\"- egs2 - TEMPLATE - asvspoof1 - \\\"asvspoof.sh\\\" (Checkpoint 1) - others - espnet_tutorial - asvspoof11 - conf - \\\"asvspoof.sh” (Checkpoint 1) - local - \\\"data_prep.py\\\" (Bouns) - \\\"data.sh\\\" (Bouns) - \\\"run.sh\\\" (Checkpoint 5) - scripts - pyscripts - utils - steps - path.sh - db.sh - cmd.sh\",\"Noted that because of the symlink, the asvspoof.sh is essentially the same for checkpoint 1.\"]},\"2531\":{\"h\":\"ASVSpoof data preparation\",\"t\":[\"As discussed in the warm-up session, ASVSpoof aims to conduct a binary classfication. As the task layout is a bit different from the ASR task we touched on the first tutorial, so we need to use a different format to formulate the data. For here, to keep the simplicity, we stil use the exact same file as the first tutorial:\",\"wav.scp text utt2spk spk2utt\",\"But on the other hand, we change the format of text into\",\"utt_id1 0 utt_id2 1 utt_id3 0\",\"where 0 represents real speech and 1 stands for fake speech.\"]},\"2532\":{\"h\":\"Download dataset\",\"t\":[\"We first download the data from google drive. Noted that the data is a subset of the ASVSpoof2019 Challenge.\",\"# a few seconds %cd /content/espnet/egs2/espnet_tutorial/asvspoof1/ !gdown 1HRdjjmGXBTXOqOq9iijuXPCA4y_46OzP !unzip espnet_tutorial_asvspoof.zip\"]},\"2533\":{\"h\":\"Prepare data (Stage1 & Stage2)\",\"t\":[\"This time, we make the task template to be as simple as possible. The data preparation will be only two stages, including basic data preparation and wave format.\",\"# It may take around 6 minutes !./asvspoof.sh --stage 1 --stop_stage 2 --train_set train --valid_set dev --test_sets \\\"eval\\\"\"]},\"2534\":{\"h\":\"ASVSpoof collect stats (✅ Checkpint 1 (1 point))\",\"t\":[\"Similar to the previous tutorial, we collect the statisitcs for the data.\",\"In the process, the data will be passed into a iterable loader. However, remember that the text file is no longer the format as the ASR recipe. Therefore, we will need to use another data loader to load the corresponding information.\",\"Fortunately, we have a wide range of data loaders for choices, which is listing in here. Please choose the correct file format and replace the [REPLACE_ME] token in asvspoof.sh\",\"After the replacement, you should be able to run the following blocks\",\"# It takes less than 2 minutes !./asvspoof.sh --stage 3 --stop_stage 3 --train_set train --valid_set dev --test_sets \\\"dev eval\\\" --asvspoof_config conf/checkpoint1_dummy.yaml # NOTE: Checkpoint 1 print_date_and_time()\"]},\"2535\":{\"h\":\"ASVSpoof Model\",\"t\":[\"In this section, we will define the ASVSpoof model and use the model to conduct the training of ASVSpoof task. For easier understanding, we first use an encoder to convert speech features into hidden representations and then use a decoder to conduct the classification.\"]},\"2536\":{\"h\":\"Encoder (✅ Checkpint 2 (1 point))\",\"t\":[\"First, we are going to focus on the encoder part. There has been a long history over the discussion of the speech encoder in our community. Given the sequential perspective, people firstly investigated recurrent neural networks. More recently, we are focusing on conformer block, which is an extension to the transformer block. In the previous settings, we used a transformer block to collect stats. However, we would want to switch to conformer.\",\"Code-reusibility is one of the major benefits of using ESPnet as a toolkit for speech tasks. As ESPnet already support conformer block in ASR, it is easy to import into this new task.\",\"In ESPnet, adding modules that we already have can be as simple as two-line codes. Please add lines into /content/espnet/espnet2/tasks/asvspoof.py. We have marked TODO in the scripts for your convenience.\",\"# It takes less than 2 minutes !./asvspoof.sh --stage 3 --stop_stage 3 --train_set train --valid_set dev --test_sets \\\"dev eval\\\" --asvspoof_config conf/checkpoint2.yaml # NOTE: Checkpoint 2 print_date_and_time()\"]},\"2537\":{\"h\":\"Decoder (✅ Checkpint 3 (1 point))\",\"t\":[\"In this stage, we will finally start the training. As the previous tutorial, we can use the Tensorboard to monitor the process.\",\"# Load the TensorBoard notebook extension %reload_ext tensorboard # Launch tensorboard before training %tensorboard --logdir /content/espnet/egs2/espnet_tutorial/asvspoof1/exp\",\"After we finished the encoder, we also need to create a decoder to conduct the prediciton. As the encoder will generate hidden representations, we want to have a simple decoder to conduct mean-pooling to all the hidden representation at the time-axis. There should be another linear layer to conclude the models into binary classification. Please fill the missing part in /conent/espnet/espnet2/asvspoof/decoder/linear_decoder.py to finally start the training. For people who are not familiar with Pytorch, please refer the related resources for details.\",\"Related resources that could be helpful for this checkpoint:\",\"https://pytorch.org/docs/stable/generated/torch.mean.html\",\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\",\"https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\",\"!nvidia-smi # Training takes around 2 minutes !./asvspoof.sh --stage 4 --stop_stage 4 --train_set train --valid_set dev --test_sets \\\"dev eval\\\" --asvspoof_config conf/checkpoint2.yaml --inference_config conf/decode_asvspoof.yaml # NOTE: Checkpoint 3 print_date_and_time()\"]},\"2538\":{\"h\":\"Model Inference\"},\"2539\":{\"h\":\"(✅ Checkpint 4 (1 point))\",\"t\":[\"As the training is finished, we expect to conduct ASVSpoof on the test set. To approach that, we first have to finish the inference codebase. For our task specifically, we need the log-probability of the prediction to compute equal error rate (EER). Therefore the output should be a float number for each utterance.\",\"Please fill the missing parts with TODOs in /content/espnet/espnet2/bin/asvspoof_inference.py\",\"!./asvspoof.sh --stage 5 --stop_stage 5 --train_set train --valid_set dev --test_sets \\\"eval\\\" --asvspoof_config conf/checkpoint2.yaml --inference_nj 1 --gpu_inference true # NOTE: Checkpoint 4 print_date_and_time()\"]},\"2540\":{\"h\":\"Scoring\"},\"2541\":{\"h\":\"(✅ Checkpint 5 (1 point))\",\"t\":[\"We have prepred the scoring script for you. We can get the EER by the following code-block\",\"!./asvspoof.sh --stage 6 --stop_stage 6 --train_set train --valid_set dev --test_sets \\\"eval\\\" --asvspoof_config conf/checkpoint2.yaml !chmod +x scripts/utils/show_asvspoof_result.sh # NOTE: Checkpoint 5 print_date_and_time()\"]},\"2542\":{\"h\":\"📗 Exercise 1 (1 point bonus)\",\"t\":[\"In the data you just downloaded, we have some extra data for training (/content/espnet/egs2/espnet_tutorial/asvspoof1/espnet_asvspoof_tutorial/extend_train). Please try to combine them with the training set and then conduct experiments the augmented set. You are also encouraged to change the model configuration. If you achieve a better equal error rate (EER) than the previous experiments, you can get a bonus point.\",\"# TODO # NOTE: Exercise 1 print_date_and_time()\"]},\"2543\":{\"h\":\"📗 Exercise 2 (1 point bonus)\",\"t\":[\"One main issue of speech anti-spoofing research is the generalization to unseen attacks, i.e., synthesis methods not seen in training the anti-spoofing models. In fact, the test set in our scenario is exact in the same case. Recently, there is a one-class learning method that compacts the natural speech representations and separate them from the fake speech with a certain margin in the embedding space.\",\"We have implemented the AM softmax method located in /content/espnet/espnet2/asvspoof/loss/am_softmax_loss.py and also prepared the template /content/espnet/espnet2/asvspoof/loss/oc_softmax_loss.py for your implementation. You can follow the TODOs to implement the methods (note that the inference/train_config should change accordingly).\",\"If you successfully implement the OC-softmax and get similar/better EER, you can get a bouns point\",\"# TODO # NOTE: Exercise 2 print_date_and_time()\"]},\"2544\":{\"h\":\"CMU 11751/18781 Fall 2022: ESPnet Tutorial\",\"t\":[\"ESPnet is a widely-used end-to-end speech processing toolkit. It has supported various speech processing tasks. ESPnet uses PyTorch as a main deep learning engine, and also follows Kaldi style data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments.\",\"Main references:\",\"ESPnet repository\",\"ESPnet documentation\",\"ESPnet tutorial in Speech Recognition and Understanding (Fall 2021)\",\"Recitation in Multilingual NLP (Spring 2022)\",\"Author: Yifan Peng (yifanpen@andrew.cmu.edu)\"]},\"2545\":{\"h\":\"❗Important Notes❗\",\"t\":[\"We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU, you may fail to connect to a GPU backend for some time.\",\"There are multiple in-class checkpoints ✅ throughout this tutorial. There will also be some after-class excersices 📗 after the tutorial. Your participation points are based on these tasks. Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\",\"Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using File -> Print in the menu bar.\",\"This tutorial covers the basics of ESPnet, which will be the foundation of the next tutorial on Wednesday.\"]},\"2546\":{\"h\":\"Objectives\",\"t\":[\"After this tutorial, you are expected to know:\",\"How to run existing recipes (data prep, training, inference and scoring) in ESPnet2\",\"How to change the training and decoding configurations\",\"How to create a new recipe from scratch\",\"Where to find resources if you encounter an issue\"]},\"2547\":{\"h\":\"Useful links\",\"t\":[\"Installation https://espnet.github.io/espnet/installation.html\",\"Usage https://espnet.github.io/espnet/espnet2_tutorial.html\",\"This is a full installation method to perform data preprocessing, training, inference, scoring, and so on.\",\"We prepare various ways of installation. Please read https://espnet.github.io/espnet/installation.html#step-2-installation-espnet for more details.\"]},\"2548\":{\"h\":\"Function to print date and time\",\"t\":[\"We first define a function to print the current date and time, which will be used in multiple places below.\",\"def print_date_and_time(): from datetime import datetime import pytz now = datetime.now(pytz.timezone(\\\"America/New_York\\\")) print(\\\"=\\\" * 60) print(f' Current date and time: {now.strftime(\\\"%m/%d/%Y %H:%M:%S\\\")}') print(\\\"=\\\" * 60) # example output print_date_and_time()\"]},\"2549\":{\"h\":\"Check GPU type\",\"t\":[\"Let's check the GPU type of this allocated environment.\",\"!nvidia-smi\"]},\"2550\":{\"h\":\"Download ESPnet\",\"t\":[\"We use git clone to download the source code of ESPnet and then go to a specific commit.\",\"Important: In other versions of ESPnet, you may encounter errors related to imcompatible package versions (numba). Please use the same commit to avoid such issues.\",\"# It takes a few seconds !git clone --depth 5 https://github.com/espnet/espnet # We use a specific commit just for reproducibility. %cd /content/espnet !git checkout 3a22d1584317ae59974aad62feab8719c003ae05\"]},\"2551\":{\"h\":\"Setup Python environment based on anaconda\",\"t\":[\"There are several other installation methods, but we highly recommend the anaconda-based one.\",\"# It takes 30 seconds %cd /content/espnet/tools !./setup_anaconda.sh anaconda espnet 3.9\"]},\"2552\":{\"h\":\"Install ESPnet\",\"t\":[\"This step installs PyTorch and other required tools.\",\"We specify CUDA_VERSION=11.6 for PyTorch 1.12.1. We also support many other versions. Please check https://github.com/espnet/espnet/blob/master/tools/installers/install_torch.sh for the detailed version list.\",\"# It may take 12 minutes %cd /content/espnet/tools !make TH_VERSION=1.12.1 CUDA_VERSION=11.6\",\"If other listed packages are necessary, install any of them using\",\". ./activation_python.sh && ./installers/install_xxx.sh\",\"We show two examples, although they are not used in this demo.\",\"# s3prl and fairseq are necessary if you want to use self-supervised pre-trained models # It takes 50s %cd /content/espnet/tools !. ./activate_python.sh && ./installers/install_s3prl.sh # install s3prl to use SSLRs !. ./activate_python.sh && ./installers/install_fairseq.sh # install s3prl to use Wav2Vec2 / HuBERT model series\"]},\"2553\":{\"h\":\"Check installation\",\"t\":[\"Now let's make sure torch, torch cuda, and espnet are successfully installed.\",\"... [x] torch=1.12.1 [x] torch cuda=11.6 [x] torch cudnn=8302 ... [x] espnet=202207 ...\"]},\"2554\":{\"h\":\"✅ Checkpoint 1 (1 point)\",\"t\":[\"Print the output of check_install.py.\",\"%cd /content/espnet/tools !. ./activate_python.sh && python3 check_install.py | head -n 40 # NOTE: Checkpoint 1 print_date_and_time()\",\"ESPnet has a number of recipes (130 recipes on Sep. 11, 2022). Please refer to https://github.com/espnet/espnet/blob/master/egs2/README.md for a complete list.\",\"Please also check the general usage of the recipe in https://espnet.github.io/espnet/espnet2_tutorial.html#recipes-using-espnet2\",\"CMU AN4 recipe\",\"In this tutorial, we will use the CMU an4 recipe. This is a small-scale speech recognition task mainly used for testing.\",\"First, let's go to the recipe directory.\",\"%cd /content/espnet/egs2/an4/asr1 !ls\",\"egs2/an4/asr1/ - conf/ # Configuration files for training, inference, etc. - scripts/ # Bash utilities of espnet2 - pyscripts/ # Python utilities of espnet2 - steps/ # From Kaldi utilities - utils/ # From Kaldi utilities - db.sh # The directory path of each corpora - path.sh # Setup script for environment variables - cmd.sh # Configuration for your backend of job scheduler - run.sh # Entry point - asr.sh # Invoked by run.sh\",\"ESPnet is designed for various use cases (local machines or cluster machines) based on Kaldi tools. If you use it in the cluster machines, please also check https://kaldi-asr.org/doc/queue.html\",\"The main stages can be parallelized by various jobs.\",\"!cat run.sh\",\"run.sh calls asr.sh, which completes the entire speech recognition experiments, including data preparation, training, inference, and scoring. They are separated into multiple stages (totally 16).\",\"Instead of executing the entire pipeline by run.sh, let's run it stage-by-stage to understand the process in each stage.\"]},\"2555\":{\"h\":\"Data preparation\",\"t\":[\"Stage 1: Data preparation: download raw data, split the entire set into train/dev/test, and prepare them in the Kaldi format\",\"Note that --stage &lt;N&gt; is to start from this stage and --stop_stage &lt;N&gt; is to stop after this stage. We also need to specify the train, dev and test sets.\",\"# a few seconds !./asr.sh --stage 1 --stop_stage 1 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"After this stage is finished, please check the newly created data directory:\",\"!ls data\",\"In this recipe, we use train_nodev as a training set, train_dev as a validation set (monitor the training progress by checking the validation score). We also use test and train_dev sets for the final speech recognition evaluation.\",\"Let's check one of the training data directories:\",\"!ls -1 data/train_nodev/\",\"These are the speech and corresponding text and speaker information in the Kaldi format. To understand their meanings, please check https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE#about-kaldi-style-data-directory.\",\"Please also check the official documentation of Kaldi: https://kaldi-asr.org/doc/data_prep.html\",\"spk2utt # Speaker information text # Transcription file utt2spk # Speaker information wav.scp # Audio file\",\"Stage 2: Speed perturbation (one of the data augmentation methods)\",\"We do not use speed perturbation for this demo. But you can turn it on by adding an argument --speed_perturb_factors \\\"0.9 1.0 1.1\\\" to the shell script.\",\"Note that we perform speed perturbation and save the augmented data in the disk before training. Another approach is to perform data augmentation during training, such as SpecAug.\",\"!./asr.sh --stage 2 --stop_stage 2 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"Stage 3: Format wav.scp: data/ -> dump/raw\",\"We dump the data with specified format (flac in this case) for the efficient use of the data.\",\"# ====== Recreating \\\"wav.scp\\\" ====== # Kaldi-wav.scp, which can describe the file path with unix-pipe, like \\\"cat /some/path |\\\", # shouldn't be used in training process. # \\\"format_wav_scp.sh\\\" dumps such pipe-style-wav to real audio file # and it can also change the audio-format and sampling rate. # If nothing is need, then format_wav_scp.sh does nothing: # i.e. the input file format and rate is same as the output.\",\"Note that --nj &lt;N&gt; means the number of CPU jobs. Please set it appropriately by considering your CPU resources and disk access.\",\"# 25 seconds !./asr.sh --stage 3 --stop_stage 3 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --nj 4\",\"Stage 4: Remove long/short data: dump/raw/org -> dump/raw\",\"Too long and too short audio data are harmful for efficient training. Those utterances are removed for training. But for inference and scoring, we still use the full data, which is important for fair comparison.\",\"!./asr.sh --stage 4 --stop_stage 4 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"Stage 5: Generate token_list from dump/raw/train_nodev/text using BPE.\",\"This is important for text processing. Here, we make a dictionary simply using the English characters. We use the sentencepiece toolkit developed by Google.\",\"!./asr.sh --stage 5 --stop_stage 5 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\"\",\"Let's check the content of the dictionary. There are several special symbols, e.g.,\",\"&lt;blank&gt; used for CTC &lt;unk&gt; unknown symbols do not appear in the training data &lt;sos/eos&gt; start and end sentence symbols\"]},\"2556\":{\"h\":\"✅ Checkpint 2 (1 point)\",\"t\":[\"Print the generated token list.\",\"!cat data/token_list/bpe_unigram30/tokens.txt # NOTE: Checkpoint 2 print_date_and_time()\"]},\"2557\":{\"h\":\"Language modeling (skipped in this tutorial)\",\"t\":[\"Stages 6--9: Stages related to language modeling.\",\"We skip the language modeling part in the recipe (stages 6 -- 9) in this tutorial.\"]},\"2558\":{\"h\":\"End-to-end ASR\",\"t\":[\"Before training, we need to set the training configs including the front-end, encoder and decoder, optimizer, scheduler, specaug, etc. These configs are usually specified in .yaml files: /content/espnet/egs2/an4/asr1/conf/train_asr_xxx.yaml, but you can also overwrite them in the command line.\",\"In this example, we will train a small Transformer model. ESPnet also supports other encoder types, such as RNN (LSTM/GRU), Conformer, and Branchformer.\",\"Please do the following:\",\"Create a new config file train_asr_demo_transformer.yaml in the directory /content/espnet/egs2/an4/asr1/conf/\",\"Copy the following lines to the new config\",\"batch_type: folded batch_size: 64 accum_grad: 1 # gradient accumulation steps max_epoch: 100 patience: none init: xavier_uniform best_model_criterion: # criterion to save best models - - valid - acc - max keep_nbest_models: 10 # save nbest models and average these checkpoints use_amp: true # whether to use automatic mixed precision num_att_plot: 0 # do not save attention plots to save time in the demo num_workers: 2 # number of workers in dataloader encoder: transformer encoder_conf: output_size: 256 attention_heads: 4 linear_units: 1024 num_blocks: 12 dropout_rate: 0.1 positional_dropout_rate: 0.1 attention_dropout_rate: 0.1 input_layer: conv2d normalize_before: true decoder: transformer decoder_conf: attention_heads: 4 linear_units: 1024 num_blocks: 3 dropout_rate: 0.1 positional_dropout_rate: 0.1 self_attention_dropout_rate: 0.1 src_attention_dropout_rate: 0.1 model_conf: ctc_weight: 0.3 # joint CTC/attention training lsm_weight: 0.1 # label smoothing weight length_normalized_loss: false optim: adam optim_conf: lr: 0.001 scheduler: warmuplr # linearly increase and exponentially decrease scheduler_conf: warmup_steps: 800\",\"Stage 10: ASR collect stats: train_set=dump/raw/train_nodev, valid_set=dump/raw/train_dev\",\"We estimate the mean and variance of the data to normalize the data.\",\"We also collect the information of input and output lengths for the efficient mini batch creation.\",\"ESPnet supports various methods to create batches, please refer to https://espnet.github.io/espnet/espnet2_training_option.html#change-mini-batch-type.\",\"# 15 seconds !./asr.sh --stage 10 --stop_stage 10 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --nj 4 --asr_config conf/train_asr_demo_transformer.yaml\",\"Stage 11: ASR Training: train_set=dump/raw/train_nodev, valid_set=dump/raw/train_dev\",\"This is the main training loop.\",\"During training, please monitor the following files\",\"log file /content/espnet/egs2/an4/asr1/exp/asr_train_asr_demo_transformer_raw_bpe30/train.log\",\"loss /content/espnet/egs2/an4/asr1/exp/asr_train_asr_demo_transformer_raw_bpe30/images/loss.png\",\"accuracy /content/espnet/egs2/an4/asr1/exp/asr_train_asr_demo_transformer_raw_bpe30/images/acc.png\",\"Good examples look like this:\",\"However, bad examples (with too large lr) are like:\",\"ESPnet supports tensorboard, you can monitor the training status using it.\",\"# Load the TensorBoard notebook extension %load_ext tensorboard # Launch tensorboard before training %tensorboard --logdir /content/espnet/egs2/an4/asr1/exp\",\"# It takes 12 minutes with a single T4 GPU. !./asr.sh --stage 11 --stop_stage 11 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --ngpu 1 --asr_config conf/train_asr_demo_transformer.yaml\",\"The training log contains all information of the current experiment. Please check it to understand the training status. If your training job fails, this file will show the error messages.\"]},\"2559\":{\"h\":\"✅ Checkpoint 3 (1 point)\",\"t\":[\"Print the training log.\",\"# NOTE: Checkpoint 3 !tail -20 exp/asr_train_asr_demo_transformer_raw_bpe30/train.log print_date_and_time()\",\"Stage 12: Decoding\",\"We need to add --use_lm false since we skip the language model.\",\"--asr_exp exp/asr_train_asr_demo_transformer_raw_bpe30 specifies the experiment directory name.\",\"--inference_nj &lt;N&gt; specifies the number of inference jobs.\",\"We can enable GPU decoding by setting --gpu_inference true. Otherwise CPU will be used for decoding.\",\"--inference_config conf/decode_asr.yaml specifies the decoding config file.\",\"Let's monitor the log /content/espnet/egs2/an4/asr1/exp/asr_train_asr_demo_transformer_raw_bpe30/decode_asr_asr_model_valid.acc.ave/train_dev/logdir/asr_inference.1.log\",\"# It would take 3 minutes with a single T4 GPU. !./asr.sh --use_lm false --gpu_inference true --inference_nj 1 --stage 12 --stop_stage 12 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --asr_exp exp/asr_train_asr_demo_transformer_raw_bpe30 --inference_config conf/decode_asr.yaml\",\"Stage 13: Scoring\",\"You can find word error rate (WER), character error rate (CER), etc. for each test set.\",\"!./asr.sh --stage 13 --stop_stage 13 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --use_lm false --asr_exp exp/asr_train_asr_demo_transformer_raw_bpe30 --inference_config conf/decode_asr.yaml\",\"You can also check the break down of the word error rate in /content/espnet/egs2/an4/asr1/exp/asr_train_asr_demo_transformer_raw_bpe30/decode_asr_asr_model_valid.acc.ave/train_dev/score_wer/result.txt\"]},\"2560\":{\"h\":\"✅ Checkpoint 4 (1 point)\",\"t\":[\"Print the scoring results.\",\"# NOTE: Checkpoint 4 !cat exp/asr_train_asr_demo_transformer_raw_bpe30/RESULTS.md print_date_and_time()\",\"❗Checkpoint Submission: If you have completed all the four checkpoints, please print your notebook and submit it to Gradescope.\"]},\"2561\":{\"h\":\"How to change the configs?\",\"t\":[\"Let's revisit the configs, since this is probably the most important part to improve the performance.\"]},\"2562\":{\"h\":\"Config file based\",\"t\":[\"All training options are changed in the config file.\",\"Pleae check https://espnet.github.io/espnet/espnet2_training_option.html\",\"Let's first check config files prepared in the an4 recipe\",\"LSTM-based E2E ASR /content/espnet/egs2/an4/asr1/conf/train_asr_rnn.yaml\",\"Transformer based E2E ASR /content/espnet/egs2/an4/asr1/conf/train_asr_transformer.yaml\",\"You can run\",\"RNN\",\"./asr.sh --stage 10 \\\\ --train_set train_nodev \\\\ --valid_set train_dev \\\\ --test_sets \\\"train_dev test\\\" \\\\ --nj 4 \\\\ --inference_nj 4 \\\\ --use_lm false \\\\ --asr_config conf/train_asr_rnn.yaml\",\"Transformer\",\"./asr.sh --stage 10 \\\\ --train_set train_nodev \\\\ --valid_set train_dev \\\\ --test_sets \\\"train_dev test\\\" \\\\ --nj 4 \\\\ --inference_nj 4 \\\\ --use_lm false \\\\ --asr_config conf/train_asr_transformer.yaml\",\"You can also find various configs in other recipes espnet/egs2/*/asr1/conf/, including\",\"Conformer egs2/librispeech/asr1/conf/tuning/train_asr_conformer10_hop_length160.yaml\",\"Branchformer egs2/librispeech/asr1/conf/tuning/train_asr_branchformer_hop_length160_e18_linear3072.yaml\"]},\"2563\":{\"h\":\"Command line argument based\",\"t\":[\"You can also customize it by passing the command line arguments, e.g.,\",\"./run.sh --stage 10 --asr_args \\\"--model_conf ctc_weight=0.3\\\"\",\"./run.sh --stage 10 --asr_args \\\"--optim_conf lr=0.1\\\"\",\"This approach has a highest priority. Thus, the arguments passed in the command line will overwrite those defined in the config file. This is convenient if you only want to change a few arguments.\",\"Please refer to https://espnet.github.io/espnet/espnet2_tutorial.html#change-the-configuration-for-training for more details.\"]},\"2564\":{\"h\":\"📗 Exercise 1 (1 point bonus)\",\"t\":[\"Run training, inference and scoring on AN4 using a new config. If you achieve a better character error rate (CER) than this following one, you can get a bonus point. I suggest tuning the total number of epochs, learning rate, warmup steps or data augmentation (speed perturbation, SpecAug) to improve the result.\",\"This AN4 dataset is very small, so the result can be unstable even for the same configs. Generally, we should compare different methods using a large dataset such as LibriSpeech 960h.\",\"Here is an example config using Branchformer (Peng et al, ICML 2022). Only the encoder is changed and the others are identical.\",\"Similarly, we create a config file named train_asr_demo_branchformer.yaml and start training.\",\"encoder: branchformer encoder_conf: output_size: 256 use_attn: true attention_heads: 4 attention_layer_type: rel_selfattn pos_enc_layer_type: rel_pos rel_pos_type: latest use_cgmlp: true cgmlp_linear_units: 1024 cgmlp_conv_kernel: 31 use_linear_after_conv: false gate_activation: identity merge_method: concat cgmlp_weight: 0.5 # used only if merge_method is \\\"fixed_ave\\\" attn_branch_drop_rate: 0.0 # used only if merge_method is \\\"learned_ave\\\" num_blocks: 12 dropout_rate: 0.1 positional_dropout_rate: 0.1 attention_dropout_rate: 0.1 input_layer: conv2d stochastic_depth_rate: 0.0\",\"My result is shown below:\",\"## asr_train_asr_demo_branchformer_raw_bpe30 ### WER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|773|85.4|11.3|3.4|0.4|15.0|46.9| |decode_asr_asr_model_valid.acc.ave/train_dev|100|591|77.3|15.7|6.9|0.7|23.4|62.0| ### CER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|2565|93.5|2.9|3.6|1.2|7.8|46.9| |decode_asr_asr_model_valid.acc.ave/train_dev|100|1915|87.8|5.0|7.2|1.8|14.0|62.0| ### TER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_asr_model_valid.acc.ave/test|130|2695|93.8|2.8|3.4|1.2|7.4|46.9| |decode_asr_asr_model_valid.acc.ave/train_dev|100|2015|88.4|4.8|6.8|1.7|13.3|62.0|\",\"image.png\",\"# Run multiple stages !./asr.sh --stage 10 --stop_stage 13 --train_set train_nodev --valid_set train_dev --test_sets \\\"train_dev test\\\" --nj 4 --ngpu 1 --use_lm false --gpu_inference true --inference_nj 1 --asr_config conf/train_asr_demo_branchformer.yaml --inference_config conf/decode_asr.yaml\",\"# NOTE: Exercise 1 Result 1 !scripts/utils/show_asr_result.sh exp print_date_and_time()\",\"# NOTE: Exercise 1 Result 2 from IPython.display import Image, display display( Image('exp/asr_train_asr_demo_transformer_raw_bpe30/images/acc.png', width=400), Image('exp/asr_train_asr_demo_branchformer_raw_bpe30/images/acc.png', width=400), ) print_date_and_time()\",\"Please carefully read the document: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE to understand how to create a new recipe. The major part is to prepare the data directory, which organizes the processed data in the Kaldi format. The other parts can be accomplished by executing some shared scripts.\"]},\"2565\":{\"h\":\"📗 Exercise 2 (1 point)\",\"t\":[\"We use the TIDIGITS dataset and we select part of it which contains around 2000 utterances for training and another 2000 for testing. Note that this dataset is provided only for private use. Please do not share it elsewhere.\",\"You need to finish the entire pipeline from data preparation to scoring.\"]},\"2566\":{\"h\":\"1. Create a new directory\",\"t\":[\"We need to create a new directory in egs2/ for the new dataset. This will automatically create several other files and directories.\",\"asr.sh cmd.sh conf db.sh local path.sh pyscripts scripts steps utils\",\"%cd /content/espnet !egs2/TEMPLATE/asr1/setup.sh egs2/tidigits/asr1 %cd egs2/tidigits/asr1 !ls\"]},\"2567\":{\"h\":\"2. Download data\",\"t\":[\"Please download the compressed dataset from Google Drive (removed in this public version) and decompress it. Then, specify the absolute path to the dataset in db.sh as follows:\",\"... TIDIGITS=/content/espnet/egs2/tidigits/asr1/downloads/TIDIGITS_children_boy # our newly added path ...\",\"!mkdir downloads %cd downloads !gdown &lt;FILE_ID&gt; !tar -xzf TIDIGITS_children_boy.tar.gz && ls TIDIGITS_children_boy\",\"%cd /content/espnet/egs2/tidigits/asr1 !echo \\\"\\\" >> db.sh !echo \\\"TIDIGITS=/content/espnet/egs2/tidigits/asr1/downloads/TIDIGITS_children_boy\\\" >> db.sh\"]},\"2568\":{\"h\":\"3. Finish the script for data preparation\",\"t\":[\"In Stage 1, asr.sh calls local/data.sh to prepare the dataset in the Kaldi format. So you need to do the following things.\",\"Create a file local/data.sh with the following content. This is task specific, but here we have prepared it for you.\",\"#!/usr/bin/env bash # Set bash to 'debug' mode, it will exit on : # -e 'error', -u 'undefined variable', -o ... 'error in pipeline', -x 'print commands', set -e set -u set -o pipefail log() { local fname=${BASH_SOURCE[1]##*/} echo -e \\\"$(date '+%Y-%m-%dT%H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*\\\" } SECONDS=0 stage=1 stop_stage=100 log \\\"$0 $*\\\" . utils/parse_options.sh . ./db.sh . ./path.sh . ./cmd.sh if [ $# -ne 0 ]; then log \\\"Error: No positional arguments are required.\\\" exit 2 fi if [ -z \\\"${TIDIGITS}\\\" ]; then log \\\"Fill the value of 'TIDIGITS' of db.sh\\\" exit 1 fi train_set=\\\"train_nodev\\\" train_dev=\\\"train_dev\\\" ndev_utt=200 if [ ${stage} -le 1 ] && [ ${stop_stage} -ge 1 ]; then log \\\"stage 1: Data preparation\\\" mkdir -p data/{train,test} if [ ! -f ${TIDIGITS}/readme.1st ]; then echo Cannot find TIDIGITS root! Exiting... exit 1 fi # Prepare data in the Kaldi format, including three files: # text, wav.scp, utt2spk python3 local/data_prep.py ${TIDIGITS} sph2pipe for x in test train; do for f in text wav.scp utt2spk; do sort data/${x}/${f} -o data/${x}/${f} done utils/utt2spk_to_spk2utt.pl data/${x}/utt2spk > \\\"data/${x}/spk2utt\\\" done # make a dev set utils/subset_data_dir.sh --first data/train \\\"${ndev_utt}\\\" \\\"data/${train_dev}\\\" n=$(($(wc -l < data/train/text) - ndev_utt)) utils/subset_data_dir.sh --last data/train \\\"${n}\\\" \\\"data/${train_set}\\\" fi log \\\"Successfully finished. [elapsed=${SECONDS}s]\\\"\",\"Create a python script local/data_prep.py which is used by the previous shell script. Similarly, we have provided most of the code for you. You only need to finish a few lines of code (see the TODO tag).\",\"import os import glob import sys if __name__ == \\\"__main__\\\": if len(sys.argv) != 3: print(\\\"Usage: python data_prep.py [root] [sph2pipe]\\\") sys.exit(1) root = sys.argv[1] sph2pipe = sys.argv[2] for x in [\\\"train\\\", \\\"test\\\"]: # We only use the data from boy children all_audio_list = glob.glob( os.path.join(root, \\\"data\\\", \\\"children\\\", x, \\\"boy\\\", \\\"*\\\", \\\"*.wav\\\") ) with open(os.path.join(\\\"data\\\", x, \\\"text\\\"), \\\"w\\\") as text_f, open( os.path.join(\\\"data\\\", x, \\\"wav.scp\\\"), \\\"w\\\" ) as wav_scp_f, open( os.path.join(\\\"data\\\", x, \\\"utt2spk\\\"), \\\"w\\\" ) as utt2spk_f: for audio_path in all_audio_list: filename = os.path.basename(audio_path) # \\\"o73a.wav\\\" etc speaker = os.path.basename(os.path.dirname(audio_path)) # \\\"lc\\\", \\\"sk\\\", etc transcript = \\\" \\\".join(list(filename[:-5])) # \\\"o73\\\" -> \\\"o 7 3\\\" uttid = f\\\"{speaker}-{filename[:-4]}\\\" # \\\"sk-o73a\\\" wav_scp_f.write( f\\\"{uttid} {sph2pipe} -f wav -p -c 1 {audio_path} |\\\\n\\\" ) ### TODO: write the other files in the Kaldi format\",\"!touch local/data.sh && chmod +x local/data.sh !touch local/data_prep.py ## TODO: copy the script and finish it\"]},\"2569\":{\"h\":\"4. Create a script as the entry point\",\"t\":[\"Now let's create a shell script run.sh as the entry point. You can directly use the following one.\",\"What are the differences compared to the AN4 recipe?\",\"We use words as modeling units.\",\"We pass decoding arguments in the command line, see inference_args.\",\"We perform data prep, training, decoding and scoring in this single job.\",\"#!/usr/bin/env bash # Set bash to 'debug' mode, it will exit on : # -e 'error', -u 'undefined variable', -o ... 'error in pipeline', -x 'print commands', set -e set -u set -o pipefail ./asr.sh \\\\ --stage 1 \\\\ --stop_stage 13 \\\\ --nj 4 \\\\ --ngpu 1 \\\\ --gpu_inference true \\\\ --inference_nj 1 \\\\ --use_lm false \\\\ --lang en \\\\ --token_type word \\\\ --asr_config conf/train_asr_demo_branchformer.yaml \\\\ --inference_args \\\"--beam_size 10 --ctc_weight 0.3\\\" \\\\ --train_set train_nodev \\\\ --valid_set train_dev \\\\ --test_sets \\\"train_dev test\\\" \\\\ --bpe_train_text \\\"data/train_nodev/text\\\" \\\\ --lm_train_text \\\"data/train_nodev/text\\\" \\\"$@\\\"\",\"!touch run.sh && chmod +x run.sh ## TODO: copy and paste the code\"]},\"2570\":{\"h\":\"5. Create the training config\",\"t\":[\"We can use the previous Branchformer config train_asr_demo_branchformer.yaml. It should work well.\",\"Note that we pass the decoding configs in the command line instead of using a separate file.\",\"!touch conf/train_asr_demo_branchformer.yaml ## TODO: copy and paste the config from an4\"]},\"2571\":{\"h\":\"6. Execute the script\",\"t\":[\"Now we are ready to launch the job. We start from Stage 1 to Stage 13.\",\"Again, we can monitor the status using Tensorboard.\",\"# Load the TensorBoard notebook extension %load_ext tensorboard # Launch tensorboard before training %tensorboard --logdir /content/espnet/egs2/tidigits/asr1/exp\",\"# It takes 30 minutes with a single T4 GPU !./run.sh\"]},\"2572\":{\"h\":\"7. Print the results\",\"t\":[\"We print the results and display the figure. Please modify the path based on your situation.\",\"The example output looks like the following:\",\"image.png\",\"## asr_train_asr_demo_branchformer_raw_en_word ### WER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |inference_beam_size10_ctc_weight0.3_asr_model_valid.acc.ave/test|1925|6325|98.4|1.2|0.3|0.3|1.9|5.9| |inference_beam_size10_ctc_weight0.3_asr_model_valid.acc.ave/train_dev|200|665|99.8|0.2|0.0|0.3|0.5|1.5| ### CER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |inference_beam_size10_ctc_weight0.3_asr_model_valid.acc.ave/test|1925|10725|98.9|0.7|0.4|0.4|1.5|5.9| |inference_beam_size10_ctc_weight0.3_asr_model_valid.acc.ave/train_dev|200|1130|99.9|0.1|0.0|0.4|0.4|1.5| ### TER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---|\",\"# NOTE: Exercise 2 # Remember to modify the path before running this cell! !cat exp/asr_train_asr_demo_branchformer_raw_en_word/RESULTS.md from IPython.display import Image, display display(Image('exp/asr_train_asr_demo_branchformer_raw_en_word/images/acc.png', width=400)) print_date_and_time()\",\"Finally, we list some resources for more advanced usage of ESPnet. Please check them after the tutorial.\"]},\"2573\":{\"h\":\"Fine-tune a pre-trained model\",\"t\":[\"We can load pre-trained weights into a new model and then fine-tune it on our target dataset. There is a separate tutorial about fine-tuning: https://espnet.github.io/espnet/notebook/espnet2_asr_transfer_learning_demo.html\",\"Notes:\",\"Here, I do not use the phrase \\\"transfer learning\\\", because transfer learning is really a large research area. Fine-tuning is just one type of transfer learning. However, these concepts are usually not distinguished in the current documentation. So, do not get confused about the terminology.\",\"There are other ways to pass the pre-trained model path, which require modifications of the scripts asr.sh. For example, you can comment out the pretrained_model and ignore_init_mismatch in asr.sh and set them in the training config instead.\"]},\"2574\":{\"h\":\"Use self-supervised pre-trained models as the front-end\",\"t\":[\"In addition to the log Mel filterbank features computed by signal processing algorithms, ESPnet also supports self-supervised speech representations extracted from large pre-trained models such as wav2vec 2.0 and HuBERT. These models are pre-trained on large amounts of unlabeled audio data, so the representations are very powerful. This approach is especially useful for low-resource applications.\",\"Please check recipes that use self-supervised pre-trained models, e.g., https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1#self-supervised-learning-features-wavlm_large-conformer-utt_mvn-with-transformer-lm\"]},\"2575\":{\"h\":\"Contribute to ESPnet\",\"t\":[\"Please follow https://github.com/espnet/espnet/blob/master/CONTRIBUTING.md to upload your pre-trained model to Hugging Face and make a pull request in the ESPnet repository.\"]},\"2576\":{\"h\":\"ESPnet2-ASR realtime demonstration\",\"t\":[\"This notebook provides a demonstration of the realtime E2E-ASR using ESPnet2-ASR.\",\"ESPnet2-ASR: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/asr1\",\"Author: Jiatong Shi (@ftshijt)\",\"# NOTE: pip shows imcompatible errors due to preinstalled libraries but you do not need to care !pip install -q espnet==0.10.0 !pip install -q espnet_model_zoo\"]},\"2577\":{\"h\":\"ASR model demo\"},\"2578\":{\"h\":\"Model Selection\",\"t\":[\"Please select model shown in espnet_model_zoo\",\"In this demonstration, we will show English, Japanese, Spanish, Mandrain, Multilingual ASR model, respectively\",\"#@title Choose English ASR model { run: \\\"auto\\\" } lang = 'en' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave' #@param [\\\"Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave\\\", \\\"kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave\\\"] {type:\\\"string\\\"}\",\"#@title Choose Japanese ASR model { run: \\\"auto\\\" } lang = 'ja' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'Shinji Watanabe/laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid.acc.ave' #@param [\\\"Shinji Watanabe/laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid.acc.ave\\\"] {type:\\\"string\\\"}\",\"#@title Choose Spanish ASR model { run: \\\"auto\\\" } lang = 'es' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'ftshijt/mls_asr_transformer_valid.acc.best' #@param [\\\"ftshijt/mls_asr_transformer_valid.acc.best\\\"] {type:\\\"string\\\"}\",\"#@title Choose Mandrain ASR model { run: \\\"auto\\\" } lang = 'zh' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'Emiru Tsunoo/aishell_asr_train_asr_streaming_transformer_raw_zh_char_sp_valid.acc.ave' #@param [\\\" Emiru Tsunoo/aishell_asr_train_asr_streaming_transformer_raw_zh_char_sp_valid.acc.ave\\\"] {type:\\\"string\\\"}\",\"#@title Choose Multilingual ASR model { run: \\\"auto\\\" } lang = 'multilingual' fs = 16000 #@param {type:\\\"integer\\\"} tag = 'ftshijt/open_li52_asr_train_asr_raw_bpe7000_valid.acc.ave_10best' #@param [\\\" ftshijt/open_li52_asr_train_asr_raw_bpe7000_valid.acc.ave_10best\\\"] {type:\\\"string\\\"}\"]},\"2579\":{\"h\":\"Model Setup\",\"t\":[\"import time import torch import string from espnet_model_zoo.downloader import ModelDownloader from espnet2.bin.asr_inference import Speech2Text d = ModelDownloader() # It may takes a while to download and build models speech2text = Speech2Text( **d.download_and_unpack(tag), device=\\\"cuda\\\", minlenratio=0.0, maxlenratio=0.0, ctc_weight=0.3, beam_size=10, batch_size=0, nbest=1 ) def text_normalizer(text): text = text.upper() return text.translate(str.maketrans('', '', string.punctuation))\"]},\"2580\":{\"h\":\"Recognize our example recordings\",\"t\":[\"!git clone https://github.com/ftshijt/ESPNet_asr_egs.git import pandas as pd import soundfile import librosa.display from IPython.display import display, Audio import matplotlib.pyplot as plt egs = pd.read_csv(\\\"ESPNet_asr_egs/egs.csv\\\") for index, row in egs.iterrows(): if row[\\\"lang\\\"] == lang or lang == \\\"multilingual\\\": speech, rate = soundfile.read(\\\"ESPNet_asr_egs/\\\" + row[\\\"path\\\"]) assert fs == int(row[\\\"sr\\\"]) nbests = speech2text(speech) text, *_ = nbests[0] print(f\\\"Input Speech: ESPNet_asr_egs/{row['path']}\\\") # let us listen to samples display(Audio(speech, rate=rate)) librosa.display.waveplot(speech, sr=rate) plt.show() print(f\\\"Reference text: {text_normalizer(row['text'])}\\\") print(f\\\"ASR hypothesis: {text_normalizer(text)}\\\") print(\\\"*\\\" * 50)\"]},\"2581\":{\"h\":\"Recognize your own pre-recordings\",\"t\":[\"Upload your own pre-recorded recordings\",\"Recognize your voice with the ASR system\",\"from google.colab import files from IPython.display import display, Audio import soundfile import librosa.display import matplotlib.pyplot as plt uploaded = files.upload() for file_name in uploaded.keys(): speech, rate = soundfile.read(file_name) assert rate == fs, \\\"mismatch in sampling rate\\\" nbests = speech2text(speech) text, *_ = nbests[0] print(f\\\"Input Speech: {file_name}\\\") display(Audio(speech, rate=rate)) librosa.display.waveplot(speech, sr=rate) plt.show() print(f\\\"ASR hypothesis: {text_normalizer(text)}\\\") print(\\\"*\\\" * 50)\"]},\"2582\":{\"h\":\"Recognize your own live-recordings\",\"t\":[\"Record your own voice\",\"Recognize your voice with the ASR system\",\"# from https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be from IPython.display import Javascript from google.colab import output from base64 import b64decode RECORD = \\\"\\\"\\\" const sleep = time => new Promise(resolve => setTimeout(resolve, time)) const b2text = blob => new Promise(resolve => { const reader = new FileReader() reader.onloadend = e => resolve(e.srcElement.result) reader.readAsDataURL(blob) }) var record = time => new Promise(async resolve => { stream = await navigator.mediaDevices.getUserMedia({ audio: true }) recorder = new MediaRecorder(stream) chunks = [] recorder.ondataavailable = e => chunks.push(e.data) recorder.start() await sleep(time) recorder.onstop = async ()=>{ blob = new Blob(chunks) text = await b2text(blob) resolve(text) } recorder.stop() }) \\\"\\\"\\\" def record(sec, filename='audio.wav'): display(Javascript(RECORD)) s = output.eval_js('record(%d)' % (sec * 1000)) b = b64decode(s.split(',')[1]) with open(filename, 'wb+') as f: f.write(b) audio = 'audio.wav' second = 5 print(f\\\"Speak to your microphone {second} sec...\\\") record(second, audio) print(\\\"Done!\\\") import librosa import librosa.display speech, rate = librosa.load(audio, sr=16000) librosa.display.waveplot(speech, sr=rate) import matplotlib.pyplot as plt plt.show() import pysndfile pysndfile.sndio.write('audio_ds.wav', speech, rate=rate, format='wav', enc='pcm16') from IPython.display import display, Audio display(Audio(speech, rate=rate))\",\"nbests = speech2text(speech) text, *_ = nbests[0] print(f\\\"ASR hypothesis: {text_normalizer(text)}\\\")\"]},\"2583\":{\"h\":\"Use transfer learning for ASR in ESPnet2\",\"t\":[\"Author : Dan Berrebbi (dberrebb@andrew.cmu.edu)\",\"Date : April 11th, 2022\",\"In that tutorial, we will introduce several options to use pre-trained models/parameters for Automatic Speech Recognition (ASR) in ESPnet2. Available options are :\",\"use a local model you (or a collegue) have already trained,\",\"use a trained model from ESPnet repository on HuggingFace.\",\"We note that this is done for ASR training, so at stage 11 of ESPnet2 models' recipe.\"]},\"2584\":{\"h\":\"Why using such (pre-)trained models ?\",\"t\":[\"Several projects may involve making use of previously trained models, this is the reason why we developed ESPnet repository on HuggingFace for instance. Example of use cases are listed below (non-exhaustive):\",\"target a low resource language, a model trained from scratch may perform badly if trained with only few hours of data,\",\"study robustness to shifts (domain, language ... shifts) of a model,\",\"make use of massively trained multilingual models.\",\"...\",\"Please use the gpu environnement provided by google colab for runing this notebook.\",\"!git clone --depth 5 https://github.com/espnet/espnet\",\"# It takes 30 seconds %cd /content/espnet/tools !./setup_anaconda.sh anaconda espnet 3.9\",\"# It may take ~8 minutes %cd /content/espnet/tools !make CUDA_VERSION=10.2\",\"In this example, we use the mini_an4 data, which has only 4 utterances for training. This is of course too small to train an ASR model, but it enables to run all the decribed transfer learning models on a colab environnement. After having run and understood those models/instructions, you can apply it to any other recipe of ESPnet2 or a new recipe that you build. First, move to the recipe directory\",\"%cd /content/espnet/egs2/mini_an4/asr1\",\"Add a configuration file\",\"As the mini_an4 does not contain any configuration file for ASR model, we add one here.\",\"config = {'accum_grad': 1, 'batch_size': 1, 'batch_type': 'folded', 'best_model_criterion': [['valid', 'acc', 'max']], 'decoder': 'transformer', 'decoder_conf': {'dropout_rate': 0.1, 'input_layer': 'embed', 'linear_units': 2048, 'num_blocks': 6}, 'encoder': 'transformer', 'encoder_conf': {'attention_dropout_rate': 0.0, 'attention_heads': 4, 'dropout_rate': 0.1, 'input_layer': 'conv2d', 'linear_units': 2048, 'num_blocks': 12, 'output_size': 256}, 'grad_clip': 5, 'init': 'xavier_uniform', 'keep_nbest_models': 1, 'max_epoch': 5, 'model_conf': {'ctc_weight': 0.3, 'length_normalized_loss': False, 'lsm_weight': 0.1}, 'optim': 'adam', 'optim_conf': {'lr': 1.0}, 'patience': 0, 'scheduler': 'noamlr', 'scheduler_conf': {'warmup_steps': 1000}}\",\"import yaml with open(\\\"conf/train_asr.yaml\\\",\\\"w\\\") as f: yaml.dump(config, f)\",\"Data preparation (stage 1 - stage 5)\",\"!./asr.sh --stage 1 --stop_stage 5 --train-set \\\"train_nodev\\\" --valid-set \\\"train_dev\\\" --test_sets \\\"test\\\"\",\"Stage 10: ASR collect stats:\",\"# takes about 10 seconds !./asr.sh --stage 10 --stop_stage 10 --train-set \\\"train_nodev\\\" --valid-set \\\"train_dev\\\" --test_sets \\\"test\\\" --asr_config \\\"conf/train_asr.yaml\\\"\",\"Stage 11: ASR training (from scratch)\",\"We train our model for only 5 epochs, just to have a pre-trained model.\",\"# takes about 1-2 minutes !./asr.sh --stage 11 --stop_stage 11 --train-set \\\"train_nodev\\\" --valid-set \\\"train_dev\\\" --test_sets \\\"test\\\" --asr_config \\\"conf/train_asr.yaml\\\" --asr_tag \\\"pre_trained_model\\\"\",\"Stage 11.2 : ASR training over a pre-trained model\",\"We train our new model over the previously trained model. (here as we use the same training data, this is not very useful, but again this is a toy example that is reproducible with any model.)\",\"Step 1 : make sure your ASR model file has the proper ESPnet format (should be ok if trained with ESPnet). It just needs to be a \\\".pth\\\" (or \\\".pt\\\" or other extension) type pytorch model.\",\"Step 2 : add the parameter --pretrained_model path/to/your/pretrained/model/file.pth to run.sh.\",\"Step 3 : step 2 will initialize your new model with the parameters of the pre-trained model. Thus your new model will be trained with a strong initialization. However, if your new model have different parameter sizes for some parts of the model (e.g. last projection layer could be modified ...). This will lead to an error because of mismatches in size. To prevent this to happen, you can add the parameter --ignore_init_mismatch true in run.sh.\",\"Step 4 (Optional) : if you only want to use some specific parts of the pre-trained model, or exclude specific parts, you can specify it in the --pretrained_model argument by passing the component names with the following syntax : --pretrained_model &lt;file_path&gt;:&lt;src_key&gt;:&lt;dst_key&gt;:&lt;exclude_Keys&gt;. src_key are the parameters you want to keep from the pre-trained model. dst_key are the parameters you want to initialize in the new model with the src_keyparameters. And exclude_Keys are the parameters from the pre-trained model that you do not want to use. You can leave src_key and dst_key fields empty and just fill exclude_Keys with the parameters that you ant to drop. For instance, if you want to re-use encoder parameters but not decoder ones, syntax will be --pretrained_model &lt;file_path&gt;:::decoder. You can see the argument expected format in more details here.\",\"# takes about 1-2 minutes !./asr.sh --stage 11 --stop_stage 11 --train-set \\\"train_nodev\\\" --valid-set \\\"train_dev\\\" \\\\ --test_sets \\\"test\\\" --asr_config \\\"conf/train_asr.yaml\\\" --asr_tag \\\"transfer_learning_with_pre_trained_model\\\"\\\\ --pretrained_model \\\"/content/espnet/egs2/mini_an4/asr1/exp/asr_train_asr_raw_bpe30/valid.acc.ave.pth\\\"\",\"Stage 11.3 : ASR training over a HuggingFace pre-trained model\",\"We train our new model over the previously trained model from HuggingFace. Any model can be used, here we take a model trained on Bengali as an example. It can be found at https://huggingface.co/espnet/bn_openslr53.\"]},\"2585\":{\"h\":\"Use a trained model from ESPnet repository on HuggingFace.\",\"t\":[\"ESPnet repository on HuggingFace contains more than 200 pre-trained models, for a wide variety of languages and dataset, and we are actively expanding this repositories with new models every week! This enable any user to perform transfer learning with a wide variety of models without having to re-train them. In order to use our pre-trained models, the first step is to download the \\\".pth\\\" model file from the HugginFace page. There are several easy way to do it, either by manually downloading them (e.g. wget https://huggingface.co/espnet/bn_openslr53/blob/main/exp/asr_train_asr_raw_bpe1000/41epoch.pth), cloning it (git clone https://huggingface.co/espnet/bn_openslr53) or downloading it through an ESPnet recipe (described in the models' pages on HuggingFace):\",\"git checkout fa1b865352475b744c37f70440de1cc6b257ba70 pip install -e . cd egs2/bn_openslr53/asr1 ./run.sh --skip_data_prep false --skip_train true --download_model espnet/bn_openslr53\",\"Then, as you have the \\\".pth\\\" model file, you can follow the steps 1 to 4 from the previous section in order to use this pre-train model.\",\"!wget https://huggingface.co/espnet/bn_openslr53/resolve/main/exp/asr_train_asr_raw_bpe1000/41epoch.pth\",\"The next command line will raise an error because of the size mismatch of some parameters, as mentionned before (step3).\",\"# will fail in about 5 seconds !./asr.sh --stage 11 --stop_stage 11 --train-set \\\"train_nodev\\\" --valid-set \\\"train_dev\\\" \\\\ --test_sets \\\"test\\\" --asr_config \\\"conf/train_asr.yaml\\\" --asr_tag \\\"transfer_learning_with_pre_trained_model\\\"\\\\ --pretrained_model \\\"/content/espnet/egs2/mini_an4/asr1/41epoch.pth\\\"\",\"To solve this issue, as mentionned, we can use the --ignore_init_mismatch \\\"true\\\" parameter.\",\"# takes about 1-2 minutes !./asr.sh --stage 11 --stop_stage 11 --train-set \\\"train_nodev\\\" --valid-set \\\"train_dev\\\" \\\\ --test_sets \\\"test\\\" --asr_config \\\"conf/train_asr.yaml\\\" --asr_tag \\\"transfer_learning_with_pre_trained_model_from_HF\\\"\\\\ --pretrained_model \\\"/content/espnet/egs2/mini_an4/asr1/41epoch.pth\\\" --ignore_init_mismatch \\\"true\\\"\",\"Additional note about the --ignore_init_mismatch true option : This option is very convenient because in lots of transfer learning use cases, you will aim to use a model trained on a language X (e.g. X=English) for another language Y. Language Y may have a vocabulary (set of tokens) different from language X, for instance if you target Y=Totonac, a Mexican low resource language, your model may be stronger if you use a different set of bpes/tokens thatn the one used to train the English model. In that situation, the last layer (projection to vocabulary space) of your ASR model needs to be initialized from scratch and may be different in shape than the one of the English model. For that reason, you should use the --ignore_init_mismatch true option. It also enables to handle the case where the scripts are differents from languages X to Y.\"]},\"2586\":{\"h\":\"ESPnet2 real streaming Transformer demonstration\",\"t\":[\"Details in \\\"Streaming Transformer ASR with Blockwise Synchronous Beam Search\\\" (https://arxiv.org/abs/2006.14941)\",\"This local notebook provides a demonstration of streaming ASR based on Transformer using ESPnet2.\",\"You can recognize a recorded audio file or a speech online.\",\"Author: Keqi Deng (UCAS)\"]},\"2587\":{\"h\":\"Train a streaming Transformer model\",\"t\":[\"You can train a streaming Transformer model on your own corpus following the example of https://github.com/espnet/espnet/blob/master/egs2/aishell/asr1/run_streaming.sh\"]},\"2588\":{\"h\":\"Download pre-trained model and audio file for demo\",\"t\":[\"You can download the pre-trained model from the ESPnet_model_zoo or directly from Huggingface.\"]},\"2589\":{\"h\":\"For Mandarin Task (Pretrained using AISHELL-1)\",\"t\":[\"tag='Emiru Tsunoo/aishell_asr_train_asr_streaming_transformer_raw_zh_char_sp_valid.acc.ave'\"]},\"2590\":{\"h\":\"For English Task (Pretrained using Tedlium2)\",\"t\":[\"tag='D-Keqi/espnet_asr_train_asr_streaming_transformer_raw_en_bpe500_sp_valid.acc.ave'\"]},\"2591\":{\"h\":\"Import packages\",\"t\":[\"Make sure that you have installed the latest ESPnet\",\"import sys import espnet from espnet2.bin.asr_inference_streaming import Speech2TextStreaming from espnet_model_zoo.downloader import ModelDownloader import argparse import numpy as np import wave\"]},\"2592\":{\"h\":\"Prepare for inference\",\"t\":[\"d=ModelDownloader() speech2text = Speech2TextStreaming( **d.download_and_unpack(tag), token_type=None, bpemodel=None, maxlenratio=0.0, minlenratio=0.0, beam_size=20, ctc_weight=0.5, lm_weight=0.0, penalty=0.0, nbest=1, device = \\\"cpu\\\", disable_repetition_detection=True, decoder_text_length_limit=0, encoded_feat_length_limit=0 )\",\"prev_lines = 0 def progress_output(text): global prev_lines lines=[''] for i in text: if len(lines[-1]) > 100: lines.append('') lines[-1] += i for i,line in enumerate(lines): if i == prev_lines: sys.stderr.write('\\\\n\\\\r') else: sys.stderr.write('\\\\r\\\\033[B\\\\033[K') sys.stderr.write(line) prev_lines = len(lines) sys.stderr.flush()\",\"def recognize(wavfile): with wave.open(wavfile, 'rb') as wavfile: ch=wavfile.getnchannels() bits=wavfile.getsampwidth() rate=wavfile.getframerate() nframes=wavfile.getnframes() buf = wavfile.readframes(-1) data=np.frombuffer(buf, dtype='int16') speech = data.astype(np.float16)/32767.0 #32767 is the upper limit of 16-bit binary numbers and is used for the normalization of int to float. sim_chunk_length = 640 if sim_chunk_length > 0: for i in range(len(speech)//sim_chunk_length): results = speech2text(speech=speech[i*sim_chunk_length:(i+1)*sim_chunk_length], is_final=False) if results is not None and len(results) > 0: nbests = [text for text, token, token_int, hyp in results] text = nbests[0] if nbests is not None and len(nbests) > 0 else \\\"\\\" progress_output(nbests[0]) else: progress_output(\\\"\\\") results = speech2text(speech[(i+1)*sim_chunk_length:len(speech)], is_final=True) else: results = speech2text(speech, is_final=True) nbests = [text for text, token, token_int, hyp in results] progress_output(nbests[0])\"]},\"2593\":{\"h\":\"Recognize the audio file\",\"t\":[\"#You can upload your own audio file for recognition, and also we provide some demo audio files that you can download from Google drive. #For Mandarin task, the demo file comes from the AISSHELL-1: https://drive.google.com/file/d/1l8w93r8Bs5FtC3A-1ydEqFQdP4k6FiUL/view?usp=sharing #wavfile='./BAC009S0724W0121.wav' #For English task, the demo file comes from the Librispeech: https://drive.google.com/file/d/1l71ZUNQ6qQk95T54H0tH_OEwZvWnEL4u/view?usp=sharing #wavfile='./61-70968-0000.wav' recognize(wavfile)\"]},\"2594\":{\"h\":\"Recognize the speech from speaker\"},\"2595\":{\"h\":\"Install pyaudio\",\"t\":[\"import pyaudio\"]},\"2596\":{\"h\":\"Streamingly recognize with pyaudio\",\"t\":[\"CHUNK=2048 FORMAT=pyaudio.paInt16 CHANNELS=1 RATE=16000 RECORD_SECONDS=5 p=pyaudio.PyAudio() stream = p.open(format=FORMAT,channels=CHANNELS,rate=RATE,input=True,frames_per_buffer=CHUNK) for i in range(0,int(RATE/CHUNK*RECORD_SECONDS)+1): data=stream.read(CHUNK) data=np.frombuffer(data, dtype='int16') data=data.astype(np.float16)/32767.0 #32767 is the upper limit of 16-bit binary numbers and is used for the normalization of int to float. if i==int(RATE/CHUNK*RECORD_SECONDS): results = speech2text(speech=data, is_final=True) break results = speech2text(speech=data, is_final=False) if results is not None and len(results) > 0: nbests = [text for text, token, token_int, hyp in results] text = nbests[0] if nbests is not None and len(nbests) > 0 else \\\"\\\" progress_output(nbests[0]) else: progress_output(\\\"\\\") nbests = [text for text, token, token_int, hyp in results] progress_output(nbests[0])\"]},\"2597\":{\"h\":\"espnet_onnx demonstration\",\"t\":[\"This notebook provides a demonstration of how to export your trained model into onnx format. Currently only ASR is supported.\",\"see also:\",\"ESPnet: https://github.com/espnet/espnet\",\"espnet_onnx: https://github.com/Masao-Someki/espnet_onnx\",\"Author: Masao Someki\"]},\"2598\":{\"h\":\"Table of Contents\",\"t\":[\"Install Dependency\",\"Export your model\",\"Inference with onnx\",\"Using streaming model\",\"To run this demo, you need to install the following packages.\",\"espnet_onnx\",\"torch >= 1.11.0 (already installed in Colab)\",\"espnet\",\"espnet_model_zoo\",\"onnx\",\"torch, espnet, espnet_model_zoo, onnx is required to run the exportation demo.\",\"!pip install -U espnet_onnx espnet espnet_model_zoo onnx # in this demo, we need to update scipy to avoid an error !pip install -U scipy\"]},\"2599\":{\"h\":\"Export model from espnet_model_zoo\",\"t\":[\"The easiest way to export a model is to use espnet_model_zoo. You can download, unpack, and export the pretrained models with export_from_pretrained method. espnet_onnx will save the onnx models into cache directory, which is ${HOME}/.cache/espnet_onnx in default.\",\"# export the model. from espnet_onnx.export import ModelExport tag_name = 'kamo-naoyuki/timit_asr_train_asr_raw_word_valid.acc.ave' m = ModelExport() m.export_from_pretrained(tag_name)\"]},\"2600\":{\"h\":\"Export from custom model\",\"t\":[\"espnet_onnx can also export your own trained model with export method.\",\"The following script shows how to export from espnet2.bin.asr_inference.Speech2Text instance. You can also export from a zipped file, by using the export_from_zip function. For this demonstration, I'm using the from_pretrained method to load parameters, but you can load your own model.\",\"# prepare the espnet2.bin.asr_inference.Speech2Text instance. from espnet2.bin.asr_inference import Speech2Text tag_name = 'kamo-naoyuki/timit_asr_train_asr_raw_word_valid.acc.ave' speech2text = Speech2Text.from_pretrained(tag_name) # export model from espnet_onnx.export import ModelExport sample_model_tag = 'demo/sample_model_1' m = ModelExport() m.export( speech2text, sample_model_tag, quantize=False )\",\"Now, let's use the exported models for inference.\",\"# please provide the tag_name to specify exported model. tag_name = 'kamo-naoyuki/timit_asr_train_asr_raw_word_valid.acc.ave' # upload wav file and let's inference! import librosa from google.colab import files wav_file = files.upload() y, sr = librosa.load(list(wav_file.keys())[0], sr=16000) # Use the exported onnx file to inference. from espnet_onnx import Speech2Text speech2text = Speech2Text(tag_name) nbest = speech2text(y) print(nbest[0][0])\",\"Model exportation is exactly the same as non-streaming model. You can follow the #Export your model chapter.\",\"As for streaming, you can specify the following configuration additionaly. Usually, these values should be the same as the training configuration.\",\"block_size\",\"hop_size\",\"look_ahead\",\"The length of the speech should be the same as streaming_model.hop_size. This value is calculated as follows\",\"$$ \\\\begin{align} h &= \\\\text{hop_size} * \\\\text{encoder.subsample} * \\\\text{stft.hop_length}\\\\ \\\\text{padding} &= (\\\\text{stft.n_fft} // \\\\text{stft.hop_length}) * \\\\text{stft.hop_length} \\\\ \\\\text{len(wav)} &= h + \\\\text{padding} \\\\end{align} $$\",\"For example, the length of the speech is 8704 with the following configuration.\",\"block_size = 40\",\"hop_size = 16\",\"look_ahead = 16\",\"encoder.subsample = 4\",\"stft.n_fft = 512\",\"stft.hop_length = 128\",\"Now, let's demonstrate the streaming inference.\",\"# Export the streaming model. # Note that the following model is very large from espnet_onnx.export import ModelExport tag_name = 'D-Keqi/espnet_asr_train_asr_streaming_transformer_raw_en_bpe500_sp_valid.acc.ave' m = ModelExport() m.export_from_pretrained(tag_name)\",\"# In this tutorial, we will use the recorded wav file to simulate streaming. import librosa from espnet_onnx import StreamingSpeech2Text tag_name = 'D-Keqi/espnet_asr_train_asr_streaming_transformer_raw_en_bpe500_sp_valid.acc.ave' streaming_model = StreamingSpeech2Text(tag_name) # upload wav file from google.colab import files wav_file = files.upload() y, sr = librosa.load(list(wav_file.keys())[0], sr=16000) num_process = len(y) // streaming_model.hop_size + 1 print(f\\\"I will split your audio file into {num_process} blocks.\\\") # simulate streaming. streaming_model.start() for i in range(num_process): # prepare wav file start = i * streaming_model.hop_size end = (i + 1) * streaming_model.hop_size wav_streaming = y[start : end] # apply padding if len(wav_streaming) < streaming_model.hop_size wav_streaming = streaming_model.pad(wav_streaming) # compute asr nbest = streaming_model(wav_streaming) print(f'Result at position {i} : {nbest[0][0]}') final_nbest = streaming_model.end() print(f'Final result : {final_nbest[0][0]}')\"]},\"2601\":{\"h\":\"ESPnet Speech Enhancement Demonstration\",\"t\":[\"Open In Colab\",\"This notebook provides a demonstration of the speech enhancement and separation using ESPnet2-SE.\",\"ESPnet2-SE: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/enh1\",\"Author: Chenda Li (@LiChenda), Wangyou Zhang (@Emrys365)\"]},\"2602\":{\"h\":\"Install\",\"t\":[\"%pip install -q espnet==0.10.1 %pip install -q espnet_model_zoo\"]},\"2603\":{\"h\":\"Speech Enhancement\"},\"2604\":{\"h\":\"Single-Channel Enhancement, the CHiME example\",\"t\":[\"# Download one utterance from real noisy speech of CHiME4 !gdown --id 1SmrN5NFSg6JuQSs2sfy3ehD8OIcqK6wS -O /content/M05_440C0213_PED_REAL.wav import os import soundfile from IPython.display import display, Audio mixwav_mc, sr = soundfile.read(\\\"/content/M05_440C0213_PED_REAL.wav\\\") # mixwav.shape: num_samples, num_channels mixwav_sc = mixwav_mc[:,4] display(Audio(mixwav_mc.T, rate=sr))\"]},\"2605\":{\"h\":\"Download and load the pretrained Conv-Tasnet\",\"t\":[\"!gdown --id 17DMWdw84wF3fz3t7ia1zssdzhkpVQGZm -O /content/chime_tasnet_singlechannel.zip !unzip /content/chime_tasnet_singlechannel.zip -d /content/enh_model_sc\",\"# Load the model # If you encounter error \\\"No module named 'espnet2'\\\", please re-run the 1st Cell. This might be a colab bug. import sys import soundfile from espnet2.bin.enh_inference import SeparateSpeech separate_speech = {} # For models downloaded from GoogleDrive, you can use the following script: enh_model_sc = SeparateSpeech( enh_train_config=\\\"/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/config.yaml\\\", enh_model_file=\\\"/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/5epoch.pth\\\", # for segment-wise process on long speech normalize_segment_scale=False, show_progressbar=True, ref_channel=4, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2606\":{\"h\":\"Enhance the single-channel real noisy speech in CHiME4\",\"t\":[\"# play the enhanced single-channel speech wave = enh_model_sc(mixwav_sc[None, ...], sr) print(\\\"Input real noisy speech\\\", flush=True) display(Audio(mixwav_sc, rate=sr)) print(\\\"Enhanced speech\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2607\":{\"h\":\"Enhance your own pre-recordings\",\"t\":[\"from google.colab import files from IPython.display import display, Audio import soundfile uploaded = files.upload() for file_name in uploaded.keys(): speech, rate = soundfile.read(file_name) assert rate == sr, \\\"mismatch in sampling rate\\\" wave = enh_model_sc(speech[None, ...], sr) print(f\\\"Your input speech {file_name}\\\", flush=True) display(Audio(speech, rate=sr)) print(f\\\"Enhanced speech for {file_name}\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2608\":{\"h\":\"Multi-Channel Enhancement\"},\"2609\":{\"h\":\"Download and load the pretrained mvdr neural beamformer.\",\"t\":[\"# Download the pretained enhancement model !gdown --id 1FohDfBlOa7ipc9v2luY-QIFQ_GJ1iW_i -O /content/mvdr_beamformer_16k_se_raw_valid.zip !unzip /content/mvdr_beamformer_16k_se_raw_valid.zip -d /content/enh_model_mc\",\"# Load the model # If you encounter error \\\"No module named 'espnet2'\\\", please re-run the 1st Cell. This might be a colab bug. import sys import soundfile from espnet2.bin.enh_inference import SeparateSpeech separate_speech = {} # For models downloaded from GoogleDrive, you can use the following script: enh_model_mc = SeparateSpeech( enh_train_config=\\\"/content/enh_model_mc/exp/enh_train_enh_beamformer_mvdr_raw/config.yaml\\\", enh_model_file=\\\"/content/enh_model_mc/exp/enh_train_enh_beamformer_mvdr_raw/11epoch.pth\\\", # for segment-wise process on long speech normalize_segment_scale=False, show_progressbar=True, ref_channel=4, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2610\":{\"h\":\"Enhance the multi-channel real noisy speech in CHiME4\",\"t\":[\"wave = enh_model_mc(mixwav_mc[None, ...], sr) print(\\\"Input real noisy speech\\\", flush=True) display(Audio(mixwav_mc.T, rate=sr)) print(\\\"Enhanced speech\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2611\":{\"h\":\"Speech Separation\"},\"2612\":{\"h\":\"Model Selection\",\"t\":[\"Please select model shown in espnet_model_zoo\",\"In this demonstration, we will show different speech separation models on wsj0_2mix.\",\"#@title Choose Speech Separation model { run: \\\"auto\\\" } fs = 8000 #@param {type:\\\"integer\\\"} tag = \\\"Chenda Li/wsj0_2mix_enh_train_enh_conv_tasnet_raw_valid.si_snr.ave\\\" #@param [\\\"Chenda Li/wsj0_2mix_enh_train_enh_conv_tasnet_raw_valid.si_snr.ave\\\", \\\"Chenda Li/wsj0_2mix_enh_train_enh_rnn_tf_raw_valid.si_snr.ave\\\", \\\"https://zenodo.org/record/4688000/files/enh_train_enh_dprnn_tasnet_raw_valid.si_snr.ave.zip\\\"]\",\"# For models uploaded to Zenodo, you can use the following python script instead: import sys import soundfile from espnet_model_zoo.downloader import ModelDownloader from espnet2.bin.enh_inference import SeparateSpeech d = ModelDownloader() cfg = d.download_and_unpack(tag) separate_speech = SeparateSpeech( enh_train_config=cfg[\\\"train_config\\\"], enh_model_file=cfg[\\\"model_file\\\"], # for segment-wise process on long speech segment_size=2.4, hop_size=0.8, normalize_segment_scale=False, show_progressbar=True, ref_channel=None, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2613\":{\"h\":\"Separate Speech Mixture\"},\"2614\":{\"h\":\"Separate the example in wsj0_2mix testing set\",\"t\":[\"!gdown --id 1ZCUkd_Lb7pO2rpPr4FqYdtJBZ7JMiInx -O /content/447c020t_1.2106_422a0112_-1.2106.wav import os import soundfile from IPython.display import display, Audio mixwav, sr = soundfile.read(\\\"447c020t_1.2106_422a0112_-1.2106.wav\\\") waves_wsj = separate_speech(mixwav[None, ...], fs=sr) print(\\\"Input mixture\\\", flush=True) display(Audio(mixwav, rate=sr)) print(f\\\"========= Separated speech with model {tag} =========\\\", flush=True) print(\\\"Separated spk1\\\", flush=True) display(Audio(waves_wsj[0].squeeze(), rate=sr)) print(\\\"Separated spk2\\\", flush=True) display(Audio(waves_wsj[1].squeeze(), rate=sr))\"]},\"2615\":{\"h\":\"Separate your own recordings\",\"t\":[\"from google.colab import files from IPython.display import display, Audio import soundfile uploaded = files.upload() for file_name in uploaded.keys(): mixwav_yours, rate = soundfile.read(file_name) assert rate == sr, \\\"mismatch in sampling rate\\\" waves_yours = separate_speech(mixwav_yours[None, ...], fs=sr) print(\\\"Input mixture\\\", flush=True) display(Audio(mixwav_yours, rate=sr)) print(f\\\"========= Separated speech with model {tag} =========\\\", flush=True) print(\\\"Separated spk1\\\", flush=True) display(Audio(waves_yours[0].squeeze(), rate=sr)) print(\\\"Separated spk2\\\", flush=True) display(Audio(waves_yours[1].squeeze(), rate=sr))\"]},\"2616\":{\"h\":\"Show spectrums of separated speech\",\"t\":[\"import matplotlib.pyplot as plt import torch from torch_complex.tensor import ComplexTensor from espnet.asr.asr_utils import plot_spectrogram from espnet2.layers.stft import Stft stft = Stft( n_fft=512, win_length=None, hop_length=128, window=\\\"hann\\\", ) ilens = torch.LongTensor([len(mixwav)]) # specs: (T, F) spec_mix = ComplexTensor( *torch.unbind( stft(torch.as_tensor(mixwav).unsqueeze(0), ilens)[0].squeeze(), dim=-1 ) ) spec_sep1 = ComplexTensor( *torch.unbind( stft(torch.as_tensor(waves_wsj[0]), ilens)[0].squeeze(), dim=-1 ) ) spec_sep2 = ComplexTensor( *torch.unbind( stft(torch.as_tensor(waves_wsj[1]), ilens)[0].squeeze(), dim=-1 ) ) # freqs = torch.linspace(0, sr / 2, spec_mix.shape[1]) # frames = torch.linspace(0, len(mixwav) / sr, spec_mix.shape[0]) samples = torch.linspace(0, len(mixwav) / sr, len(mixwav)) plt.figure(figsize=(24, 12)) plt.subplot(3, 2, 1) plt.title('Mixture Spectrogram') plot_spectrogram( plt, abs(spec_mix).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 2) plt.title('Mixture Wavform') plt.plot(samples, mixwav) plt.xlim(0, len(mixwav) / sr) plt.subplot(3, 2, 3) plt.title('Separated Spectrogram (spk1)') plot_spectrogram( plt, abs(spec_sep1).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 4) plt.title('Separated Wavform (spk1)') plt.plot(samples, waves_wsj[0].squeeze()) plt.xlim(0, len(mixwav) / sr) plt.subplot(3, 2, 5) plt.title('Separated Spectrogram (spk2)') plot_spectrogram( plt, abs(spec_sep2).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 6) plt.title('Separated Wavform (spk2)') plt.plot(samples, waves_wsj[1].squeeze()) plt.xlim(0, len(mixwav) / sr) plt.xlabel(\\\"Time (s)\\\") plt.show()\"]},\"2617\":{\"h\":\"Evluate separated speech with pretrained ASR model\",\"t\":[\"The ground truths are:\",\"text_1: SOME CRITICS INCLUDING HIGH REAGAN ADMINISTRATION OFFICIALS ARE RAISING THE ALARM THAT THE FED'S POLICY IS TOO TIGHT AND COULD CAUSE A RECESSION NEXT YEAR\",\"text_2: THE UNITED STATES UNDERTOOK TO DEFEND WESTERN EUROPE AGAINST SOVIET ATTACK\",\"(This may take a while for the speech recognition.)\",\"import espnet_model_zoo from espnet_model_zoo.downloader import ModelDownloader from espnet2.bin.asr_inference import Speech2Text wsj_8k_model_url=\\\"https://zenodo.org/record/4012264/files/asr_train_asr_transformer_raw_char_1gpu_valid.acc.ave.zip?download=1\\\" d = ModelDownloader() speech2text = Speech2Text( **d.download_and_unpack(wsj_8k_model_url), device=\\\"cuda:0\\\", ) text_est = [None, None] text_est[0], *_ = speech2text(waves_wsj[0].squeeze())[0] text_est[1], *_ = speech2text(waves_wsj[1].squeeze())[0] text_m, *_ = speech2text(mixwav)[0] print(\\\"Mix Speech to Text: \\\", text_m) print(\\\"Separated Speech 1 to Text: \\\", text_est[0]) print(\\\"Separated Speech 2 to Text: \\\", text_est[1])\",\"import difflib from itertools import permutations import editdistance import numpy as np colors = dict( red=lambda text: f\\\"\\\\033[38;2;255;0;0m{text}\\\\033[0m\\\" if text else \\\"\\\", green=lambda text: f\\\"\\\\033[38;2;0;255;0m{text}\\\\033[0m\\\" if text else \\\"\\\", yellow=lambda text: f\\\"\\\\033[38;2;225;225;0m{text}\\\\033[0m\\\" if text else \\\"\\\", white=lambda text: f\\\"\\\\033[38;2;255;255;255m{text}\\\\033[0m\\\" if text else \\\"\\\", black=lambda text: f\\\"\\\\033[38;2;0;0;0m{text}\\\\033[0m\\\" if text else \\\"\\\", ) def diff_strings(ref, est): \\\"\\\"\\\"Reference: https://stackoverflow.com/a/64404008/7384873\\\"\\\"\\\" ref_str, est_str, err_str = [], [], [] matcher = difflib.SequenceMatcher(None, ref, est) for opcode, a0, a1, b0, b1 in matcher.get_opcodes(): if opcode == \\\"equal\\\": txt = ref[a0:a1] ref_str.append(txt) est_str.append(txt) err_str.append(\\\" \\\" * (a1 - a0)) elif opcode == \\\"insert\\\": ref_str.append(\\\"*\\\" * (b1 - b0)) est_str.append(colors[\\\"green\\\"](est[b0:b1])) err_str.append(colors[\\\"black\\\"](\\\"I\\\" * (b1 - b0))) elif opcode == \\\"delete\\\": ref_str.append(ref[a0:a1]) est_str.append(colors[\\\"red\\\"](\\\"*\\\" * (a1 - a0))) err_str.append(colors[\\\"black\\\"](\\\"D\\\" * (a1 - a0))) elif opcode == \\\"replace\\\": diff = a1 - a0 - b1 + b0 if diff >= 0: txt_ref = ref[a0:a1] txt_est = colors[\\\"yellow\\\"](est[b0:b1]) + colors[\\\"red\\\"](\\\"*\\\" * diff) txt_err = \\\"S\\\" * (b1 - b0) + \\\"D\\\" * diff elif diff < 0: txt_ref = ref[a0:a1] + \\\"*\\\" * -diff txt_est = colors[\\\"yellow\\\"](est[b0:b1]) + colors[\\\"green\\\"](\\\"*\\\" * -diff) txt_err = \\\"S\\\" * (b1 - b0) + \\\"I\\\" * -diff ref_str.append(txt_ref) est_str.append(txt_est) err_str.append(colors[\\\"black\\\"](txt_err)) return \\\"\\\".join(ref_str), \\\"\\\".join(est_str), \\\"\\\".join(err_str) text_ref = [ \\\"SOME CRITICS INCLUDING HIGH REAGAN ADMINISTRATION OFFICIALS ARE RAISING THE ALARM THAT THE FED'S POLICY IS TOO TIGHT AND COULD CAUSE A RECESSION NEXT YEAR\\\", \\\"THE UNITED STATES UNDERTOOK TO DEFEND WESTERN EUROPE AGAINST SOVIET ATTACK\\\", ] print(\\\"=====================\\\" , flush=True) perms = list(permutations(range(2))) string_edit = [ [ editdistance.eval(text_ref[m], text_est[n]) for m, n in enumerate(p) ] for p in perms ] dist = [sum(edist) for edist in string_edit] perm_idx = np.argmin(dist) perm = perms[perm_idx] for i, p in enumerate(perm): print(\\\"\\\\n--------------- Text %d ---------------\\\" % (i + 1), flush=True) ref, est, err = diff_strings(text_ref[i], text_est[p]) print(\\\"REF: \\\" + ref + \\\"\\\\n\\\" + \\\"HYP: \\\" + est + \\\"\\\\n\\\" + \\\"ERR: \\\" + err, flush=True) print(\\\"Edit Distance = {}\\\\n\\\".format(string_edit[perm_idx][i]), flush=True)\"]},\"2618\":{\"h\":\"ESPnet Speech Enhancement Demonstration\",\"t\":[\"Open In Colab\",\"This notebook provides a demonstration of the speech enhancement and separation using ESPnet2-SE.\",\"ESPnet2-SE: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/enh1\",\"Presenters:\",\"Shinji Watanabe (shinjiw@cmu.edu)\",\"Chenda Li (lichenda1996@sjtu.edu.cn)\",\"Jing Shi (shijing2014@ia.ac.cn)\",\"Wangyou Zhang (wyz-97@sjtu.edu.cn)\",\"Yen-Ju Lu (neil.lu@citi.sinica.edu.tw)\",\"This notebook is created by: Chenda Li (@LiChenda) and Wangyou Zhang (@Emrys365)\",\"(1) Tutorials on the Basic Usage\",\"Install\",\"Speech Enhancement with Pretrained Models\",\"We support various interfaces, e.g. Python API, HuggingFace API, portable speech enhancement scripts for other tasks, etc.\",\"2.1 Single-channel Enhancement (CHiME-4)\",\"2.2 Enhance Your Own Recordings\",\"2.3 Multi-channel Enhancement (CHiME-4)\",\"Speech Separation with Pretrained Models\",\"3.1 Model Selection\",\"3.2 Separate Speech Mixture\",\"Evaluate Separated Speech with the Pretrained ASR Model\",\"(2) Tutorials for Adding New Recipe and Contributing to ESPnet-SE Project\",\"Creating a New Recipe\",\"Implementing a New Speech Enhancement/Separation Model\"]},\"2619\":{\"h\":\"Install\",\"t\":[\"%pip install -q espnet==0.10.1 %pip install -q espnet_model_zoo\"]},\"2620\":{\"h\":\"Speech Enhancement with Pretrained Models\"},\"2621\":{\"h\":\"Single-Channel Enhancement, the CHiME example\",\"t\":[\"# Download one utterance from real noisy speech of CHiME4 !gdown --id 1SmrN5NFSg6JuQSs2sfy3ehD8OIcqK6wS -O /content/M05_440C0213_PED_REAL.wav import os import soundfile from IPython.display import display, Audio mixwav_mc, sr = soundfile.read(\\\"/content/M05_440C0213_PED_REAL.wav\\\") # mixwav.shape: num_samples, num_channels mixwav_sc = mixwav_mc[:,4] display(Audio(mixwav_mc.T, rate=sr))\"]},\"2622\":{\"h\":\"Download and load the pretrained Conv-Tasnet\",\"t\":[\"!gdown --id 17DMWdw84wF3fz3t7ia1zssdzhkpVQGZm -O /content/chime_tasnet_singlechannel.zip !unzip /content/chime_tasnet_singlechannel.zip -d /content/enh_model_sc\",\"# Load the model # If you encounter error \\\"No module named 'espnet2'\\\", please re-run the 1st Cell. This might be a colab bug. import sys import soundfile from espnet2.bin.enh_inference import SeparateSpeech separate_speech = {} # For models downloaded from GoogleDrive, you can use the following script: enh_model_sc = SeparateSpeech( train_config=\\\"/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/config.yaml\\\", model_file=\\\"/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/5epoch.pth\\\", # for segment-wise process on long speech normalize_segment_scale=False, show_progressbar=True, ref_channel=4, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2623\":{\"h\":\"Enhance the single-channel real noisy speech in CHiME4\",\"t\":[\"# play the enhanced single-channel speech wave = enh_model_sc(mixwav_sc[None, ...], sr) print(\\\"Input real noisy speech\\\", flush=True) display(Audio(mixwav_sc, rate=sr)) print(\\\"Enhanced speech\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2624\":{\"h\":\"Enhance your own pre-recordings\",\"t\":[\"from google.colab import files from IPython.display import display, Audio import soundfile uploaded = files.upload() for file_name in uploaded.keys(): speech, rate = soundfile.read(file_name) assert rate == sr, \\\"mismatch in sampling rate\\\" wave = enh_model_sc(speech[None, ...], sr) print(f\\\"Your input speech {file_name}\\\", flush=True) display(Audio(speech, rate=sr)) print(f\\\"Enhanced speech for {file_name}\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2625\":{\"h\":\"Multi-Channel Enhancement\"},\"2626\":{\"h\":\"Download and load the pretrained mvdr neural beamformer.\",\"t\":[\"# Download the pretained enhancement model !gdown --id 1FohDfBlOa7ipc9v2luY-QIFQ_GJ1iW_i -O /content/mvdr_beamformer_16k_se_raw_valid.zip !unzip /content/mvdr_beamformer_16k_se_raw_valid.zip -d /content/enh_model_mc\",\"# Load the model # If you encounter error \\\"No module named 'espnet2'\\\", please re-run the 1st Cell. This might be a colab bug. import sys import soundfile from espnet2.bin.enh_inference import SeparateSpeech separate_speech = {} # For models downloaded from GoogleDrive, you can use the following script: enh_model_mc = SeparateSpeech( train_config=\\\"/content/enh_model_mc/exp/enh_train_enh_beamformer_mvdr_raw/config.yaml\\\", model_file=\\\"/content/enh_model_mc/exp/enh_train_enh_beamformer_mvdr_raw/11epoch.pth\\\", # for segment-wise process on long speech normalize_segment_scale=False, show_progressbar=True, ref_channel=4, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2627\":{\"h\":\"Enhance the multi-channel real noisy speech in CHiME4\",\"t\":[\"wave = enh_model_mc(mixwav_mc[None, ...], sr) print(\\\"Input real noisy speech\\\", flush=True) display(Audio(mixwav_mc.T, rate=sr)) print(\\\"Enhanced speech\\\", flush=True) display(Audio(wave[0].squeeze(), rate=sr))\"]},\"2628\":{\"h\":\"Portable speech enhancement scripts for other tasks\",\"t\":[\"For an ESPNet ASR or TTS dataset like below:\",\"data `-- et05_real_isolated_6ch_track |-- spk2utt |-- text |-- utt2spk |-- utt2uniq `-- wav.scp\",\"Run the following scripts to create an enhanced dataset:\",\"scripts/utils/enhance_dataset.sh \\\\ --spk_num 1 \\\\ --gpu_inference true \\\\ --inference_nj 4 \\\\ --fs 16k \\\\ --id_prefix \\\"\\\" \\\\ dump/raw/et05_real_isolated_6ch_track \\\\ data/et05_real_isolated_6ch_track_enh \\\\ exp/enh_train_enh_beamformer_mvdr_raw/valid.loss.best.pth\",\"The above script will generate a new directory data/et05_real_isolated_6ch_track_enh:\",\"data `-- et05_real_isolated_6ch_track_enh |-- spk2utt |-- text |-- utt2spk |-- utt2uniq |-- wav.scp `-- wavs/\",\"where wav.scp contains paths to the enhanced audios (stored in wavs/).\"]},\"2629\":{\"h\":\"Speech Separation\"},\"2630\":{\"h\":\"Model Selection\",\"t\":[\"In this demonstration, we will show different speech separation models on wsj0_2mix.\",\"The pretrained models can be download from direct URL, or from zenodo and huggingface with model ID.\",\"#@title Choose Speech Separation model { run: \\\"auto\\\" } fs = 8000 #@param {type:\\\"integer\\\"} tag = \\\"espnet/Chenda_Li_wsj0_2mix_enh_train_enh_conv_tasnet_raw_valid.si_snr.ave\\\" #@param [\\\"Chenda Li/wsj0_2mix_enh_train_enh_conv_tasnet_raw_valid.si_snr.ave\\\", \\\"Chenda Li/wsj0_2mix_enh_train_enh_rnn_tf_raw_valid.si_snr.ave\\\", \\\"https://zenodo.org/record/4688000/files/enh_train_enh_dprnn_tasnet_raw_valid.si_snr.ave.zip\\\", \\\"espnet/Chenda_Li_wsj0_2mix_enh_train_enh_conv_tasnet_raw_valid.si_snr.ave\\\"]\",\"# For models uploaded to Zenodo, you can use the following python script instead: import sys import soundfile from espnet_model_zoo.downloader import ModelDownloader from espnet2.bin.enh_inference import SeparateSpeech d = ModelDownloader() cfg = d.download_and_unpack(tag) separate_speech = SeparateSpeech( enh_train_config=cfg[\\\"train_config\\\"], enh_model_file=cfg[\\\"model_file\\\"], # for segment-wise process on long speech segment_size=2.4, hop_size=0.8, normalize_segment_scale=False, show_progressbar=True, ref_channel=None, normalize_output_wav=True, device=\\\"cuda:0\\\", )\"]},\"2631\":{\"h\":\"Separate Speech Mixture\"},\"2632\":{\"h\":\"Separate the example in wsj0_2mix testing set\",\"t\":[\"!gdown --id 1ZCUkd_Lb7pO2rpPr4FqYdtJBZ7JMiInx -O /content/447c020t_1.2106_422a0112_-1.2106.wav import os import soundfile from IPython.display import display, Audio mixwav, sr = soundfile.read(\\\"447c020t_1.2106_422a0112_-1.2106.wav\\\") waves_wsj = separate_speech(mixwav[None, ...], fs=sr) print(\\\"Input mixture\\\", flush=True) display(Audio(mixwav, rate=sr)) print(f\\\"========= Separated speech with model {tag} =========\\\", flush=True) print(\\\"Separated spk1\\\", flush=True) display(Audio(waves_wsj[0].squeeze(), rate=sr)) print(\\\"Separated spk2\\\", flush=True) display(Audio(waves_wsj[1].squeeze(), rate=sr))\"]},\"2633\":{\"h\":\"Separate your own recordings\",\"t\":[\"from google.colab import files from IPython.display import display, Audio import soundfile uploaded = files.upload() for file_name in uploaded.keys(): mixwav_yours, rate = soundfile.read(file_name) assert rate == sr, \\\"mismatch in sampling rate\\\" waves_yours = separate_speech(mixwav_yours[None, ...], fs=sr) print(\\\"Input mixture\\\", flush=True) display(Audio(mixwav_yours, rate=sr)) print(f\\\"========= Separated speech with model {tag} =========\\\", flush=True) print(\\\"Separated spk1\\\", flush=True) display(Audio(waves_yours[0].squeeze(), rate=sr)) print(\\\"Separated spk2\\\", flush=True) display(Audio(waves_yours[1].squeeze(), rate=sr))\"]},\"2634\":{\"h\":\"Show spectrums of separated speech\",\"t\":[\"import matplotlib.pyplot as plt import torch from torch_complex.tensor import ComplexTensor from espnet.asr.asr_utils import plot_spectrogram from espnet2.layers.stft import Stft stft = Stft( n_fft=512, win_length=None, hop_length=128, window=\\\"hann\\\", ) ilens = torch.LongTensor([len(mixwav)]) # specs: (T, F) spec_mix = ComplexTensor( *torch.unbind( stft(torch.as_tensor(mixwav).unsqueeze(0), ilens)[0].squeeze(), dim=-1 ) ) spec_sep1 = ComplexTensor( *torch.unbind( stft(torch.as_tensor(waves_wsj[0]), ilens)[0].squeeze(), dim=-1 ) ) spec_sep2 = ComplexTensor( *torch.unbind( stft(torch.as_tensor(waves_wsj[1]), ilens)[0].squeeze(), dim=-1 ) ) # freqs = torch.linspace(0, sr / 2, spec_mix.shape[1]) # frames = torch.linspace(0, len(mixwav) / sr, spec_mix.shape[0]) samples = torch.linspace(0, len(mixwav) / sr, len(mixwav)) plt.figure(figsize=(24, 12)) plt.subplot(3, 2, 1) plt.title('Mixture Spectrogram') plot_spectrogram( plt, abs(spec_mix).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 2) plt.title('Mixture Wavform') plt.plot(samples, mixwav) plt.xlim(0, len(mixwav) / sr) plt.subplot(3, 2, 3) plt.title('Separated Spectrogram (spk1)') plot_spectrogram( plt, abs(spec_sep1).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 4) plt.title('Separated Wavform (spk1)') plt.plot(samples, waves_wsj[0].squeeze()) plt.xlim(0, len(mixwav) / sr) plt.subplot(3, 2, 5) plt.title('Separated Spectrogram (spk2)') plot_spectrogram( plt, abs(spec_sep2).transpose(-1, -2).numpy(), fs=sr, mode='db', frame_shift=None, bottom=False, labelbottom=False ) plt.subplot(3, 2, 6) plt.title('Separated Wavform (spk2)') plt.plot(samples, waves_wsj[1].squeeze()) plt.xlim(0, len(mixwav) / sr) plt.xlabel(\\\"Time (s)\\\") plt.show()\"]},\"2635\":{\"h\":\"Evluate separated speech with pretrained ASR model\",\"t\":[\"The ground truths are:\",\"text_1: SOME CRITICS INCLUDING HIGH REAGAN ADMINISTRATION OFFICIALS ARE RAISING THE ALARM THAT THE FED'S POLICY IS TOO TIGHT AND COULD CAUSE A RECESSION NEXT YEAR\",\"text_2: THE UNITED STATES UNDERTOOK TO DEFEND WESTERN EUROPE AGAINST SOVIET ATTACK\",\"(This may take a while for the speech recognition.)\",\"%pip install -q https://github.com/kpu/kenlm/archive/master.zip # ASR need kenlm\",\"import espnet_model_zoo from espnet_model_zoo.downloader import ModelDownloader from espnet2.bin.asr_inference import Speech2Text wsj_8k_model_url=\\\"https://zenodo.org/record/4012264/files/asr_train_asr_transformer_raw_char_1gpu_valid.acc.ave.zip?download=1\\\" d = ModelDownloader() speech2text = Speech2Text( **d.download_and_unpack(wsj_8k_model_url), device=\\\"cuda:0\\\", ) text_est = [None, None] text_est[0], *_ = speech2text(waves_wsj[0].squeeze())[0] text_est[1], *_ = speech2text(waves_wsj[1].squeeze())[0] text_m, *_ = speech2text(mixwav)[0] print(\\\"Mix Speech to Text: \\\", text_m) print(\\\"Separated Speech 1 to Text: \\\", text_est[0]) print(\\\"Separated Speech 2 to Text: \\\", text_est[1])\",\"import difflib from itertools import permutations import editdistance import numpy as np colors = dict( red=lambda text: f\\\"\\\\033[38;2;255;0;0m{text}\\\\033[0m\\\" if text else \\\"\\\", green=lambda text: f\\\"\\\\033[38;2;0;255;0m{text}\\\\033[0m\\\" if text else \\\"\\\", yellow=lambda text: f\\\"\\\\033[38;2;225;225;0m{text}\\\\033[0m\\\" if text else \\\"\\\", white=lambda text: f\\\"\\\\033[38;2;255;255;255m{text}\\\\033[0m\\\" if text else \\\"\\\", black=lambda text: f\\\"\\\\033[38;2;0;0;0m{text}\\\\033[0m\\\" if text else \\\"\\\", ) def diff_strings(ref, est): \\\"\\\"\\\"Reference: https://stackoverflow.com/a/64404008/7384873\\\"\\\"\\\" ref_str, est_str, err_str = [], [], [] matcher = difflib.SequenceMatcher(None, ref, est) for opcode, a0, a1, b0, b1 in matcher.get_opcodes(): if opcode == \\\"equal\\\": txt = ref[a0:a1] ref_str.append(txt) est_str.append(txt) err_str.append(\\\" \\\" * (a1 - a0)) elif opcode == \\\"insert\\\": ref_str.append(\\\"*\\\" * (b1 - b0)) est_str.append(colors[\\\"green\\\"](est[b0:b1])) err_str.append(colors[\\\"black\\\"](\\\"I\\\" * (b1 - b0))) elif opcode == \\\"delete\\\": ref_str.append(ref[a0:a1]) est_str.append(colors[\\\"red\\\"](\\\"*\\\" * (a1 - a0))) err_str.append(colors[\\\"black\\\"](\\\"D\\\" * (a1 - a0))) elif opcode == \\\"replace\\\": diff = a1 - a0 - b1 + b0 if diff >= 0: txt_ref = ref[a0:a1] txt_est = colors[\\\"yellow\\\"](est[b0:b1]) + colors[\\\"red\\\"](\\\"*\\\" * diff) txt_err = \\\"S\\\" * (b1 - b0) + \\\"D\\\" * diff elif diff < 0: txt_ref = ref[a0:a1] + \\\"*\\\" * -diff txt_est = colors[\\\"yellow\\\"](est[b0:b1]) + colors[\\\"green\\\"](\\\"*\\\" * -diff) txt_err = \\\"S\\\" * (b1 - b0) + \\\"I\\\" * -diff ref_str.append(txt_ref) est_str.append(txt_est) err_str.append(colors[\\\"black\\\"](txt_err)) return \\\"\\\".join(ref_str), \\\"\\\".join(est_str), \\\"\\\".join(err_str) text_ref = [ \\\"SOME CRITICS INCLUDING HIGH REAGAN ADMINISTRATION OFFICIALS ARE RAISING THE ALARM THAT THE FED'S POLICY IS TOO TIGHT AND COULD CAUSE A RECESSION NEXT YEAR\\\", \\\"THE UNITED STATES UNDERTOOK TO DEFEND WESTERN EUROPE AGAINST SOVIET ATTACK\\\", ] print(\\\"=====================\\\" , flush=True) perms = list(permutations(range(2))) string_edit = [ [ editdistance.eval(text_ref[m], text_est[n]) for m, n in enumerate(p) ] for p in perms ] dist = [sum(edist) for edist in string_edit] perm_idx = np.argmin(dist) perm = perms[perm_idx] for i, p in enumerate(perm): print(\\\"\\\\n--------------- Text %d ---------------\\\" % (i + 1), flush=True) ref, est, err = diff_strings(text_ref[i], text_est[p]) print(\\\"REF: \\\" + ref + \\\"\\\\n\\\" + \\\"HYP: \\\" + est + \\\"\\\\n\\\" + \\\"ERR: \\\" + err, flush=True) print(\\\"Edit Distance = {}\\\\n\\\".format(string_edit[perm_idx][i]), flush=True)\",\"If you would like to contribute to the ESPnet-SE project, or if you would like to make modifications based on the current speech enhancement/separation functionality, the following tutorials will provide you detailed information about how to creating new recipes or new models in ESPnet-SE.\"]},\"2636\":{\"h\":\"Creating a New Recipe\"},\"2637\":{\"h\":\"Step 1 Create recipe directory\",\"t\":[\"First, run the following command to create the directory for the new recipe from our template:\",\"egs2/TEMPLATE/enh1/setup.sh egs2/&lt;your-recipe-name&gt;/enh1\",\"For the following steps, we assume the operations are done under the directory egs2/&lt;your-recipe-name&gt;/enh1/.\"]},\"2638\":{\"h\":\"Step 2 Write scripts for data preparation\",\"t\":[\"Prepare local/data.sh, which will be used in stage 1 in enh.sh. It can take some arguments as input, see egs2/wsj0_2mix/enh1/local/data.sh for reference.\",\"The script local/data.sh should finally generate Kaldi-style data directories under &lt;recipe-dir&gt;/data/. Each subset directory should contains at least 4 files:\",\"&lt;recipe-dir&gt;/data/&lt;subset-name&gt;/ ├── spk{1,2,3...}.scp (clean speech references) ├── spk2utt ├── utt2spk └── wav.scp (noisy speech)\",\"Optionally, it can also contain noise{}.scp and dereverb{}.scp, which point to the corresponding noise and dereverberated references respectively. {} can be 1, 2, ..., depending on the number of noise types (dereverberated signals) in the input signal in wav.scp.\",\"Make sure to sort the scp and other related files as in Kaldi. Also, remember to run . ./path.sh in local/data.sh before sorting, because it will force sorting to be byte-wise, i.e. export LC_ALL=C.\",\"Remember to check your new scripts with shellcheck, otherwise they may fail the tests in ci/test_shell.sh.\"]},\"2639\":{\"h\":\"Step 3 Prepare training configuration\",\"t\":[\"Prepare training configuration files (e.g. train.yaml) under conf/.\",\"If you have multiple configuration files, it is recommended to put them under conf/tuning/, and create a symbolic link conf/tuning/train.yaml pointing to the config file with the best performance.\"]},\"2640\":{\"h\":\"Step 4 Prepare run.sh\",\"t\":[\"Write run.sh to provide a template entry script, so that users can easily run your recipe by ./run.sh. See egs2/wsj0_2mix/enh1/run.sh for reference.\",\"If your recipes provide references for noise and/or dereverberation, you can add the argument --use_noise_ref true and/or --use_dereverb_ref true in run.sh.\"]},\"2641\":{\"h\":\"Implementing a New Speech Enhancement/Separation Model\",\"t\":[\"The current ESPnet-SE tool adopts an encoder-separator-decoder architecture for all models, e.g.\",\"For Time-Frequency masking models, the encoder and decoder would be stft_encoder.py and stft_decoder.py respectively, and the separator can be any of dprnn_separator.py, rnn_separator.py, tcn_separator.py, and transformer_separator.py. For TasNet, the encoder and decoder are conv_encoder.py and conv_decoder.py respectively. The separator is tcn_separator.py.\"]},\"2642\":{\"h\":\"Step 1 Create model scripts\",\"t\":[\"For encoder, separator, and decoder models, create new scripts under espnet2/enh/encoder/, espnet2/enh/separator/, and espnet2/enh/decoder/, respectively.\",\"For a separator model, please make sure it implements the num_spk property. See espnet2/enh/separator/rnn_separator.py for reference.\",\"Remember to format your new scripts to match the styles in black and flake8, otherwise they may fail the tests in ci/test_python.sh.\"]},\"2643\":{\"h\":\"Step 2 Add the new model to related scripts\",\"t\":[\"In espnet2/tasks/enh.py, add your new model to the corresponding ClassChoices, e.g.\",\"For encoders, add &lt;key&gt;=&lt;your-model&gt; to encoder_choices.\",\"For decoders, add &lt;key&gt;=&lt;your-model&gt; to decoder_choices.\",\"For separators, add &lt;key&gt;=&lt;your-model&gt; to separator_choices.\"]},\"2644\":{\"h\":\"Step 3 [Optional] Create new loss functions\",\"t\":[\"If you want to use a new loss function for your model, you can add it to espnet2/enh/espnet_model.py, such as:\",\" @staticmethod def new_loss(ref, inf): \\\"\\\"\\\"Your new loss Args: ref: (Batch, samples) inf: (Batch, samples) Returns: loss: (Batch,) \\\"\\\"\\\" ... return loss\",\"Then add your loss name to ALL_LOSS_TYPES, and handle the loss calculation in _compute_loss.\"]},\"2645\":{\"h\":\"Step 4 Create unit tests for the new model\",\"t\":[\"Finally, it would be nice to make some unit tests for your new model under test/espnet2/enh/encoder, test/espnet2/enh/decoder, or test/espnet2/enh/separator.\"]},\"2646\":{\"h\":\"ESPNET 2 pass SLU Demonstration\",\"t\":[\"This notebook provides a demonstration of the Two Pass End-to-End Spoken Language Understanding model\",\"Paper Link: https://arxiv.org/abs/2207.06670\",\"ESPnet2-SLU: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/slu1\",\"Author: Siddhant Arora\",\"! python -m pip install transformers !git clone https://github.com/espnet/espnet /espnet !pip install /espnet %pip install -q espnet_model_zoo %pip install fairseq@git+https://github.com//pytorch/fairseq.git@f2146bdc7abf293186de9449bfa2272775e39e1d#egg=fairseq\"]},\"2647\":{\"h\":\"Download Audio File\",\"t\":[\"# !gdown --id 1LxoxCoFgx3u8CvKb1loybGFtArKKPcAH -O /content/audio_file.wav !gdown --id 18ANT62ittt7Ai2E8bQRlvT0ZVXXsf1eE -O /content/audio_file.wav\",\"import os import soundfile from IPython.display import display, Audio mixwav_mc, sr = soundfile.read(\\\"/content/audio_file.wav\\\") display(Audio(mixwav_mc.T, rate=sr))\"]},\"2648\":{\"h\":\"Download and Load pretrained First Pass Model\",\"t\":[\"!git lfs clone https://huggingface.co/espnet/siddhana_slurp_new_asr_train_asr_conformer_raw_en_word_valid.acc.ave_10best /content/slurp_first_pass_model\",\"from espnet2.bin.asr_inference import Speech2Text speech2text_slurp = Speech2Text.from_pretrained( asr_train_config=\\\"/content/slurp_first_pass_model/exp/asr_train_asr_conformer_raw_en_word/config.yaml\\\", asr_model_file=\\\"/content/slurp_first_pass_model/exp/asr_train_asr_conformer_raw_en_word/valid.acc.ave_10best.pth\\\", nbest=1, )\",\"nbests_orig = speech2text_slurp(mixwav_mc) text, *_ = nbests_orig[0] def text_normalizer(sub_word_transcript): transcript = sub_word_transcript[0].replace(\\\"▁\\\", \\\"\\\") for sub_word in sub_word_transcript[1:]: if \\\"▁\\\" in sub_word: transcript = transcript + \\\" \\\" + sub_word.replace(\\\"▁\\\", \\\"\\\") else: transcript = transcript + sub_word return transcript intent_text=\\\"{scenario: \\\"+text.split()[0].split(\\\"_\\\")[0]+\\\", action: \\\"+\\\"_\\\".join(text.split()[0].split(\\\"_\\\")[1:])+\\\"}\\\" print(f\\\"INTENT: {intent_text}\\\") transcript=text_normalizer(text.split()[1:]) print(f\\\"ASR hypothesis: {transcript}\\\") print(f\\\"First pass SLU model fails to predict the correct action.\\\")\"]},\"2649\":{\"h\":\"Download and Load pretrained Second Pass Model\",\"t\":[\"!git lfs clone https://huggingface.co/espnet/slurp_slu_2pass /content/slurp_second_pass_model\",\"from espnet2.bin.slu_inference import Speech2Understand from transformers import AutoModel, AutoTokenizer speech2text_second_pass_slurp = Speech2Understand.from_pretrained( slu_train_config=\\\"/content/slurp_second_pass_model/exp/slu_train_asr_bert_conformer_deliberation_raw_en_word/config.yaml\\\", slu_model_file=\\\"/content/slurp_second_pass_model/exp/slu_train_asr_bert_conformer_deliberation_raw_en_word/valid.acc.ave_10best.pth\\\", nbest=1, )\",\"from espnet2.tasks.slu import SLUTask preprocess_fn=SLUTask.build_preprocess_fn( speech2text_second_pass_slurp.asr_train_args, False )\",\"import numpy as np transcript = preprocess_fn.text_cleaner(transcript) tokens = preprocess_fn.transcript_tokenizer.text2tokens(transcript) text_ints = np.array(preprocess_fn.transcript_token_id_converter.tokens2ids(tokens), dtype=np.int64)\",\"import torch nbests = speech2text_second_pass_slurp(mixwav_mc,torch.tensor(text_ints)) text1, *_ = nbests[0] intent_text=\\\"{scenario: \\\"+text1.split()[0].split(\\\"_\\\")[0]+\\\", action: \\\"+\\\"_\\\".join(text1.split()[0].split(\\\"_\\\")[1:])+\\\"}\\\" print(f\\\"INTENT: {intent_text}\\\") transcript=text_normalizer(text1.split()[1:]) print(f\\\"ASR hypothesis: {transcript}\\\") print(f\\\"Second pass SLU model successfully recognizes the correct action.\\\")\"]},\"2650\":{\"h\":\"ESPnet2-TTS realtime demonstration\",\"t\":[\"Open In Colab\",\"This notebook provides a demonstration of the realtime E2E-TTS using ESPnet2-TTS and ParallelWaveGAN repo.\",\"ESPnet2-TTS: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1\",\"ParallelWaveGAN: https://github.com/kan-bayashi/ParallelWaveGAN\",\"Author: Tomoki Hayashi (@kan-bayashi)\"]},\"2651\":{\"h\":\"Installation\",\"t\":[\"# NOTE: pip shows imcompatible errors due to preinstalled libraries but you do not need to care !pip install -q espnet==202308 pypinyin==0.44.0 parallel_wavegan==0.5.4 gdown==4.4.0 espnet_model_zoo\"]},\"2652\":{\"h\":\"Single speaker model demo\"},\"2653\":{\"h\":\"Model Selection\",\"t\":[\"Please select model: English, Japanese, and Mandarin are supported.\",\"You can try end-to-end text2wav model & combination of text2mel and vocoder. If you use text2wav model, you do not need to use vocoder (automatically disabled).\",\"Text2wav models:\",\"VITS\",\"Text2mel models:\",\"Tacotron2\",\"Transformer-TTS\",\"(Conformer) FastSpeech\",\"(Conformer) FastSpeech2\",\"Vocoders:\",\"Parallel WaveGAN\",\"Multi-band MelGAN\",\"HiFiGAN\",\"Style MelGAN.\",\"The terms of use follow that of each corpus. We use the following corpora:\",\"ljspeech_*: LJSpeech dataset \",\"https://keithito.com/LJ-Speech-Dataset/\",\"jsut_*: JSUT corpus \",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"jvs_*: JVS corpus + JSUT corpus \",\"https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus\",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"tsukuyomi_*: つくよみちゃんコーパス + JSUT corpus \",\"https://tyc.rei-yumesaki.net/material/corpus/\",\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\",\"csmsc_*: Chinese Standard Mandarin Speech Corpus \",\"https://www.data-baker.com/open_source.html\",\"#@title Choose English model { run: \\\"auto\\\" } lang = 'English' tag = 'kan-bayashi/ljspeech_vits' #@param [\\\"kan-bayashi/ljspeech_tacotron2\\\", \\\"kan-bayashi/ljspeech_fastspeech\\\", \\\"kan-bayashi/ljspeech_fastspeech2\\\", \\\"kan-bayashi/ljspeech_conformer_fastspeech2\\\", \\\"kan-bayashi/ljspeech_joint_finetune_conformer_fastspeech2_hifigan\\\", \\\"kan-bayashi/ljspeech_joint_train_conformer_fastspeech2_hifigan\\\", \\\"kan-bayashi/ljspeech_vits\\\"] {type:\\\"string\\\"} vocoder_tag = \\\"none\\\" #@param [\\\"none\\\", \\\"parallel_wavegan/ljspeech_parallel_wavegan.v1\\\", \\\"parallel_wavegan/ljspeech_full_band_melgan.v2\\\", \\\"parallel_wavegan/ljspeech_multi_band_melgan.v2\\\", \\\"parallel_wavegan/ljspeech_hifigan.v1\\\", \\\"parallel_wavegan/ljspeech_style_melgan.v1\\\"] {type:\\\"string\\\"}\",\"#@title Choose Japanese model { run: \\\"auto\\\" } lang = 'Japanese' tag = 'kan-bayashi/jsut_full_band_vits_prosody' #@param [\\\"kan-bayashi/jsut_tacotron2\\\", \\\"kan-bayashi/jsut_transformer\\\", \\\"kan-bayashi/jsut_fastspeech\\\", \\\"kan-bayashi/jsut_fastspeech2\\\", \\\"kan-bayashi/jsut_conformer_fastspeech2\\\", \\\"kan-bayashi/jsut_conformer_fastspeech2_accent\\\", \\\"kan-bayashi/jsut_conformer_fastspeech2_accent_with_pause\\\", \\\"kan-bayashi/jsut_vits_accent_with_pause\\\", \\\"kan-bayashi/jsut_full_band_vits_accent_with_pause\\\", \\\"kan-bayashi/jsut_tacotron2_prosody\\\", \\\"kan-bayashi/jsut_transformer_prosody\\\", \\\"kan-bayashi/jsut_conformer_fastspeech2_tacotron2_prosody\\\", \\\"kan-bayashi/jsut_vits_prosody\\\", \\\"kan-bayashi/jsut_full_band_vits_prosody\\\", \\\"kan-bayashi/jvs_jvs010_vits_prosody\\\", \\\"kan-bayashi/tsukuyomi_full_band_vits_prosody\\\"] {type:\\\"string\\\"} vocoder_tag = 'none' #@param [\\\"none\\\", \\\"parallel_wavegan/jsut_parallel_wavegan.v1\\\", \\\"parallel_wavegan/jsut_multi_band_melgan.v2\\\", \\\"parallel_wavegan/jsut_style_melgan.v1\\\", \\\"parallel_wavegan/jsut_hifigan.v1\\\"] {type:\\\"string\\\"}\",\"#@title Choose Mandarin model { run: \\\"auto\\\" } lang = 'Mandarin' tag = 'kan-bayashi/csmsc_full_band_vits' #@param [\\\"kan-bayashi/csmsc_tacotron2\\\", \\\"kan-bayashi/csmsc_transformer\\\", \\\"kan-bayashi/csmsc_fastspeech\\\", \\\"kan-bayashi/csmsc_fastspeech2\\\", \\\"kan-bayashi/csmsc_conformer_fastspeech2\\\", \\\"kan-bayashi/csmsc_vits\\\", \\\"kan-bayashi/csmsc_full_band_vits\\\"] {type: \\\"string\\\"} vocoder_tag = \\\"none\\\" #@param [\\\"none\\\", \\\"parallel_wavegan/csmsc_parallel_wavegan.v1\\\", \\\"parallel_wavegan/csmsc_multi_band_melgan.v2\\\", \\\"parallel_wavegan/csmsc_hifigan.v1\\\", \\\"parallel_wavegan/csmsc_style_melgan.v1\\\"] {type:\\\"string\\\"}\"]},\"2654\":{\"h\":\"Model Setup\",\"t\":[\"from espnet2.bin.tts_inference import Text2Speech from espnet2.utils.types import str_or_none text2speech = Text2Speech.from_pretrained( model_tag=str_or_none(tag), vocoder_tag=str_or_none(vocoder_tag), device=\\\"cuda\\\", # Only for Tacotron 2 & Transformer threshold=0.5, # Only for Tacotron 2 minlenratio=0.0, maxlenratio=10.0, use_att_constraint=False, backward_window=1, forward_window=3, # Only for FastSpeech & FastSpeech2 & VITS speed_control_alpha=1.0, # Only for VITS noise_scale=0.333, noise_scale_dur=0.333, )\"]},\"2655\":{\"h\":\"Synthesis\",\"t\":[\"import time import torch # decide the input sentence by yourself print(f\\\"Input your favorite sentence in {lang}.\\\") x = input() # synthesis with torch.no_grad(): start = time.time() wav = text2speech(x)[\\\"wav\\\"] rtf = (time.time() - start) / (len(wav) / text2speech.fs) print(f\\\"RTF = {rtf:5f}\\\") # let us listen to generated samples from IPython.display import display, Audio display(Audio(wav.view(-1).cpu().numpy(), rate=text2speech.fs))\"]},\"2656\":{\"h\":\"Multi-speaker Model Demo\"},\"2657\":{\"h\":\"Model Selection\",\"t\":[\"Now we provide only English multi-speaker pretrained model.\",\"The terms of use follow that of each corpus. We use the following corpora:\",\"libritts_*: LibriTTS corpus \",\"http://www.openslr.org/60\",\"vctk_*: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit \",\"http://www.udialogue.org/download/cstr-vctk-corpus.html\",\"#@title English multi-speaker pretrained model { run: \\\"auto\\\" } lang = 'English' tag = 'kan-bayashi/vctk_full_band_multi_spk_vits' #@param [\\\"kan-bayashi/vctk_gst_tacotron2\\\", \\\"kan-bayashi/vctk_gst_transformer\\\", \\\"kan-bayashi/vctk_xvector_tacotron2\\\", \\\"kan-bayashi/vctk_xvector_transformer\\\", \\\"kan-bayashi/vctk_xvector_conformer_fastspeech2\\\", \\\"kan-bayashi/vctk_gst+xvector_tacotron2\\\", \\\"kan-bayashi/vctk_gst+xvector_transformer\\\", \\\"kan-bayashi/vctk_gst+xvector_conformer_fastspeech2\\\", \\\"kan-bayashi/vctk_multi_spk_vits\\\", \\\"kan-bayashi/vctk_full_band_multi_spk_vits\\\", \\\"kan-bayashi/libritts_xvector_transformer\\\", \\\"kan-bayashi/libritts_xvector_conformer_fastspeech2\\\", \\\"kan-bayashi/libritts_gst+xvector_transformer\\\", \\\"kan-bayashi/libritts_gst+xvector_conformer_fastspeech2\\\", \\\"kan-bayashi/libritts_xvector_vits\\\"] {type:\\\"string\\\"} vocoder_tag = \\\"none\\\" #@param [\\\"none\\\", \\\"parallel_wavegan/vctk_parallel_wavegan.v1.long\\\", \\\"parallel_wavegan/vctk_multi_band_melgan.v2\\\", \\\"parallel_wavegan/vctk_style_melgan.v1\\\", \\\"parallel_wavegan/vctk_hifigan.v1\\\", \\\"parallel_wavegan/libritts_parallel_wavegan.v1.long\\\", \\\"parallel_wavegan/libritts_multi_band_melgan.v2\\\", \\\"parallel_wavegan/libritts_hifigan.v1\\\", \\\"parallel_wavegan/libritts_style_melgan.v1\\\"] {type:\\\"string\\\"}\"]},\"2658\":{\"h\":\"Model Setup\",\"t\":[\"from espnet2.bin.tts_inference import Text2Speech from espnet2.utils.types import str_or_none text2speech = Text2Speech.from_pretrained( model_tag=str_or_none(tag), vocoder_tag=str_or_none(vocoder_tag), device=\\\"cuda\\\", # Only for Tacotron 2 & Transformer threshold=0.5, # Only for Tacotron 2 minlenratio=0.0, maxlenratio=10.0, use_att_constraint=False, backward_window=1, forward_window=3, # Only for FastSpeech & FastSpeech2 & VITS speed_control_alpha=1.0, # Only for VITS noise_scale=0.333, noise_scale_dur=0.333, )\"]},\"2659\":{\"h\":\"Speaker selection\",\"t\":[\"For multi-speaker model, we need to provide X-vector and/or the reference speech to decide the speaker characteristics. For X-vector, you can select the speaker from the dumped x-vectors. For the reference speech, you can use any speech but please make sure the sampling rate is matched.\",\"import glob import os import numpy as np import kaldiio # Get model directory path from espnet_model_zoo.downloader import ModelDownloader d = ModelDownloader() model_dir = os.path.dirname(d.download_and_unpack(tag)[\\\"train_config\\\"]) # X-vector selection spembs = None if text2speech.use_spembs: xvector_ark = [p for p in glob.glob(f\\\"{model_dir}/../../dump/**/spk_xvector.ark\\\", recursive=True) if \\\"tr\\\" in p][0] xvectors = {k: v for k, v in kaldiio.load_ark(xvector_ark)} spks = list(xvectors.keys()) # randomly select speaker random_spk_idx = np.random.randint(0, len(spks)) spk = spks[random_spk_idx] spembs = xvectors[spk] print(f\\\"selected spk: {spk}\\\") # Speaker ID selection sids = None if text2speech.use_sids: spk2sid = glob.glob(f\\\"{model_dir}/../../dump/**/spk2sid\\\", recursive=True)[0] with open(spk2sid) as f: lines = [line.strip() for line in f.readlines()] sid2spk = {int(line.split()[1]): line.split()[0] for line in lines} # randomly select speaker sids = np.array(np.random.randint(1, len(sid2spk))) spk = sid2spk[int(sids)] print(f\\\"selected spk: {spk}\\\") # Reference speech selection for GST speech = None if text2speech.use_speech: # you can change here to load your own reference speech # e.g. # import soundfile as sf # speech, fs = sf.read(\\\"/path/to/reference.wav\\\") # speech = torch.from_numpy(speech).float() speech = torch.randn(50000,) * 0.01\"]},\"2660\":{\"h\":\"Synthesis\",\"t\":[\"import time import torch # decide the input sentence by yourself print(f\\\"Input your favorite sentence in {lang}.\\\") x = input() # synthesis with torch.no_grad(): start = time.time() wav = text2speech(x, speech=speech, spembs=spembs, sids=sids)[\\\"wav\\\"] rtf = (time.time() - start) / (len(wav) / text2speech.fs) print(f\\\"RTF = {rtf:5f}\\\") # let us listen to generated samples from IPython.display import display, Audio display(Audio(wav.view(-1).cpu().numpy(), rate=text2speech.fs))\"]},\"2661\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"2662\":{\"h\":\"Espnet Bin\"},\"2663\":{\"h\":\"Tools\"},\"2664\":{\"h\":\"Spm\"},\"2665\":{\"h\":\"Utils\"},\"2666\":{\"h\":\"Espnet2 Bin\"},\"2667\":{\"h\":\"Utils Py\"},\"2668\":{\"h\":\"Distributed\"},\"2669\":{\"h\":\"Espnet\"},\"2670\":{\"h\":\"Guide\"},\"2671\":{\"h\":\"Mt\"},\"2672\":{\"h\":\"Lm\"},\"2673\":{\"h\":\"Asr\"},\"2674\":{\"h\":\"Optimizer\"},\"2675\":{\"h\":\"Scheduler\"},\"2676\":{\"h\":\"Nets\"},\"2677\":{\"h\":\"St\"},\"2678\":{\"h\":\"Transform\"},\"2679\":{\"h\":\"Tts\"},\"2680\":{\"h\":\"Utils\"},\"2681\":{\"h\":\"Vc\"},\"2682\":{\"h\":\"Asr Transducer\"},\"2683\":{\"h\":\"Espnet2\"},\"2684\":{\"h\":\"Asvspoof\"},\"2685\":{\"h\":\"Asr\"},\"2686\":{\"h\":\"Diar\"},\"2687\":{\"h\":\"Fileio\"},\"2688\":{\"h\":\"Fst\"},\"2689\":{\"h\":\"Enh\"},\"2690\":{\"h\":\"Gan Svs\"},\"2691\":{\"h\":\"Gan Tts\"},\"2692\":{\"h\":\"Hubert\"},\"2693\":{\"h\":\"Iterators\"},\"2694\":{\"h\":\"Layers\"},\"2695\":{\"h\":\"Lm\"},\"2696\":{\"h\":\"Main Funcs\"},\"2697\":{\"h\":\"Mt\"},\"2698\":{\"h\":\"Optimizers\"},\"2699\":{\"h\":\"S2t\"},\"2700\":{\"h\":\"S2st\"},\"2701\":{\"h\":\"Samplers\"},\"2702\":{\"h\":\"Schedulers\"},\"2703\":{\"h\":\"Slu\"},\"2704\":{\"h\":\"Spk\"},\"2705\":{\"h\":\"St\"},\"2706\":{\"h\":\"Svs\"},\"2707\":{\"h\":\"Tasks\"},\"2708\":{\"h\":\"Text\"},\"2709\":{\"h\":\"Torch Utils\"},\"2710\":{\"h\":\"Train\"},\"2711\":{\"h\":\"Tts\"},\"2712\":{\"h\":\"Tts2\"},\"2713\":{\"h\":\"Uasr\"},\"2714\":{\"h\":\"Config\"},\"2715\":{\"h\":\"Espnetez\"},\"2716\":{\"h\":\"Utils\"},\"2717\":{\"h\":\"Data\"},\"2718\":{\"h\":\"Dataloader\"},\"2719\":{\"h\":\"Dataset\"},\"2720\":{\"h\":\"Preprocess\"},\"2721\":{\"h\":\"Trainer\"},\"2722\":{\"h\":\"Task\"},\"2723\":{\"h\":\"CMU Speech Recognition Fall2021\"},\"2724\":{\"h\":\"Course\"},\"2725\":{\"h\":\"CMU Speech Processing Spring2023\"},\"2726\":{\"h\":\"CMU Speech Recognition Fall2022\"},\"2727\":{\"h\":\"ASR\"},\"2728\":{\"h\":\"Demo\"},\"2729\":{\"h\":\"Others\"},\"2730\":{\"h\":\"SE\"},\"2731\":{\"h\":\"SLU\"},\"2732\":{\"h\":\"TTS\"}},\"dirtCount\":0,\"index\":[[\"└──\",{\"1\":{\"2638\":1}}],[\"├──\",{\"1\":{\"2638\":3}}],[\"❗checkpoint\",{\"1\":{\"2560\":1}}],[\"❗important\",{\"0\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"▁sep\",{\"1\":{\"2476\":3}}],[\"▁\",{\"1\":{\"2472\":3,\"2476\":3,\"2648\":3}}],[\"última\",{\"1\":{\"2457\":1}}],[\"👀\",{\"1\":{\"2433\":1}}],[\"📗\",{\"0\":{\"2432\":1,\"2438\":1,\"2441\":1,\"2542\":1,\"2543\":1,\"2564\":1,\"2565\":1},\"1\":{\"2422\":1,\"2525\":1,\"2545\":1}}],[\"⭕\",{\"0\":{\"2431\":1,\"2433\":1,\"2439\":1,\"2440\":1},\"1\":{\"2429\":1}}],[\"✅\",{\"0\":{\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2414\":1,\"2418\":1,\"2419\":1,\"2420\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2462\":1,\"2471\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2479\":1,\"2485\":1,\"2490\":1,\"2497\":1,\"2500\":1,\"2501\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2554\":1,\"2556\":1,\"2559\":1,\"2560\":1},\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"つくよみちゃんコーパス\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"こんにちは\",{\"1\":{\"2143\":1}}],[\"ᅡᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵ\",{\"1\":{\"2126\":1}}],[\"ᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ\",{\"1\":{\"2126\":1}}],[\"ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑ하ᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ\",{\"1\":{\"2126\":1}}],[\"ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑᄒ\",{\"1\":{\"2126\":1}}],[\"｜\",{\"1\":{\"2086\":1}}],[\"啊\",{\"1\":{\"1423\":1}}],[\"^t\",{\"1\":{\"1618\":1,\"1696\":1}}],[\"^\",{\"1\":{\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"1144\":1,\"1228\":1,\"1345\":1,\"1347\":1,\"1705\":1,\"1715\":1,\"2143\":1}}],[\"^virbr\",{\"1\":{\"45\":1}}],[\"~8\",{\"1\":{\"2372\":1,\"2584\":1}}],[\"~10\",{\"1\":{\"2372\":1,\"2375\":1,\"2440\":1}}],[\"~wang\",{\"1\":{\"1523\":1}}],[\"~torch\",{\"1\":{\"1049\":3,\"1050\":5,\"1054\":1,\"1056\":6,\"1065\":2,\"1068\":2,\"1070\":2,\"1074\":1}}],[\"~typing\",{\"1\":{\"698\":2,\"699\":2,\"1049\":1,\"1050\":1,\"1054\":1,\"1056\":1,\"1068\":2,\"1074\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1272\":1}}],[\"~\",{\"1\":{\"956\":1,\"973\":1,\"1145\":1}}],[\"~espnet\",{\"1\":{\"698\":1,\"699\":1}}],[\"~55\",{\"1\":{\"49\":1}}],[\"請用中文輸入您喜歡的句子\",{\"1\":{\"232\":1}}],[\"ü\",{\"1\":{\"231\":1}}],[\"日本語で好きな文章を入力してください\",{\"1\":{\"225\":1}}],[\"初回の辞書のインストールが必要です\",{\"1\":{\"224\":1}}],[\"<30\",{\"1\":{\"2194\":1}}],[\"<0\",{\"1\":{\"2194\":1}}],[\"<notimestamps>\",{\"1\":{\"2194\":1}}],[\"<na>\",{\"1\":{\"1400\":14,\"1975\":1,\"2194\":1}}],[\"<dst\",{\"1\":{\"2157\":1}}],[\"<file\",{\"1\":{\"2157\":1}}],[\"<file>\",{\"1\":{\"564\":1}}],[\"<unk>\",{\"1\":{\"2135\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2348\":1}}],[\"<exclude\",{\"1\":{\"2157\":1}}],[\"<espnet2\",{\"1\":{\"2096\":9,\"2097\":4,\"2098\":6,\"2100\":7,\"2101\":24,\"2102\":6,\"2103\":10,\"2104\":7,\"2105\":6,\"2107\":2,\"2108\":7,\"2109\":14,\"2110\":9,\"2111\":10,\"2112\":13,\"2113\":9,\"2114\":8,\"2115\":6,\"2116\":7,\"2117\":4,\"2118\":5}}],[\"<eos>\",{\"1\":{\"612\":1,\"852\":2,\"905\":2,\"1975\":1}}],[\"<generatespeech>\",{\"1\":{\"1955\":1}}],[\"<generatetext>\",{\"1\":{\"1955\":1}}],[\"<=\",{\"1\":{\"1082\":2,\"2022\":1,\"2023\":1}}],[\"<built\",{\"1\":{\"2106\":2}}],[\"<blank>\",{\"1\":{\"1057\":1,\"1171\":1,\"1172\":1,\"1206\":1,\"1892\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":2,\"2348\":1}}],[\"<br\",{\"1\":{\"940\":1,\"1257\":1,\"1398\":2,\"1705\":1,\"1707\":1,\"1735\":2,\"1905\":2,\"2022\":2,\"2023\":1,\"2086\":2,\"2087\":2}}],[\"<class\",{\"1\":{\"1049\":1,\"1050\":1,\"1054\":1,\"1056\":1,\"1065\":1,\"1068\":1,\"1070\":2,\"1074\":1,\"2099\":1}}],[\"<mask>\",{\"1\":{\"905\":3,\"1206\":1}}],[\"<s>\",{\"1\":{\"2294\":1}}],[\"<sil>\",{\"1\":{\"2294\":1}}],[\"<src\",{\"1\":{\"2157\":1}}],[\"<sc>\",{\"1\":{\"2128\":1,\"2129\":1}}],[\"<stdout>\",{\"1\":{\"2099\":1}}],[\"<sop>\",{\"1\":{\"1975\":1}}],[\"<sos\",{\"1\":{\"1171\":2,\"1955\":1,\"2076\":2,\"2348\":1}}],[\"<sos>\",{\"1\":{\"612\":1,\"852\":2,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1975\":1}}],[\"<space>\",{\"1\":{\"1057\":1,\"1171\":1,\"1172\":1,\"1206\":1,\"1892\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":2,\"2120\":1,\"2122\":1,\"2130\":1,\"2137\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1}}],[\"<shinjiw\",{\"1\":{\"198\":1}}],[\"<type>\",{\"1\":{\"564\":1}}],[\"<key>\",{\"1\":{\"564\":1}}],[\"<queue\",{\"1\":{\"294\":1}}],[\"<\",{\"1\":{\"203\":1,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"377\":1,\"380\":1,\"384\":1,\"391\":1,\"397\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"493\":1,\"496\":1,\"499\":1,\"501\":1,\"503\":1,\"506\":1,\"509\":1,\"512\":1,\"515\":1,\"517\":1,\"519\":1,\"522\":1,\"525\":1,\"528\":1,\"530\":1,\"533\":1,\"536\":1,\"538\":1,\"541\":1,\"544\":1,\"546\":1,\"549\":1,\"551\":1,\"554\":1,\"557\":1,\"560\":1,\"562\":1,\"564\":1,\"566\":1,\"568\":1,\"570\":1,\"572\":1,\"574\":1,\"576\":1,\"579\":1,\"582\":1,\"585\":1,\"588\":1,\"590\":1,\"600\":1,\"601\":1,\"602\":1,\"603\":1,\"604\":1,\"606\":1,\"607\":1,\"610\":1,\"614\":1,\"615\":1,\"616\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"636\":1,\"638\":1,\"641\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"659\":1,\"660\":1,\"662\":1,\"663\":1,\"664\":1,\"666\":1,\"667\":1,\"669\":1,\"670\":1,\"671\":1,\"673\":1,\"676\":1,\"697\":1,\"706\":1,\"742\":1,\"750\":1,\"767\":1,\"781\":1,\"788\":1,\"789\":1,\"790\":1,\"793\":1,\"812\":1,\"820\":1,\"829\":1,\"857\":1,\"867\":1,\"873\":1,\"874\":1,\"880\":1,\"890\":1,\"937\":1,\"938\":1,\"939\":1,\"941\":1,\"943\":1,\"944\":1,\"945\":1,\"948\":1,\"949\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"955\":1,\"956\":1,\"960\":1,\"962\":1,\"963\":1,\"964\":1,\"965\":1,\"966\":1,\"968\":1,\"969\":1,\"970\":1,\"972\":1,\"973\":1,\"977\":1,\"978\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"985\":1,\"986\":1,\"987\":1,\"989\":1,\"990\":1,\"991\":1,\"993\":1,\"994\":1,\"999\":1,\"1000\":1,\"1008\":1,\"1011\":3,\"1012\":1,\"1019\":1,\"1027\":1,\"1029\":1,\"1031\":1,\"1037\":1,\"1038\":1,\"1039\":1,\"1040\":1,\"1043\":1,\"1044\":1,\"1045\":1,\"1061\":3,\"1067\":1,\"1082\":1,\"1084\":2,\"1117\":1,\"1119\":1,\"1127\":1,\"1130\":1,\"1136\":1,\"1145\":1,\"1160\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1171\":1,\"1177\":1,\"1184\":1,\"1188\":1,\"1204\":1,\"1205\":1,\"1206\":1,\"1217\":1,\"1220\":1,\"1222\":1,\"1233\":1,\"1239\":1,\"1241\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1256\":1,\"1257\":1,\"1279\":1,\"1280\":1,\"1297\":1,\"1311\":1,\"1314\":1,\"1319\":1,\"1323\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1339\":1,\"1341\":1,\"1343\":1,\"1351\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1364\":1,\"1366\":1,\"1369\":1,\"1375\":1,\"1381\":1,\"1388\":1,\"1393\":1,\"1394\":1,\"1396\":1,\"1398\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1409\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1419\":1,\"1422\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1431\":1,\"1435\":1,\"1439\":1,\"1460\":1,\"1462\":1,\"1464\":2,\"1465\":1,\"1470\":1,\"1471\":1,\"1473\":1,\"1478\":1,\"1482\":1,\"1517\":1,\"1522\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1543\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1572\":1,\"1575\":1,\"1576\":1,\"1586\":1,\"1594\":1,\"1598\":1,\"1602\":1,\"1605\":1,\"1607\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1638\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1663\":1,\"1664\":1,\"1670\":1,\"1679\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1694\":1,\"1700\":1,\"1701\":1,\"1710\":1,\"1713\":1,\"1718\":1,\"1721\":1,\"1722\":1,\"1727\":1,\"1728\":1,\"1734\":1,\"1738\":1,\"1740\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1748\":1,\"1749\":1,\"1751\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1767\":1,\"1768\":1,\"1782\":1,\"1784\":1,\"1789\":1,\"1793\":1,\"1795\":1,\"1805\":1,\"1806\":1,\"1811\":1,\"1816\":1,\"1820\":1,\"1823\":1,\"1824\":1,\"1826\":1,\"1827\":1,\"1828\":1,\"1833\":1,\"1840\":1,\"1841\":1,\"1850\":2,\"1853\":1,\"1855\":1,\"1860\":1,\"1875\":1,\"1877\":1,\"1879\":1,\"1880\":1,\"1902\":1,\"1904\":1,\"1906\":1,\"1911\":1,\"1912\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1925\":1,\"1926\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1941\":1,\"1943\":1,\"1945\":1,\"1946\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1951\":1,\"1958\":1,\"1960\":1,\"1961\":1,\"1967\":1,\"1968\":1,\"1970\":1,\"1971\":1,\"1972\":1,\"1975\":1,\"1978\":1,\"1984\":1,\"1997\":1,\"2003\":1,\"2005\":1,\"2019\":1,\"2021\":1,\"2027\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2047\":1,\"2076\":1,\"2077\":1,\"2082\":1,\"2086\":1,\"2091\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2125\":1,\"2126\":1,\"2127\":1,\"2131\":1,\"2133\":1,\"2136\":1,\"2156\":1,\"2165\":1,\"2166\":1,\"2167\":1,\"2171\":1,\"2175\":1,\"2176\":1,\"2177\":1,\"2182\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2206\":1,\"2207\":1,\"2208\":1,\"2211\":1,\"2212\":1,\"2218\":1,\"2222\":1,\"2223\":1,\"2224\":1,\"2226\":1,\"2228\":1,\"2229\":1,\"2230\":1,\"2232\":1,\"2235\":1,\"2236\":1,\"2240\":1,\"2241\":1,\"2252\":1,\"2255\":1,\"2256\":1,\"2258\":1,\"2259\":1,\"2261\":1,\"2263\":1,\"2266\":1,\"2273\":1,\"2277\":1,\"2278\":1,\"2288\":1,\"2294\":2,\"2306\":1,\"2307\":1,\"2308\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2316\":1,\"2319\":1,\"2321\":1,\"2324\":1,\"2325\":1,\"2327\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2336\":1,\"2338\":1,\"2339\":1,\"2341\":1,\"2343\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2348\":1,\"2350\":1,\"2351\":1,\"2352\":1,\"2353\":1,\"2500\":1,\"2568\":1,\"2600\":1,\"2617\":1,\"2635\":1}}],[\"λ\",{\"1\":{\"118\":3}}],[\"q=tokenizer\",{\"1\":{\"2414\":1}}],[\"q3\",{\"1\":{\"2387\":1}}],[\"q2\",{\"1\":{\"2387\":1}}],[\"q1\",{\"1\":{\"2387\":1}}],[\"q^\",{\"1\":{\"1339\":1}}],[\"qmf\",{\"1\":{\"429\":4,\"1860\":1}}],[\"qq\",{\"1\":{\"200\":1}}],[\"q\",{\"1\":{\"144\":11,\"200\":3,\"207\":2,\"1069\":2,\"1076\":1,\"1339\":2,\"1578\":2,\"1579\":2,\"1580\":2,\"1660\":2,\"1661\":2,\"1662\":2,\"1670\":1,\"1671\":1,\"1853\":2,\"1854\":4,\"1905\":3,\"1921\":4,\"1922\":3,\"1936\":3,\"1938\":3,\"1939\":3,\"2253\":1,\"2355\":1,\"2362\":1,\"2450\":1,\"2466\":1,\"2482\":2,\"2499\":1,\"2517\":1,\"2576\":2,\"2602\":2,\"2619\":2,\"2635\":1,\"2646\":1,\"2651\":1}}],[\"qsub\",{\"1\":{\"144\":4}}],[\"qifq\",{\"1\":{\"2490\":1,\"2609\":1,\"2626\":1}}],[\"qiu\",{\"1\":{\"130\":1}}],[\"qint8\",{\"1\":{\"249\":1,\"307\":1,\"333\":1,\"384\":1,\"408\":1,\"416\":1,\"422\":1,\"478\":1}}],[\"qin\",{\"1\":{\"130\":1}}],[\"qian\",{\"1\":{\"130\":2,\"1670\":1,\"1671\":1}}],[\"quite\",{\"1\":{\"2385\":1}}],[\"quickstart\",{\"1\":{\"70\":1}}],[\"quick\",{\"0\":{\"47\":1},\"1\":{\"2394\":1,\"2530\":1}}],[\"quotes\",{\"0\":{\"2332\":1},\"1\":{\"2332\":2}}],[\"quality\",{\"1\":{\"2003\":2,\"2090\":1,\"2236\":1,\"2244\":1,\"2255\":1,\"2265\":1}}],[\"quantile\",{\"1\":{\"1925\":4}}],[\"quantization\",{\"0\":{\"311\":1,\"388\":1,\"412\":1,\"420\":1,\"426\":1,\"482\":1},\"1\":{\"700\":3,\"766\":3,\"835\":1}}],[\"quantize=false\",{\"1\":{\"2600\":1}}],[\"quantize=256\",{\"1\":{\"835\":1}}],[\"quantized\",{\"1\":{\"835\":2,\"869\":2,\"877\":2,\"2244\":1}}],[\"quantize\",{\"1\":{\"249\":8,\"307\":8,\"333\":6,\"384\":6,\"408\":8,\"416\":6,\"422\":8,\"478\":8,\"835\":2}}],[\"quadratic\",{\"0\":{\"1886\":1,\"1887\":1,\"1888\":1},\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"que\",{\"1\":{\"2457\":4}}],[\"question5\",{\"0\":{\"2479\":1}}],[\"question4\",{\"0\":{\"2420\":1,\"2462\":1,\"2478\":1}}],[\"question3\",{\"0\":{\"2419\":1,\"2461\":1,\"2476\":1}}],[\"question2\",{\"0\":{\"2418\":1,\"2474\":1,\"2510\":1}}],[\"question\",{\"1\":{\"2414\":1,\"2418\":1,\"2419\":1,\"2420\":1,\"2461\":1,\"2462\":1}}],[\"question1\",{\"0\":{\"2414\":1,\"2471\":1}}],[\"questions\",{\"0\":{\"2441\":1},\"1\":{\"2387\":1}}],[\"quert\",{\"1\":{\"740\":1}}],[\"query\",{\"1\":{\"116\":1,\"740\":3,\"741\":4,\"771\":3,\"775\":4,\"776\":4,\"785\":8,\"809\":3,\"1001\":4,\"1025\":4,\"1026\":1,\"1065\":4,\"1066\":4,\"1076\":14,\"1209\":9,\"1470\":1,\"1662\":1,\"1932\":2,\"1993\":3,\"2040\":1}}],[\"queue\",{\"1\":{\"142\":3,\"143\":1,\"144\":6,\"275\":2,\"276\":2,\"279\":2,\"280\":2,\"281\":2,\"282\":2,\"283\":2,\"284\":2,\"2372\":1,\"2429\":1,\"2554\":1}}],[\"qkv\",{\"1\":{\"785\":1,\"1076\":1,\"1209\":1}}],[\"qk\",{\"1\":{\"116\":2,\"1065\":4,\"1066\":3,\"1578\":1,\"1579\":1,\"1580\":1,\"1660\":2,\"1661\":2,\"1662\":2}}],[\"zenodo\",{\"1\":{\"2357\":2,\"2371\":2,\"2494\":1,\"2612\":2,\"2617\":1,\"2630\":3,\"2635\":1}}],[\"zeroed\",{\"1\":{\"1352\":2}}],[\"zeroes\",{\"1\":{\"915\":1,\"1352\":2}}],[\"zero=false\",{\"1\":{\"1019\":1,\"1037\":1,\"1039\":1,\"1231\":1}}],[\"zero=true\",{\"1\":{\"965\":1,\"968\":1,\"972\":1}}],[\"zeros\",{\"1\":{\"900\":3,\"902\":3,\"1376\":1,\"1693\":1,\"1697\":1,\"1755\":1,\"1929\":1,\"1941\":1,\"1947\":1,\"2295\":1,\"2296\":1}}],[\"zero\",{\"0\":{\"1343\":1},\"1\":{\"60\":1,\"80\":1,\"174\":1,\"609\":1,\"611\":1,\"632\":1,\"771\":3,\"792\":1,\"807\":1,\"809\":3,\"915\":1,\"932\":1,\"943\":2,\"950\":2,\"955\":2,\"965\":2,\"968\":2,\"972\":2,\"1019\":1,\"1037\":1,\"1039\":1,\"1140\":1,\"1145\":4,\"1148\":3,\"1169\":1,\"1186\":3,\"1199\":1,\"1203\":3,\"1210\":2,\"1211\":1,\"1216\":1,\"1221\":1,\"1224\":1,\"1257\":1,\"1287\":1,\"1298\":2,\"1299\":2,\"1301\":1,\"1302\":2,\"1303\":2,\"1304\":1,\"1336\":1,\"1337\":2,\"1343\":2,\"1348\":1,\"1349\":2,\"1350\":2,\"1559\":1,\"1560\":2,\"1639\":3,\"1640\":2,\"1693\":1,\"1696\":1,\"1718\":1,\"1755\":1,\"1758\":1,\"1759\":1,\"1778\":1,\"1850\":1,\"1851\":3,\"1852\":1,\"1914\":1,\"1915\":1,\"1940\":1,\"1949\":1,\"1959\":2,\"1973\":1,\"2003\":1,\"2026\":1,\"2054\":3,\"2090\":1,\"2151\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2279\":3}}],[\"zwicker\",{\"1\":{\"1904\":1}}],[\"zcaceres\",{\"1\":{\"1037\":1}}],[\"zongo\",{\"1\":{\"2387\":1}}],[\"zone\",{\"1\":{\"838\":1}}],[\"zoneoutcell\",{\"0\":{\"837\":1},\"1\":{\"837\":1,\"838\":1}}],[\"zoneout\",{\"1\":{\"821\":2,\"837\":4,\"838\":2,\"2002\":3,\"2078\":3,\"2095\":3,\"2263\":3}}],[\"zoo\",{\"0\":{\"2599\":1},\"1\":{\"97\":1,\"2355\":3,\"2357\":1,\"2358\":1,\"2371\":2,\"2431\":1,\"2450\":1,\"2466\":1,\"2482\":1,\"2500\":1,\"2504\":1,\"2514\":1,\"2517\":1,\"2520\":1,\"2576\":1,\"2578\":1,\"2579\":1,\"2588\":1,\"2591\":1,\"2598\":3,\"2599\":1,\"2602\":1,\"2612\":2,\"2617\":2,\"2619\":1,\"2630\":1,\"2635\":2,\"2646\":1,\"2651\":1,\"2659\":1}}],[\"z\",{\"1\":{\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"758\":2,\"1221\":2,\"1464\":1,\"1604\":1,\"1618\":2,\"1619\":2,\"1638\":3,\"1655\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1670\":1,\"1671\":1,\"1712\":1,\"1715\":1,\"1719\":1,\"1808\":8,\"1853\":4,\"1862\":4,\"1871\":2,\"2568\":1}}],[\"zh\",{\"1\":{\"229\":6,\"230\":6,\"295\":4,\"2357\":3,\"2578\":3,\"2589\":1}}],[\"zhong\",{\"1\":{\"130\":1}}],[\"zhaoheng\",{\"1\":{\"130\":1}}],[\"zhang\",{\"1\":{\"120\":1,\"130\":4,\"1604\":1,\"1655\":1,\"1670\":1,\"1671\":1,\"1719\":1,\"2054\":1,\"2366\":1,\"2388\":1,\"2524\":1,\"2601\":1,\"2618\":2}}],[\"zhuo\",{\"1\":{\"130\":1,\"1515\":1}}],[\"zipped\",{\"1\":{\"2600\":1}}],[\"zip\",{\"1\":{\"99\":1,\"277\":1,\"2368\":2,\"2371\":1,\"2396\":1,\"2454\":1,\"2460\":1,\"2486\":2,\"2490\":2,\"2494\":2,\"2499\":1,\"2500\":2,\"2506\":2,\"2510\":4,\"2512\":2,\"2519\":2,\"2532\":1,\"2600\":1,\"2605\":2,\"2609\":2,\"2612\":1,\"2617\":1,\"2622\":2,\"2626\":2,\"2630\":1,\"2635\":2}}],[\"|inference\",{\"1\":{\"2572\":4}}],[\"|decode\",{\"1\":{\"2440\":6,\"2441\":6,\"2564\":6}}],[\"|dataset|snt|wrd|corr|sub|del|ins|err|s\",{\"1\":{\"2440\":3,\"2441\":3,\"2564\":3,\"2572\":3}}],[\"||rtf||\",{\"1\":{\"1713\":1}}],[\"||^2\",{\"1\":{\"1010\":1}}],[\"||x\",{\"1\":{\"1010\":1}}],[\"|c\",{\"1\":{\"52\":1}}],[\"|\",{\"1\":{\"52\":2,\"53\":2,\"201\":1,\"202\":2,\"240\":1,\"295\":189,\"377\":1,\"503\":1,\"691\":6,\"692\":1,\"693\":3,\"697\":7,\"700\":3,\"712\":5,\"725\":6,\"751\":8,\"752\":4,\"756\":2,\"765\":4,\"778\":1,\"797\":1,\"798\":3,\"806\":7,\"815\":1,\"824\":12,\"825\":1,\"827\":1,\"857\":1,\"865\":1,\"866\":1,\"915\":2,\"918\":1,\"929\":1,\"1013\":1,\"1015\":1,\"1046\":10,\"1048\":1,\"1049\":1,\"1050\":1,\"1051\":2,\"1052\":6,\"1053\":3,\"1054\":2,\"1055\":2,\"1056\":1,\"1057\":4,\"1060\":6,\"1062\":1,\"1063\":8,\"1065\":7,\"1066\":1,\"1068\":2,\"1069\":4,\"1073\":4,\"1074\":1,\"1075\":2,\"1076\":2,\"1081\":1,\"1083\":4,\"1092\":1,\"1093\":2,\"1098\":2,\"1101\":1,\"1113\":6,\"1114\":1,\"1115\":2,\"1119\":1,\"1127\":1,\"1133\":3,\"1138\":3,\"1139\":3,\"1140\":5,\"1141\":2,\"1145\":1,\"1148\":4,\"1149\":4,\"1150\":4,\"1158\":6,\"1169\":4,\"1170\":2,\"1171\":9,\"1172\":7,\"1176\":8,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1186\":1,\"1190\":1,\"1193\":11,\"1195\":2,\"1198\":2,\"1200\":2,\"1203\":2,\"1206\":7,\"1207\":1,\"1210\":1,\"1211\":2,\"1214\":2,\"1215\":3,\"1216\":2,\"1218\":1,\"1219\":6,\"1222\":2,\"1224\":2,\"1239\":3,\"1248\":2,\"1255\":1,\"1256\":2,\"1257\":5,\"1269\":6,\"1270\":7,\"1272\":3,\"1273\":7,\"1282\":1,\"1284\":2,\"1285\":2,\"1336\":2,\"1348\":2,\"1371\":9,\"1375\":1,\"1377\":1,\"1382\":1,\"1384\":2,\"1386\":3,\"1394\":1,\"1396\":2,\"1403\":2,\"1407\":3,\"1411\":1,\"1415\":2,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":2,\"1431\":1,\"1432\":1,\"1435\":1,\"1441\":1,\"1445\":1,\"1454\":3,\"1455\":1,\"1463\":3,\"1505\":2,\"1510\":2,\"1511\":1,\"1515\":2,\"1516\":3,\"1523\":2,\"1524\":4,\"1525\":2,\"1528\":2,\"1529\":2,\"1534\":2,\"1539\":2,\"1551\":1,\"1552\":3,\"1553\":12,\"1554\":2,\"1558\":1,\"1595\":1,\"1600\":2,\"1611\":2,\"1616\":1,\"1617\":1,\"1626\":2,\"1643\":2,\"1644\":2,\"1645\":1,\"1654\":2,\"1658\":2,\"1659\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":2,\"1671\":4,\"1678\":2,\"1681\":1,\"1685\":1,\"1695\":6,\"1696\":2,\"1697\":2,\"1698\":5,\"1701\":1,\"1702\":1,\"1703\":2,\"1704\":3,\"1705\":4,\"1707\":5,\"1708\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1719\":2,\"1722\":1,\"1727\":2,\"1733\":1,\"1736\":2,\"1737\":2,\"1740\":1,\"1741\":2,\"1742\":2,\"1744\":1,\"1751\":1,\"1758\":1,\"1759\":1,\"1765\":1,\"1773\":69,\"1778\":19,\"1785\":2,\"1791\":4,\"1798\":1,\"1800\":1,\"1803\":1,\"1804\":28,\"1805\":23,\"1810\":1,\"1830\":2,\"1831\":2,\"1832\":2,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":4,\"1837\":24,\"1839\":2,\"1843\":2,\"1844\":2,\"1850\":6,\"1851\":13,\"1859\":5,\"1862\":2,\"1863\":1,\"1864\":1,\"1865\":1,\"1868\":2,\"1871\":1,\"1876\":1,\"1877\":9,\"1878\":13,\"1880\":3,\"1892\":5,\"1893\":5,\"1894\":1,\"1895\":4,\"1896\":6,\"1897\":1,\"1898\":1,\"1900\":3,\"1902\":1,\"1905\":1,\"1906\":3,\"1909\":1,\"1910\":1,\"1912\":4,\"1914\":3,\"1915\":3,\"1917\":1,\"1918\":5,\"1919\":1,\"1920\":1,\"1926\":3,\"1929\":2,\"1932\":1,\"1941\":2,\"1947\":2,\"1949\":1,\"1954\":1,\"1955\":1,\"1956\":1,\"1958\":1,\"1959\":1,\"1960\":1,\"1962\":2,\"1964\":3,\"1966\":1,\"1967\":4,\"1968\":2,\"1970\":5,\"1975\":7,\"1984\":30,\"1985\":6,\"1987\":3,\"1989\":7,\"1991\":3,\"2001\":7,\"2002\":11,\"2003\":3,\"2004\":6,\"2005\":1,\"2006\":1,\"2007\":2,\"2008\":1,\"2009\":1,\"2011\":1,\"2012\":2,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2019\":2,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2027\":17,\"2029\":2,\"2044\":1,\"2046\":10,\"2052\":1,\"2054\":3,\"2068\":1,\"2076\":21,\"2082\":70,\"2084\":8,\"2086\":25,\"2087\":26,\"2089\":13,\"2090\":25,\"2095\":27,\"2099\":10,\"2102\":1,\"2106\":2,\"2109\":5,\"2113\":5,\"2115\":5,\"2116\":5,\"2120\":4,\"2123\":1,\"2124\":1,\"2128\":3,\"2129\":1,\"2130\":4,\"2131\":3,\"2132\":1,\"2133\":1,\"2135\":3,\"2136\":4,\"2137\":12,\"2152\":2,\"2175\":8,\"2176\":3,\"2177\":1,\"2178\":21,\"2179\":21,\"2180\":12,\"2181\":4,\"2182\":3,\"2184\":6,\"2185\":2,\"2186\":4,\"2188\":1,\"2189\":2,\"2191\":17,\"2193\":12,\"2194\":16,\"2195\":20,\"2196\":14,\"2197\":3,\"2199\":19,\"2200\":8,\"2201\":8,\"2202\":8,\"2203\":2,\"2204\":4,\"2205\":24,\"2208\":1,\"2213\":1,\"2214\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1,\"2230\":9,\"2236\":2,\"2237\":4,\"2240\":31,\"2241\":7,\"2243\":11,\"2244\":13,\"2246\":3,\"2248\":7,\"2250\":3,\"2254\":3,\"2255\":13,\"2260\":3,\"2263\":11,\"2264\":10,\"2265\":1,\"2266\":4,\"2278\":29,\"2279\":12,\"2290\":2,\"2292\":2,\"2294\":14,\"2295\":1,\"2316\":1,\"2317\":7,\"2325\":3,\"2330\":2,\"2343\":2,\"2344\":1,\"2345\":1,\"2347\":2,\"2349\":2,\"2387\":1,\"2430\":1,\"2440\":30,\"2441\":30,\"2492\":9,\"2554\":1,\"2555\":1,\"2564\":30,\"2568\":1,\"2572\":30,\"2628\":9}}],[\"○\",{\"1\":{\"31\":1}}],[\"◎\",{\"1\":{\"31\":3}}],[\"`thu\",{\"1\":{\"2441\":1}}],[\"`translatotron\",{\"1\":{\"2003\":2}}],[\"`15a6dc1501b65211725a4fb514fcf5dd24f7ae95`\",{\"1\":{\"2441\":1}}],[\"`pytorch\",{\"1\":{\"2441\":1}}],[\"`espnet\",{\"1\":{\"2441\":1}}],[\"`enc\",{\"1\":{\"28\":3}}],[\"`3\",{\"1\":{\"2441\":1}}],[\"`local\",{\"1\":{\"2385\":1}}],[\"`mish\",{\"1\":{\"2252\":1}}],[\"`sat\",{\"1\":{\"2441\":1}}],[\"`singing\",{\"1\":{\"2079\":1,\"2083\":1,\"2095\":1}}],[\"`short\",{\"1\":{\"121\":2}}],[\"`https\",{\"1\":{\"2078\":1}}],[\"`direct\",{\"1\":{\"2004\":1}}],[\"``python\",{\"1\":{\"1421\":1}}],[\"``\",{\"1\":{\"1337\":1,\"1421\":1,\"2049\":1,\"2054\":1}}],[\"`validate\",{\"1\":{\"124\":1}}],[\"`freeze\",{\"1\":{\"29\":3}}],[\"`\",{\"1\":{\"28\":2,\"29\":3,\"697\":1,\"1011\":1,\"1688\":2,\"1693\":2,\"1755\":2,\"1756\":2,\"1850\":1,\"2441\":1,\"2492\":4,\"2628\":4}}],[\"x=english\",{\"1\":{\"2585\":1}}],[\"xzf\",{\"1\":{\"2567\":1}}],[\"xvectors\",{\"1\":{\"2415\":1,\"2514\":3,\"2659\":3}}],[\"xvectorprojector\",{\"0\":{\"2072\":1},\"1\":{\"2072\":1}}],[\"xvectorencoder\",{\"0\":{\"2070\":1},\"1\":{\"2070\":1}}],[\"xvector\",{\"0\":{\"2070\":1,\"2072\":1},\"1\":{\"2070\":1,\"2072\":1,\"2415\":3,\"2416\":2,\"2417\":3,\"2418\":3,\"2419\":1,\"2512\":6,\"2514\":3,\"2657\":6,\"2659\":3}}],[\"xiaoicesing\",{\"0\":{\"2090\":2},\"1\":{\"2090\":5}}],[\"xiaoicesing2loss\",{\"0\":{\"2091\":1},\"1\":{\"2091\":2}}],[\"xiaoicesing2\",{\"1\":{\"1778\":1,\"2090\":2}}],[\"xiaoice\",{\"0\":{\"2090\":1,\"2091\":1},\"1\":{\"1778\":1,\"2090\":1,\"2091\":2}}],[\"xinjian\",{\"1\":{\"130\":1}}],[\"x1\",{\"1\":{\"1754\":1}}],[\"x^t\",{\"1\":{\"1696\":1}}],[\"x|args\",{\"1\":{\"1618\":2,\"1619\":2,\"1638\":2}}],[\"x0\",{\"1\":{\"1618\":1,\"1619\":1,\"1754\":1}}],[\"xmlscpwriter\",{\"1\":{\"1416\":1}}],[\"xmlscpreader\",{\"1\":{\"1389\":1,\"1414\":1}}],[\"xmlwriter\",{\"0\":{\"1415\":1},\"1\":{\"1415\":2}}],[\"xml\",{\"1\":{\"1413\":1,\"1414\":5,\"1416\":3,\"2387\":1}}],[\"xmlreader\",{\"0\":{\"1413\":1},\"1\":{\"1413\":2}}],[\"xlim\",{\"1\":{\"2498\":3,\"2616\":3,\"2634\":3}}],[\"xlabel\",{\"1\":{\"2498\":1,\"2616\":1,\"2634\":1}}],[\"xla\",{\"0\":{\"1330\":1},\"1\":{\"1330\":1}}],[\"xl\",{\"1\":{\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1}}],[\"xlens\",{\"1\":{\"705\":2}}],[\"xlen\",{\"1\":{\"695\":1,\"696\":1,\"734\":1,\"773\":1,\"828\":1,\"1133\":1,\"1190\":1,\"1214\":1,\"1244\":1,\"1273\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1957\":1,\"1958\":1,\"1960\":1,\"2001\":1}}],[\"xavier\",{\"1\":{\"893\":1,\"1778\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"2003\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2264\":1,\"2279\":1,\"2440\":1,\"2558\":1,\"2584\":1}}],[\"xx\",{\"1\":{\"748\":1,\"1696\":2}}],[\"xxx\",{\"0\":{\"179\":1},\"1\":{\"26\":1,\"169\":1,\"181\":1,\"182\":2,\"185\":1,\"187\":1,\"2429\":1,\"2552\":1,\"2558\":1}}],[\"xy\",{\"1\":{\"731\":1,\"732\":1,\"1025\":1}}],[\"xp\",{\"1\":{\"704\":1,\"731\":4,\"898\":1}}],[\"xs=none\",{\"1\":{\"899\":1,\"901\":1}}],[\"xs\",{\"1\":{\"676\":6,\"695\":2,\"696\":2,\"697\":5,\"701\":2,\"717\":1,\"734\":2,\"735\":2,\"737\":4,\"742\":4,\"749\":4,\"754\":4,\"773\":2,\"774\":2,\"777\":1,\"781\":4,\"782\":2,\"802\":2,\"821\":4,\"826\":4,\"828\":2,\"887\":2,\"899\":1,\"900\":7,\"901\":1,\"902\":7,\"903\":2,\"906\":2,\"928\":10,\"959\":3,\"1119\":1,\"1133\":3,\"1140\":2,\"1148\":2,\"1149\":6,\"1150\":6,\"1169\":2,\"1178\":2,\"1179\":4,\"1180\":2,\"1181\":2,\"1182\":2,\"1190\":2,\"1200\":2,\"1203\":2,\"1214\":2,\"1215\":1,\"1222\":1,\"1244\":2,\"1269\":2,\"1272\":2,\"1273\":2,\"1282\":1,\"1354\":1,\"1487\":1,\"1491\":1,\"1592\":1,\"1595\":2,\"1627\":1,\"1711\":2,\"1881\":2,\"1917\":3,\"1957\":2,\"1958\":2,\"1960\":2,\"2001\":2,\"2029\":2,\"2081\":2,\"2083\":2,\"2085\":1,\"2260\":6,\"2265\":2,\"2271\":2,\"2283\":1,\"2285\":1,\"2287\":2,\"2295\":1,\"2296\":2}}],[\"xf\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1}}],[\"xuankai\",{\"1\":{\"130\":5}}],[\"xu\",{\"1\":{\"130\":2}}],[\"x\",{\"1\":{\"26\":5,\"94\":1,\"98\":5,\"109\":1,\"115\":1,\"118\":3,\"174\":2,\"203\":3,\"218\":2,\"225\":2,\"231\":3,\"232\":2,\"605\":3,\"633\":1,\"648\":1,\"676\":6,\"677\":4,\"678\":3,\"679\":3,\"681\":4,\"684\":7,\"685\":3,\"686\":4,\"687\":5,\"688\":6,\"689\":6,\"691\":19,\"692\":8,\"693\":4,\"696\":2,\"697\":26,\"698\":1,\"699\":1,\"701\":2,\"704\":1,\"705\":4,\"706\":10,\"708\":2,\"710\":4,\"711\":12,\"713\":2,\"714\":5,\"715\":5,\"716\":5,\"717\":3,\"718\":5,\"719\":5,\"720\":5,\"721\":5,\"722\":5,\"723\":2,\"725\":10,\"731\":3,\"734\":5,\"737\":4,\"740\":1,\"741\":1,\"742\":2,\"744\":3,\"749\":7,\"752\":1,\"754\":2,\"756\":1,\"758\":7,\"760\":1,\"764\":2,\"767\":3,\"770\":3,\"771\":2,\"772\":2,\"773\":2,\"775\":1,\"776\":1,\"777\":3,\"781\":5,\"785\":1,\"786\":2,\"793\":2,\"796\":2,\"797\":9,\"804\":2,\"806\":1,\"807\":1,\"809\":2,\"810\":3,\"812\":5,\"813\":2,\"815\":4,\"816\":1,\"817\":7,\"818\":2,\"819\":1,\"821\":2,\"825\":4,\"826\":2,\"828\":5,\"829\":4,\"830\":2,\"831\":1,\"835\":4,\"836\":1,\"857\":2,\"863\":2,\"865\":2,\"869\":1,\"875\":4,\"876\":4,\"877\":2,\"895\":4,\"898\":1,\"904\":3,\"907\":3,\"922\":4,\"926\":2,\"927\":2,\"930\":1,\"932\":2,\"943\":1,\"944\":3,\"950\":1,\"956\":2,\"963\":1,\"965\":2,\"966\":1,\"967\":1,\"968\":2,\"969\":1,\"970\":1,\"971\":1,\"973\":3,\"981\":1,\"1001\":2,\"1010\":3,\"1028\":1,\"1038\":1,\"1047\":2,\"1049\":6,\"1050\":6,\"1051\":3,\"1052\":6,\"1053\":3,\"1054\":3,\"1055\":3,\"1056\":6,\"1058\":11,\"1061\":5,\"1062\":5,\"1065\":3,\"1066\":8,\"1067\":4,\"1068\":6,\"1069\":5,\"1070\":3,\"1072\":3,\"1073\":4,\"1074\":6,\"1075\":11,\"1076\":3,\"1077\":4,\"1079\":3,\"1080\":2,\"1081\":6,\"1082\":4,\"1083\":1,\"1084\":4,\"1093\":3,\"1127\":2,\"1130\":1,\"1133\":9,\"1134\":1,\"1141\":2,\"1143\":1,\"1144\":1,\"1148\":7,\"1149\":7,\"1150\":7,\"1151\":1,\"1153\":2,\"1156\":1,\"1160\":2,\"1161\":2,\"1162\":3,\"1163\":1,\"1164\":2,\"1165\":3,\"1166\":2,\"1170\":2,\"1177\":2,\"1182\":2,\"1187\":1,\"1188\":1,\"1190\":3,\"1203\":7,\"1204\":1,\"1211\":3,\"1212\":1,\"1213\":1,\"1214\":2,\"1221\":4,\"1224\":3,\"1229\":1,\"1230\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1244\":4,\"1249\":1,\"1251\":1,\"1252\":1,\"1253\":2,\"1254\":2,\"1256\":1,\"1257\":2,\"1259\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1270\":1,\"1272\":7,\"1273\":2,\"1274\":1,\"1276\":1,\"1279\":3,\"1280\":1,\"1281\":1,\"1287\":3,\"1288\":1,\"1307\":1,\"1311\":1,\"1316\":1,\"1318\":1,\"1325\":1,\"1328\":1,\"1329\":1,\"1335\":1,\"1336\":3,\"1338\":1,\"1348\":3,\"1355\":3,\"1356\":1,\"1358\":1,\"1369\":2,\"1370\":2,\"1378\":2,\"1379\":2,\"1430\":2,\"1447\":1,\"1449\":1,\"1451\":4,\"1452\":1,\"1456\":1,\"1458\":1,\"1462\":2,\"1464\":2,\"1468\":1,\"1470\":2,\"1471\":2,\"1473\":2,\"1474\":1,\"1482\":1,\"1485\":1,\"1489\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1505\":7,\"1508\":1,\"1514\":4,\"1517\":2,\"1518\":1,\"1520\":1,\"1522\":2,\"1530\":1,\"1538\":2,\"1539\":2,\"1546\":2,\"1549\":1,\"1557\":4,\"1560\":4,\"1561\":1,\"1563\":1,\"1572\":2,\"1573\":1,\"1576\":2,\"1577\":2,\"1578\":2,\"1579\":2,\"1580\":2,\"1581\":1,\"1583\":1,\"1585\":4,\"1586\":1,\"1588\":1,\"1590\":1,\"1594\":2,\"1600\":2,\"1603\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1612\":4,\"1613\":1,\"1615\":4,\"1618\":2,\"1619\":2,\"1620\":1,\"1622\":1,\"1623\":5,\"1624\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1637\":4,\"1638\":8,\"1646\":1,\"1647\":1,\"1655\":1,\"1663\":2,\"1664\":2,\"1665\":2,\"1669\":7,\"1672\":1,\"1676\":1,\"1679\":2,\"1686\":2,\"1688\":4,\"1693\":2,\"1719\":1,\"1720\":1,\"1721\":1,\"1729\":1,\"1730\":1,\"1754\":2,\"1755\":2,\"1756\":4,\"1758\":1,\"1759\":1,\"1766\":2,\"1768\":2,\"1769\":1,\"1771\":4,\"1772\":4,\"1777\":2,\"1779\":1,\"1781\":4,\"1782\":1,\"1786\":2,\"1787\":4,\"1788\":4,\"1789\":2,\"1795\":1,\"1799\":2,\"1801\":1,\"1806\":2,\"1808\":2,\"1810\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1829\":2,\"1833\":4,\"1835\":4,\"1838\":4,\"1840\":2,\"1845\":2,\"1846\":2,\"1847\":2,\"1848\":1,\"1849\":2,\"1855\":4,\"1856\":2,\"1858\":2,\"1860\":4,\"1861\":2,\"1863\":4,\"1864\":4,\"1865\":4,\"1866\":2,\"1868\":4,\"1869\":6,\"1870\":2,\"1872\":2,\"1873\":2,\"1874\":4,\"1875\":1,\"1880\":4,\"1884\":4,\"1885\":2,\"1904\":1,\"1906\":3,\"1907\":1,\"1911\":3,\"1916\":1,\"1917\":3,\"1919\":4,\"1920\":2,\"1935\":1,\"1943\":2,\"1948\":2,\"1949\":2,\"1957\":2,\"1959\":2,\"1960\":2,\"1963\":2,\"1994\":1,\"2001\":8,\"2004\":7,\"2029\":7,\"2030\":1,\"2032\":1,\"2034\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2049\":2,\"2050\":1,\"2052\":1,\"2054\":2,\"2055\":1,\"2057\":1,\"2059\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":2,\"2070\":4,\"2072\":1,\"2074\":1,\"2075\":1,\"2078\":2,\"2081\":1,\"2083\":2,\"2149\":3,\"2210\":5,\"2211\":1,\"2252\":2,\"2258\":2,\"2265\":2,\"2266\":1,\"2267\":3,\"2270\":3,\"2272\":3,\"2290\":1,\"2297\":1,\"2299\":1,\"2303\":2,\"2304\":1,\"2365\":2,\"2372\":3,\"2508\":2,\"2510\":2,\"2514\":4,\"2515\":2,\"2553\":4,\"2568\":11,\"2569\":1,\"2585\":3,\"2655\":2,\"2659\":4,\"2660\":2}}],[\"y=totonac\",{\"1\":{\"2585\":1}}],[\"y1\",{\"1\":{\"1754\":1}}],[\"y0\",{\"1\":{\"1754\":1}}],[\"yy\",{\"1\":{\"731\":2,\"732\":1}}],[\"yyy=zzz\",{\"1\":{\"187\":1}}],[\"ylens\",{\"1\":{\"1136\":2,\"1137\":1}}],[\"ylen\",{\"1\":{\"695\":1,\"696\":1,\"734\":1,\"773\":1,\"828\":1,\"1133\":1,\"1190\":1,\"1214\":1,\"1244\":1,\"1273\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1957\":1,\"1958\":1,\"1960\":1,\"2001\":1}}],[\"ys\",{\"1\":{\"676\":6,\"695\":5,\"696\":4,\"706\":4,\"731\":1,\"734\":6,\"735\":2,\"742\":4,\"750\":6,\"754\":4,\"755\":2,\"768\":4,\"773\":5,\"781\":4,\"796\":2,\"815\":2,\"817\":2,\"821\":4,\"822\":2,\"826\":4,\"828\":6,\"852\":2,\"905\":2,\"920\":2,\"924\":2,\"1117\":2,\"1133\":11,\"1136\":2,\"1137\":1,\"1145\":4,\"1171\":8,\"1181\":2,\"1190\":13,\"1204\":4,\"1206\":8,\"1214\":9,\"1220\":2,\"1221\":2,\"1244\":11,\"1269\":2,\"1273\":11,\"1552\":8,\"1767\":4,\"1957\":6,\"1958\":4,\"1959\":2,\"1960\":6,\"2000\":2,\"2001\":5,\"2002\":1,\"2078\":2,\"2088\":2,\"2091\":2,\"2245\":2,\"2256\":2,\"2260\":2,\"2280\":2}}],[\"yseq\",{\"1\":{\"175\":1,\"194\":1,\"694\":3,\"751\":1,\"765\":3,\"797\":1,\"798\":6,\"1060\":1,\"1063\":3,\"1133\":1,\"1176\":1,\"1193\":2,\"1221\":1}}],[\"y\",{\"1\":{\"144\":2,\"175\":2,\"194\":2,\"218\":3,\"225\":3,\"232\":3,\"648\":1,\"706\":6,\"734\":2,\"744\":2,\"750\":2,\"773\":2,\"796\":3,\"815\":2,\"817\":2,\"828\":2,\"869\":1,\"1010\":4,\"1130\":1,\"1133\":4,\"1156\":1,\"1188\":1,\"1190\":1,\"1214\":2,\"1221\":1,\"1233\":1,\"1244\":1,\"1273\":2,\"1288\":1,\"1307\":1,\"1335\":1,\"1368\":3,\"1372\":3,\"1451\":1,\"1462\":2,\"1463\":2,\"1472\":3,\"1474\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1514\":1,\"1557\":1,\"1575\":3,\"1585\":1,\"1604\":1,\"1606\":1,\"1612\":1,\"1615\":1,\"1618\":5,\"1619\":5,\"1623\":1,\"1637\":1,\"1638\":1,\"1647\":3,\"1655\":1,\"1660\":4,\"1661\":4,\"1662\":4,\"1670\":1,\"1671\":1,\"1686\":2,\"1695\":3,\"1701\":2,\"1702\":2,\"1719\":1,\"1736\":2,\"1737\":2,\"1754\":2,\"1758\":2,\"1759\":2,\"1761\":2,\"1763\":2,\"1793\":2,\"1859\":4,\"1869\":4,\"1892\":4,\"1893\":4,\"1935\":2,\"1943\":1,\"1957\":2,\"1959\":2,\"1960\":2,\"1996\":2,\"2001\":2,\"2054\":1,\"2267\":1,\"2301\":4,\"2392\":1,\"2425\":1,\"2457\":1,\"2528\":1,\"2548\":1,\"2568\":1,\"2585\":3,\"2600\":5}}],[\"yin\",{\"0\":{\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1},\"1\":{\"1805\":3,\"1808\":38,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1}}],[\"yingram\",{\"1\":{\"2267\":9}}],[\"yingdecoder\",{\"0\":{\"1808\":1},\"1\":{\"1808\":2}}],[\"ying\",{\"0\":{\"1808\":1,\"2266\":2},\"1\":{\"1773\":5,\"1804\":5,\"1805\":3,\"1808\":2,\"2082\":5,\"2095\":1,\"2266\":6}}],[\"yi\",{\"1\":{\"130\":1}}],[\"yifanpen\",{\"1\":{\"2421\":1,\"2544\":1}}],[\"yifan\",{\"1\":{\"130\":2,\"1956\":1,\"2421\":1,\"2544\":1}}],[\"york\",{\"1\":{\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1}}],[\"yo\",{\"1\":{\"201\":1}}],[\"yoshiki\",{\"1\":{\"130\":1}}],[\"yoshimura\",{\"1\":{\"130\":2,\"195\":1}}],[\"yooncheol\",{\"1\":{\"130\":1}}],[\"youtube\",{\"1\":{\"2501\":1}}],[\"yours\",{\"1\":{\"2615\":6,\"2633\":6}}],[\"yourself\",{\"1\":{\"97\":1,\"133\":1,\"295\":1,\"2365\":1,\"2501\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2655\":1,\"2660\":1}}],[\"yourespnetmodel\",{\"1\":{\"2168\":2,\"2170\":2}}],[\"yourtask\",{\"1\":{\"2099\":1,\"2168\":1,\"2170\":1}}],[\"yourmodel\",{\"1\":{\"173\":1}}],[\"your\",{\"0\":{\"99\":1,\"2360\":1,\"2429\":1,\"2458\":1,\"2522\":1,\"2523\":1,\"2581\":1,\"2582\":1,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1},\"1\":{\"1\":2,\"4\":1,\"11\":1,\"13\":2,\"19\":1,\"45\":2,\"46\":1,\"48\":1,\"49\":1,\"56\":1,\"70\":1,\"80\":1,\"85\":5,\"91\":1,\"93\":1,\"94\":1,\"96\":3,\"99\":2,\"100\":1,\"113\":2,\"119\":1,\"124\":3,\"126\":1,\"128\":1,\"133\":1,\"137\":1,\"141\":1,\"144\":2,\"148\":1,\"173\":2,\"198\":1,\"218\":1,\"243\":1,\"760\":1,\"778\":1,\"831\":1,\"1198\":3,\"1432\":1,\"1436\":1,\"1476\":1,\"1510\":1,\"1511\":1,\"1598\":1,\"1643\":1,\"1644\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2084\":1,\"2089\":1,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2168\":1,\"2360\":3,\"2365\":1,\"2372\":1,\"2373\":1,\"2385\":1,\"2387\":3,\"2389\":3,\"2393\":2,\"2394\":1,\"2400\":1,\"2408\":3,\"2414\":1,\"2422\":3,\"2429\":1,\"2430\":1,\"2431\":1,\"2446\":1,\"2449\":3,\"2458\":3,\"2461\":1,\"2462\":1,\"2465\":3,\"2481\":4,\"2501\":6,\"2503\":3,\"2508\":1,\"2510\":1,\"2514\":1,\"2515\":1,\"2522\":2,\"2523\":4,\"2525\":3,\"2529\":2,\"2530\":1,\"2536\":1,\"2543\":1,\"2545\":3,\"2554\":1,\"2555\":1,\"2558\":1,\"2560\":1,\"2572\":1,\"2575\":1,\"2581\":2,\"2582\":3,\"2584\":5,\"2585\":2,\"2587\":1,\"2593\":1,\"2597\":1,\"2598\":1,\"2600\":4,\"2607\":1,\"2618\":1,\"2624\":1,\"2637\":2,\"2638\":1,\"2640\":2,\"2642\":1,\"2643\":4,\"2644\":3,\"2645\":1,\"2655\":1,\"2659\":1,\"2660\":1}}],[\"you\",{\"1\":{\"1\":4,\"2\":2,\"3\":3,\"4\":1,\"11\":4,\"12\":1,\"14\":1,\"16\":1,\"17\":4,\"18\":1,\"19\":4,\"20\":1,\"25\":3,\"26\":3,\"33\":2,\"34\":2,\"38\":1,\"45\":2,\"46\":1,\"47\":4,\"48\":3,\"49\":4,\"51\":1,\"54\":3,\"56\":1,\"57\":2,\"58\":1,\"59\":1,\"60\":1,\"62\":2,\"63\":1,\"72\":1,\"74\":2,\"75\":1,\"76\":2,\"77\":1,\"78\":1,\"84\":2,\"85\":9,\"90\":2,\"91\":1,\"92\":2,\"94\":1,\"95\":2,\"96\":4,\"97\":2,\"99\":2,\"100\":1,\"106\":2,\"112\":1,\"115\":1,\"124\":3,\"126\":3,\"127\":3,\"132\":2,\"133\":1,\"134\":8,\"135\":9,\"136\":3,\"137\":1,\"141\":2,\"144\":6,\"148\":7,\"149\":2,\"150\":4,\"166\":1,\"168\":3,\"173\":1,\"177\":1,\"179\":1,\"180\":2,\"182\":1,\"187\":1,\"188\":1,\"197\":2,\"198\":2,\"203\":2,\"209\":1,\"213\":1,\"228\":1,\"233\":1,\"235\":4,\"236\":1,\"238\":1,\"240\":5,\"295\":7,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"745\":1,\"746\":1,\"754\":1,\"760\":1,\"778\":1,\"820\":1,\"821\":1,\"826\":1,\"831\":1,\"1171\":1,\"1187\":1,\"1202\":1,\"1206\":1,\"1243\":2,\"1286\":1,\"1287\":1,\"1427\":1,\"1432\":1,\"1436\":1,\"1476\":1,\"1510\":1,\"1511\":1,\"1543\":1,\"1552\":1,\"1598\":1,\"1603\":1,\"1622\":1,\"1643\":1,\"1644\":1,\"1655\":1,\"1656\":1,\"1660\":3,\"1661\":3,\"1662\":3,\"1719\":3,\"1905\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1953\":1,\"1955\":1,\"2019\":2,\"2084\":1,\"2089\":1,\"2096\":1,\"2098\":1,\"2099\":2,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2131\":1,\"2168\":1,\"2185\":1,\"2201\":1,\"2203\":1,\"2355\":1,\"2362\":1,\"2363\":3,\"2368\":2,\"2371\":1,\"2372\":1,\"2373\":1,\"2375\":2,\"2377\":2,\"2378\":1,\"2380\":1,\"2381\":1,\"2385\":1,\"2387\":4,\"2388\":1,\"2389\":5,\"2391\":1,\"2393\":3,\"2394\":4,\"2398\":1,\"2405\":1,\"2406\":1,\"2407\":1,\"2408\":5,\"2414\":1,\"2419\":1,\"2420\":1,\"2422\":5,\"2423\":2,\"2427\":1,\"2429\":2,\"2430\":1,\"2436\":2,\"2437\":2,\"2441\":2,\"2447\":1,\"2448\":1,\"2449\":5,\"2451\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2463\":1,\"2464\":1,\"2465\":5,\"2467\":1,\"2480\":1,\"2481\":6,\"2486\":2,\"2490\":2,\"2494\":1,\"2500\":1,\"2501\":1,\"2502\":1,\"2503\":5,\"2504\":1,\"2506\":3,\"2514\":3,\"2523\":1,\"2524\":1,\"2525\":5,\"2527\":1,\"2529\":3,\"2530\":4,\"2534\":1,\"2541\":1,\"2542\":4,\"2543\":3,\"2545\":5,\"2546\":2,\"2550\":1,\"2552\":1,\"2554\":1,\"2555\":1,\"2558\":2,\"2559\":2,\"2560\":1,\"2562\":2,\"2563\":2,\"2564\":2,\"2565\":1,\"2568\":4,\"2569\":1,\"2573\":1,\"2576\":1,\"2583\":1,\"2584\":12,\"2585\":6,\"2586\":1,\"2587\":1,\"2588\":1,\"2591\":1,\"2593\":2,\"2598\":1,\"2599\":1,\"2600\":4,\"2605\":2,\"2609\":2,\"2612\":1,\"2622\":2,\"2626\":2,\"2630\":1,\"2635\":3,\"2639\":1,\"2640\":1,\"2644\":2,\"2651\":1,\"2653\":3,\"2659\":3}}],[\"yang\",{\"1\":{\"1605\":1}}],[\"yanmin\",{\"1\":{\"130\":1}}],[\"yan\",{\"1\":{\"130\":3}}],[\"yasuda\",{\"1\":{\"130\":1}}],[\"yamamoto\",{\"1\":{\"130\":2}}],[\"yaml\",{\"0\":{\"187\":1,\"503\":1,\"544\":1,\"2307\":1,\"2315\":1,\"2342\":2},\"1\":{\"25\":17,\"27\":1,\"62\":1,\"63\":4,\"64\":1,\"92\":1,\"186\":2,\"187\":5,\"189\":2,\"240\":9,\"241\":2,\"242\":1,\"285\":2,\"286\":1,\"296\":1,\"503\":3,\"544\":3,\"1198\":1,\"1603\":1,\"1622\":1,\"1967\":1,\"2099\":1,\"2307\":2,\"2309\":1,\"2315\":1,\"2342\":3,\"2368\":1,\"2377\":7,\"2398\":1,\"2400\":1,\"2401\":2,\"2403\":1,\"2405\":1,\"2429\":1,\"2436\":6,\"2440\":3,\"2455\":1,\"2460\":1,\"2461\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":1,\"2534\":1,\"2536\":1,\"2537\":2,\"2539\":1,\"2541\":1,\"2558\":5,\"2559\":3,\"2562\":6,\"2564\":3,\"2569\":1,\"2570\":2,\"2584\":6,\"2585\":2,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2639\":2,\"2648\":1,\"2649\":1}}],[\"yalta\",{\"1\":{\"130\":2}}],[\"yellow\",{\"1\":{\"2500\":2,\"2617\":2,\"2635\":2}}],[\"yellow=lambda\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"yen\",{\"1\":{\"130\":1,\"2618\":1}}],[\"year\",{\"1\":{\"130\":1,\"161\":1,\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"year=2022\",{\"1\":{\"130\":1}}],[\"year=\",{\"1\":{\"130\":8}}],[\"yes\",{\"1\":{\"73\":3,\"201\":1,\"202\":1}}],[\"yet\",{\"1\":{\"24\":1,\"48\":1,\"83\":2,\"118\":1,\"1255\":1,\"1391\":2,\"2125\":1}}],[\"yu23b\",{\"1\":{\"1462\":1,\"1463\":1}}],[\"yumesaki\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"yum\",{\"1\":{\"132\":2}}],[\"yuning\",{\"1\":{\"130\":1}}],[\"yuekai\",{\"1\":{\"130\":1}}],[\"yusuf\",{\"1\":{\"1528\":1}}],[\"yusuke\",{\"1\":{\"130\":1}}],[\"yushiueda\",{\"1\":{\"2478\":1}}],[\"yushi\",{\"1\":{\"130\":1}}],[\"yuya\",{\"1\":{\"130\":1}}],[\"yu\",{\"1\":{\"24\":1,\"113\":1,\"130\":2,\"1462\":2,\"1463\":2}}],[\">>\",{\"1\":{\"295\":1,\"2567\":2}}],[\">>>\",{\"1\":{\"128\":1,\"744\":7,\"838\":2,\"876\":6,\"900\":9,\"902\":9,\"904\":4,\"907\":3,\"921\":1,\"928\":6,\"944\":2,\"959\":5,\"984\":1,\"987\":3,\"989\":4,\"992\":2,\"995\":2,\"1014\":2,\"1015\":4,\"1017\":4,\"1024\":1,\"1028\":2,\"1030\":1,\"1383\":1,\"1389\":2,\"1391\":3,\"1395\":2,\"1397\":3,\"1400\":2,\"1402\":2,\"1404\":3,\"1406\":4,\"1408\":6,\"1410\":2,\"1412\":3,\"1414\":2,\"1416\":3,\"1418\":2,\"1423\":2,\"1425\":1,\"1897\":6,\"1951\":3,\"1969\":1,\"2096\":8,\"2098\":8,\"2099\":11,\"2100\":8,\"2101\":8,\"2102\":10,\"2103\":8,\"2104\":8,\"2105\":8,\"2107\":8,\"2108\":8,\"2109\":8,\"2110\":8,\"2111\":6,\"2112\":8,\"2113\":8,\"2114\":8,\"2115\":8,\"2116\":8,\"2117\":8,\"2118\":8,\"2134\":2,\"2143\":2,\"2149\":6,\"2154\":6,\"2158\":5,\"2168\":3,\"2170\":3,\"2176\":11,\"2183\":1,\"2190\":2,\"2193\":5,\"2201\":1,\"2209\":9,\"2210\":4,\"2314\":6,\"2320\":7,\"2323\":2,\"2328\":7,\"2335\":5,\"2337\":1,\"2340\":7}}],[\">=\",{\"1\":{\"134\":1,\"778\":2,\"1067\":2,\"1084\":1,\"1142\":1,\"1186\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1301\":1,\"1304\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1375\":1,\"1912\":2,\"2500\":1,\"2598\":1,\"2617\":1,\"2635\":1}}],[\">1\",{\"1\":{\"23\":5}}],[\">\",{\"0\":{\"201\":1,\"2433\":1},\"1\":{\"22\":1,\"23\":2,\"28\":6,\"29\":6,\"46\":4,\"118\":1,\"119\":1,\"143\":4,\"167\":1,\"178\":1,\"196\":1,\"198\":1,\"200\":1,\"231\":1,\"234\":1,\"240\":1,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"286\":1,\"294\":1,\"295\":2,\"296\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"377\":1,\"380\":1,\"384\":1,\"391\":1,\"397\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"493\":1,\"496\":1,\"499\":1,\"501\":1,\"503\":1,\"506\":1,\"509\":1,\"512\":1,\"515\":1,\"517\":1,\"519\":1,\"522\":1,\"525\":1,\"528\":1,\"530\":1,\"533\":1,\"536\":1,\"538\":1,\"541\":1,\"544\":1,\"546\":1,\"549\":1,\"551\":1,\"554\":1,\"557\":1,\"560\":1,\"562\":1,\"564\":1,\"566\":1,\"568\":1,\"570\":1,\"572\":1,\"574\":1,\"576\":1,\"579\":1,\"582\":1,\"585\":1,\"588\":1,\"590\":1,\"600\":1,\"601\":1,\"602\":1,\"603\":1,\"604\":1,\"606\":1,\"607\":1,\"610\":1,\"614\":1,\"615\":1,\"616\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":2,\"629\":2,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"636\":1,\"638\":4,\"641\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"659\":1,\"660\":1,\"662\":1,\"663\":1,\"664\":1,\"666\":1,\"667\":1,\"669\":1,\"670\":1,\"671\":1,\"673\":1,\"676\":1,\"697\":1,\"698\":1,\"699\":1,\"706\":1,\"711\":2,\"742\":1,\"747\":1,\"749\":3,\"750\":1,\"767\":1,\"778\":3,\"781\":1,\"788\":1,\"789\":1,\"790\":1,\"793\":1,\"799\":2,\"812\":1,\"820\":1,\"829\":1,\"857\":1,\"867\":1,\"873\":1,\"874\":1,\"880\":1,\"890\":1,\"937\":1,\"938\":1,\"939\":1,\"940\":1,\"941\":1,\"943\":1,\"944\":1,\"945\":1,\"948\":1,\"949\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"955\":1,\"956\":1,\"960\":1,\"962\":1,\"963\":1,\"964\":1,\"965\":1,\"966\":1,\"968\":1,\"969\":1,\"970\":1,\"972\":1,\"973\":1,\"977\":1,\"978\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"985\":1,\"986\":1,\"987\":1,\"989\":1,\"990\":1,\"991\":1,\"993\":1,\"994\":1,\"999\":1,\"1000\":1,\"1008\":1,\"1011\":1,\"1012\":1,\"1019\":1,\"1027\":1,\"1029\":1,\"1031\":1,\"1037\":1,\"1038\":1,\"1039\":1,\"1040\":1,\"1043\":1,\"1044\":1,\"1045\":1,\"1049\":1,\"1050\":1,\"1054\":1,\"1056\":1,\"1061\":1,\"1065\":1,\"1067\":3,\"1068\":1,\"1070\":2,\"1074\":1,\"1082\":3,\"1084\":1,\"1117\":1,\"1119\":1,\"1127\":5,\"1130\":1,\"1133\":3,\"1136\":1,\"1145\":1,\"1148\":2,\"1149\":3,\"1150\":3,\"1158\":4,\"1160\":3,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":3,\"1165\":2,\"1167\":1,\"1168\":1,\"1171\":1,\"1177\":3,\"1184\":1,\"1188\":1,\"1196\":1,\"1197\":1,\"1198\":5,\"1203\":2,\"1204\":2,\"1205\":1,\"1206\":1,\"1209\":2,\"1217\":1,\"1220\":1,\"1222\":1,\"1233\":1,\"1239\":1,\"1241\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1252\":3,\"1253\":2,\"1254\":3,\"1256\":1,\"1257\":2,\"1271\":1,\"1272\":3,\"1273\":1,\"1279\":1,\"1280\":1,\"1297\":1,\"1311\":1,\"1314\":1,\"1319\":1,\"1323\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1339\":1,\"1341\":1,\"1343\":1,\"1351\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1364\":1,\"1366\":1,\"1369\":1,\"1375\":1,\"1381\":1,\"1388\":2,\"1393\":1,\"1394\":1,\"1396\":1,\"1398\":3,\"1399\":1,\"1401\":1,\"1405\":1,\"1407\":1,\"1409\":2,\"1411\":1,\"1413\":2,\"1415\":1,\"1419\":1,\"1422\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1431\":1,\"1435\":1,\"1439\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1465\":1,\"1470\":1,\"1471\":1,\"1473\":1,\"1478\":1,\"1482\":1,\"1501\":1,\"1505\":2,\"1506\":1,\"1517\":1,\"1522\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1543\":2,\"1559\":1,\"1560\":1,\"1561\":1,\"1564\":1,\"1572\":1,\"1575\":1,\"1576\":1,\"1586\":1,\"1594\":1,\"1598\":1,\"1602\":1,\"1605\":1,\"1607\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1629\":1,\"1638\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1655\":1,\"1656\":1,\"1663\":1,\"1664\":1,\"1669\":2,\"1670\":1,\"1679\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1694\":1,\"1700\":1,\"1701\":1,\"1705\":1,\"1707\":1,\"1710\":1,\"1713\":1,\"1718\":1,\"1721\":1,\"1722\":1,\"1727\":1,\"1728\":1,\"1734\":1,\"1735\":2,\"1738\":1,\"1740\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1748\":1,\"1749\":1,\"1751\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1767\":1,\"1768\":1,\"1782\":1,\"1784\":1,\"1789\":1,\"1793\":1,\"1795\":1,\"1804\":3,\"1805\":1,\"1806\":1,\"1811\":1,\"1816\":1,\"1820\":1,\"1823\":1,\"1824\":1,\"1826\":1,\"1827\":1,\"1828\":1,\"1833\":1,\"1840\":1,\"1841\":1,\"1850\":2,\"1851\":3,\"1853\":1,\"1855\":1,\"1860\":1,\"1875\":1,\"1877\":1,\"1878\":3,\"1879\":1,\"1880\":1,\"1902\":1,\"1904\":1,\"1905\":2,\"1906\":1,\"1911\":1,\"1912\":4,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1925\":1,\"1926\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1941\":1,\"1943\":1,\"1945\":1,\"1946\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1958\":1,\"1960\":1,\"1961\":1,\"1967\":1,\"1968\":1,\"1970\":1,\"1971\":2,\"1972\":1,\"1975\":1,\"1978\":1,\"1984\":1,\"1987\":1,\"1989\":2,\"1991\":1,\"1997\":1,\"2001\":6,\"2002\":3,\"2003\":1,\"2004\":6,\"2005\":1,\"2019\":1,\"2021\":1,\"2022\":2,\"2023\":1,\"2027\":1,\"2029\":3,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2047\":1,\"2076\":1,\"2077\":1,\"2082\":1,\"2086\":7,\"2087\":6,\"2090\":3,\"2091\":1,\"2095\":3,\"2096\":1,\"2098\":1,\"2099\":3,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2125\":1,\"2126\":1,\"2127\":1,\"2131\":1,\"2133\":1,\"2136\":1,\"2155\":1,\"2156\":1,\"2165\":1,\"2166\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2171\":1,\"2175\":1,\"2176\":1,\"2177\":1,\"2182\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2206\":1,\"2207\":1,\"2208\":1,\"2211\":1,\"2212\":1,\"2218\":1,\"2222\":1,\"2223\":1,\"2224\":1,\"2226\":1,\"2228\":1,\"2229\":1,\"2230\":1,\"2232\":1,\"2235\":1,\"2236\":1,\"2240\":1,\"2241\":1,\"2243\":3,\"2244\":3,\"2246\":1,\"2248\":2,\"2250\":1,\"2252\":1,\"2255\":4,\"2256\":1,\"2258\":1,\"2259\":1,\"2261\":1,\"2263\":4,\"2264\":3,\"2266\":1,\"2273\":1,\"2277\":1,\"2278\":1,\"2279\":3,\"2288\":1,\"2294\":1,\"2306\":1,\"2307\":1,\"2308\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2316\":1,\"2319\":1,\"2321\":1,\"2324\":1,\"2325\":1,\"2327\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2336\":1,\"2338\":1,\"2339\":1,\"2341\":1,\"2343\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2348\":1,\"2350\":1,\"2351\":1,\"2352\":1,\"2353\":1,\"2373\":2,\"2389\":1,\"2408\":1,\"2422\":1,\"2430\":1,\"2433\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1,\"2555\":2,\"2568\":2,\"2592\":4,\"2596\":2}}],[\"+text1\",{\"1\":{\"2474\":1,\"2649\":1}}],[\"+text\",{\"1\":{\"2472\":1,\"2476\":1,\"2648\":1}}],[\"+1\",{\"1\":{\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"2596\":1}}],[\"+=\",{\"1\":{\"217\":4,\"224\":4,\"231\":3,\"2386\":1,\"2592\":1}}],[\"+x\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2405\":1,\"2429\":1,\"2541\":1,\"2568\":1,\"2569\":1}}],[\"+\",{\"0\":{\"349\":1,\"2394\":1,\"2530\":1},\"1\":{\"21\":1,\"23\":1,\"26\":2,\"76\":1,\"115\":2,\"118\":2,\"124\":1,\"171\":5,\"175\":5,\"185\":3,\"193\":3,\"194\":4,\"206\":1,\"231\":1,\"238\":3,\"294\":1,\"670\":1,\"689\":1,\"697\":1,\"711\":2,\"713\":1,\"749\":2,\"835\":1,\"904\":1,\"917\":1,\"944\":1,\"1048\":1,\"1061\":1,\"1082\":1,\"1113\":5,\"1115\":4,\"1132\":1,\"1133\":2,\"1148\":2,\"1149\":2,\"1150\":2,\"1171\":4,\"1172\":4,\"1203\":2,\"1206\":3,\"1210\":2,\"1243\":1,\"1248\":1,\"1272\":2,\"1302\":2,\"1303\":2,\"1304\":4,\"1334\":1,\"1337\":4,\"1342\":1,\"1371\":5,\"1376\":3,\"1452\":3,\"1505\":2,\"1506\":2,\"1551\":4,\"1552\":5,\"1553\":4,\"1554\":3,\"1604\":1,\"1618\":1,\"1619\":2,\"1638\":2,\"1669\":2,\"1696\":1,\"1702\":1,\"1712\":1,\"1715\":1,\"1735\":2,\"1736\":1,\"1741\":1,\"1771\":1,\"1788\":1,\"1803\":1,\"1805\":1,\"1834\":1,\"1847\":2,\"1850\":1,\"1859\":1,\"1877\":1,\"1883\":1,\"1892\":3,\"1893\":3,\"1911\":1,\"1935\":1,\"1970\":4,\"1975\":4,\"1984\":1,\"2001\":2,\"2004\":2,\"2027\":4,\"2029\":2,\"2040\":1,\"2076\":4,\"2090\":2,\"2099\":1,\"2102\":1,\"2142\":2,\"2216\":1,\"2217\":1,\"2236\":2,\"2243\":4,\"2244\":12,\"2255\":12,\"2279\":12,\"2294\":4,\"2325\":1,\"2330\":1,\"2359\":1,\"2363\":2,\"2386\":1,\"2456\":1,\"2460\":1,\"2472\":6,\"2474\":3,\"2476\":6,\"2500\":14,\"2506\":2,\"2521\":1,\"2568\":1,\"2580\":1,\"2600\":3,\"2617\":14,\"2635\":14,\"2648\":6,\"2649\":3,\"2653\":2}}],[\"=transcript\",{\"1\":{\"2476\":1}}],[\"=>\",{\"1\":{\"629\":2,\"689\":1,\"799\":2,\"1695\":2,\"2360\":9,\"2458\":9,\"2523\":9,\"2582\":9}}],[\"======\",{\"1\":{\"2430\":2,\"2431\":2,\"2555\":2}}],[\"=====================\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"============================================================\",{\"1\":{\"2441\":2}}],[\"=========\",{\"1\":{\"2372\":2,\"2497\":2,\"2614\":2,\"2615\":2,\"2632\":2,\"2633\":2}}],[\"==\",{\"1\":{\"56\":1,\"111\":1,\"172\":1,\"217\":1,\"231\":1,\"1133\":1,\"1204\":1,\"1214\":1,\"1244\":1,\"1273\":1,\"1385\":2,\"1387\":2,\"1391\":1,\"1427\":3,\"1553\":1,\"1661\":1,\"2086\":1,\"2087\":1,\"2099\":1,\"2102\":1,\"2210\":2,\"2359\":3,\"2386\":1,\"2456\":3,\"2460\":3,\"2500\":4,\"2501\":1,\"2521\":3,\"2522\":1,\"2568\":1,\"2580\":3,\"2581\":1,\"2592\":1,\"2607\":1,\"2615\":1,\"2617\":4,\"2624\":1,\"2633\":1,\"2635\":4}}],[\"=c\",{\"1\":{\"52\":1}}],[\"=n\",{\"1\":{\"32\":2}}],[\"=\",{\"1\":{\"21\":21,\"22\":9,\"23\":1,\"24\":1,\"56\":2,\"57\":1,\"59\":5,\"60\":5,\"69\":2,\"77\":1,\"78\":1,\"80\":1,\"99\":1,\"113\":6,\"115\":57,\"116\":22,\"117\":2,\"118\":6,\"119\":5,\"121\":4,\"122\":2,\"130\":9,\"136\":1,\"171\":5,\"172\":3,\"173\":6,\"174\":15,\"175\":7,\"185\":4,\"193\":4,\"194\":11,\"202\":1,\"203\":3,\"210\":3,\"211\":3,\"212\":3,\"214\":1,\"215\":1,\"216\":1,\"217\":20,\"218\":7,\"222\":3,\"223\":3,\"224\":15,\"225\":7,\"229\":3,\"230\":3,\"231\":22,\"232\":7,\"238\":8,\"239\":2,\"609\":2,\"611\":2,\"638\":6,\"668\":1,\"669\":1,\"670\":1,\"672\":1,\"689\":2,\"690\":1,\"691\":9,\"692\":4,\"693\":8,\"694\":6,\"697\":12,\"698\":7,\"699\":12,\"700\":13,\"712\":7,\"714\":2,\"715\":2,\"716\":2,\"718\":2,\"719\":2,\"720\":2,\"721\":2,\"722\":2,\"725\":9,\"726\":11,\"729\":1,\"730\":11,\"734\":1,\"744\":5,\"751\":4,\"752\":9,\"756\":17,\"760\":3,\"765\":3,\"766\":2,\"767\":1,\"778\":7,\"797\":2,\"798\":4,\"806\":4,\"815\":1,\"817\":1,\"818\":1,\"825\":18,\"827\":1,\"828\":1,\"830\":1,\"831\":3,\"835\":1,\"838\":2,\"857\":5,\"858\":9,\"868\":1,\"876\":2,\"885\":2,\"887\":1,\"900\":5,\"902\":5,\"904\":2,\"907\":1,\"928\":4,\"929\":1,\"932\":3,\"944\":2,\"959\":5,\"984\":1,\"987\":3,\"989\":4,\"992\":2,\"995\":2,\"996\":1,\"1010\":1,\"1013\":3,\"1015\":7,\"1017\":1,\"1028\":1,\"1031\":1,\"1046\":1,\"1047\":1,\"1048\":11,\"1049\":5,\"1050\":5,\"1051\":5,\"1052\":11,\"1053\":4,\"1054\":6,\"1055\":3,\"1056\":5,\"1057\":16,\"1058\":3,\"1059\":3,\"1060\":4,\"1061\":5,\"1062\":1,\"1063\":2,\"1064\":4,\"1065\":21,\"1066\":22,\"1067\":4,\"1068\":5,\"1069\":9,\"1070\":5,\"1071\":1,\"1072\":2,\"1073\":7,\"1074\":5,\"1075\":13,\"1076\":8,\"1077\":4,\"1078\":2,\"1079\":3,\"1080\":1,\"1081\":1,\"1082\":5,\"1083\":5,\"1084\":4,\"1093\":14,\"1096\":10,\"1097\":1,\"1098\":2,\"1099\":2,\"1101\":2,\"1106\":4,\"1107\":1,\"1108\":5,\"1113\":2,\"1115\":66,\"1119\":1,\"1127\":1,\"1132\":5,\"1133\":12,\"1134\":1,\"1138\":15,\"1139\":15,\"1140\":23,\"1141\":3,\"1145\":8,\"1148\":30,\"1149\":27,\"1150\":22,\"1158\":14,\"1166\":1,\"1167\":14,\"1168\":14,\"1169\":28,\"1170\":1,\"1171\":17,\"1172\":13,\"1173\":2,\"1176\":4,\"1178\":5,\"1179\":27,\"1180\":20,\"1181\":14,\"1182\":2,\"1187\":1,\"1190\":4,\"1191\":1,\"1192\":2,\"1193\":2,\"1194\":2,\"1195\":5,\"1196\":14,\"1197\":14,\"1198\":10,\"1200\":6,\"1201\":1,\"1202\":1,\"1203\":29,\"1204\":11,\"1206\":13,\"1207\":11,\"1211\":3,\"1212\":1,\"1214\":4,\"1215\":8,\"1216\":3,\"1218\":1,\"1219\":9,\"1220\":9,\"1222\":9,\"1224\":2,\"1225\":2,\"1226\":2,\"1239\":5,\"1244\":8,\"1255\":4,\"1256\":2,\"1257\":11,\"1269\":38,\"1270\":7,\"1271\":12,\"1272\":19,\"1273\":21,\"1282\":9,\"1284\":4,\"1285\":2,\"1286\":1,\"1287\":1,\"1289\":14,\"1336\":2,\"1339\":2,\"1348\":2,\"1352\":1,\"1371\":7,\"1373\":3,\"1374\":1,\"1375\":2,\"1376\":4,\"1377\":7,\"1383\":3,\"1384\":2,\"1385\":3,\"1386\":3,\"1387\":3,\"1389\":2,\"1390\":1,\"1391\":2,\"1395\":2,\"1397\":3,\"1400\":2,\"1402\":2,\"1404\":3,\"1405\":2,\"1406\":4,\"1407\":4,\"1408\":5,\"1410\":2,\"1412\":3,\"1414\":2,\"1416\":3,\"1417\":1,\"1418\":1,\"1423\":1,\"1424\":1,\"1426\":5,\"1427\":2,\"1428\":2,\"1429\":1,\"1431\":1,\"1432\":1,\"1435\":1,\"1441\":2,\"1444\":1,\"1445\":1,\"1452\":1,\"1454\":5,\"1455\":1,\"1463\":7,\"1465\":4,\"1505\":23,\"1510\":2,\"1511\":1,\"1515\":8,\"1516\":12,\"1517\":11,\"1522\":1,\"1523\":20,\"1524\":24,\"1525\":19,\"1528\":12,\"1529\":8,\"1531\":4,\"1534\":10,\"1539\":14,\"1551\":3,\"1552\":6,\"1553\":18,\"1554\":6,\"1558\":4,\"1600\":2,\"1602\":2,\"1611\":36,\"1616\":1,\"1617\":1,\"1618\":2,\"1619\":2,\"1626\":8,\"1638\":1,\"1643\":11,\"1644\":12,\"1645\":6,\"1654\":11,\"1658\":13,\"1659\":17,\"1660\":1,\"1661\":4,\"1662\":4,\"1665\":3,\"1669\":16,\"1671\":21,\"1688\":1,\"1695\":5,\"1696\":4,\"1697\":3,\"1698\":6,\"1702\":1,\"1703\":1,\"1704\":6,\"1705\":5,\"1706\":4,\"1707\":6,\"1708\":4,\"1711\":1,\"1712\":9,\"1713\":7,\"1714\":5,\"1715\":9,\"1719\":1,\"1735\":2,\"1741\":3,\"1742\":1,\"1746\":2,\"1756\":1,\"1759\":4,\"1761\":4,\"1763\":6,\"1765\":17,\"1771\":18,\"1773\":58,\"1776\":7,\"1777\":7,\"1778\":45,\"1785\":5,\"1786\":1,\"1787\":17,\"1788\":18,\"1791\":6,\"1797\":5,\"1798\":17,\"1800\":4,\"1801\":8,\"1803\":16,\"1804\":85,\"1805\":47,\"1810\":1,\"1833\":4,\"1834\":7,\"1835\":3,\"1836\":2,\"1837\":19,\"1838\":1,\"1839\":3,\"1840\":1,\"1841\":1,\"1843\":2,\"1844\":16,\"1845\":2,\"1846\":5,\"1847\":7,\"1848\":12,\"1849\":12,\"1850\":25,\"1851\":93,\"1852\":22,\"1855\":2,\"1856\":13,\"1857\":14,\"1858\":15,\"1859\":13,\"1860\":5,\"1861\":11,\"1862\":18,\"1863\":12,\"1864\":13,\"1865\":13,\"1866\":7,\"1867\":8,\"1868\":10,\"1869\":1,\"1870\":5,\"1871\":15,\"1872\":6,\"1873\":8,\"1874\":16,\"1876\":4,\"1877\":31,\"1878\":61,\"1879\":2,\"1880\":21,\"1883\":3,\"1890\":3,\"1892\":10,\"1893\":1,\"1894\":1,\"1895\":7,\"1896\":11,\"1897\":6,\"1898\":3,\"1900\":7,\"1902\":1,\"1905\":1,\"1906\":5,\"1909\":1,\"1910\":4,\"1912\":9,\"1914\":5,\"1915\":6,\"1917\":6,\"1918\":9,\"1919\":3,\"1920\":4,\"1921\":4,\"1922\":2,\"1925\":2,\"1926\":3,\"1928\":3,\"1929\":5,\"1931\":2,\"1932\":5,\"1935\":2,\"1936\":3,\"1938\":2,\"1939\":2,\"1940\":4,\"1941\":5,\"1943\":2,\"1947\":4,\"1948\":2,\"1949\":4,\"1951\":2,\"1953\":2,\"1954\":1,\"1955\":6,\"1956\":1,\"1958\":7,\"1960\":9,\"1961\":1,\"1962\":1,\"1967\":1,\"1968\":1,\"1970\":11,\"1971\":3,\"1972\":5,\"1975\":14,\"1984\":15,\"1985\":13,\"1987\":8,\"1989\":14,\"1991\":8,\"1993\":3,\"1996\":5,\"1997\":1,\"1999\":3,\"2000\":5,\"2001\":22,\"2002\":39,\"2003\":32,\"2004\":21,\"2006\":4,\"2007\":5,\"2008\":5,\"2009\":5,\"2010\":3,\"2011\":2,\"2012\":7,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":6,\"2019\":3,\"2020\":3,\"2021\":4,\"2022\":4,\"2023\":8,\"2026\":21,\"2027\":21,\"2029\":14,\"2044\":2,\"2046\":4,\"2049\":4,\"2052\":2,\"2054\":23,\"2055\":4,\"2059\":4,\"2064\":6,\"2068\":2,\"2070\":5,\"2076\":25,\"2079\":2,\"2082\":59,\"2084\":13,\"2086\":52,\"2087\":58,\"2089\":18,\"2090\":82,\"2091\":3,\"2095\":79,\"2096\":8,\"2097\":5,\"2098\":8,\"2099\":24,\"2100\":8,\"2101\":8,\"2102\":10,\"2103\":8,\"2104\":8,\"2105\":8,\"2107\":8,\"2108\":8,\"2109\":12,\"2110\":8,\"2111\":7,\"2112\":8,\"2113\":12,\"2114\":8,\"2115\":12,\"2116\":12,\"2117\":8,\"2118\":8,\"2120\":4,\"2121\":1,\"2125\":4,\"2126\":6,\"2128\":5,\"2129\":5,\"2130\":3,\"2131\":4,\"2132\":1,\"2133\":1,\"2134\":1,\"2135\":1,\"2136\":3,\"2137\":11,\"2142\":1,\"2149\":4,\"2151\":3,\"2157\":1,\"2161\":1,\"2163\":1,\"2170\":1,\"2176\":8,\"2177\":3,\"2178\":28,\"2179\":28,\"2180\":11,\"2181\":8,\"2182\":6,\"2183\":2,\"2184\":24,\"2188\":10,\"2189\":4,\"2190\":1,\"2191\":25,\"2193\":19,\"2194\":30,\"2195\":23,\"2196\":17,\"2197\":8,\"2198\":1,\"2199\":5,\"2200\":28,\"2201\":4,\"2208\":3,\"2209\":5,\"2210\":6,\"2211\":4,\"2213\":1,\"2214\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1,\"2222\":1,\"2230\":1,\"2236\":9,\"2237\":4,\"2240\":25,\"2241\":14,\"2243\":69,\"2244\":85,\"2245\":2,\"2246\":8,\"2248\":14,\"2250\":8,\"2254\":1,\"2255\":84,\"2256\":2,\"2257\":6,\"2259\":4,\"2260\":13,\"2261\":10,\"2262\":5,\"2263\":60,\"2264\":72,\"2265\":6,\"2266\":12,\"2267\":1,\"2274\":3,\"2278\":25,\"2279\":76,\"2280\":3,\"2290\":12,\"2292\":10,\"2294\":20,\"2295\":5,\"2296\":4,\"2301\":4,\"2302\":3,\"2303\":1,\"2304\":6,\"2305\":2,\"2314\":2,\"2316\":2,\"2317\":7,\"2320\":2,\"2325\":3,\"2328\":2,\"2330\":2,\"2335\":2,\"2340\":2,\"2345\":1,\"2347\":1,\"2349\":4,\"2352\":1,\"2357\":15,\"2358\":3,\"2359\":4,\"2360\":20,\"2363\":9,\"2364\":1,\"2365\":5,\"2367\":2,\"2368\":2,\"2369\":1,\"2371\":5,\"2372\":2,\"2386\":6,\"2392\":3,\"2425\":3,\"2432\":1,\"2455\":2,\"2456\":4,\"2457\":2,\"2458\":18,\"2460\":6,\"2470\":1,\"2472\":6,\"2474\":6,\"2476\":7,\"2478\":4,\"2482\":1,\"2485\":2,\"2486\":2,\"2487\":1,\"2490\":2,\"2491\":1,\"2494\":1,\"2497\":2,\"2498\":6,\"2500\":24,\"2501\":4,\"2506\":3,\"2507\":1,\"2508\":5,\"2510\":10,\"2512\":3,\"2513\":1,\"2514\":19,\"2515\":5,\"2520\":7,\"2521\":8,\"2522\":6,\"2523\":22,\"2528\":3,\"2548\":3,\"2568\":8,\"2578\":15,\"2579\":3,\"2580\":4,\"2581\":4,\"2582\":20,\"2584\":1,\"2592\":13,\"2596\":6,\"2599\":2,\"2600\":31,\"2604\":2,\"2605\":2,\"2606\":1,\"2607\":3,\"2609\":2,\"2610\":1,\"2612\":5,\"2614\":2,\"2615\":3,\"2616\":8,\"2617\":25,\"2621\":2,\"2622\":2,\"2623\":1,\"2624\":3,\"2626\":2,\"2627\":1,\"2630\":5,\"2632\":2,\"2633\":3,\"2634\":8,\"2635\":25,\"2643\":3,\"2647\":1,\"2648\":6,\"2649\":6,\"2653\":9,\"2654\":1,\"2655\":5,\"2657\":3,\"2658\":1,\"2659\":19,\"2660\":5}}],[\"kpu\",{\"1\":{\"2499\":1,\"2635\":1}}],[\"kpad\",{\"1\":{\"1369\":1,\"1473\":1}}],[\"korakot\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"koreascience\",{\"1\":{\"2032\":1}}],[\"koreancleaner\",{\"0\":{\"2127\":1},\"1\":{\"2127\":2}}],[\"korean\",{\"0\":{\"2127\":1},\"1\":{\"461\":3,\"2127\":2}}],[\"k=3\",{\"1\":{\"2040\":1}}],[\"k=none\",{\"1\":{\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1}}],[\"kr\",{\"1\":{\"2032\":1}}],[\"ks=4\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"ks\",{\"1\":{\"1578\":1,\"1579\":1,\"1580\":1,\"1660\":1,\"1661\":2,\"1662\":1}}],[\"ksz\",{\"1\":{\"1543\":1,\"1655\":4,\"1656\":1,\"1719\":4}}],[\"ksz=3\",{\"1\":{\"1656\":1}}],[\"ksz=\",{\"1\":{\"1506\":1,\"1543\":1}}],[\"kmeans\",{\"1\":{\"1528\":1}}],[\"k$\",{\"1\":{\"1517\":1}}],[\"kürzinger\",{\"1\":{\"1198\":1}}],[\"kv\",{\"1\":{\"785\":2}}],[\"kv=false\",{\"1\":{\"785\":2}}],[\"kwds\",{\"1\":{\"989\":1,\"2167\":1}}],[\"kw\",{\"1\":{\"709\":3}}],[\"kwargs2args\",{\"0\":{\"2321\":1,\"2329\":2},\"1\":{\"2321\":2,\"2329\":4}}],[\"kwargs\",{\"0\":{\"1008\":2,\"2312\":1,\"2322\":2},\"1\":{\"661\":1,\"672\":1,\"673\":1,\"676\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"692\":1,\"710\":4,\"754\":3,\"759\":1,\"767\":1,\"781\":1,\"797\":1,\"799\":1,\"820\":3,\"821\":3,\"826\":3,\"943\":1,\"944\":1,\"950\":1,\"955\":1,\"956\":1,\"959\":2,\"981\":1,\"989\":1,\"1008\":8,\"1057\":4,\"1086\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1113\":3,\"1116\":1,\"1130\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1164\":1,\"1165\":1,\"1171\":3,\"1172\":2,\"1177\":2,\"1181\":1,\"1187\":1,\"1199\":1,\"1202\":1,\"1206\":1,\"1209\":3,\"1212\":1,\"1213\":1,\"1219\":1,\"1243\":3,\"1245\":1,\"1246\":1,\"1251\":2,\"1252\":2,\"1253\":3,\"1254\":3,\"1278\":1,\"1279\":1,\"1281\":1,\"1286\":1,\"1287\":1,\"1327\":3,\"1371\":3,\"1452\":4,\"1454\":2,\"1517\":1,\"1551\":4,\"1552\":2,\"1553\":3,\"1554\":3,\"1559\":1,\"1605\":1,\"1612\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1646\":2,\"1647\":2,\"1665\":1,\"1681\":1,\"1718\":1,\"1744\":1,\"1760\":1,\"1773\":3,\"1806\":1,\"1828\":1,\"1830\":1,\"1832\":1,\"1837\":3,\"1838\":1,\"1840\":1,\"1850\":2,\"1852\":2,\"1855\":1,\"1892\":3,\"1893\":3,\"1953\":2,\"1955\":2,\"1970\":3,\"1975\":3,\"1980\":2,\"1984\":3,\"2001\":1,\"2006\":1,\"2027\":3,\"2030\":1,\"2034\":1,\"2046\":2,\"2049\":1,\"2055\":1,\"2064\":1,\"2070\":1,\"2076\":3,\"2077\":2,\"2082\":3,\"2099\":1,\"2102\":1,\"2131\":1,\"2132\":1,\"2137\":1,\"2149\":1,\"2235\":2,\"2240\":3,\"2277\":2,\"2278\":3,\"2294\":2,\"2309\":1,\"2312\":2,\"2321\":1,\"2322\":2,\"2323\":1,\"2329\":1,\"2342\":1,\"2345\":1,\"2348\":1,\"2350\":1}}],[\"k\",{\"0\":{\"917\":1},\"1\":{\"686\":4,\"687\":4,\"688\":4,\"689\":4,\"743\":1,\"754\":1,\"785\":4,\"875\":1,\"917\":6,\"1001\":2,\"1031\":2,\"1048\":6,\"1076\":11,\"1144\":1,\"1209\":4,\"1228\":1,\"1245\":1,\"1248\":1,\"1345\":1,\"1347\":1,\"1368\":3,\"1369\":1,\"1370\":2,\"1372\":3,\"1375\":3,\"1378\":2,\"1379\":2,\"1381\":2,\"1464\":2,\"1472\":3,\"1473\":1,\"1528\":1,\"1529\":1,\"1546\":3,\"1568\":1,\"1575\":3,\"1594\":1,\"1663\":2,\"1664\":2,\"1665\":2,\"1670\":1,\"1671\":1,\"1683\":2,\"1688\":1,\"1693\":1,\"1696\":1,\"1698\":1,\"1755\":1,\"1756\":1,\"1761\":2,\"1763\":2,\"1768\":2,\"1805\":2,\"1950\":1,\"2040\":3,\"2063\":1,\"2074\":2,\"2075\":2,\"2143\":1,\"2154\":2,\"2168\":1,\"2170\":1,\"2253\":1,\"2514\":2,\"2592\":1,\"2659\":2}}],[\"khz\",{\"1\":{\"648\":1}}],[\"khudanpur\",{\"1\":{\"130\":1}}],[\"kumar\",{\"1\":{\"130\":1}}],[\"kuang\",{\"1\":{\"113\":1}}],[\"kab\",{\"1\":{\"2460\":1}}],[\"kaiser\",{\"1\":{\"952\":1,\"1860\":1,\"1883\":2}}],[\"kanda\",{\"1\":{\"940\":1}}],[\"kana\",{\"0\":{\"2141\":1},\"1\":{\"461\":1,\"2141\":1}}],[\"kana=false\",{\"1\":{\"224\":1}}],[\"kan\",{\"1\":{\"206\":2,\"240\":1,\"1860\":1,\"2361\":2,\"2363\":31,\"2450\":1,\"2506\":4,\"2510\":4,\"2512\":16,\"2517\":1,\"2650\":2,\"2653\":33,\"2657\":16}}],[\"karthik\",{\"1\":{\"130\":1}}],[\"karita\",{\"1\":{\"130\":2,\"166\":1,\"167\":1,\"176\":1,\"199\":1}}],[\"kamo\",{\"1\":{\"130\":1,\"1906\":1,\"2019\":2,\"2357\":1,\"2578\":1,\"2599\":1,\"2600\":2}}],[\"kazuya\",{\"1\":{\"130\":1}}],[\"katsuki\",{\"1\":{\"130\":1}}],[\"kaldiwriter\",{\"0\":{\"986\":1},\"1\":{\"986\":2}}],[\"kaldireader\",{\"0\":{\"985\":1},\"1\":{\"985\":2}}],[\"kaldiio\",{\"1\":{\"171\":2,\"174\":1,\"175\":2,\"185\":2,\"193\":2,\"194\":2,\"202\":2,\"238\":6,\"2384\":1,\"2386\":3,\"2432\":1,\"2514\":2,\"2659\":2}}],[\"kaldi\",{\"0\":{\"134\":1,\"169\":1,\"181\":1,\"2222\":1,\"2224\":1},\"1\":{\"5\":1,\"7\":1,\"15\":2,\"16\":2,\"49\":1,\"57\":1,\"58\":1,\"84\":6,\"85\":2,\"134\":18,\"135\":3,\"141\":1,\"167\":9,\"168\":1,\"169\":4,\"178\":9,\"179\":1,\"181\":4,\"196\":8,\"200\":1,\"204\":1,\"234\":8,\"235\":2,\"237\":2,\"301\":2,\"536\":1,\"952\":2,\"1013\":3,\"1014\":1,\"1015\":6,\"1382\":1,\"2222\":2,\"2224\":2,\"2354\":1,\"2372\":4,\"2373\":2,\"2381\":1,\"2382\":3,\"2384\":7,\"2385\":2,\"2386\":1,\"2387\":1,\"2388\":1,\"2412\":1,\"2421\":1,\"2429\":4,\"2430\":6,\"2431\":2,\"2524\":1,\"2544\":1,\"2554\":4,\"2555\":6,\"2564\":1,\"2568\":3,\"2638\":2}}],[\"k2\",{\"0\":{\"315\":1,\"485\":1},\"1\":{\"111\":1,\"113\":5,\"127\":1,\"315\":4,\"485\":4,\"1047\":1,\"1057\":6,\"1076\":1,\"1101\":1,\"1102\":1,\"1427\":2}}],[\"knowledge\",{\"1\":{\"2388\":1,\"2524\":1}}],[\"known\",{\"1\":{\"143\":1,\"1245\":2,\"1255\":1,\"2309\":1}}],[\"know\",{\"1\":{\"82\":1,\"1243\":1,\"2381\":1,\"2391\":1,\"2423\":1,\"2527\":1,\"2546\":1}}],[\"k80s\",{\"1\":{\"44\":1}}],[\"kindly\",{\"1\":{\"2388\":1,\"2524\":1}}],[\"kinds\",{\"1\":{\"32\":1}}],[\"kinoshita\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"kiyono\",{\"1\":{\"130\":1}}],[\"kim\",{\"1\":{\"23\":2,\"119\":1,\"1660\":2,\"1661\":2,\"1662\":2}}],[\"keqi\",{\"1\":{\"2586\":1,\"2590\":1,\"2600\":2}}],[\"keithito\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"kenlm\",{\"1\":{\"2294\":1,\"2499\":2,\"2635\":2}}],[\"keops\",{\"0\":{\"1291\":1}}],[\"keops=false\",{\"1\":{\"1248\":1}}],[\"kernerl\",{\"1\":{\"723\":1,\"1148\":1,\"1203\":1,\"1505\":1,\"2054\":1}}],[\"kernel=\",{\"1\":{\"1508\":1,\"1605\":1,\"1631\":1}}],[\"kernels=\",{\"1\":{\"2074\":1,\"2075\":1}}],[\"kernels=2\",{\"1\":{\"2063\":1}}],[\"kernels\",{\"0\":{\"1818\":1},\"1\":{\"722\":2,\"1143\":1,\"1154\":1,\"1186\":2,\"1210\":2,\"1765\":1,\"1800\":1,\"1804\":2,\"1805\":1,\"1818\":1}}],[\"kernel\",{\"0\":{\"1100\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":1},\"1\":{\"21\":8,\"115\":14,\"116\":1,\"701\":1,\"708\":1,\"712\":4,\"713\":3,\"722\":1,\"723\":2,\"736\":1,\"737\":2,\"740\":7,\"741\":7,\"749\":3,\"754\":2,\"770\":1,\"775\":7,\"776\":7,\"786\":3,\"835\":2,\"1051\":5,\"1052\":4,\"1054\":4,\"1055\":4,\"1069\":3,\"1081\":1,\"1097\":2,\"1100\":2,\"1132\":3,\"1140\":1,\"1142\":1,\"1143\":2,\"1148\":5,\"1149\":4,\"1150\":3,\"1151\":1,\"1153\":1,\"1167\":1,\"1168\":1,\"1169\":2,\"1170\":3,\"1186\":1,\"1196\":1,\"1197\":1,\"1198\":3,\"1203\":5,\"1227\":1,\"1228\":1,\"1241\":4,\"1243\":2,\"1245\":5,\"1247\":1,\"1248\":3,\"1249\":1,\"1269\":4,\"1272\":3,\"1298\":4,\"1299\":4,\"1301\":4,\"1302\":4,\"1303\":4,\"1304\":4,\"1334\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1370\":1,\"1377\":3,\"1378\":1,\"1379\":1,\"1478\":3,\"1480\":1,\"1505\":5,\"1508\":2,\"1510\":1,\"1511\":1,\"1512\":1,\"1516\":5,\"1518\":1,\"1520\":1,\"1522\":9,\"1523\":9,\"1540\":1,\"1543\":1,\"1545\":6,\"1546\":1,\"1555\":1,\"1576\":3,\"1577\":3,\"1596\":1,\"1645\":3,\"1655\":2,\"1656\":1,\"1658\":3,\"1659\":3,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1669\":3,\"1674\":1,\"1687\":1,\"1689\":1,\"1716\":1,\"1719\":2,\"1752\":2,\"1753\":1,\"1754\":1,\"1761\":1,\"1763\":1,\"1765\":9,\"1769\":1,\"1771\":6,\"1772\":3,\"1776\":3,\"1777\":3,\"1778\":10,\"1782\":1,\"1787\":6,\"1788\":6,\"1795\":1,\"1798\":6,\"1800\":10,\"1801\":3,\"1803\":9,\"1804\":25,\"1805\":11,\"1806\":1,\"1808\":3,\"1814\":1,\"1815\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":3,\"1834\":3,\"1835\":3,\"1844\":9,\"1845\":1,\"1846\":2,\"1847\":3,\"1848\":3,\"1849\":3,\"1850\":15,\"1851\":34,\"1852\":15,\"1856\":7,\"1857\":6,\"1858\":5,\"1861\":2,\"1862\":3,\"1863\":3,\"1864\":3,\"1865\":3,\"1866\":3,\"1867\":3,\"1868\":3,\"1870\":1,\"1871\":3,\"1872\":3,\"1873\":3,\"1874\":6,\"1876\":3,\"1877\":11,\"1878\":24,\"1880\":3,\"1917\":5,\"2003\":2,\"2026\":2,\"2029\":3,\"2042\":2,\"2047\":1,\"2054\":5,\"2064\":1,\"2070\":1,\"2087\":3,\"2090\":7,\"2095\":3,\"2243\":14,\"2244\":26,\"2255\":25,\"2257\":3,\"2261\":3,\"2263\":3,\"2264\":6,\"2265\":3,\"2279\":23,\"2290\":1,\"2292\":1,\"2297\":1,\"2440\":1,\"2564\":1}}],[\"kept\",{\"1\":{\"691\":1,\"693\":1,\"697\":1,\"797\":1,\"857\":1,\"1406\":1,\"1551\":3,\"1553\":3,\"1640\":1}}],[\"kevin\",{\"1\":{\"130\":1}}],[\"keeps\",{\"1\":{\"2387\":1}}],[\"keepdim=false\",{\"1\":{\"1685\":1}}],[\"keeping\",{\"1\":{\"1145\":1,\"1643\":1,\"1644\":1}}],[\"keep\",{\"1\":{\"49\":1,\"57\":1,\"85\":1,\"92\":1,\"112\":1,\"196\":1,\"200\":1,\"234\":1,\"247\":2,\"265\":2,\"269\":2,\"525\":2,\"826\":3,\"952\":1,\"987\":1,\"1340\":1,\"1375\":1,\"1379\":1,\"1552\":1,\"1664\":1,\"2186\":1,\"2202\":2,\"2204\":1,\"2275\":1,\"2281\":1,\"2395\":1,\"2440\":1,\"2531\":1,\"2558\":1,\"2584\":2}}],[\"keyparameters\",{\"1\":{\"2584\":1}}],[\"key>\",{\"1\":{\"2157\":2}}],[\"key4\",{\"1\":{\"1389\":1,\"1391\":1,\"1395\":1,\"1397\":1,\"1402\":1,\"1404\":1,\"1406\":1,\"1414\":1,\"1416\":1}}],[\"key3\",{\"1\":{\"1389\":1,\"1391\":1,\"1395\":1,\"1397\":1,\"1402\":1,\"1404\":1,\"1406\":1,\"1414\":1,\"1416\":1,\"1425\":2}}],[\"key2\",{\"1\":{\"1389\":1,\"1391\":1,\"1395\":1,\"1397\":1,\"1402\":1,\"1404\":1,\"1406\":2,\"1410\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":2,\"1423\":1,\"1425\":2,\"2193\":7}}],[\"key1\",{\"1\":{\"1389\":2,\"1391\":2,\"1395\":2,\"1397\":1,\"1402\":2,\"1404\":1,\"1406\":4,\"1410\":2,\"1412\":1,\"1414\":2,\"1416\":1,\"1418\":2,\"1421\":2,\"1423\":2,\"1425\":2,\"2193\":2}}],[\"keywords=yes\",{\"1\":{\"2414\":1}}],[\"keyword\",{\"1\":{\"1008\":1,\"1905\":1}}],[\"key=\",{\"1\":{\"605\":1,\"710\":1,\"1028\":1}}],[\"key=value\",{\"1\":{\"25\":1}}],[\"keylist\",{\"1\":{\"528\":2}}],[\"key\",{\"1\":{\"28\":1,\"30\":1,\"57\":3,\"59\":1,\"66\":2,\"116\":1,\"171\":4,\"174\":1,\"175\":2,\"182\":1,\"185\":4,\"193\":2,\"194\":1,\"238\":5,\"265\":1,\"269\":1,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"363\":4,\"368\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"437\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"570\":2,\"574\":2,\"605\":1,\"610\":3,\"623\":4,\"629\":1,\"640\":1,\"658\":1,\"668\":1,\"669\":1,\"670\":1,\"672\":5,\"691\":3,\"693\":3,\"697\":3,\"710\":2,\"725\":1,\"740\":3,\"741\":3,\"754\":2,\"771\":3,\"775\":3,\"776\":3,\"785\":8,\"797\":2,\"806\":1,\"809\":3,\"820\":2,\"821\":2,\"824\":2,\"826\":2,\"857\":3,\"909\":2,\"984\":1,\"992\":1,\"995\":1,\"1003\":2,\"1004\":2,\"1005\":2,\"1028\":1,\"1065\":4,\"1066\":4,\"1074\":1,\"1076\":14,\"1081\":4,\"1086\":6,\"1209\":7,\"1270\":1,\"1327\":1,\"1340\":1,\"1389\":1,\"1392\":1,\"1395\":1,\"1397\":1,\"1402\":1,\"1404\":1,\"1406\":1,\"1407\":2,\"1408\":1,\"1414\":1,\"1416\":1,\"1470\":1,\"1527\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1778\":7,\"1805\":7,\"1896\":1,\"1905\":1,\"1924\":2,\"1932\":2,\"1937\":2,\"1963\":1,\"1993\":3,\"2011\":2,\"2086\":9,\"2087\":9,\"2090\":10,\"2095\":9,\"2099\":1,\"2189\":1,\"2193\":9,\"2199\":1,\"2209\":2,\"2232\":1,\"2432\":2,\"2584\":6,\"2643\":3}}],[\"keys=true\",{\"1\":{\"2315\":1}}],[\"keys2\",{\"1\":{\"2193\":2}}],[\"keys1\",{\"1\":{\"2193\":1}}],[\"keys>\",{\"1\":{\"2157\":1}}],[\"keys\",{\"0\":{\"1340\":1},\"1\":{\"24\":3,\"59\":3,\"66\":1,\"217\":1,\"224\":1,\"231\":1,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"635\":1,\"691\":5,\"692\":2,\"697\":6,\"725\":1,\"754\":3,\"797\":2,\"806\":1,\"820\":3,\"821\":3,\"826\":3,\"913\":1,\"989\":1,\"1015\":1,\"1270\":1,\"1340\":2,\"1389\":1,\"1392\":1,\"1395\":1,\"1398\":1,\"1400\":1,\"1402\":1,\"1406\":1,\"1410\":1,\"1414\":1,\"1659\":1,\"2099\":3,\"2102\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2193\":2,\"2209\":3,\"2501\":1,\"2514\":1,\"2522\":1,\"2581\":1,\"2584\":3,\"2600\":2,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1,\"2659\":1}}],[\"kldivloss\",{\"1\":{\"1996\":1}}],[\"kldivergencelosswithoutflow\",{\"0\":{\"1854\":1},\"1\":{\"1854\":1}}],[\"kldivergenceloss\",{\"0\":{\"1853\":1},\"1\":{\"1853\":2}}],[\"kl\",{\"1\":{\"22\":6,\"825\":9,\"933\":3,\"1805\":3,\"1853\":3,\"1854\":2,\"1877\":3}}],[\"59\",{\"1\":{\"2444\":1}}],[\"591\",{\"1\":{\"2444\":1}}],[\"5|5\",{\"1\":{\"2572\":1}}],[\"5|\",{\"1\":{\"2572\":2}}],[\"5|1\",{\"1\":{\"2572\":1}}],[\"5|13\",{\"1\":{\"2441\":1}}],[\"5|2\",{\"1\":{\"2564\":1}}],[\"5|28\",{\"1\":{\"2440\":1}}],[\"5|0\",{\"1\":{\"2440\":1}}],[\"5epoch\",{\"1\":{\"2368\":1,\"2486\":1,\"2605\":1,\"2622\":1}}],[\"5e15\",{\"1\":{\"2154\":1}}],[\"55\",{\"1\":{\"632\":1,\"2151\":2,\"2441\":1}}],[\"5f\",{\"1\":{\"218\":1,\"225\":1,\"232\":1,\"2365\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2655\":1,\"2660\":1}}],[\"5th\",{\"1\":{\"149\":1,\"1400\":1}}],[\"52581\",{\"1\":{\"110\":1}}],[\"5268\",{\"1\":{\"17\":1}}],[\"58th\",{\"1\":{\"130\":1}}],[\"58\",{\"1\":{\"87\":2}}],[\"5gpus\",{\"0\":{\"41\":1}}],[\"5\",{\"0\":{\"242\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":2,\"2418\":1,\"2419\":1,\"2420\":1,\"2431\":1,\"2479\":1,\"2501\":1,\"2541\":1,\"2570\":1},\"1\":{\"19\":1,\"21\":1,\"22\":1,\"72\":1,\"113\":2,\"149\":1,\"190\":1,\"217\":1,\"224\":1,\"231\":4,\"235\":1,\"242\":2,\"286\":1,\"296\":1,\"691\":1,\"693\":1,\"694\":1,\"697\":1,\"698\":1,\"699\":1,\"730\":1,\"756\":1,\"802\":1,\"803\":1,\"807\":1,\"825\":2,\"838\":1,\"857\":1,\"900\":1,\"902\":1,\"904\":6,\"928\":1,\"940\":1,\"948\":1,\"1062\":2,\"1074\":2,\"1075\":10,\"1081\":1,\"1106\":1,\"1108\":1,\"1115\":4,\"1140\":1,\"1141\":1,\"1156\":1,\"1158\":1,\"1166\":1,\"1171\":1,\"1172\":1,\"1179\":1,\"1180\":1,\"1206\":1,\"1219\":1,\"1220\":2,\"1239\":1,\"1257\":1,\"1269\":1,\"1289\":2,\"1418\":1,\"1516\":2,\"1523\":1,\"1524\":1,\"1525\":1,\"1528\":1,\"1551\":1,\"1553\":1,\"1604\":1,\"1611\":1,\"1618\":2,\"1643\":1,\"1644\":1,\"1646\":1,\"1761\":50,\"1763\":50,\"1765\":4,\"1778\":9,\"1791\":1,\"1800\":4,\"1801\":3,\"1803\":3,\"1804\":6,\"1805\":59,\"1833\":1,\"1844\":3,\"1845\":2,\"1846\":1,\"1847\":3,\"1848\":1,\"1849\":1,\"1850\":12,\"1851\":7,\"1852\":15,\"1856\":4,\"1858\":1,\"1863\":1,\"1864\":1,\"1865\":2,\"1866\":1,\"1868\":1,\"1870\":1,\"1877\":9,\"1878\":6,\"1896\":1,\"1929\":1,\"1975\":1,\"1985\":1,\"1994\":1,\"2002\":4,\"2003\":3,\"2021\":6,\"2022\":6,\"2023\":6,\"2027\":1,\"2042\":1,\"2070\":1,\"2074\":2,\"2075\":2,\"2078\":1,\"2079\":1,\"2081\":1,\"2083\":1,\"2086\":5,\"2087\":5,\"2090\":3,\"2095\":6,\"2154\":1,\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2194\":3,\"2195\":1,\"2196\":2,\"2197\":1,\"2200\":1,\"2243\":3,\"2244\":7,\"2255\":6,\"2263\":6,\"2264\":8,\"2265\":1,\"2266\":1,\"2271\":1,\"2279\":7,\"2314\":2,\"2320\":2,\"2360\":1,\"2362\":2,\"2364\":1,\"2372\":1,\"2373\":3,\"2383\":1,\"2386\":1,\"2387\":1,\"2393\":1,\"2394\":1,\"2403\":2,\"2405\":1,\"2409\":1,\"2413\":1,\"2427\":1,\"2432\":1,\"2433\":3,\"2440\":1,\"2441\":1,\"2444\":1,\"2450\":1,\"2458\":1,\"2460\":2,\"2461\":1,\"2498\":1,\"2501\":1,\"2504\":3,\"2507\":1,\"2510\":2,\"2513\":1,\"2517\":1,\"2523\":1,\"2529\":1,\"2530\":1,\"2539\":2,\"2541\":1,\"2550\":1,\"2555\":3,\"2564\":1,\"2568\":1,\"2582\":1,\"2584\":6,\"2585\":1,\"2592\":1,\"2616\":1,\"2634\":1,\"2651\":1,\"2654\":1,\"2658\":1}}],[\"540\",{\"1\":{\"1082\":1}}],[\"5462\",{\"1\":{\"130\":1}}],[\"5458\",{\"1\":{\"130\":1}}],[\"54\",{\"1\":{\"17\":1,\"87\":1,\"2441\":2,\"2442\":1}}],[\"513\",{\"1\":{\"1804\":1,\"1863\":1,\"1878\":1}}],[\"512\",{\"1\":{\"116\":1,\"752\":1,\"778\":1,\"1065\":1,\"1066\":1,\"1075\":1,\"1115\":6,\"1158\":1,\"1207\":1,\"1269\":7,\"1373\":1,\"1376\":1,\"1377\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1604\":1,\"1626\":1,\"1639\":1,\"1643\":1,\"1644\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1763\":2,\"1765\":1,\"1766\":1,\"1778\":2,\"1786\":3,\"1801\":2,\"1803\":1,\"1804\":1,\"1805\":1,\"1844\":1,\"1850\":1,\"1851\":1,\"1852\":2,\"1857\":1,\"1866\":1,\"1870\":2,\"1877\":1,\"1878\":1,\"1910\":1,\"1912\":1,\"1918\":1,\"1993\":1,\"2002\":3,\"2003\":1,\"2070\":1,\"2084\":1,\"2086\":1,\"2087\":1,\"2089\":1,\"2090\":1,\"2095\":5,\"2210\":1,\"2211\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":5,\"2264\":2,\"2279\":1,\"2600\":1}}],[\"51890\",{\"1\":{\"44\":1}}],[\"5186\",{\"1\":{\"17\":1}}],[\"51\",{\"1\":{\"17\":1,\"44\":1}}],[\"5678\",{\"1\":{\"58\":1}}],[\"56\",{\"1\":{\"17\":1,\"51\":1,\"87\":1}}],[\"50s\",{\"1\":{\"2429\":1,\"2552\":1}}],[\"500\",{\"1\":{\"1400\":1,\"1427\":1,\"1428\":1,\"1528\":1,\"1778\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2519\":1}}],[\"50000steps\",{\"1\":{\"2520\":1}}],[\"50000\",{\"1\":{\"2514\":1,\"2659\":1}}],[\"5000\",{\"1\":{\"76\":2,\"115\":1,\"1077\":1,\"1093\":1,\"1148\":1,\"1169\":1,\"1410\":1,\"1412\":1,\"2054\":1,\"2349\":2}}],[\"5089420\",{\"1\":{\"885\":1,\"1706\":1,\"1707\":1}}],[\"50\",{\"1\":{\"17\":1,\"119\":1,\"240\":1,\"242\":1,\"689\":1,\"700\":1,\"880\":1,\"1048\":1,\"1138\":1,\"1139\":1,\"1652\":1,\"1654\":1,\"2359\":1,\"2456\":1,\"2460\":1,\"2521\":1,\"2522\":1,\"2580\":1,\"2581\":1}}],[\"506\",{\"1\":{\"17\":1,\"87\":1}}],[\"9cff98a78ceaa4d85843be0a50b369ec826b27f6\",{\"1\":{\"2529\":1}}],[\"9|5\",{\"1\":{\"2572\":1}}],[\"9|3\",{\"1\":{\"2564\":1}}],[\"9|0\",{\"1\":{\"2564\":1,\"2572\":2}}],[\"9|16\",{\"1\":{\"2440\":1}}],[\"9|\",{\"1\":{\"2440\":3,\"2564\":3,\"2572\":2}}],[\"9|2\",{\"1\":{\"2440\":2,\"2441\":1}}],[\"960h\",{\"1\":{\"2431\":3,\"2564\":1}}],[\"960hr\",{\"1\":{\"2377\":2}}],[\"962\",{\"1\":{\"87\":1}}],[\"9205\",{\"1\":{\"1604\":1,\"1655\":1,\"1719\":1}}],[\"9201\",{\"1\":{\"1604\":1,\"1655\":1,\"1719\":1}}],[\"9292\",{\"1\":{\"1082\":1}}],[\"9250505\",{\"1\":{\"700\":1,\"1048\":1,\"1138\":1,\"1139\":1}}],[\"9+\",{\"1\":{\"132\":1}}],[\"9995\",{\"1\":{\"2349\":2}}],[\"99995\",{\"1\":{\"2294\":1}}],[\"999995\",{\"1\":{\"1115\":2}}],[\"999\",{\"1\":{\"62\":1}}],[\"9\",{\"1\":{\"62\":1,\"174\":1,\"207\":2,\"294\":1,\"952\":1,\"1198\":1,\"1410\":1,\"1412\":1,\"1761\":1,\"1763\":1,\"1778\":1,\"1805\":1,\"1851\":2,\"1852\":1,\"1860\":1,\"1870\":3,\"1871\":1,\"1872\":1,\"1873\":1,\"1883\":1,\"1905\":1,\"1925\":1,\"1946\":1,\"1947\":1,\"2244\":2,\"2255\":2,\"2279\":2,\"2292\":1,\"2314\":2,\"2372\":2,\"2373\":1,\"2374\":2,\"2384\":1,\"2394\":1,\"2409\":1,\"2428\":1,\"2430\":1,\"2431\":1,\"2434\":2,\"2441\":1,\"2442\":1,\"2444\":1,\"2530\":1,\"2551\":1,\"2555\":1,\"2557\":2,\"2584\":1}}],[\"91300\",{\"1\":{\"17\":1}}],[\"970010\",{\"1\":{\"44\":1}}],[\"97\",{\"1\":{\"17\":1,\"1132\":1,\"1935\":1,\"1943\":1,\"2618\":1}}],[\"90\",{\"1\":{\"1946\":1,\"1947\":1}}],[\"907\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"903\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"9053040\",{\"1\":{\"700\":2,\"1048\":2,\"1138\":2,\"1139\":2}}],[\"900\",{\"1\":{\"26\":2}}],[\"90000\",{\"1\":{\"17\":1}}],[\"90200\",{\"1\":{\"17\":1}}],[\"90100\",{\"1\":{\"17\":1}}],[\"98epoch\",{\"1\":{\"2494\":1}}],[\"98\",{\"1\":{\"51\":1,\"174\":1}}],[\"9823\",{\"1\":{\"17\":1}}],[\"9897\",{\"1\":{\"17\":1}}],[\"945\",{\"1\":{\"87\":1}}],[\"9459\",{\"1\":{\"17\":1}}],[\"94\",{\"1\":{\"17\":2}}],[\"93\",{\"1\":{\"17\":1}}],[\"70968\",{\"1\":{\"2593\":1}}],[\"707\",{\"1\":{\"1905\":3,\"1921\":1,\"1922\":1,\"1936\":1,\"1938\":1,\"1939\":1}}],[\"79\",{\"1\":{\"2444\":1,\"2445\":1,\"2446\":2}}],[\"792\",{\"1\":{\"130\":1}}],[\"7|13\",{\"1\":{\"2564\":1}}],[\"7|6\",{\"1\":{\"2564\":1}}],[\"7|82\",{\"1\":{\"2441\":1}}],[\"7|23\",{\"1\":{\"2564\":1}}],[\"7|21\",{\"1\":{\"2441\":1}}],[\"7|28\",{\"1\":{\"2440\":1}}],[\"7|79\",{\"1\":{\"2441\":1}}],[\"7|0\",{\"1\":{\"2440\":2,\"2572\":1}}],[\"7|3\",{\"1\":{\"2440\":2}}],[\"773\",{\"1\":{\"2444\":1}}],[\"77\",{\"1\":{\"1523\":1}}],[\"776\",{\"1\":{\"87\":1}}],[\"7c\",{\"1\":{\"211\":1}}],[\"7+\",{\"1\":{\"132\":1}}],[\"78\",{\"1\":{\"2445\":1}}],[\"785\",{\"1\":{\"130\":1}}],[\"7861\",{\"1\":{\"17\":1}}],[\"7600\",{\"1\":{\"295\":10,\"1207\":1,\"1989\":1,\"2248\":1}}],[\"7658\",{\"1\":{\"130\":1}}],[\"7654\",{\"1\":{\"130\":1}}],[\"768\",{\"1\":{\"17\":1,\"1115\":4,\"1269\":1,\"1771\":1,\"1787\":1,\"1788\":2,\"1798\":1,\"1874\":1}}],[\"75\",{\"1\":{\"121\":1,\"689\":1,\"940\":1,\"1093\":1,\"1180\":1,\"1928\":1,\"2266\":1}}],[\"7471631\",{\"1\":{\"1529\":1,\"1568\":1}}],[\"74\",{\"1\":{\"17\":2}}],[\"740617\",{\"1\":{\"17\":1}}],[\"7\",{\"0\":{\"2572\":1},\"1\":{\"17\":4,\"21\":1,\"87\":1,\"110\":1,\"134\":1,\"197\":1,\"628\":1,\"1410\":1,\"1412\":1,\"1505\":1,\"1659\":1,\"1665\":1,\"1705\":1,\"1761\":27,\"1763\":27,\"1765\":3,\"1771\":1,\"1778\":4,\"1787\":1,\"1788\":2,\"1798\":1,\"1800\":2,\"1801\":1,\"1803\":2,\"1804\":4,\"1805\":32,\"1844\":2,\"1845\":1,\"1847\":1,\"1850\":4,\"1851\":3,\"1852\":4,\"1857\":1,\"1874\":1,\"1877\":4,\"1878\":3,\"2003\":1,\"2042\":1,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2279\":1,\"2444\":1,\"2446\":2,\"2461\":1,\"2568\":1}}],[\"7384873\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"73\",{\"1\":{\"2372\":1}}],[\"731593\",{\"1\":{\"17\":1}}],[\"731425\",{\"1\":{\"17\":1}}],[\"733486\",{\"1\":{\"17\":1}}],[\"730052\",{\"1\":{\"17\":1}}],[\"72476\",{\"1\":{\"17\":1}}],[\"72\",{\"1\":{\"17\":2}}],[\"72843\",{\"1\":{\"17\":1}}],[\"7171\",{\"1\":{\"130\":1}}],[\"7167\",{\"1\":{\"130\":1}}],[\"71428\",{\"1\":{\"17\":1}}],[\"71\",{\"1\":{\"17\":1}}],[\"8704\",{\"1\":{\"2600\":1}}],[\"873\",{\"1\":{\"87\":1}}],[\"82\",{\"1\":{\"2444\":1,\"2445\":1,\"2446\":1}}],[\"82r\",{\"1\":{\"230\":1}}],[\"8|6\",{\"1\":{\"2564\":1}}],[\"8|3\",{\"1\":{\"2564\":1}}],[\"8|2\",{\"1\":{\"2564\":1}}],[\"8|22\",{\"1\":{\"2441\":1}}],[\"8|5\",{\"1\":{\"2564\":1}}],[\"8|46\",{\"1\":{\"2564\":1}}],[\"8|79\",{\"1\":{\"2441\":2}}],[\"8|14\",{\"1\":{\"2564\":1}}],[\"8|1\",{\"1\":{\"2441\":1,\"2564\":1}}],[\"8|82\",{\"1\":{\"2441\":2}}],[\"8|0\",{\"1\":{\"2440\":2,\"2572\":1}}],[\"8691481\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"8k\",{\"1\":{\"1551\":2,\"1553\":2,\"2617\":2,\"2635\":2}}],[\"8khz\",{\"1\":{\"48\":1}}],[\"8oog7ee4sz8\",{\"1\":{\"952\":1}}],[\"8\",{\"1\":{\"44\":1,\"49\":1,\"64\":2,\"115\":1,\"135\":2,\"150\":2,\"241\":2,\"242\":1,\"611\":1,\"721\":3,\"1211\":1,\"1269\":1,\"1286\":1,\"1336\":1,\"1337\":1,\"1377\":1,\"1523\":1,\"1645\":1,\"1658\":1,\"1659\":1,\"1763\":2,\"1765\":2,\"1766\":2,\"1778\":3,\"1786\":2,\"1800\":4,\"1801\":2,\"1803\":2,\"1804\":5,\"1805\":3,\"1844\":2,\"1850\":2,\"1851\":2,\"1852\":2,\"1857\":2,\"1861\":1,\"1870\":1,\"1877\":3,\"1878\":3,\"1930\":2,\"1932\":4,\"1950\":1,\"2049\":1,\"2055\":1,\"2059\":1,\"2064\":1,\"2099\":1,\"2155\":1,\"2268\":1,\"2290\":1,\"2317\":1,\"2364\":1,\"2371\":1,\"2372\":1,\"2444\":1,\"2445\":4,\"2446\":1,\"2482\":1,\"2494\":1,\"2612\":1,\"2630\":1}}],[\"801\",{\"1\":{\"87\":1}}],[\"8000\",{\"1\":{\"2184\":1,\"2200\":1,\"2371\":1,\"2612\":1,\"2630\":1}}],[\"800\",{\"1\":{\"26\":4,\"2558\":1}}],[\"80\",{\"1\":{\"21\":1,\"75\":3,\"102\":1,\"171\":1,\"175\":1,\"239\":2,\"295\":10,\"752\":1,\"778\":1,\"959\":2,\"987\":1,\"1158\":1,\"1207\":1,\"1765\":1,\"1778\":1,\"1803\":1,\"1805\":1,\"1834\":1,\"1844\":1,\"1850\":1,\"1852\":1,\"1857\":1,\"1859\":1,\"1862\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1877\":1,\"1912\":1,\"1919\":1,\"1948\":1,\"1989\":2,\"2012\":1,\"2236\":1,\"2248\":2,\"2261\":1,\"2267\":1,\"2445\":1}}],[\"8041\",{\"1\":{\"17\":1}}],[\"8192\",{\"1\":{\"1761\":1,\"1763\":1}}],[\"81\",{\"1\":{\"17\":1,\"2446\":1}}],[\"84\",{\"1\":{\"17\":2}}],[\"890\",{\"1\":{\"75\":2}}],[\"89900\",{\"1\":{\"17\":1}}],[\"89800\",{\"1\":{\"17\":1}}],[\"89700\",{\"1\":{\"17\":1}}],[\"83\",{\"1\":{\"17\":1,\"1385\":4,\"1387\":4}}],[\"61\",{\"1\":{\"2593\":1}}],[\"613215\",{\"1\":{\"17\":1}}],[\"6ch\",{\"1\":{\"2492\":5,\"2628\":5}}],[\"6|2\",{\"1\":{\"2441\":2}}],[\"6|22\",{\"1\":{\"2441\":1}}],[\"6|0\",{\"1\":{\"2440\":2,\"2441\":1}}],[\"6|17\",{\"1\":{\"2441\":1}}],[\"6|16\",{\"1\":{\"2441\":1}}],[\"6|18\",{\"1\":{\"2441\":1}}],[\"6|1\",{\"1\":{\"2440\":1,\"2564\":1}}],[\"667\",{\"1\":{\"1778\":1,\"1804\":1,\"1805\":1,\"1877\":1,\"1878\":1,\"2364\":1}}],[\"62\",{\"1\":{\"1778\":1,\"1852\":1,\"1860\":1,\"1870\":3,\"1883\":1}}],[\"6730918\",{\"1\":{\"1712\":1,\"1715\":1}}],[\"650\",{\"1\":{\"1958\":1}}],[\"65\",{\"1\":{\"1115\":4}}],[\"6vhc20v\",{\"1\":{\"216\":1}}],[\"60\",{\"1\":{\"2392\":2,\"2425\":2,\"2512\":1,\"2528\":2,\"2548\":2,\"2657\":1}}],[\"6006\",{\"1\":{\"17\":1}}],[\"6084\",{\"1\":{\"17\":1}}],[\"6901\",{\"1\":{\"17\":1}}],[\"6931\",{\"1\":{\"17\":1}}],[\"64404008\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"640\",{\"1\":{\"122\":1,\"2592\":1}}],[\"6462\",{\"1\":{\"17\":1}}],[\"64\",{\"1\":{\"17\":1,\"144\":2,\"1180\":1,\"1516\":1,\"1522\":1,\"1523\":1,\"1671\":2,\"1761\":12,\"1763\":12,\"1776\":1,\"1803\":1,\"1804\":1,\"1805\":12,\"1850\":3,\"1851\":3,\"1852\":2,\"1861\":1,\"1862\":2,\"1871\":1,\"1872\":1,\"1873\":1,\"1880\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2257\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2304\":1,\"2558\":1}}],[\"63\",{\"1\":{\"17\":1,\"87\":1,\"239\":2,\"2444\":1}}],[\"6\",{\"0\":{\"2571\":1},\"1\":{\"17\":3,\"58\":1,\"65\":1,\"73\":1,\"91\":1,\"115\":1,\"116\":1,\"720\":3,\"900\":3,\"902\":3,\"961\":2,\"1115\":2,\"1148\":1,\"1149\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1203\":1,\"1204\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1418\":1,\"1463\":1,\"1505\":1,\"1539\":1,\"1669\":1,\"1671\":1,\"1696\":1,\"1698\":1,\"1705\":1,\"1761\":1,\"1763\":1,\"1771\":1,\"1778\":2,\"1788\":2,\"1798\":1,\"1804\":1,\"1805\":2,\"1850\":1,\"1851\":1,\"1852\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"2001\":1,\"2004\":1,\"2026\":1,\"2029\":1,\"2054\":1,\"2090\":2,\"2095\":1,\"2243\":3,\"2244\":3,\"2255\":2,\"2257\":1,\"2259\":2,\"2261\":1,\"2263\":1,\"2264\":3,\"2270\":1,\"2272\":1,\"2279\":2,\"2374\":2,\"2394\":1,\"2397\":1,\"2405\":2,\"2409\":1,\"2429\":2,\"2434\":2,\"2444\":2,\"2445\":3,\"2446\":2,\"2498\":1,\"2530\":1,\"2533\":1,\"2541\":2,\"2552\":2,\"2553\":1,\"2557\":2,\"2584\":1,\"2616\":1,\"2634\":1}}],[\"jldw4lfod1p8fk2mgoizo\",{\"1\":{\"2512\":1}}],[\"jxoun5m2\",{\"1\":{\"2510\":1}}],[\"jxttwmwxzfo8lt5kjzybjng\",{\"1\":{\"2500\":1}}],[\"jvs010\",{\"1\":{\"2363\":1,\"2653\":1}}],[\"jvs\",{\"1\":{\"2363\":4,\"2506\":3,\"2653\":4}}],[\"jvp\",{\"1\":{\"1187\":1,\"1202\":1}}],[\"jr\",{\"1\":{\"2134\":1}}],[\"jetsgenerator\",{\"0\":{\"1851\":1},\"1\":{\"1851\":1}}],[\"jets\",{\"0\":{\"1829\":1,\"1841\":1,\"1842\":1,\"1850\":3,\"1851\":1,\"1879\":1,\"1881\":1,\"1889\":1},\"1\":{\"1829\":1,\"1841\":2,\"1842\":1,\"1850\":12,\"1851\":3,\"1879\":3,\"1881\":1,\"1889\":1}}],[\"jeeweon\",{\"1\":{\"1132\":1}}],[\"jensen\",{\"1\":{\"825\":1}}],[\"jhu\",{\"1\":{\"940\":1}}],[\"js\",{\"1\":{\"600\":2,\"633\":2,\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"jsut\",{\"1\":{\"222\":1,\"223\":1,\"295\":8,\"2363\":26,\"2506\":7,\"2653\":26}}],[\"json2trn\",{\"0\":{\"551\":1,\"554\":1,\"557\":1},\"1\":{\"551\":2,\"554\":2,\"557\":2}}],[\"json2text\",{\"0\":{\"549\":1},\"1\":{\"549\":2}}],[\"json2sctm\",{\"0\":{\"546\":1},\"1\":{\"546\":2}}],[\"jsons\",{\"1\":{\"493\":2,\"515\":2,\"517\":2,\"566\":9}}],[\"json\",{\"0\":{\"185\":1,\"193\":1,\"239\":1,\"298\":1,\"515\":1,\"560\":1,\"600\":1,\"633\":1},\"1\":{\"16\":1,\"84\":4,\"168\":1,\"169\":1,\"171\":9,\"172\":2,\"175\":6,\"179\":1,\"181\":1,\"185\":7,\"191\":1,\"193\":6,\"194\":4,\"203\":2,\"235\":1,\"239\":6,\"245\":2,\"247\":2,\"249\":2,\"251\":4,\"255\":4,\"257\":2,\"259\":4,\"261\":2,\"263\":2,\"265\":4,\"267\":2,\"269\":4,\"276\":1,\"281\":1,\"298\":3,\"493\":1,\"515\":3,\"517\":1,\"546\":2,\"549\":2,\"551\":2,\"554\":2,\"557\":2,\"560\":8,\"566\":1,\"570\":1,\"574\":1,\"576\":2,\"600\":3,\"629\":1,\"633\":3,\"641\":2,\"765\":1,\"798\":1,\"909\":1,\"987\":1,\"1003\":1,\"1004\":1,\"1005\":2,\"1028\":2,\"1402\":4,\"1404\":4}}],[\"jp\",{\"1\":{\"222\":6,\"223\":6,\"295\":4}}],[\"j4\",{\"1\":{\"167\":1,\"178\":1}}],[\"jan\",{\"1\":{\"2384\":1,\"2441\":1,\"2442\":1}}],[\"javascript\",{\"1\":{\"2360\":2,\"2458\":2,\"2523\":2,\"2582\":2}}],[\"ja\",{\"1\":{\"2357\":1,\"2578\":1}}],[\"jamo\",{\"1\":{\"2126\":3}}],[\"jako202029757857763\",{\"1\":{\"2032\":1}}],[\"jax\",{\"1\":{\"1757\":1}}],[\"jaso\",{\"0\":{\"2126\":1},\"1\":{\"461\":2,\"2126\":2}}],[\"jaconv\",{\"1\":{\"461\":1}}],[\"japanese\",{\"0\":{\"219\":1,\"220\":1},\"1\":{\"136\":1,\"2357\":2,\"2363\":3,\"2506\":1,\"2578\":2,\"2653\":3}}],[\"jahn\",{\"1\":{\"130\":1}}],[\"j\",{\"1\":{\"134\":3,\"144\":2,\"196\":1,\"234\":1,\"1010\":2,\"1011\":7,\"1462\":2,\"1463\":2,\"1604\":1,\"1655\":1,\"1715\":1,\"1719\":1,\"2055\":1,\"2064\":1,\"2510\":1}}],[\"jiankang\",{\"1\":{\"2030\":1}}],[\"jiatongs\",{\"1\":{\"2380\":1,\"2388\":1,\"2406\":1,\"2447\":1,\"2524\":1}}],[\"jiatong\",{\"1\":{\"130\":4,\"1132\":1,\"2002\":2,\"2356\":1,\"2380\":1,\"2388\":1,\"2406\":1,\"2447\":1,\"2516\":1,\"2524\":1,\"2576\":1}}],[\"jik876\",{\"1\":{\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1}}],[\"jinchuan\",{\"1\":{\"130\":1}}],[\"jin\",{\"1\":{\"130\":1}}],[\"jing\",{\"1\":{\"130\":1,\"2618\":1}}],[\"jiro\",{\"1\":{\"130\":1}}],[\"junior\",{\"1\":{\"2134\":1}}],[\"jung\",{\"1\":{\"2055\":1,\"2064\":1}}],[\"june\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"jupyter\",{\"1\":{\"138\":1}}],[\"ju\",{\"1\":{\"130\":2,\"2618\":1}}],[\"july\",{\"1\":{\"176\":1}}],[\"jul\",{\"1\":{\"130\":1}}],[\"just\",{\"1\":{\"56\":1,\"58\":1,\"59\":1,\"69\":1,\"71\":1,\"74\":1,\"80\":1,\"84\":1,\"626\":1,\"727\":2,\"728\":2,\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"952\":1,\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1,\"1356\":1,\"1552\":1,\"2050\":1,\"2148\":1,\"2168\":1,\"2372\":1,\"2441\":1,\"2529\":1,\"2542\":1,\"2550\":1,\"2573\":1,\"2584\":3}}],[\"john\",{\"1\":{\"1529\":1,\"1568\":1,\"1705\":1}}],[\"jose\",{\"1\":{\"201\":1,\"202\":1}}],[\"josé\",{\"1\":{\"201\":1}}],[\"joined\",{\"1\":{\"2344\":2}}],[\"joinsegmenter\",{\"0\":{\"2295\":1},\"1\":{\"2295\":1}}],[\"join\",{\"0\":{\"2295\":1,\"2344\":1},\"1\":{\"175\":1,\"194\":1,\"217\":1,\"2295\":2,\"2296\":1,\"2344\":2,\"2472\":1,\"2474\":1,\"2476\":4,\"2500\":3,\"2568\":5,\"2617\":3,\"2635\":3,\"2648\":1,\"2649\":1}}],[\"jointtext2wav\",{\"0\":{\"1852\":1},\"1\":{\"1852\":2}}],[\"jointscore2wav\",{\"0\":{\"1778\":1},\"1\":{\"1778\":2}}],[\"jointnetwork\",{\"0\":{\"766\":1,\"1064\":1},\"1\":{\"700\":1,\"766\":1,\"1048\":1,\"1057\":1,\"1059\":1,\"1064\":2,\"1138\":1,\"1139\":1}}],[\"joint\",{\"0\":{\"117\":1,\"766\":1,\"1064\":1,\"1778\":2,\"1852\":2},\"1\":{\"114\":1,\"117\":7,\"692\":1,\"698\":2,\"699\":2,\"700\":3,\"725\":3,\"766\":15,\"812\":1,\"825\":13,\"1048\":3,\"1057\":3,\"1059\":3,\"1064\":11,\"1138\":3,\"1139\":3,\"1171\":1,\"1173\":1,\"1206\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1778\":2,\"1852\":2,\"2027\":1,\"2046\":1,\"2076\":1,\"2086\":1,\"2087\":5,\"2090\":5,\"2095\":4,\"2243\":4,\"2244\":4,\"2255\":4,\"2263\":4,\"2264\":4,\"2279\":4,\"2440\":1,\"2558\":1,\"2653\":2}}],[\"jointly\",{\"1\":{\"22\":1,\"1778\":1,\"1850\":1,\"1852\":1}}],[\"journal=\",{\"1\":{\"130\":3}}],[\"job=1\",{\"1\":{\"143\":2}}],[\"jobs\",{\"0\":{\"93\":1},\"1\":{\"32\":3,\"47\":1,\"93\":2,\"141\":1,\"143\":2,\"144\":3,\"148\":2,\"275\":2,\"276\":2,\"279\":2,\"280\":2,\"281\":2,\"282\":2,\"283\":2,\"284\":2,\"294\":2,\"297\":2,\"562\":2,\"2372\":1,\"2373\":1,\"2375\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2554\":1,\"2555\":1,\"2559\":1}}],[\"job\",{\"0\":{\"141\":1,\"142\":1,\"2220\":1},\"1\":{\"15\":1,\"40\":1,\"47\":1,\"85\":3,\"93\":1,\"94\":1,\"141\":3,\"143\":6,\"144\":1,\"235\":1,\"2220\":1,\"2372\":1,\"2385\":1,\"2429\":1,\"2554\":1,\"2558\":1,\"2569\":1,\"2571\":1}}],[\"4|4\",{\"1\":{\"2564\":1}}],[\"4|46\",{\"1\":{\"2564\":1}}],[\"4|42\",{\"1\":{\"2441\":1}}],[\"4|62\",{\"1\":{\"2564\":1}}],[\"4|1\",{\"1\":{\"2564\":1,\"2572\":3}}],[\"4|15\",{\"1\":{\"2564\":1}}],[\"4|11\",{\"1\":{\"2564\":1}}],[\"4|16\",{\"1\":{\"2441\":2}}],[\"4|0\",{\"1\":{\"2441\":1,\"2564\":1,\"2572\":2}}],[\"4|20\",{\"1\":{\"2441\":1}}],[\"46ozp\",{\"1\":{\"2396\":1,\"2532\":1}}],[\"4688000\",{\"1\":{\"2371\":1,\"2612\":1,\"2630\":1}}],[\"4e12\",{\"1\":{\"2154\":1}}],[\"41epoch\",{\"1\":{\"2585\":4}}],[\"41\",{\"1\":{\"1761\":4,\"1763\":4,\"1778\":1,\"1801\":1,\"1805\":5,\"1846\":1,\"1847\":1,\"1849\":1,\"1850\":1,\"1852\":1,\"1877\":1}}],[\"4d\",{\"1\":{\"1655\":1}}],[\"42\",{\"1\":{\"2444\":1}}],[\"4256c702685249202f333348a87c13143985b90b\",{\"1\":{\"2431\":1}}],[\"422a0112\",{\"1\":{\"2372\":2,\"2497\":2,\"2614\":2,\"2632\":2}}],[\"4281\",{\"1\":{\"130\":1}}],[\"4277\",{\"1\":{\"130\":1}}],[\"48k\",{\"1\":{\"1551\":1,\"1553\":2}}],[\"48khz\",{\"1\":{\"48\":1,\"49\":1,\"1551\":1,\"1553\":1}}],[\"48000\",{\"1\":{\"1463\":1}}],[\"481\",{\"1\":{\"110\":1}}],[\"404\",{\"1\":{\"2661\":1}}],[\"4012264\",{\"1\":{\"2617\":1,\"2635\":1}}],[\"40block\",{\"1\":{\"2460\":3,\"2461\":2}}],[\"4023\",{\"1\":{\"1400\":2}}],[\"4000\",{\"1\":{\"1400\":1}}],[\"4000000\",{\"1\":{\"2440\":1}}],[\"4000000steps\",{\"1\":{\"215\":1}}],[\"400000steps\",{\"1\":{\"214\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1}}],[\"400\",{\"1\":{\"1207\":1,\"1255\":1,\"1971\":2,\"2236\":1}}],[\"4096\",{\"1\":{\"1179\":1,\"1870\":1}}],[\"40\",{\"1\":{\"104\":1,\"1149\":1,\"1150\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"2059\":1,\"2255\":1,\"2260\":2,\"2440\":1,\"2554\":1,\"2600\":1}}],[\"40ms\",{\"1\":{\"102\":1}}],[\"40epoch\",{\"1\":{\"87\":1}}],[\"471\",{\"1\":{\"87\":1}}],[\"456\",{\"1\":{\"2328\":1}}],[\"45\",{\"1\":{\"1778\":1,\"1805\":2,\"1850\":1,\"1852\":1,\"1877\":1}}],[\"458\",{\"1\":{\"1076\":1}}],[\"4581968193699de14b56527296262dd76ab43557\",{\"1\":{\"817\":1,\"1958\":1}}],[\"4583\",{\"1\":{\"17\":1}}],[\"453\",{\"1\":{\"87\":2}}],[\"4gpus\",{\"0\":{\"34\":1},\"1\":{\"94\":1}}],[\"447c020t\",{\"1\":{\"2372\":2,\"2497\":2,\"2614\":2,\"2632\":2}}],[\"440c0213\",{\"1\":{\"2367\":2,\"2485\":2,\"2604\":2,\"2621\":2}}],[\"44\",{\"1\":{\"17\":1,\"48\":1,\"2504\":1,\"2651\":1}}],[\"4909\",{\"1\":{\"133\":1}}],[\"4994583\",{\"1\":{\"44\":2}}],[\"49\",{\"1\":{\"17\":2}}],[\"43\",{\"1\":{\"17\":1,\"51\":1}}],[\"4\",{\"0\":{\"171\":1,\"172\":1,\"173\":1,\"174\":2,\"186\":1,\"241\":1,\"2403\":1,\"2420\":1,\"2458\":1,\"2462\":1,\"2477\":1,\"2478\":1,\"2500\":1,\"2539\":1,\"2560\":1,\"2569\":1,\"2640\":1,\"2645\":1},\"1\":{\"16\":1,\"21\":3,\"22\":1,\"26\":2,\"34\":2,\"35\":1,\"58\":2,\"63\":2,\"94\":1,\"115\":6,\"116\":3,\"132\":1,\"134\":1,\"143\":2,\"186\":1,\"187\":2,\"235\":1,\"238\":1,\"241\":4,\"638\":10,\"694\":1,\"717\":1,\"718\":2,\"722\":2,\"762\":1,\"763\":1,\"798\":1,\"900\":1,\"902\":1,\"904\":4,\"907\":1,\"928\":1,\"940\":1,\"987\":2,\"1011\":4,\"1025\":1,\"1053\":1,\"1065\":1,\"1066\":2,\"1069\":1,\"1075\":1,\"1082\":1,\"1115\":4,\"1140\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1167\":2,\"1168\":2,\"1169\":1,\"1196\":2,\"1197\":2,\"1203\":1,\"1204\":1,\"1211\":1,\"1220\":2,\"1222\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1282\":1,\"1286\":1,\"1289\":2,\"1336\":1,\"1337\":1,\"1410\":1,\"1412\":1,\"1505\":1,\"1522\":1,\"1523\":1,\"1531\":2,\"1539\":1,\"1545\":1,\"1645\":2,\"1669\":1,\"1671\":1,\"1713\":2,\"1761\":13,\"1763\":15,\"1765\":2,\"1766\":2,\"1778\":7,\"1786\":2,\"1800\":4,\"1801\":5,\"1803\":2,\"1804\":7,\"1805\":21,\"1844\":2,\"1846\":3,\"1847\":3,\"1849\":2,\"1850\":8,\"1851\":5,\"1852\":9,\"1856\":4,\"1858\":5,\"1860\":2,\"1861\":1,\"1862\":4,\"1864\":2,\"1868\":1,\"1870\":4,\"1877\":9,\"1878\":6,\"1941\":4,\"1960\":1,\"1993\":1,\"1999\":1,\"2001\":1,\"2002\":2,\"2003\":1,\"2004\":1,\"2026\":1,\"2029\":1,\"2054\":1,\"2090\":1,\"2095\":2,\"2099\":2,\"2102\":2,\"2154\":1,\"2176\":1,\"2196\":1,\"2243\":2,\"2244\":2,\"2255\":2,\"2261\":1,\"2262\":1,\"2263\":2,\"2264\":3,\"2267\":2,\"2271\":1,\"2279\":1,\"2314\":5,\"2320\":1,\"2367\":1,\"2371\":1,\"2373\":4,\"2375\":2,\"2377\":4,\"2394\":1,\"2398\":1,\"2401\":2,\"2403\":1,\"2430\":1,\"2433\":4,\"2436\":4,\"2440\":3,\"2444\":1,\"2445\":3,\"2446\":3,\"2460\":1,\"2461\":1,\"2481\":2,\"2485\":1,\"2492\":1,\"2494\":1,\"2498\":1,\"2500\":1,\"2504\":3,\"2530\":1,\"2537\":2,\"2539\":1,\"2555\":4,\"2558\":3,\"2560\":1,\"2562\":4,\"2564\":2,\"2568\":1,\"2569\":1,\"2584\":3,\"2585\":1,\"2600\":1,\"2604\":1,\"2612\":1,\"2616\":1,\"2618\":2,\"2621\":1,\"2628\":1,\"2630\":1,\"2634\":1,\"2638\":1,\"2651\":2}}],[\"hmm\",{\"1\":{\"2385\":1}}],[\"hc\",{\"1\":{\"1598\":3,\"1599\":1,\"1648\":1,\"1652\":2,\"1654\":2}}],[\"h=1\",{\"1\":{\"1314\":1}}],[\"h=hidden\",{\"1\":{\"1243\":1}}],[\"h=8\",{\"1\":{\"732\":1,\"748\":1,\"784\":1}}],[\"h2\",{\"1\":{\"1160\":2,\"1161\":2,\"1164\":2,\"1165\":2,\"1177\":2,\"1209\":2,\"1252\":2,\"1253\":2,\"1254\":2}}],[\"h2r3\",{\"1\":{\"794\":1}}],[\"h2r2\",{\"1\":{\"794\":2}}],[\"h2r1\",{\"1\":{\"794\":2}}],[\"h1\",{\"1\":{\"1160\":2,\"1161\":2,\"1164\":2,\"1165\":2,\"1177\":2,\"1209\":2,\"1252\":2,\"1253\":2,\"1254\":2}}],[\"h1r3\",{\"1\":{\"794\":1}}],[\"h1r2\",{\"1\":{\"794\":2}}],[\"h1r1\",{\"1\":{\"794\":2}}],[\"hlens\",{\"1\":{\"1117\":1,\"1133\":2,\"1136\":2,\"1137\":2,\"1145\":2,\"1190\":3,\"1204\":2,\"1214\":2,\"1220\":1,\"1244\":2,\"1273\":2,\"2001\":1,\"2004\":1,\"2078\":2}}],[\"h5filewrapper\",{\"0\":{\"2187\":1},\"1\":{\"2187\":2}}],[\"h5\",{\"1\":{\"984\":1,\"989\":1,\"992\":1,\"1014\":1,\"1015\":3,\"1800\":1}}],[\"h3r3\",{\"1\":{\"794\":1}}],[\"h3r2\",{\"1\":{\"794\":1}}],[\"h3r1\",{\"1\":{\"794\":1}}],[\"hxed\",{\"1\":{\"2500\":1}}],[\"hx\",{\"1\":{\"685\":1}}],[\"hsun\",{\"1\":{\"2259\":1}}],[\"hs=1\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"hs=false\",{\"1\":{\"1133\":1}}],[\"hs\",{\"1\":{\"677\":4,\"678\":4,\"679\":4,\"681\":4,\"682\":4,\"684\":4,\"685\":4,\"686\":4,\"687\":4,\"688\":4,\"689\":4,\"691\":2,\"693\":2,\"694\":3,\"697\":2,\"703\":2,\"758\":4,\"782\":1,\"797\":1,\"1117\":1,\"1133\":9,\"1145\":8,\"1148\":2,\"1190\":2,\"1203\":2,\"1204\":2,\"1214\":2,\"1220\":1,\"1221\":1,\"1244\":2,\"1272\":2,\"1273\":6,\"1578\":1,\"1579\":1,\"1580\":1,\"1595\":1,\"1660\":1,\"1661\":2,\"1662\":1,\"1842\":2,\"1984\":1,\"2001\":3,\"2004\":1,\"2078\":2}}],[\"hdim\",{\"1\":{\"805\":2,\"808\":2,\"827\":2}}],[\"hdd\",{\"1\":{\"528\":2,\"564\":1,\"2155\":1}}],[\"hdf5writer\",{\"0\":{\"983\":1},\"1\":{\"983\":2,\"984\":1}}],[\"hdf5reader\",{\"0\":{\"982\":1},\"1\":{\"982\":2}}],[\"hdf5\",{\"1\":{\"168\":1,\"169\":1,\"179\":1,\"181\":1,\"247\":2,\"253\":2,\"276\":1,\"279\":1,\"281\":1,\"283\":1,\"284\":1,\"496\":4,\"506\":3,\"509\":1,\"512\":1,\"519\":1,\"522\":4,\"525\":2,\"533\":2,\"541\":1,\"619\":1,\"989\":1,\"1013\":4,\"1014\":2,\"1015\":6}}],[\"hybrid\",{\"0\":{\"150\":1},\"1\":{\"150\":7,\"704\":1,\"705\":1,\"880\":1,\"1171\":1,\"1206\":1,\"1975\":1,\"2027\":1,\"2076\":1,\"2452\":1}}],[\"hypernetwork\",{\"1\":{\"1242\":1}}],[\"hyper\",{\"1\":{\"632\":1,\"672\":1,\"1210\":1,\"1211\":1,\"1241\":1,\"1242\":1,\"1286\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":1,\"2151\":1}}],[\"hyperparameter\",{\"0\":{\"436\":1},\"1\":{\"2044\":1,\"2052\":1,\"2068\":1}}],[\"hyperparameters\",{\"1\":{\"240\":1,\"1016\":1,\"1217\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1848\":1,\"1849\":1,\"1851\":1,\"1856\":2,\"1857\":2,\"1858\":2,\"1866\":1,\"1867\":2,\"1871\":1}}],[\"hyps\",{\"0\":{\"912\":1},\"1\":{\"546\":3,\"551\":3,\"554\":3,\"557\":3,\"600\":2,\"633\":2,\"691\":8,\"692\":4,\"693\":2,\"697\":6,\"698\":4,\"699\":4,\"700\":9,\"706\":1,\"725\":2,\"797\":6,\"806\":2,\"824\":2,\"880\":2,\"912\":3,\"917\":2,\"1046\":2,\"1048\":14,\"1066\":2,\"1073\":2,\"1075\":2,\"1083\":2,\"1138\":9,\"1139\":9,\"1270\":2,\"2457\":1}}],[\"hypothesis\",{\"0\":{\"601\":1,\"647\":1,\"765\":1,\"1063\":1,\"1193\":1},\"1\":{\"150\":3,\"600\":1,\"601\":4,\"633\":1,\"647\":4,\"691\":14,\"692\":10,\"693\":7,\"694\":1,\"697\":24,\"698\":1,\"699\":1,\"700\":9,\"725\":4,\"751\":2,\"765\":3,\"797\":12,\"798\":1,\"806\":4,\"824\":4,\"857\":1,\"912\":1,\"917\":2,\"1046\":1,\"1048\":13,\"1060\":3,\"1063\":2,\"1066\":2,\"1073\":2,\"1075\":2,\"1083\":2,\"1138\":10,\"1139\":9,\"1176\":2,\"1193\":2,\"1270\":4,\"2359\":1,\"2360\":1,\"2472\":1,\"2473\":1,\"2474\":1,\"2476\":1,\"2521\":1,\"2580\":1,\"2581\":1,\"2582\":1,\"2648\":1,\"2649\":1}}],[\"hypotheses\",{\"1\":{\"23\":2,\"119\":2,\"333\":2,\"691\":8,\"692\":1,\"693\":2,\"697\":9,\"698\":1,\"699\":1,\"700\":3,\"705\":1,\"725\":2,\"797\":9,\"806\":2,\"824\":2,\"857\":1,\"912\":3,\"917\":5,\"922\":2,\"1046\":2,\"1048\":9,\"1066\":2,\"1073\":2,\"1075\":2,\"1083\":2,\"1138\":3,\"1139\":3,\"1270\":2}}],[\"hypo\",{\"1\":{\"108\":3,\"110\":1,\"501\":1}}],[\"hyp\",{\"1\":{\"98\":3,\"549\":1,\"572\":2,\"601\":2,\"647\":2,\"691\":10,\"692\":3,\"693\":1,\"697\":10,\"700\":1,\"725\":2,\"797\":6,\"806\":2,\"824\":2,\"1138\":2,\"1139\":1,\"1219\":1,\"1270\":2,\"2294\":1,\"2500\":1,\"2592\":2,\"2596\":2,\"2617\":1,\"2635\":1}}],[\"hz\",{\"1\":{\"109\":1,\"295\":4,\"778\":2,\"940\":1,\"1132\":1,\"1510\":1,\"1511\":1,\"1616\":1,\"1617\":1,\"1643\":1,\"1644\":1,\"1797\":1,\"1810\":2,\"1904\":2,\"1912\":2,\"1916\":2,\"1921\":1,\"1922\":1,\"1923\":2,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1947\":1}}],[\"h\",{\"1\":{\"108\":2,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"271\":1,\"272\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"377\":1,\"380\":1,\"384\":1,\"391\":1,\"397\":1,\"398\":11,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"493\":1,\"496\":1,\"499\":1,\"501\":1,\"503\":1,\"506\":1,\"509\":1,\"512\":1,\"515\":1,\"517\":1,\"519\":1,\"522\":1,\"525\":1,\"528\":1,\"530\":1,\"533\":1,\"536\":1,\"538\":1,\"541\":1,\"544\":1,\"546\":1,\"549\":1,\"551\":1,\"554\":1,\"557\":1,\"560\":1,\"562\":1,\"564\":1,\"566\":1,\"568\":1,\"570\":1,\"572\":1,\"574\":1,\"576\":1,\"579\":1,\"582\":1,\"585\":1,\"588\":1,\"590\":1,\"629\":1,\"677\":1,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"692\":2,\"698\":1,\"699\":1,\"700\":1,\"725\":1,\"731\":1,\"732\":1,\"747\":1,\"748\":1,\"758\":1,\"763\":1,\"784\":2,\"799\":1,\"806\":1,\"835\":4,\"885\":1,\"929\":1,\"1048\":1,\"1073\":1,\"1076\":15,\"1116\":1,\"1138\":1,\"1139\":1,\"1162\":1,\"1179\":1,\"1243\":9,\"1245\":6,\"1246\":3,\"1247\":3,\"1248\":8,\"1251\":3,\"1270\":1,\"1279\":1,\"1351\":2,\"1369\":2,\"1370\":1,\"1379\":2,\"1462\":1,\"1463\":1,\"1473\":2,\"1546\":1,\"1576\":2,\"1577\":2,\"1598\":1,\"1652\":1,\"1654\":1,\"1664\":2,\"1665\":2,\"1687\":1,\"1688\":4,\"1689\":1,\"1693\":5,\"1695\":2,\"1696\":1,\"1698\":1,\"1704\":2,\"1705\":2,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1715\":1,\"1755\":5,\"1756\":4,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1784\":1,\"1793\":1,\"1804\":6,\"1805\":1,\"1842\":2,\"1853\":4,\"1854\":4,\"1878\":6,\"2079\":2,\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1,\"2568\":1,\"2600\":2}}],[\"hugginface\",{\"1\":{\"2585\":1}}],[\"hugging\",{\"0\":{\"375\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1320\":2,\"1321\":2,\"2028\":1,\"2123\":1,\"2124\":1},\"1\":{\"97\":1,\"99\":2,\"307\":4,\"375\":3,\"443\":4,\"449\":2,\"1190\":3,\"1191\":2,\"1192\":2,\"1320\":2,\"1321\":2,\"2026\":1,\"2028\":2,\"2123\":1,\"2124\":1,\"2446\":1,\"2575\":1}}],[\"huggingfacetokenizer\",{\"0\":{\"2124\":1},\"1\":{\"2124\":1}}],[\"huggingfacetokenidconverter\",{\"0\":{\"2123\":1},\"1\":{\"2123\":1}}],[\"huggingfacetransformerspostdecoder\",{\"0\":{\"2028\":1},\"1\":{\"2028\":1}}],[\"huggingfacetransformerspostencoder\",{\"0\":{\"1192\":1},\"1\":{\"1192\":1}}],[\"huggingfacetransformersencoder\",{\"0\":{\"1191\":1},\"1\":{\"1191\":1}}],[\"huggingfacetransformersdecoder\",{\"0\":{\"1190\":1},\"1\":{\"1190\":1}}],[\"huggingfaceoptmodel\",{\"0\":{\"1957\":1},\"1\":{\"1957\":1}}],[\"huggingface\",{\"0\":{\"1957\":1,\"2585\":1},\"1\":{\"91\":1,\"99\":1,\"1454\":1,\"1957\":1,\"2355\":2,\"2357\":2,\"2446\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2481\":1,\"2494\":1,\"2583\":1,\"2584\":4,\"2585\":5,\"2588\":1,\"2618\":1,\"2630\":1,\"2648\":1,\"2649\":1}}],[\"humana\",{\"1\":{\"2457\":1}}],[\"humanfriendly\",{\"0\":{\"2326\":1},\"1\":{\"2326\":1}}],[\"human\",{\"0\":{\"2154\":1},\"1\":{\"2154\":8}}],[\"hungarian\",{\"1\":{\"461\":1}}],[\"hung\",{\"1\":{\"130\":1}}],[\"huazhe\",{\"1\":{\"130\":1}}],[\"huo\",{\"1\":{\"130\":1}}],[\"hubret\",{\"1\":{\"2441\":1}}],[\"hubert6\",{\"1\":{\"2519\":1}}],[\"hubertcollatefn\",{\"0\":{\"2188\":1},\"1\":{\"2188\":2}}],[\"huberttask\",{\"0\":{\"2105\":1},\"1\":{\"2105\":2}}],[\"hubertpretrainmodel\",{\"0\":{\"1892\":1},\"1\":{\"1892\":1}}],[\"hubertpretrainloss\",{\"0\":{\"1890\":1},\"1\":{\"1890\":1}}],[\"hubert\",{\"0\":{\"1180\":1,\"1181\":1,\"1269\":1,\"1309\":2,\"1890\":2,\"1892\":1,\"1893\":1,\"2105\":1,\"2692\":1},\"1\":{\"101\":1,\"102\":3,\"1180\":11,\"1181\":6,\"1269\":6,\"1309\":2,\"1804\":3,\"1805\":3,\"1890\":3,\"1892\":2,\"1893\":2,\"2105\":2,\"2377\":2,\"2429\":2,\"2431\":25,\"2432\":16,\"2440\":1,\"2441\":4,\"2552\":1,\"2574\":1}}],[\"hub\",{\"1\":{\"5\":1,\"6\":1,\"102\":1,\"1454\":1,\"2355\":1,\"2357\":1}}],[\"hf\",{\"1\":{\"91\":2,\"99\":2,\"1454\":1,\"2585\":1}}],[\"houlsby\",{\"0\":{\"1907\":2,\"1931\":1,\"1933\":1},\"1\":{\"1907\":2,\"1931\":1,\"1933\":2}}],[\"hours\",{\"1\":{\"2584\":1}}],[\"hour\",{\"1\":{\"233\":1}}],[\"hook\",{\"1\":{\"1245\":1,\"1248\":1}}],[\"hook=none\",{\"1\":{\"979\":1}}],[\"hooks\",{\"0\":{\"1973\":1},\"1\":{\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1973\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1}}],[\"hot\",{\"1\":{\"743\":1,\"793\":2,\"875\":1,\"2046\":1}}],[\"holds\",{\"1\":{\"743\":1,\"1155\":1,\"2385\":2}}],[\"hold\",{\"1\":{\"449\":2,\"692\":1,\"699\":1,\"1139\":1,\"2385\":1}}],[\"horizon\",{\"1\":{\"1267\":1}}],[\"hori\",{\"1\":{\"130\":1}}],[\"hop\",{\"1\":{\"104\":2,\"343\":2,\"350\":2,\"368\":2,\"692\":1,\"693\":5,\"1149\":3,\"1150\":3,\"1158\":1,\"1207\":1,\"1255\":2,\"1373\":1,\"1526\":1,\"1604\":3,\"1643\":2,\"1644\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1763\":1,\"1776\":4,\"1777\":4,\"1778\":1,\"1786\":4,\"1799\":1,\"1801\":1,\"1803\":1,\"1804\":2,\"1805\":1,\"1811\":3,\"1850\":1,\"1852\":1,\"1859\":3,\"1877\":1,\"1910\":1,\"1918\":1,\"1929\":3,\"1941\":3,\"1947\":3,\"1987\":1,\"1989\":1,\"1991\":1,\"2084\":1,\"2089\":1,\"2196\":1,\"2236\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2357\":3,\"2371\":1,\"2436\":2,\"2494\":1,\"2498\":1,\"2562\":2,\"2578\":3,\"2600\":12,\"2612\":1,\"2616\":1,\"2630\":1,\"2634\":1}}],[\"hosts\",{\"1\":{\"93\":1}}],[\"host2\",{\"1\":{\"36\":1,\"39\":1,\"42\":1}}],[\"host1\",{\"1\":{\"36\":3,\"39\":5,\"42\":1}}],[\"hostxn\",{\"1\":{\"32\":2}}],[\"host\",{\"0\":{\"36\":1},\"1\":{\"32\":9,\"38\":3,\"40\":1,\"42\":1,\"44\":1,\"87\":3,\"377\":2,\"727\":1}}],[\"home\",{\"1\":{\"28\":1,\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"940\":1,\"2467\":1,\"2599\":1}}],[\"how\",{\"0\":{\"26\":1,\"27\":1,\"90\":1,\"124\":1,\"126\":1,\"2376\":1,\"2379\":1,\"2386\":1,\"2435\":1,\"2561\":1},\"1\":{\"96\":1,\"113\":1,\"134\":1,\"135\":1,\"155\":1,\"159\":1,\"161\":1,\"162\":1,\"164\":1,\"171\":1,\"195\":1,\"233\":1,\"235\":1,\"275\":1,\"276\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"294\":1,\"297\":1,\"754\":2,\"762\":1,\"763\":2,\"826\":3,\"997\":2,\"1028\":1,\"1269\":2,\"1851\":2,\"2001\":1,\"2002\":1,\"2004\":1,\"2086\":3,\"2087\":3,\"2090\":2,\"2095\":1,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":1,\"2264\":3,\"2279\":2,\"2391\":3,\"2394\":1,\"2423\":3,\"2441\":1,\"2461\":1,\"2527\":3,\"2530\":1,\"2546\":3,\"2564\":1,\"2597\":1,\"2600\":1,\"2635\":1}}],[\"however\",{\"1\":{\"5\":1,\"6\":1,\"28\":1,\"48\":1,\"119\":1,\"204\":1,\"1719\":1,\"2384\":1,\"2385\":1,\"2387\":1,\"2389\":1,\"2394\":1,\"2398\":1,\"2400\":1,\"2408\":1,\"2420\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2467\":1,\"2473\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2530\":1,\"2534\":1,\"2536\":1,\"2545\":1,\"2558\":1,\"2573\":1,\"2584\":1}}],[\"his\",{\"1\":{\"2388\":1,\"2524\":1}}],[\"history\",{\"0\":{\"898\":1},\"1\":{\"685\":1,\"745\":1,\"746\":1,\"898\":3,\"1670\":1,\"1671\":1,\"2400\":1,\"2536\":1}}],[\"hier\",{\"1\":{\"2076\":1}}],[\"hierarchical\",{\"1\":{\"677\":1,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"758\":1,\"892\":1}}],[\"hint\",{\"1\":{\"2441\":2}}],[\"hinge\",{\"1\":{\"1836\":1,\"1843\":1}}],[\"hindi\",{\"1\":{\"461\":1}}],[\"hifiganscalediscriminator\",{\"0\":{\"1849\":1},\"1\":{\"1849\":1}}],[\"hifiganperioddiscriminator\",{\"0\":{\"1848\":1},\"1\":{\"1848\":2}}],[\"hifiganmultiscalemultiperioddiscriminator\",{\"0\":{\"1847\":1},\"1\":{\"1847\":1}}],[\"hifiganmultiscalediscriminator\",{\"0\":{\"1846\":1},\"1\":{\"1846\":1}}],[\"hifiganmultiperioddiscriminator\",{\"0\":{\"1845\":1},\"1\":{\"1845\":2}}],[\"hifigangenerator\",{\"0\":{\"1844\":1},\"1\":{\"1800\":1,\"1803\":1,\"1844\":2}}],[\"hifigan\",{\"0\":{\"1836\":1,\"1839\":1,\"1843\":1,\"1844\":2,\"1845\":2,\"1846\":2,\"1847\":2,\"1848\":2,\"1849\":2,\"1859\":1,\"1866\":1},\"1\":{\"1778\":2,\"1804\":1,\"1805\":3,\"1836\":1,\"1839\":1,\"1843\":1,\"1844\":3,\"1845\":3,\"1846\":3,\"1847\":3,\"1848\":3,\"1849\":3,\"1850\":1,\"1852\":2,\"1859\":1,\"1866\":2,\"1877\":1,\"2363\":4,\"2506\":1,\"2512\":2,\"2653\":6,\"2657\":2}}],[\"hifi\",{\"1\":{\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1845\":1,\"1846\":2,\"1847\":3,\"1849\":1,\"1850\":1}}],[\"hid\",{\"1\":{\"1522\":2,\"1523\":2,\"1543\":2,\"1545\":2,\"1655\":4,\"1719\":4}}],[\"hiddens\",{\"1\":{\"2004\":1}}],[\"hidden\",{\"1\":{\"21\":6,\"22\":1,\"102\":1,\"115\":7,\"116\":5,\"676\":1,\"677\":3,\"678\":3,\"679\":3,\"681\":3,\"682\":3,\"684\":3,\"685\":3,\"686\":3,\"687\":3,\"688\":3,\"689\":3,\"691\":1,\"693\":1,\"697\":1,\"713\":4,\"725\":11,\"732\":1,\"736\":1,\"747\":1,\"748\":1,\"754\":2,\"758\":3,\"766\":1,\"770\":4,\"781\":1,\"782\":1,\"786\":4,\"797\":1,\"801\":3,\"806\":13,\"812\":1,\"824\":8,\"825\":1,\"826\":3,\"827\":1,\"837\":1,\"838\":8,\"863\":3,\"865\":3,\"866\":3,\"891\":2,\"915\":2,\"918\":3,\"929\":1,\"1046\":9,\"1047\":2,\"1049\":1,\"1051\":4,\"1054\":3,\"1055\":3,\"1056\":1,\"1062\":5,\"1064\":1,\"1065\":2,\"1066\":12,\"1070\":3,\"1072\":2,\"1073\":15,\"1074\":4,\"1075\":14,\"1080\":2,\"1081\":5,\"1083\":11,\"1114\":1,\"1133\":4,\"1145\":2,\"1148\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1203\":1,\"1220\":1,\"1222\":3,\"1244\":1,\"1251\":1,\"1252\":2,\"1253\":4,\"1254\":2,\"1269\":1,\"1270\":15,\"1272\":1,\"1273\":4,\"1279\":1,\"1282\":3,\"1374\":1,\"1376\":1,\"1377\":2,\"1430\":3,\"1460\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"1531\":3,\"1532\":3,\"1534\":1,\"1535\":3,\"1537\":3,\"1539\":1,\"1543\":1,\"1558\":3,\"1560\":1,\"1572\":5,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":3,\"1595\":1,\"1598\":5,\"1602\":3,\"1609\":1,\"1626\":1,\"1645\":3,\"1648\":3,\"1650\":3,\"1652\":5,\"1654\":3,\"1658\":2,\"1659\":2,\"1660\":3,\"1661\":3,\"1662\":3,\"1670\":3,\"1671\":3,\"1763\":1,\"1765\":1,\"1766\":3,\"1769\":1,\"1776\":4,\"1777\":4,\"1786\":3,\"1787\":3,\"1789\":1,\"1798\":3,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":5,\"1805\":1,\"1806\":1,\"1808\":3,\"1816\":1,\"1820\":1,\"1833\":3,\"1842\":3,\"1844\":1,\"1851\":3,\"1853\":1,\"1863\":4,\"1864\":3,\"1865\":3,\"1874\":1,\"1877\":1,\"1878\":5,\"1951\":1,\"1957\":2,\"1958\":1,\"1960\":2,\"1983\":3,\"1986\":3,\"2001\":1,\"2002\":1,\"2004\":2,\"2049\":1,\"2055\":1,\"2064\":1,\"2070\":1,\"2078\":2,\"2079\":1,\"2083\":1,\"2086\":1,\"2087\":1,\"2090\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2260\":1,\"2264\":3,\"2279\":2,\"2385\":1,\"2399\":1,\"2401\":2,\"2535\":1,\"2537\":2}}],[\"hippo\",{\"1\":{\"1245\":1,\"1339\":1}}],[\"hitachi\",{\"1\":{\"940\":1}}],[\"hira\",{\"1\":{\"130\":1}}],[\"hirofumi\",{\"1\":{\"130\":1}}],[\"highly\",{\"1\":{\"2372\":1,\"2384\":1,\"2428\":1,\"2551\":1}}],[\"highlight\",{\"1\":{\"1810\":2}}],[\"highpass\",{\"0\":{\"1938\":1},\"1\":{\"1905\":1,\"1938\":2}}],[\"high=10\",{\"1\":{\"1387\":1}}],[\"highest\",{\"1\":{\"778\":1,\"1912\":1,\"2437\":1,\"2563\":1}}],[\"higher\",{\"1\":{\"217\":1,\"745\":1,\"746\":1,\"2387\":1}}],[\"highwaynet\",{\"0\":{\"764\":1},\"1\":{\"764\":1}}],[\"highway\",{\"0\":{\"1188\":1},\"1\":{\"701\":6,\"764\":4,\"821\":4,\"1188\":2}}],[\"high\",{\"1\":{\"23\":1,\"119\":1,\"1386\":1,\"1462\":1,\"1463\":1,\"2003\":2,\"2090\":1,\"2236\":1,\"2244\":1,\"2255\":1,\"2265\":1,\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"harmful\",{\"1\":{\"2373\":1,\"2433\":1,\"2555\":1}}],[\"harmonics\",{\"1\":{\"1776\":1,\"1803\":1}}],[\"harmonic\",{\"0\":{\"1817\":1},\"1\":{\"1776\":9,\"1797\":4,\"1803\":3,\"1804\":3,\"1817\":1}}],[\"harm\",{\"0\":{\"1776\":1},\"1\":{\"1776\":3}}],[\"hard\",{\"1\":{\"2040\":2,\"2294\":1}}],[\"hardness\",{\"1\":{\"1528\":1}}],[\"hardtanh\",{\"1\":{\"115\":4,\"1096\":6}}],[\"haeb\",{\"1\":{\"1704\":1}}],[\"hawkaaron\",{\"1\":{\"1337\":2,\"1349\":2,\"1350\":2}}],[\"hal\",{\"1\":{\"1712\":2,\"1715\":2}}],[\"halves\",{\"1\":{\"1143\":2}}],[\"half\",{\"1\":{\"80\":1,\"1011\":1,\"1143\":1,\"1248\":1}}],[\"having\",{\"1\":{\"997\":1,\"998\":1,\"1245\":1,\"1377\":1,\"1420\":1,\"1424\":1,\"2148\":1,\"2201\":1,\"2468\":1,\"2584\":1,\"2585\":1}}],[\"haven\",{\"1\":{\"83\":1}}],[\"have\",{\"0\":{\"39\":1},\"1\":{\"1\":1,\"11\":1,\"26\":1,\"30\":1,\"56\":2,\"57\":1,\"62\":1,\"85\":1,\"96\":1,\"102\":1,\"119\":1,\"124\":1,\"126\":1,\"127\":1,\"134\":1,\"135\":4,\"141\":1,\"143\":1,\"150\":2,\"198\":1,\"226\":1,\"235\":1,\"237\":1,\"734\":1,\"1011\":2,\"1028\":2,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1177\":1,\"1187\":1,\"1202\":1,\"1209\":1,\"1248\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1257\":1,\"1286\":1,\"1287\":1,\"1337\":1,\"1371\":1,\"1392\":1,\"1427\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1551\":2,\"1552\":1,\"1553\":2,\"1554\":1,\"1602\":1,\"1648\":1,\"1650\":1,\"1972\":2,\"2170\":2,\"2384\":1,\"2387\":3,\"2394\":2,\"2398\":1,\"2400\":2,\"2401\":1,\"2403\":1,\"2405\":1,\"2420\":1,\"2431\":1,\"2450\":1,\"2457\":1,\"2462\":1,\"2467\":1,\"2468\":1,\"2473\":1,\"2530\":2,\"2534\":1,\"2536\":2,\"2537\":1,\"2539\":1,\"2541\":1,\"2542\":1,\"2543\":1,\"2560\":1,\"2568\":2,\"2583\":1,\"2584\":2,\"2585\":2,\"2591\":1,\"2639\":1}}],[\"hat\",{\"1\":{\"750\":4,\"1646\":1,\"1767\":2,\"1793\":1,\"1836\":2,\"1839\":2,\"1859\":2}}],[\"hats\",{\"1\":{\"750\":1,\"1761\":1,\"1763\":1}}],[\"happen\",{\"1\":{\"2584\":1}}],[\"happened\",{\"1\":{\"593\":1,\"594\":1}}],[\"happens\",{\"1\":{\"49\":1,\"2385\":1}}],[\"hamming\",{\"1\":{\"509\":1,\"512\":1,\"519\":1,\"1198\":1,\"1207\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"1917\":4}}],[\"han\",{\"1\":{\"677\":2,\"678\":2,\"679\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"758\":2,\"892\":2,\"1220\":7,\"1289\":7}}],[\"hanning\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1}}],[\"hann\",{\"1\":{\"509\":1,\"512\":1,\"519\":1,\"945\":1,\"947\":1,\"951\":1,\"953\":1,\"966\":1,\"967\":1,\"969\":1,\"970\":1,\"1158\":1,\"1643\":1,\"1644\":1,\"1660\":1,\"1661\":1,\"1719\":1,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1859\":1,\"1877\":1,\"1918\":1,\"1929\":1,\"1941\":1,\"1947\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"2084\":1,\"2089\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2317\":1,\"2325\":1,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"hands\",{\"1\":{\"2388\":1,\"2524\":1}}],[\"handling\",{\"1\":{\"1551\":1,\"1553\":1}}],[\"handles\",{\"1\":{\"1198\":1}}],[\"handler\",{\"1\":{\"823\":1,\"2155\":4}}],[\"handle\",{\"1\":{\"49\":1,\"80\":1,\"1278\":1,\"1375\":1,\"1463\":1,\"1622\":1,\"2415\":1,\"2585\":1,\"2644\":1}}],[\"hand\",{\"1\":{\"240\":1,\"2395\":1,\"2412\":1,\"2531\":1}}],[\"hackathon\",{\"1\":{\"176\":1}}],[\"hayashi2021espnet2\",{\"1\":{\"130\":1}}],[\"hayashi2020espnet\",{\"1\":{\"130\":1}}],[\"hayashi\",{\"1\":{\"130\":6,\"206\":1,\"233\":1,\"2361\":1,\"2650\":1}}],[\"hash\",{\"1\":{\"2441\":1,\"2442\":1}}],[\"has\",{\"1\":{\"14\":1,\"18\":1,\"21\":1,\"26\":4,\"46\":2,\"47\":1,\"49\":1,\"58\":1,\"60\":1,\"63\":1,\"64\":1,\"74\":1,\"76\":1,\"77\":1,\"78\":1,\"90\":1,\"106\":1,\"107\":2,\"114\":1,\"115\":2,\"121\":1,\"149\":1,\"170\":1,\"235\":2,\"237\":5,\"238\":1,\"691\":4,\"692\":2,\"695\":1,\"697\":4,\"706\":2,\"745\":1,\"746\":1,\"770\":1,\"796\":1,\"797\":2,\"815\":1,\"836\":1,\"1015\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1177\":1,\"1187\":1,\"1190\":1,\"1202\":1,\"1209\":2,\"1210\":1,\"1221\":1,\"1244\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1286\":1,\"1287\":1,\"1375\":1,\"1400\":1,\"1427\":1,\"1639\":1,\"1735\":1,\"1904\":1,\"2050\":1,\"2167\":1,\"2183\":1,\"2190\":1,\"2193\":1,\"2271\":1,\"2309\":1,\"2346\":1,\"2372\":1,\"2384\":1,\"2385\":1,\"2388\":1,\"2389\":1,\"2400\":1,\"2408\":1,\"2411\":1,\"2421\":1,\"2422\":1,\"2429\":1,\"2437\":1,\"2449\":1,\"2465\":1,\"2473\":1,\"2481\":1,\"2503\":1,\"2524\":1,\"2525\":1,\"2536\":1,\"2544\":1,\"2545\":1,\"2554\":1,\"2563\":1,\"2584\":2}}],[\"htm\",{\"1\":{\"1082\":1}}],[\"html\",{\"0\":{\"12\":1},\"1\":{\"10\":1,\"12\":2,\"13\":1,\"38\":1,\"45\":1,\"82\":1,\"84\":1,\"1269\":1,\"1462\":1,\"1463\":1,\"1528\":1,\"1529\":1,\"1568\":1,\"1695\":1,\"2355\":2,\"2363\":1,\"2372\":4,\"2373\":1,\"2377\":1,\"2378\":1,\"2382\":2,\"2384\":2,\"2390\":3,\"2401\":3,\"2414\":1,\"2424\":3,\"2429\":2,\"2430\":1,\"2436\":1,\"2437\":1,\"2506\":1,\"2512\":1,\"2526\":3,\"2537\":3,\"2547\":3,\"2554\":2,\"2555\":1,\"2558\":1,\"2562\":1,\"2563\":1,\"2573\":1,\"2653\":1,\"2657\":1}}],[\"htk\",{\"1\":{\"778\":3,\"989\":1,\"1158\":1,\"1207\":1,\"1912\":3,\"1927\":1,\"1989\":1,\"2248\":1}}],[\"http\",{\"1\":{\"130\":1,\"940\":1,\"1524\":1,\"2512\":2,\"2657\":2}}],[\"https\",{\"1\":{\"13\":1,\"38\":1,\"44\":1,\"45\":2,\"47\":1,\"70\":4,\"82\":1,\"84\":1,\"97\":1,\"130\":2,\"134\":1,\"135\":1,\"166\":2,\"167\":4,\"177\":2,\"178\":4,\"195\":2,\"196\":4,\"199\":3,\"200\":2,\"202\":1,\"206\":2,\"207\":1,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":2,\"223\":2,\"229\":2,\"230\":2,\"233\":2,\"234\":4,\"277\":1,\"295\":2,\"628\":1,\"668\":3,\"678\":1,\"681\":1,\"682\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"692\":1,\"693\":1,\"700\":6,\"706\":2,\"729\":1,\"734\":1,\"740\":1,\"741\":1,\"758\":1,\"770\":2,\"771\":2,\"772\":2,\"775\":1,\"776\":1,\"809\":2,\"810\":2,\"813\":1,\"817\":1,\"885\":1,\"950\":1,\"952\":1,\"968\":1,\"1031\":1,\"1037\":2,\"1047\":1,\"1048\":5,\"1049\":1,\"1056\":1,\"1061\":1,\"1066\":1,\"1067\":1,\"1072\":1,\"1075\":1,\"1076\":1,\"1080\":1,\"1082\":1,\"1084\":3,\"1101\":1,\"1102\":1,\"1138\":7,\"1139\":6,\"1144\":1,\"1148\":1,\"1150\":1,\"1180\":1,\"1198\":1,\"1203\":2,\"1210\":1,\"1211\":1,\"1214\":1,\"1215\":1,\"1228\":1,\"1269\":1,\"1284\":1,\"1286\":2,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":2,\"1337\":3,\"1345\":1,\"1347\":1,\"1349\":1,\"1350\":1,\"1371\":3,\"1400\":1,\"1454\":3,\"1462\":2,\"1463\":2,\"1466\":1,\"1515\":1,\"1523\":1,\"1528\":1,\"1529\":2,\"1566\":1,\"1568\":2,\"1605\":2,\"1645\":1,\"1695\":2,\"1696\":1,\"1698\":1,\"1706\":1,\"1707\":1,\"1712\":2,\"1715\":3,\"1717\":1,\"1735\":1,\"1765\":1,\"1767\":1,\"1782\":1,\"1793\":1,\"1800\":1,\"1803\":1,\"1829\":1,\"1841\":1,\"1842\":1,\"1844\":1,\"1850\":1,\"1857\":1,\"1858\":1,\"1860\":1,\"1917\":2,\"1921\":1,\"1922\":1,\"1932\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1958\":1,\"1973\":1,\"1976\":1,\"1986\":2,\"2003\":1,\"2019\":1,\"2032\":2,\"2040\":2,\"2054\":1,\"2078\":1,\"2081\":1,\"2083\":1,\"2095\":1,\"2125\":1,\"2131\":2,\"2154\":1,\"2259\":1,\"2260\":1,\"2294\":1,\"2309\":1,\"2324\":1,\"2354\":3,\"2355\":3,\"2357\":2,\"2359\":1,\"2360\":1,\"2361\":2,\"2363\":7,\"2366\":1,\"2371\":1,\"2372\":6,\"2373\":1,\"2377\":3,\"2378\":1,\"2379\":1,\"2382\":3,\"2383\":1,\"2384\":5,\"2387\":1,\"2390\":3,\"2393\":1,\"2394\":1,\"2401\":3,\"2409\":1,\"2414\":1,\"2424\":3,\"2427\":1,\"2429\":6,\"2430\":2,\"2431\":2,\"2432\":2,\"2436\":1,\"2437\":1,\"2446\":3,\"2450\":3,\"2456\":1,\"2458\":1,\"2466\":1,\"2467\":2,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2482\":1,\"2499\":1,\"2500\":1,\"2504\":1,\"2506\":7,\"2517\":3,\"2521\":1,\"2523\":1,\"2526\":3,\"2529\":1,\"2530\":1,\"2537\":3,\"2547\":3,\"2550\":1,\"2552\":1,\"2554\":3,\"2555\":2,\"2558\":1,\"2562\":1,\"2563\":1,\"2564\":1,\"2573\":1,\"2574\":1,\"2575\":1,\"2576\":1,\"2580\":1,\"2582\":1,\"2584\":2,\"2585\":3,\"2586\":1,\"2587\":1,\"2593\":2,\"2597\":2,\"2601\":1,\"2612\":1,\"2617\":2,\"2618\":1,\"2630\":1,\"2635\":3,\"2646\":3,\"2648\":1,\"2649\":1,\"2650\":2,\"2653\":7}}],[\"he\",{\"1\":{\"1905\":1}}],[\"hermitian\",{\"1\":{\"1695\":3}}],[\"hershey\",{\"1\":{\"1529\":1,\"1568\":1}}],[\"here\",{\"1\":{\"11\":1,\"21\":1,\"98\":1,\"102\":1,\"106\":1,\"112\":1,\"113\":2,\"115\":1,\"118\":1,\"121\":1,\"170\":1,\"197\":1,\"198\":1,\"221\":1,\"237\":1,\"239\":2,\"240\":2,\"1052\":2,\"1248\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1337\":1,\"1383\":1,\"1639\":1,\"1713\":1,\"1951\":1,\"2387\":3,\"2395\":1,\"2398\":1,\"2414\":2,\"2418\":1,\"2419\":1,\"2420\":1,\"2433\":1,\"2438\":1,\"2457\":1,\"2461\":1,\"2462\":1,\"2479\":1,\"2501\":1,\"2514\":1,\"2531\":1,\"2534\":1,\"2555\":1,\"2564\":1,\"2568\":1,\"2573\":1,\"2584\":4,\"2659\":1}}],[\"height=0\",{\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"heights\",{\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"height\",{\"1\":{\"1009\":1,\"1011\":4,\"1018\":1,\"1021\":1,\"1022\":1,\"1025\":1}}],[\"heavily\",{\"1\":{\"944\":1,\"1917\":1}}],[\"heavy\",{\"1\":{\"795\":1}}],[\"head=4\",{\"1\":{\"1578\":1,\"1579\":1,\"1580\":1,\"1660\":1,\"1661\":1,\"1662\":1}}],[\"heads=4\",{\"1\":{\"747\":1,\"749\":1,\"1430\":1,\"1470\":1,\"1537\":1,\"1670\":1}}],[\"heads\",{\"1\":{\"21\":6,\"115\":7,\"116\":3,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"731\":1,\"732\":1,\"747\":1,\"748\":1,\"749\":2,\"754\":1,\"771\":1,\"784\":2,\"785\":1,\"809\":1,\"826\":4,\"892\":1,\"1065\":3,\"1066\":3,\"1069\":11,\"1076\":3,\"1115\":4,\"1133\":2,\"1140\":1,\"1148\":3,\"1149\":3,\"1150\":3,\"1167\":1,\"1168\":1,\"1169\":1,\"1179\":1,\"1181\":3,\"1182\":4,\"1196\":1,\"1197\":1,\"1203\":3,\"1204\":1,\"1209\":1,\"1220\":1,\"1241\":1,\"1245\":1,\"1269\":3,\"1271\":1,\"1272\":3,\"1273\":1,\"1289\":1,\"1430\":2,\"1470\":2,\"1505\":1,\"1537\":3,\"1539\":3,\"1581\":3,\"1660\":1,\"1661\":2,\"1662\":1,\"1669\":1,\"1670\":3,\"1671\":4,\"1771\":3,\"1787\":3,\"1788\":3,\"1798\":3,\"1804\":3,\"1805\":1,\"1850\":1,\"1851\":3,\"1852\":1,\"1874\":3,\"1877\":1,\"1878\":3,\"2001\":3,\"2004\":3,\"2026\":1,\"2029\":3,\"2054\":3,\"2095\":3,\"2239\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2261\":3,\"2262\":3,\"2263\":3,\"2264\":8,\"2440\":2,\"2558\":2,\"2564\":1,\"2584\":1}}],[\"head\",{\"0\":{\"909\":1,\"1069\":1,\"1320\":1},\"1\":{\"5\":1,\"21\":2,\"115\":3,\"183\":1,\"184\":1,\"191\":1,\"237\":1,\"238\":2,\"239\":1,\"629\":1,\"686\":6,\"687\":4,\"688\":6,\"689\":9,\"749\":1,\"754\":1,\"763\":2,\"771\":4,\"784\":3,\"785\":8,\"799\":1,\"809\":4,\"826\":1,\"892\":1,\"909\":4,\"1069\":1,\"1133\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1181\":1,\"1203\":1,\"1209\":8,\"1272\":1,\"1320\":1,\"1505\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":1,\"1960\":1,\"1993\":1,\"2001\":1,\"2004\":1,\"2029\":1,\"2040\":1,\"2046\":1,\"2054\":1,\"2253\":3,\"2262\":1,\"2264\":1,\"2385\":4,\"2387\":3,\"2554\":1}}],[\"henc\",{\"1\":{\"676\":1,\"781\":1,\"812\":1}}],[\"heuristics\",{\"1\":{\"150\":1,\"815\":1}}],[\"heymann\",{\"1\":{\"130\":1}}],[\"held\",{\"1\":{\"607\":1}}],[\"hello\",{\"1\":{\"58\":1,\"2134\":2}}],[\"helping\",{\"1\":{\"2388\":1,\"2524\":1}}],[\"help=none\",{\"1\":{\"2313\":1}}],[\"helps\",{\"1\":{\"802\":1,\"803\":1}}],[\"helper\",{\"0\":{\"1013\":1,\"1015\":1,\"1288\":1,\"1300\":1,\"1306\":1,\"1307\":1,\"1316\":1,\"1318\":1,\"1324\":1,\"1325\":1,\"1331\":1,\"1333\":1,\"1335\":1,\"1338\":1},\"1\":{\"169\":1,\"181\":1,\"1013\":1,\"1014\":2,\"1015\":3,\"1142\":1,\"1186\":1,\"1210\":1,\"1227\":1,\"1288\":1,\"1300\":1,\"1306\":1,\"1307\":1,\"1316\":1,\"1318\":1,\"1324\":1,\"1325\":1,\"1331\":1,\"1333\":1,\"1335\":1,\"1338\":1,\"1344\":1,\"1346\":1,\"2012\":1,\"2137\":1,\"2176\":1,\"2318\":1}}],[\"help\",{\"1\":{\"58\":2,\"62\":2,\"90\":2,\"108\":2,\"126\":1,\"205\":2,\"273\":1,\"289\":1,\"291\":1,\"293\":1,\"952\":1,\"1639\":1,\"2394\":1,\"2414\":1,\"2468\":1,\"2530\":1}}],[\"helpful\",{\"1\":{\"49\":1,\"2401\":1,\"2537\":1}}],[\"ucas\",{\"1\":{\"2586\":1}}],[\"udialogue\",{\"1\":{\"2512\":1,\"2657\":1}}],[\"uds\",{\"1\":{\"2457\":1}}],[\"ulaw\",{\"1\":{\"1927\":1}}],[\"uv\",{\"1\":{\"1797\":2}}],[\"umbach\",{\"1\":{\"1704\":1}}],[\"uhifigangenerator\",{\"0\":{\"1800\":1},\"1\":{\"1800\":1}}],[\"uhifigan\",{\"0\":{\"1797\":1,\"1800\":2},\"1\":{\"1797\":1,\"1800\":3}}],[\"uhlenbeck\",{\"1\":{\"1618\":1,\"1619\":1}}],[\"uhh\",{\"1\":{\"1605\":1}}],[\"u+1\",{\"1\":{\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1}}],[\"u=none\",{\"1\":{\"1247\":1}}],[\"uk\",{\"1\":{\"940\":1,\"2040\":1}}],[\"ukrainian\",{\"1\":{\"461\":1}}],[\"uasrsmoothnesspenalty\",{\"0\":{\"2305\":1},\"1\":{\"2305\":1}}],[\"uasrpseudolabelloss\",{\"0\":{\"2304\":1},\"1\":{\"2304\":1}}],[\"uasrphonemediversityloss\",{\"0\":{\"2303\":1},\"1\":{\"2303\":1}}],[\"uasrprefixscorer\",{\"0\":{\"829\":1},\"1\":{\"829\":2}}],[\"uasrgradientpenalty\",{\"0\":{\"2302\":1},\"1\":{\"2302\":1}}],[\"uasrdiscriminatorloss\",{\"0\":{\"2301\":1},\"1\":{\"2301\":1}}],[\"uasrtraineroptions\",{\"0\":{\"2204\":1},\"1\":{\"2203\":2,\"2204\":1}}],[\"uasrtrainer\",{\"0\":{\"2203\":1},\"1\":{\"2118\":1,\"2203\":2,\"2204\":1}}],[\"uasrtask\",{\"0\":{\"2118\":1},\"1\":{\"2118\":2}}],[\"uasr\",{\"0\":{\"476\":1,\"478\":1,\"485\":1,\"829\":1,\"2118\":1,\"2203\":1,\"2204\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2713\":1},\"1\":{\"476\":7,\"478\":9,\"485\":7,\"829\":2,\"1398\":1,\"1409\":1,\"2118\":2,\"2203\":4,\"2204\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":2,\"2290\":2,\"2292\":2,\"2294\":2,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":2,\"2302\":2,\"2303\":2,\"2304\":2,\"2305\":2}}],[\"uen\",{\"1\":{\"231\":1}}],[\"uei\",{\"1\":{\"231\":1}}],[\"ueda\",{\"1\":{\"130\":1}}],[\"uid\",{\"1\":{\"2190\":1}}],[\"ui\",{\"1\":{\"231\":1}}],[\"uint8\",{\"1\":{\"48\":1,\"731\":2,\"899\":1,\"900\":4,\"901\":1,\"902\":4,\"1133\":2,\"1214\":1,\"1273\":1,\"2001\":1}}],[\"url=\",{\"1\":{\"130\":2,\"2617\":1,\"2635\":1}}],[\"url\",{\"1\":{\"70\":1,\"130\":1,\"277\":1,\"1178\":3,\"1179\":2,\"1180\":3,\"1214\":1,\"1215\":1,\"1284\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"2431\":1,\"2432\":1,\"2482\":1,\"2494\":1,\"2617\":1,\"2630\":1,\"2635\":1}}],[\"upon\",{\"1\":{\"2121\":1,\"2122\":1}}],[\"up=1\",{\"1\":{\"1753\":1}}],[\"up=false\",{\"1\":{\"1508\":1,\"1631\":1}}],[\"upfirdn2d\",{\"0\":{\"1753\":2,\"1754\":2},\"1\":{\"1753\":2,\"1754\":2}}],[\"upenn\",{\"1\":{\"1400\":1}}],[\"uppool\",{\"0\":{\"1279\":1},\"1\":{\"1279\":2}}],[\"upper=1\",{\"1\":{\"952\":1,\"961\":1}}],[\"upper=\",{\"1\":{\"948\":1}}],[\"upper=0\",{\"1\":{\"940\":1}}],[\"upper\",{\"1\":{\"202\":1,\"295\":1,\"771\":1,\"809\":1,\"1148\":1,\"1203\":1,\"1925\":1,\"2054\":1,\"2358\":1,\"2520\":1,\"2579\":1,\"2592\":1,\"2596\":1}}],[\"upsamplenetwork\",{\"0\":{\"1876\":1},\"1\":{\"1876\":2}}],[\"upsampled\",{\"1\":{\"1834\":1,\"1872\":1,\"1873\":1,\"1876\":1,\"1986\":1}}],[\"upsamples\",{\"1\":{\"1755\":1}}],[\"upsampleconv\",{\"0\":{\"1674\":1},\"1\":{\"1674\":1}}],[\"upsample=false\",{\"1\":{\"1506\":1}}],[\"upsample\",{\"0\":{\"1280\":1,\"1358\":1,\"1672\":1,\"1730\":1,\"1755\":1,\"1756\":1,\"1827\":1,\"1832\":1,\"1834\":1,\"1869\":1,\"1876\":1},\"1\":{\"1280\":2,\"1358\":2,\"1551\":1,\"1553\":1,\"1672\":1,\"1730\":1,\"1755\":2,\"1756\":2,\"1765\":5,\"1778\":2,\"1800\":5,\"1803\":5,\"1804\":5,\"1805\":2,\"1827\":2,\"1832\":1,\"1834\":4,\"1842\":1,\"1844\":6,\"1850\":2,\"1851\":6,\"1852\":2,\"1857\":4,\"1862\":7,\"1869\":1,\"1871\":12,\"1872\":8,\"1873\":8,\"1876\":3,\"1877\":2,\"1878\":5,\"1986\":1}}],[\"upsampling\",{\"0\":{\"830\":1},\"1\":{\"830\":6,\"835\":3,\"1508\":1,\"1655\":1,\"1719\":1,\"1752\":1,\"1755\":3,\"1756\":2,\"1765\":1,\"1800\":2,\"1803\":1,\"1804\":2,\"1834\":2,\"1842\":1,\"1844\":1,\"1851\":1,\"1857\":1,\"1862\":3,\"1871\":4,\"1876\":2,\"1878\":2,\"1986\":2}}],[\"upstream\",{\"1\":{\"102\":6,\"2431\":2,\"2432\":2,\"2441\":1}}],[\"upgrade\",{\"1\":{\"207\":1,\"2396\":1,\"2450\":1,\"2517\":1}}],[\"uploads\",{\"1\":{\"2040\":1}}],[\"uploaded\",{\"1\":{\"99\":2,\"2371\":1,\"2501\":2,\"2522\":2,\"2581\":2,\"2607\":2,\"2612\":1,\"2615\":2,\"2624\":2,\"2630\":1,\"2633\":2}}],[\"uploading\",{\"1\":{\"91\":2}}],[\"upload\",{\"1\":{\"91\":3,\"99\":1,\"2431\":1,\"2446\":1,\"2501\":1,\"2522\":2,\"2575\":1,\"2581\":2,\"2593\":1,\"2600\":4,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1}}],[\"updated\",{\"1\":{\"639\":1,\"727\":1,\"728\":1,\"1186\":1,\"1210\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1670\":1,\"1671\":1}}],[\"updates\",{\"1\":{\"627\":2,\"1142\":1,\"1178\":1,\"1179\":1,\"1180\":2,\"1186\":3,\"1210\":1,\"1269\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"2294\":1}}],[\"updater\",{\"1\":{\"604\":1,\"627\":1,\"652\":1,\"656\":1,\"709\":4,\"727\":4,\"728\":4,\"742\":4,\"834\":1,\"976\":1,\"1043\":1}}],[\"update\",{\"0\":{\"174\":1,\"298\":1,\"2308\":1},\"1\":{\"80\":1,\"83\":1,\"170\":1,\"195\":1,\"233\":1,\"298\":2,\"604\":1,\"627\":5,\"667\":1,\"671\":1,\"727\":6,\"728\":5,\"792\":1,\"976\":4,\"1043\":4,\"1186\":1,\"1210\":1,\"1451\":2,\"1514\":2,\"1557\":2,\"1585\":2,\"1612\":2,\"1615\":2,\"1623\":3,\"1637\":2,\"2041\":1,\"2308\":2,\"2598\":1}}],[\"updating\",{\"1\":{\"80\":1}}],[\"up\",{\"0\":{\"1307\":1,\"1508\":1,\"1688\":1,\"1693\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1755\":1,\"1756\":1},\"1\":{\"62\":1,\"69\":1,\"85\":1,\"113\":1,\"1003\":1,\"1307\":1,\"1508\":1,\"1551\":1,\"1553\":1,\"1639\":1,\"1661\":1,\"1688\":1,\"1693\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1754\":2,\"1755\":1,\"1756\":1,\"1941\":1,\"1947\":2,\"2199\":1,\"2394\":1,\"2395\":1,\"2530\":1,\"2531\":1}}],[\"utc\",{\"1\":{\"2441\":1,\"2442\":1}}],[\"utf\",{\"1\":{\"2099\":1,\"2482\":1}}],[\"utmos\",{\"1\":{\"1850\":1,\"1877\":1}}],[\"utt=200\",{\"1\":{\"2568\":1}}],[\"uttb\",{\"1\":{\"1385\":2,\"1387\":2}}],[\"utta\",{\"1\":{\"1385\":2,\"1387\":2,\"2012\":1}}],[\"utt4\",{\"1\":{\"1028\":2}}],[\"utt3\",{\"1\":{\"1028\":2}}],[\"utt1\",{\"1\":{\"987\":1,\"1028\":2}}],[\"uttidb\",{\"1\":{\"1015\":3,\"1383\":1}}],[\"uttida\",{\"1\":{\"1015\":3,\"1383\":2}}],[\"uttid\",{\"1\":{\"909\":2,\"1013\":1,\"1015\":2,\"2183\":2,\"2568\":2}}],[\"utt2uniq\",{\"1\":{\"2492\":2,\"2628\":2}}],[\"utt2category\",{\"1\":{\"2007\":1,\"2011\":1,\"2012\":1}}],[\"utt2\",{\"1\":{\"1028\":2}}],[\"utt2rir\",{\"1\":{\"949\":1}}],[\"utt2ratio=none\",{\"1\":{\"948\":1,\"952\":1,\"961\":1}}],[\"utt2noise=none\",{\"1\":{\"948\":1}}],[\"utt2num\",{\"1\":{\"238\":2,\"279\":2,\"294\":2}}],[\"utt2text\",{\"1\":{\"564\":1}}],[\"utt2feat2\",{\"1\":{\"564\":1}}],[\"utt2feat\",{\"1\":{\"564\":1}}],[\"utt2spk=none\",{\"1\":{\"941\":1}}],[\"utt2spk\",{\"1\":{\"237\":4,\"496\":2,\"564\":2,\"2181\":1,\"2373\":1,\"2385\":3,\"2387\":1,\"2395\":1,\"2412\":2,\"2430\":1,\"2492\":2,\"2531\":1,\"2555\":1,\"2568\":6,\"2628\":2,\"2638\":1}}],[\"utt\",{\"0\":{\"864\":1,\"1095\":1},\"1\":{\"237\":6,\"238\":2,\"245\":2,\"274\":1,\"301\":4,\"429\":4,\"629\":1,\"823\":1,\"864\":1,\"1085\":1,\"1095\":1,\"1113\":1,\"1171\":1,\"1172\":1,\"1371\":1,\"1551\":1,\"1553\":1,\"1554\":1,\"1773\":1,\"1837\":1,\"1892\":1,\"1893\":1,\"1970\":1,\"1975\":1,\"2027\":1,\"2038\":1,\"2046\":1,\"2076\":1,\"2082\":1,\"2183\":2,\"2190\":2,\"2240\":1,\"2278\":1,\"2385\":2,\"2386\":2,\"2387\":3,\"2395\":3,\"2412\":3,\"2440\":3,\"2531\":3,\"2568\":2,\"2574\":1}}],[\"utts\",{\"1\":{\"171\":2,\"175\":1,\"185\":1,\"193\":1,\"560\":2,\"909\":1,\"1028\":2,\"1057\":2}}],[\"uttmvn\",{\"1\":{\"102\":1,\"251\":6,\"259\":6,\"752\":3}}],[\"utterancecmvn\",{\"0\":{\"960\":1},\"1\":{\"960\":2}}],[\"utterancemvn\",{\"0\":{\"831\":1,\"1920\":1},\"1\":{\"831\":1,\"1920\":2}}],[\"utterances\",{\"1\":{\"107\":1,\"237\":2,\"1670\":1,\"1671\":1,\"2197\":1,\"2385\":1,\"2387\":1,\"2433\":1,\"2457\":1,\"2467\":1,\"2508\":2,\"2555\":1,\"2565\":1,\"2584\":1}}],[\"utterance\",{\"0\":{\"932\":1,\"1920\":1,\"1949\":2},\"1\":{\"51\":1,\"150\":1,\"506\":1,\"600\":2,\"633\":2,\"692\":1,\"693\":1,\"698\":1,\"699\":1,\"864\":1,\"909\":1,\"932\":2,\"1409\":1,\"1920\":2,\"1949\":5,\"2044\":1,\"2046\":2,\"2052\":1,\"2068\":1,\"2367\":1,\"2385\":1,\"2403\":1,\"2410\":2,\"2440\":2,\"2467\":1,\"2485\":1,\"2539\":1,\"2604\":1,\"2621\":1}}],[\"utilize\",{\"1\":{\"2410\":1,\"2467\":2}}],[\"utilities\",{\"0\":{\"159\":1},\"1\":{\"85\":4,\"745\":1,\"2372\":4,\"2385\":4,\"2429\":4,\"2554\":4}}],[\"utility\",{\"1\":{\"15\":1,\"25\":1,\"46\":1}}],[\"util\",{\"1\":{\"148\":1}}],[\"utils\",{\"0\":{\"600\":1,\"601\":1,\"610\":1,\"612\":1,\"616\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"623\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"638\":1,\"641\":1,\"647\":1,\"648\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"863\":1,\"865\":1,\"866\":1,\"868\":1,\"883\":1,\"884\":1,\"889\":1,\"891\":1,\"895\":1,\"899\":1,\"901\":1,\"903\":1,\"906\":1,\"908\":1,\"912\":1,\"913\":1,\"917\":1,\"918\":1,\"922\":1,\"925\":1,\"926\":1,\"927\":1,\"929\":1,\"930\":1,\"933\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"985\":1,\"986\":1,\"987\":2,\"988\":1,\"989\":2,\"990\":1,\"991\":1,\"993\":1,\"994\":1,\"996\":1,\"997\":1,\"998\":1,\"999\":1,\"1000\":1,\"1001\":1,\"1002\":2,\"1003\":1,\"1004\":1,\"1005\":1,\"1006\":1,\"1007\":2,\"1008\":1,\"1009\":1,\"1010\":1,\"1011\":1,\"1012\":1,\"1013\":1,\"1015\":1,\"1016\":1,\"1018\":1,\"1019\":1,\"1020\":2,\"1021\":1,\"1022\":1,\"1023\":1,\"1025\":1,\"1026\":1,\"1027\":2,\"1028\":1,\"1029\":1,\"1031\":1,\"1032\":2,\"1033\":2,\"1034\":2,\"1035\":1,\"1036\":1,\"1037\":1,\"1038\":2,\"1039\":1,\"1040\":1,\"1085\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1102\":1,\"1142\":2,\"1143\":2,\"1154\":2,\"1155\":2,\"1186\":2,\"1194\":2,\"1202\":2,\"1210\":2,\"1225\":1,\"1226\":2,\"1227\":2,\"1288\":1,\"1298\":2,\"1299\":2,\"1300\":1,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1306\":1,\"1307\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1324\":1,\"1325\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1331\":1,\"1333\":1,\"1334\":2,\"1335\":1,\"1338\":1,\"1340\":1,\"1344\":2,\"1346\":2,\"1353\":1,\"1355\":1,\"1356\":1,\"1359\":1,\"1456\":1,\"1458\":1,\"1468\":1,\"1474\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1508\":1,\"1512\":1,\"1542\":1,\"1549\":1,\"1573\":1,\"1583\":1,\"1592\":1,\"1596\":1,\"1607\":1,\"1613\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1681\":1,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1699\":1,\"1709\":1,\"1716\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1727\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":1,\"1740\":1,\"1742\":1,\"1744\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1751\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":1,\"1811\":1,\"1884\":1,\"1885\":1,\"1924\":1,\"1937\":1,\"1944\":1,\"2148\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":1,\"2166\":1,\"2180\":1,\"2212\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2227\":1,\"2239\":1,\"2254\":1,\"2309\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2313\":1,\"2315\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2321\":1,\"2322\":1,\"2324\":1,\"2325\":1,\"2326\":1,\"2327\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2336\":1,\"2338\":1,\"2339\":1,\"2341\":1,\"2342\":1,\"2665\":1,\"2667\":1,\"2680\":1,\"2709\":1,\"2716\":1},\"1\":{\"11\":3,\"15\":4,\"18\":2,\"26\":1,\"60\":2,\"80\":1,\"84\":1,\"85\":1,\"90\":2,\"98\":1,\"107\":1,\"110\":1,\"169\":1,\"172\":1,\"174\":3,\"181\":1,\"197\":2,\"198\":2,\"200\":1,\"201\":3,\"202\":1,\"203\":1,\"205\":2,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":4,\"222\":2,\"223\":2,\"224\":4,\"229\":2,\"230\":2,\"231\":5,\"235\":1,\"275\":1,\"276\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"600\":2,\"601\":2,\"610\":2,\"612\":1,\"616\":2,\"618\":2,\"619\":2,\"620\":2,\"621\":2,\"623\":2,\"629\":1,\"630\":2,\"631\":2,\"632\":2,\"633\":2,\"634\":2,\"638\":2,\"641\":2,\"647\":2,\"648\":2,\"651\":2,\"652\":2,\"653\":2,\"654\":2,\"655\":2,\"656\":2,\"863\":1,\"865\":1,\"866\":1,\"868\":1,\"883\":1,\"884\":1,\"889\":1,\"891\":1,\"895\":1,\"899\":1,\"901\":1,\"903\":1,\"906\":1,\"908\":1,\"912\":1,\"913\":1,\"917\":1,\"918\":1,\"922\":1,\"925\":1,\"926\":1,\"927\":1,\"929\":1,\"930\":1,\"933\":1,\"979\":1,\"980\":2,\"981\":3,\"982\":2,\"983\":2,\"985\":2,\"986\":2,\"987\":4,\"988\":1,\"989\":4,\"990\":2,\"991\":2,\"993\":2,\"994\":2,\"996\":1,\"997\":1,\"998\":1,\"999\":2,\"1000\":2,\"1001\":1,\"1002\":2,\"1003\":1,\"1004\":1,\"1005\":1,\"1006\":1,\"1007\":2,\"1008\":2,\"1009\":1,\"1010\":1,\"1011\":2,\"1012\":2,\"1013\":1,\"1015\":1,\"1016\":1,\"1018\":1,\"1019\":2,\"1020\":2,\"1021\":1,\"1022\":1,\"1023\":1,\"1025\":1,\"1026\":1,\"1027\":4,\"1028\":1,\"1029\":2,\"1031\":2,\"1032\":2,\"1033\":2,\"1034\":2,\"1035\":1,\"1036\":1,\"1037\":2,\"1038\":4,\"1039\":2,\"1040\":2,\"1085\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":2,\"1102\":2,\"1142\":2,\"1143\":2,\"1154\":2,\"1155\":2,\"1186\":2,\"1194\":2,\"1202\":2,\"1210\":2,\"1225\":1,\"1226\":2,\"1227\":2,\"1288\":1,\"1298\":2,\"1299\":2,\"1300\":1,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1306\":1,\"1307\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":2,\"1324\":1,\"1325\":1,\"1327\":2,\"1328\":2,\"1329\":2,\"1331\":1,\"1333\":1,\"1334\":2,\"1335\":1,\"1338\":1,\"1340\":1,\"1344\":2,\"1346\":2,\"1353\":1,\"1355\":2,\"1356\":2,\"1359\":1,\"1456\":1,\"1458\":1,\"1468\":1,\"1474\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":2,\"1503\":1,\"1508\":1,\"1512\":1,\"1542\":1,\"1549\":1,\"1573\":1,\"1583\":1,\"1592\":1,\"1596\":1,\"1607\":2,\"1613\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1681\":2,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":2,\"1699\":1,\"1709\":1,\"1716\":1,\"1722\":2,\"1723\":1,\"1724\":1,\"1727\":2,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":2,\"1740\":2,\"1742\":2,\"1744\":2,\"1747\":1,\"1748\":2,\"1749\":2,\"1751\":2,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":1,\"1811\":2,\"1884\":1,\"1885\":1,\"1924\":1,\"1937\":1,\"1944\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2148\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":2,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":2,\"2166\":2,\"2180\":1,\"2212\":2,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":2,\"2219\":1,\"2220\":1,\"2221\":1,\"2227\":1,\"2239\":1,\"2254\":1,\"2309\":1,\"2310\":2,\"2311\":2,\"2312\":2,\"2313\":1,\"2315\":1,\"2316\":2,\"2317\":1,\"2318\":1,\"2319\":2,\"2321\":2,\"2322\":1,\"2324\":2,\"2325\":2,\"2326\":1,\"2327\":2,\"2329\":2,\"2330\":2,\"2331\":2,\"2332\":2,\"2333\":2,\"2334\":2,\"2336\":2,\"2338\":2,\"2339\":2,\"2341\":2,\"2342\":1,\"2364\":1,\"2372\":1,\"2385\":1,\"2394\":1,\"2405\":1,\"2429\":1,\"2440\":3,\"2461\":1,\"2492\":1,\"2498\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2530\":1,\"2541\":1,\"2554\":1,\"2564\":1,\"2566\":1,\"2568\":4,\"2616\":1,\"2628\":1,\"2634\":1,\"2654\":1,\"2658\":1}}],[\"unbind\",{\"1\":{\"2498\":3,\"2616\":3,\"2634\":3}}],[\"unbatchfy\",{\"1\":{\"691\":1}}],[\"una\",{\"1\":{\"2457\":1}}],[\"unzip\",{\"1\":{\"2368\":1,\"2396\":1,\"2454\":1,\"2460\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2506\":1,\"2510\":2,\"2512\":1,\"2519\":2,\"2532\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"unpack\",{\"0\":{\"1968\":1},\"1\":{\"1968\":2,\"1969\":1,\"2358\":1,\"2371\":1,\"2514\":1,\"2520\":1,\"2579\":1,\"2592\":1,\"2599\":1,\"2612\":1,\"2617\":1,\"2630\":1,\"2635\":1,\"2659\":1}}],[\"unpaired\",{\"1\":{\"1398\":1}}],[\"unvoiced\",{\"1\":{\"1797\":1,\"2142\":3}}],[\"unet\",{\"0\":{\"1726\":1,\"1752\":1},\"1\":{\"1726\":1,\"1752\":1,\"1800\":1}}],[\"unconstrained\",{\"0\":{\"1888\":1},\"1\":{\"1670\":1,\"1671\":1,\"1888\":1}}],[\"uncompressed\",{\"1\":{\"49\":1}}],[\"unfolded\",{\"1\":{\"1719\":1}}],[\"unfold\",{\"1\":{\"1719\":1}}],[\"unfolding\",{\"1\":{\"1660\":2,\"1661\":2,\"1662\":2,\"1719\":1}}],[\"unfortunately\",{\"1\":{\"235\":1,\"2125\":1}}],[\"unmasked\",{\"1\":{\"1269\":1,\"1890\":1}}],[\"unlabeled\",{\"1\":{\"2574\":1}}],[\"unless\",{\"1\":{\"1241\":1,\"1243\":1}}],[\"unlike\",{\"1\":{\"57\":1,\"74\":1,\"745\":1,\"746\":1,\"1015\":1}}],[\"unrolling\",{\"1\":{\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1177\":1,\"1209\":1,\"1252\":1,\"1253\":1,\"1254\":1}}],[\"untie\",{\"1\":{\"1115\":2}}],[\"until\",{\"1\":{\"112\":1,\"700\":1,\"1138\":1,\"1139\":1,\"1926\":1}}],[\"unused\",{\"1\":{\"429\":2,\"1463\":1,\"1605\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1719\":2,\"2186\":1,\"2202\":2,\"2204\":1}}],[\"un\",{\"1\":{\"231\":1,\"2457\":1}}],[\"unk\",{\"1\":{\"217\":1,\"224\":1,\"231\":1,\"276\":1,\"298\":1,\"618\":2,\"620\":1,\"2135\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2373\":1,\"2555\":1}}],[\"unknown\",{\"1\":{\"59\":2,\"618\":1,\"1551\":1,\"1553\":1,\"1645\":1,\"1735\":1,\"2373\":1,\"2555\":1}}],[\"unstable\",{\"1\":{\"2564\":1}}],[\"unseen\",{\"1\":{\"2543\":1}}],[\"unsqueeze\",{\"1\":{\"2498\":1,\"2521\":1,\"2522\":1,\"2523\":1,\"2616\":1,\"2634\":1}}],[\"unsigned\",{\"1\":{\"1927\":1}}],[\"unsplit\",{\"1\":{\"1424\":1}}],[\"unsupervised\",{\"1\":{\"130\":1,\"1398\":1,\"2257\":1,\"2261\":1,\"2262\":1,\"2294\":2}}],[\"unsortedbatchsampler\",{\"0\":{\"2011\":1},\"1\":{\"2011\":1}}],[\"unsorted\",{\"0\":{\"2011\":1},\"1\":{\"73\":1,\"74\":1,\"429\":2,\"2011\":1,\"2012\":2}}],[\"unnorm\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"unnormalized\",{\"1\":{\"1886\":3,\"1887\":3,\"1888\":3}}],[\"unno\",{\"1\":{\"130\":1}}],[\"unnecessary\",{\"1\":{\"59\":1,\"1697\":1}}],[\"undocumented\",{\"1\":{\"126\":1,\"398\":11}}],[\"undefined\",{\"1\":{\"98\":1,\"2568\":1,\"2569\":1}}],[\"understood\",{\"1\":{\"2584\":1}}],[\"understand\",{\"1\":{\"135\":1,\"226\":1,\"233\":1,\"235\":1,\"1972\":1,\"2372\":1,\"2381\":1,\"2394\":2,\"2407\":1,\"2414\":1,\"2429\":1,\"2430\":1,\"2448\":1,\"2464\":1,\"2530\":2,\"2554\":1,\"2555\":1,\"2558\":1,\"2564\":1}}],[\"understanding\",{\"0\":{\"157\":1,\"2463\":1,\"2467\":1},\"1\":{\"83\":1,\"130\":2,\"150\":1,\"157\":1,\"161\":1,\"2380\":2,\"2388\":3,\"2399\":1,\"2421\":1,\"2463\":1,\"2464\":1,\"2467\":4,\"2524\":2,\"2535\":1,\"2544\":1,\"2646\":1}}],[\"undertook\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"undernormalization\",{\"1\":{\"1302\":1,\"1303\":1,\"1304\":1,\"1337\":1}}],[\"under\",{\"1\":{\"3\":1,\"16\":1,\"26\":2,\"83\":1,\"112\":3,\"134\":2,\"135\":2,\"940\":1,\"1211\":1,\"1286\":1,\"1336\":1,\"2637\":1,\"2638\":1,\"2639\":2,\"2642\":1,\"2645\":1}}],[\"uninstall\",{\"1\":{\"2482\":1}}],[\"unix\",{\"1\":{\"2430\":1,\"2555\":1}}],[\"unigram30\",{\"1\":{\"2373\":1,\"2556\":1}}],[\"unicode=none\",{\"1\":{\"2315\":1}}],[\"universidad\",{\"1\":{\"2457\":1}}],[\"universal\",{\"1\":{\"1462\":1,\"1463\":1,\"1670\":1,\"1671\":1}}],[\"univnet\",{\"1\":{\"1786\":1}}],[\"unidirectional\",{\"1\":{\"1522\":1,\"1523\":1,\"1572\":1}}],[\"unidecode\",{\"1\":{\"207\":1}}],[\"union\",{\"1\":{\"1141\":1,\"1148\":1,\"1170\":1,\"1203\":1,\"1375\":1,\"1454\":1,\"1463\":1,\"1505\":2,\"1515\":1,\"1516\":2,\"1523\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1558\":1,\"1603\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":1,\"1671\":1,\"1719\":1,\"1800\":1,\"1836\":2,\"1839\":2,\"1843\":1,\"2054\":1,\"2106\":2,\"2258\":1,\"2343\":1,\"2344\":1,\"2347\":2,\"2349\":2}}],[\"unifying\",{\"1\":{\"1551\":1,\"1553\":1}}],[\"uniform\",{\"0\":{\"905\":1,\"931\":1},\"1\":{\"905\":2,\"931\":2,\"956\":1,\"973\":1,\"1269\":2,\"1778\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"2003\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2264\":1,\"2279\":1,\"2440\":1,\"2558\":1,\"2584\":1}}],[\"unified\",{\"1\":{\"60\":1,\"80\":1,\"130\":1,\"143\":1,\"1696\":1,\"1698\":1,\"2384\":1}}],[\"unique\",{\"1\":{\"21\":1,\"56\":1,\"115\":1,\"237\":1,\"2473\":1}}],[\"united\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"unitysynthesizer\",{\"0\":{\"2004\":1},\"1\":{\"2004\":1}}],[\"unity\",{\"0\":{\"2004\":1},\"1\":{\"2004\":2}}],[\"unitarily\",{\"1\":{\"1339\":1}}],[\"units`\",{\"1\":{\"2004\":1}}],[\"units=192\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"units=128\",{\"1\":{\"701\":1,\"1994\":1}}],[\"units=2048\",{\"1\":{\"747\":1,\"749\":1}}],[\"units=256\",{\"1\":{\"701\":1,\"803\":1,\"2078\":1}}],[\"units=0\",{\"1\":{\"732\":1,\"748\":1,\"801\":1}}],[\"units\",{\"1\":{\"30\":1,\"175\":1,\"194\":1,\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"239\":1,\"276\":1,\"298\":1,\"614\":3,\"635\":3,\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"701\":4,\"731\":3,\"732\":4,\"747\":3,\"748\":4,\"749\":2,\"754\":2,\"758\":2,\"782\":1,\"784\":4,\"800\":2,\"801\":5,\"802\":1,\"804\":2,\"805\":2,\"806\":2,\"807\":3,\"808\":2,\"821\":8,\"826\":4,\"892\":2,\"1059\":1,\"1133\":2,\"1140\":1,\"1145\":1,\"1148\":3,\"1149\":3,\"1150\":3,\"1151\":1,\"1167\":1,\"1168\":1,\"1169\":2,\"1181\":2,\"1196\":1,\"1197\":1,\"1200\":2,\"1203\":3,\"1204\":1,\"1270\":1,\"1271\":1,\"1272\":3,\"1273\":1,\"1505\":3,\"1516\":3,\"1576\":1,\"1577\":1,\"1595\":1,\"1607\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":3,\"1771\":3,\"1787\":3,\"1788\":3,\"1798\":3,\"1850\":1,\"1851\":5,\"1852\":1,\"1874\":3,\"2001\":4,\"2002\":4,\"2003\":3,\"2004\":3,\"2026\":1,\"2029\":3,\"2054\":3,\"2078\":3,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":8,\"2243\":5,\"2244\":5,\"2255\":5,\"2257\":4,\"2261\":3,\"2263\":8,\"2264\":8,\"2279\":2,\"2414\":1,\"2440\":2,\"2558\":2,\"2564\":1,\"2569\":1,\"2584\":2}}],[\"unit\",{\"0\":{\"405\":1,\"2645\":1},\"1\":{\"15\":2,\"51\":1,\"150\":1,\"648\":2,\"1054\":1,\"1153\":1,\"1376\":1,\"1515\":2,\"1528\":2,\"1529\":2,\"1534\":2,\"1539\":2,\"1626\":2,\"1654\":2,\"1958\":1,\"1960\":3,\"1984\":3,\"2001\":5,\"2004\":4,\"2277\":1,\"2452\":1,\"2518\":1,\"2519\":3,\"2520\":3,\"2521\":1,\"2645\":1}}],[\"u\",{\"1\":{\"10\":1,\"23\":1,\"48\":1,\"98\":2,\"119\":1,\"249\":2,\"690\":1,\"700\":2,\"725\":12,\"766\":2,\"806\":1,\"825\":19,\"827\":5,\"884\":1,\"885\":1,\"1014\":2,\"1048\":2,\"1059\":1,\"1062\":2,\"1064\":2,\"1066\":2,\"1073\":1,\"1075\":2,\"1081\":2,\"1083\":1,\"1086\":6,\"1099\":3,\"1138\":2,\"1139\":2,\"1142\":15,\"1154\":2,\"1155\":5,\"1173\":1,\"1186\":5,\"1210\":3,\"1228\":3,\"1243\":5,\"1246\":3,\"1247\":2,\"1248\":1,\"1270\":1,\"1298\":4,\"1299\":4,\"1301\":6,\"1302\":4,\"1303\":4,\"1304\":6,\"1334\":7,\"1337\":3,\"1345\":3,\"1347\":3,\"1349\":3,\"1350\":3,\"1455\":1,\"1696\":1,\"1706\":1,\"1708\":1,\"1712\":1,\"1715\":1,\"1752\":1,\"1761\":1,\"1763\":1,\"1768\":1,\"1797\":1,\"1805\":1,\"1927\":1,\"2568\":2,\"2569\":2,\"2598\":2}}],[\"u18\",{\"1\":{\"8\":1}}],[\"ubuntu16\",{\"1\":{\"167\":3,\"178\":3,\"196\":2,\"234\":2}}],[\"ubuntu18\",{\"1\":{\"133\":1}}],[\"ubuntu\",{\"0\":{\"8\":1},\"1\":{\"5\":3,\"6\":2,\"132\":3,\"133\":1,\"134\":2}}],[\"usp=sharing\",{\"1\":{\"2593\":2}}],[\"usually\",{\"1\":{\"1067\":2,\"1082\":2,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"2046\":1,\"2385\":2,\"2414\":1,\"2558\":1,\"2573\":1,\"2600\":1}}],[\"us\",{\"1\":{\"56\":1,\"235\":2,\"240\":1,\"461\":1,\"2359\":1,\"2365\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2521\":1,\"2580\":1,\"2655\":1,\"2660\":1}}],[\"usr\",{\"1\":{\"18\":1,\"28\":1,\"90\":1,\"98\":1,\"134\":1,\"136\":1,\"1950\":1,\"2568\":1,\"2569\":1}}],[\"usages\",{\"0\":{\"145\":1},\"1\":{\"1132\":1}}],[\"usage\",{\"0\":{\"14\":1,\"47\":1,\"62\":1,\"100\":1,\"102\":1,\"108\":1,\"113\":1,\"143\":1,\"179\":1,\"190\":1},\"1\":{\"85\":1,\"92\":1,\"108\":1,\"109\":1,\"112\":1,\"148\":1,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"271\":1,\"272\":1,\"274\":1,\"275\":1,\"276\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"285\":1,\"286\":1,\"287\":1,\"288\":1,\"290\":1,\"292\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"377\":1,\"380\":1,\"384\":1,\"391\":1,\"397\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"493\":1,\"496\":1,\"499\":1,\"501\":1,\"503\":1,\"506\":1,\"509\":1,\"512\":1,\"515\":1,\"517\":1,\"519\":1,\"522\":1,\"525\":1,\"528\":1,\"530\":1,\"533\":1,\"536\":1,\"538\":1,\"541\":1,\"544\":1,\"546\":1,\"549\":1,\"551\":1,\"554\":1,\"557\":1,\"560\":1,\"562\":1,\"564\":1,\"566\":1,\"568\":1,\"570\":1,\"572\":1,\"574\":1,\"576\":1,\"579\":1,\"582\":1,\"585\":1,\"588\":1,\"590\":1,\"700\":1,\"785\":1,\"944\":1,\"1138\":1,\"1139\":1,\"1171\":1,\"1206\":1,\"1409\":1,\"1427\":1,\"1516\":1,\"1552\":1,\"2046\":1,\"2199\":1,\"2355\":1,\"2372\":1,\"2384\":1,\"2389\":1,\"2390\":1,\"2411\":1,\"2424\":1,\"2429\":1,\"2431\":2,\"2481\":2,\"2525\":1,\"2526\":1,\"2547\":1,\"2554\":1,\"2568\":1,\"2572\":1,\"2618\":1}}],[\"usebias\",{\"1\":{\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1}}],[\"useful\",{\"0\":{\"2355\":1,\"2382\":1,\"2390\":1,\"2424\":1,\"2526\":1,\"2547\":1},\"1\":{\"115\":1,\"168\":1,\"785\":1,\"1015\":1,\"1252\":1,\"1390\":1,\"1451\":1,\"1484\":2,\"1514\":1,\"1551\":1,\"1553\":1,\"1557\":1,\"1585\":1,\"1601\":2,\"1612\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1623\":1,\"1637\":1,\"1638\":2,\"1905\":1,\"2574\":1,\"2584\":1}}],[\"used\",{\"1\":{\"17\":1,\"18\":1,\"21\":1,\"22\":1,\"23\":1,\"45\":4,\"46\":1,\"48\":1,\"56\":2,\"59\":1,\"60\":2,\"79\":2,\"85\":3,\"98\":2,\"102\":1,\"107\":1,\"112\":2,\"113\":1,\"115\":1,\"119\":1,\"132\":1,\"134\":1,\"135\":1,\"136\":1,\"142\":1,\"150\":2,\"152\":1,\"237\":1,\"239\":1,\"241\":1,\"294\":1,\"626\":1,\"668\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"710\":1,\"711\":4,\"727\":1,\"728\":1,\"734\":1,\"740\":2,\"741\":2,\"745\":1,\"746\":1,\"766\":2,\"767\":1,\"774\":1,\"775\":2,\"776\":2,\"785\":2,\"817\":1,\"828\":1,\"834\":1,\"836\":1,\"933\":1,\"975\":1,\"976\":1,\"987\":3,\"989\":1,\"1007\":1,\"1008\":1,\"1013\":1,\"1031\":1,\"1034\":1,\"1042\":1,\"1043\":1,\"1052\":2,\"1057\":1,\"1068\":2,\"1072\":1,\"1098\":1,\"1113\":1,\"1132\":1,\"1133\":1,\"1140\":2,\"1141\":2,\"1142\":2,\"1144\":1,\"1148\":2,\"1149\":5,\"1150\":5,\"1155\":3,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1169\":2,\"1171\":1,\"1172\":1,\"1178\":1,\"1179\":1,\"1180\":2,\"1181\":2,\"1182\":1,\"1186\":1,\"1187\":4,\"1200\":1,\"1202\":4,\"1203\":2,\"1210\":2,\"1216\":1,\"1243\":1,\"1252\":1,\"1253\":3,\"1254\":2,\"1255\":1,\"1269\":3,\"1272\":1,\"1273\":2,\"1279\":1,\"1286\":1,\"1287\":1,\"1337\":1,\"1343\":1,\"1352\":1,\"1371\":3,\"1377\":1,\"1429\":1,\"1462\":1,\"1463\":2,\"1505\":1,\"1510\":1,\"1511\":1,\"1516\":1,\"1524\":2,\"1529\":1,\"1534\":1,\"1539\":1,\"1543\":2,\"1545\":1,\"1551\":6,\"1552\":2,\"1553\":6,\"1558\":2,\"1611\":1,\"1616\":1,\"1617\":1,\"1626\":1,\"1633\":1,\"1638\":1,\"1639\":1,\"1645\":1,\"1654\":1,\"1656\":1,\"1658\":1,\"1659\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":3,\"1671\":5,\"1692\":1,\"1712\":1,\"1715\":1,\"1739\":3,\"1752\":3,\"1776\":1,\"1778\":2,\"1781\":1,\"1797\":1,\"1801\":3,\"1803\":1,\"1804\":5,\"1805\":2,\"1849\":1,\"1850\":2,\"1852\":2,\"1856\":2,\"1858\":2,\"1864\":1,\"1871\":1,\"1877\":2,\"1878\":2,\"1892\":1,\"1893\":1,\"1925\":1,\"1927\":2,\"1928\":1,\"1929\":2,\"1935\":1,\"1941\":2,\"1943\":1,\"1946\":1,\"1947\":3,\"1964\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2012\":5,\"2027\":1,\"2029\":1,\"2046\":2,\"2050\":1,\"2076\":1,\"2096\":2,\"2098\":2,\"2099\":5,\"2100\":2,\"2101\":2,\"2102\":3,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2197\":1,\"2199\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2279\":1,\"2372\":1,\"2373\":1,\"2385\":2,\"2388\":1,\"2392\":1,\"2400\":1,\"2411\":1,\"2414\":1,\"2415\":1,\"2418\":1,\"2421\":1,\"2425\":1,\"2429\":2,\"2430\":1,\"2440\":4,\"2462\":1,\"2468\":1,\"2510\":1,\"2524\":1,\"2528\":1,\"2536\":1,\"2544\":1,\"2548\":1,\"2552\":1,\"2554\":1,\"2555\":2,\"2559\":1,\"2564\":2,\"2568\":1,\"2584\":1,\"2585\":1,\"2592\":1,\"2596\":1,\"2638\":1}}],[\"usesseparator\",{\"0\":{\"1671\":1},\"1\":{\"1671\":1}}],[\"uses\",{\"0\":{\"1430\":1,\"1470\":1,\"1471\":1,\"1586\":1,\"1670\":2,\"1671\":1},\"1\":{\"17\":1,\"19\":1,\"134\":1,\"689\":1,\"692\":1,\"693\":1,\"697\":1,\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"797\":1,\"834\":1,\"857\":1,\"952\":1,\"1430\":2,\"1470\":2,\"1471\":2,\"1586\":2,\"1670\":6,\"1671\":2,\"1785\":1,\"1846\":1,\"1847\":1,\"1921\":2,\"1951\":1,\"2125\":1,\"2279\":1,\"2354\":1,\"2388\":1,\"2421\":1,\"2441\":1,\"2524\":1,\"2544\":1}}],[\"use\",{\"0\":{\"19\":1,\"27\":1,\"50\":1,\"52\":1,\"89\":1,\"96\":1,\"2415\":1,\"2574\":1,\"2583\":1,\"2585\":1},\"1\":{\"4\":1,\"6\":1,\"10\":1,\"17\":2,\"19\":6,\"21\":12,\"22\":5,\"30\":3,\"38\":1,\"47\":1,\"48\":1,\"49\":2,\"62\":1,\"66\":1,\"70\":2,\"73\":1,\"74\":1,\"79\":1,\"80\":1,\"81\":1,\"83\":1,\"85\":1,\"94\":1,\"100\":1,\"104\":2,\"105\":2,\"106\":1,\"110\":1,\"113\":3,\"115\":4,\"127\":1,\"132\":1,\"134\":5,\"135\":1,\"144\":3,\"148\":1,\"155\":1,\"164\":1,\"168\":1,\"170\":1,\"173\":2,\"179\":1,\"192\":1,\"217\":1,\"235\":1,\"240\":1,\"245\":2,\"251\":9,\"263\":2,\"265\":4,\"267\":2,\"269\":4,\"271\":1,\"272\":1,\"295\":3,\"315\":2,\"363\":6,\"399\":4,\"429\":12,\"455\":2,\"464\":4,\"470\":4,\"485\":2,\"605\":1,\"607\":1,\"615\":1,\"617\":1,\"626\":3,\"627\":9,\"677\":1,\"685\":1,\"686\":1,\"687\":1,\"689\":1,\"700\":1,\"702\":2,\"710\":1,\"711\":1,\"712\":2,\"725\":3,\"730\":1,\"740\":6,\"741\":6,\"749\":1,\"754\":4,\"755\":4,\"756\":3,\"775\":6,\"776\":6,\"778\":2,\"797\":1,\"802\":3,\"806\":3,\"808\":1,\"821\":10,\"822\":4,\"824\":2,\"826\":9,\"933\":2,\"936\":3,\"943\":1,\"950\":1,\"952\":1,\"955\":1,\"965\":1,\"968\":1,\"972\":1,\"987\":4,\"996\":1,\"997\":2,\"999\":1,\"1003\":1,\"1004\":1,\"1011\":1,\"1015\":1,\"1028\":3,\"1046\":1,\"1048\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":4,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":4,\"1066\":1,\"1067\":3,\"1068\":1,\"1073\":1,\"1075\":1,\"1076\":4,\"1083\":1,\"1084\":4,\"1093\":2,\"1133\":5,\"1138\":1,\"1139\":1,\"1140\":3,\"1148\":7,\"1149\":4,\"1150\":3,\"1151\":1,\"1153\":1,\"1158\":3,\"1167\":1,\"1168\":1,\"1169\":3,\"1178\":1,\"1180\":1,\"1181\":4,\"1190\":1,\"1196\":1,\"1197\":1,\"1198\":3,\"1200\":1,\"1203\":7,\"1204\":2,\"1214\":1,\"1215\":1,\"1216\":1,\"1222\":3,\"1239\":3,\"1241\":1,\"1242\":1,\"1244\":1,\"1245\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1270\":3,\"1271\":1,\"1272\":2,\"1273\":3,\"1282\":3,\"1375\":1,\"1400\":1,\"1430\":1,\"1505\":7,\"1508\":1,\"1516\":11,\"1522\":2,\"1523\":1,\"1524\":3,\"1525\":2,\"1526\":1,\"1537\":1,\"1539\":1,\"1543\":2,\"1545\":2,\"1572\":1,\"1581\":1,\"1611\":6,\"1639\":2,\"1644\":1,\"1646\":1,\"1655\":2,\"1656\":2,\"1658\":1,\"1659\":4,\"1660\":5,\"1661\":6,\"1662\":5,\"1664\":1,\"1665\":1,\"1669\":4,\"1670\":3,\"1671\":5,\"1719\":2,\"1738\":1,\"1761\":1,\"1763\":1,\"1765\":6,\"1767\":1,\"1768\":1,\"1771\":14,\"1778\":24,\"1781\":9,\"1782\":1,\"1786\":1,\"1787\":6,\"1788\":4,\"1791\":3,\"1793\":1,\"1795\":1,\"1798\":10,\"1800\":10,\"1801\":4,\"1803\":6,\"1804\":30,\"1805\":16,\"1844\":6,\"1845\":2,\"1846\":1,\"1847\":3,\"1848\":8,\"1849\":6,\"1850\":16,\"1851\":31,\"1852\":22,\"1857\":5,\"1858\":3,\"1859\":4,\"1861\":3,\"1862\":5,\"1863\":3,\"1864\":6,\"1865\":6,\"1866\":3,\"1870\":2,\"1871\":3,\"1872\":1,\"1873\":1,\"1874\":7,\"1877\":13,\"1878\":22,\"1879\":4,\"1880\":12,\"1912\":2,\"1956\":1,\"1968\":1,\"1985\":2,\"2000\":2,\"2001\":6,\"2002\":13,\"2003\":4,\"2004\":6,\"2026\":1,\"2029\":1,\"2049\":1,\"2054\":6,\"2055\":1,\"2064\":1,\"2078\":5,\"2079\":4,\"2083\":4,\"2086\":10,\"2087\":10,\"2088\":4,\"2090\":15,\"2091\":4,\"2095\":26,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2125\":1,\"2178\":2,\"2179\":2,\"2184\":1,\"2185\":1,\"2186\":5,\"2197\":1,\"2200\":1,\"2201\":1,\"2202\":10,\"2203\":1,\"2204\":5,\"2216\":1,\"2217\":1,\"2236\":3,\"2241\":1,\"2243\":25,\"2244\":26,\"2245\":4,\"2255\":25,\"2256\":4,\"2263\":24,\"2264\":22,\"2266\":1,\"2279\":22,\"2280\":4,\"2294\":3,\"2352\":1,\"2363\":4,\"2364\":1,\"2368\":1,\"2371\":1,\"2372\":3,\"2373\":5,\"2375\":3,\"2377\":2,\"2383\":1,\"2385\":4,\"2386\":1,\"2387\":2,\"2389\":2,\"2393\":4,\"2395\":2,\"2398\":1,\"2399\":3,\"2401\":1,\"2408\":2,\"2411\":1,\"2412\":1,\"2414\":2,\"2415\":1,\"2417\":1,\"2418\":2,\"2419\":1,\"2422\":1,\"2427\":2,\"2429\":5,\"2430\":4,\"2431\":2,\"2432\":3,\"2433\":3,\"2436\":2,\"2440\":6,\"2449\":2,\"2450\":2,\"2459\":1,\"2462\":1,\"2465\":2,\"2473\":1,\"2481\":2,\"2482\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2503\":2,\"2506\":4,\"2507\":1,\"2510\":2,\"2512\":2,\"2513\":1,\"2514\":4,\"2525\":2,\"2529\":5,\"2531\":2,\"2534\":1,\"2535\":3,\"2537\":1,\"2545\":1,\"2550\":3,\"2552\":3,\"2554\":3,\"2555\":6,\"2558\":2,\"2559\":3,\"2562\":2,\"2564\":4,\"2565\":2,\"2568\":1,\"2569\":3,\"2570\":1,\"2573\":1,\"2574\":1,\"2583\":3,\"2584\":9,\"2585\":7,\"2599\":1,\"2600\":3,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2640\":2,\"2644\":1,\"2653\":4,\"2654\":1,\"2657\":2,\"2658\":1,\"2659\":4}}],[\"users\",{\"1\":{\"24\":1,\"113\":1,\"607\":1,\"613\":1,\"2468\":1,\"2640\":1}}],[\"user\",{\"1\":{\"1\":1,\"4\":2,\"14\":1,\"21\":1,\"27\":1,\"99\":1,\"112\":2,\"121\":1,\"124\":1,\"1007\":1,\"1785\":1,\"2349\":3,\"2467\":2,\"2585\":1}}],[\"using\",{\"0\":{\"3\":1,\"4\":1,\"11\":1,\"40\":1,\"41\":1,\"42\":1,\"46\":1,\"54\":1,\"66\":1,\"85\":1,\"97\":1,\"98\":1,\"141\":1,\"197\":1,\"198\":1,\"2584\":1,\"2589\":1,\"2590\":1},\"1\":{\"1\":2,\"3\":1,\"4\":2,\"5\":3,\"11\":1,\"12\":1,\"17\":1,\"19\":1,\"24\":2,\"47\":2,\"57\":1,\"58\":1,\"69\":2,\"71\":1,\"74\":1,\"80\":3,\"85\":1,\"93\":1,\"94\":1,\"97\":1,\"98\":1,\"99\":1,\"102\":1,\"113\":1,\"118\":1,\"130\":1,\"132\":1,\"136\":1,\"144\":1,\"150\":1,\"161\":2,\"166\":1,\"177\":1,\"195\":1,\"198\":1,\"202\":1,\"206\":1,\"233\":1,\"235\":2,\"239\":1,\"241\":1,\"242\":1,\"243\":1,\"245\":2,\"249\":1,\"257\":1,\"261\":1,\"263\":1,\"267\":1,\"286\":2,\"296\":2,\"519\":1,\"541\":1,\"568\":1,\"595\":2,\"605\":2,\"608\":1,\"648\":1,\"686\":1,\"688\":1,\"689\":1,\"710\":2,\"745\":1,\"754\":2,\"929\":1,\"940\":1,\"952\":1,\"999\":1,\"1011\":1,\"1025\":1,\"1057\":1,\"1132\":1,\"1144\":1,\"1198\":3,\"1207\":1,\"1228\":1,\"1247\":1,\"1257\":1,\"1284\":1,\"1345\":1,\"1347\":1,\"1398\":1,\"1528\":1,\"1603\":1,\"1639\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1688\":1,\"1756\":1,\"1785\":1,\"1788\":1,\"1917\":1,\"1919\":1,\"1940\":1,\"1948\":1,\"1961\":1,\"2099\":1,\"2156\":1,\"2193\":1,\"2212\":2,\"2325\":1,\"2361\":1,\"2372\":1,\"2373\":1,\"2377\":1,\"2384\":1,\"2385\":1,\"2389\":2,\"2393\":1,\"2400\":1,\"2407\":1,\"2408\":2,\"2410\":1,\"2422\":2,\"2429\":2,\"2433\":2,\"2438\":2,\"2449\":2,\"2451\":1,\"2465\":2,\"2468\":1,\"2471\":1,\"2473\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2479\":1,\"2481\":2,\"2503\":2,\"2516\":1,\"2525\":2,\"2529\":1,\"2536\":1,\"2545\":2,\"2552\":1,\"2554\":1,\"2555\":2,\"2558\":1,\"2564\":3,\"2570\":1,\"2571\":1,\"2576\":1,\"2586\":1,\"2598\":1,\"2600\":2,\"2601\":1,\"2618\":1,\"2650\":1}}],[\"3a22d1584317ae59974aad62feab8719c003ae05\",{\"1\":{\"2550\":1}}],[\"3a3\",{\"1\":{\"2372\":1}}],[\"333\",{\"1\":{\"2507\":2,\"2510\":4,\"2513\":2,\"2654\":2,\"2658\":2}}],[\"3|0\",{\"1\":{\"2572\":2}}],[\"3|62\",{\"1\":{\"2564\":1}}],[\"3|1\",{\"1\":{\"2572\":1}}],[\"3|15\",{\"1\":{\"2564\":1}}],[\"3|19\",{\"1\":{\"2441\":1}}],[\"3|3\",{\"1\":{\"2564\":1}}],[\"3|2\",{\"1\":{\"2441\":1}}],[\"3e9\",{\"1\":{\"2154\":1}}],[\"362epoch\",{\"1\":{\"2520\":1}}],[\"36787944117144233\",{\"1\":{\"1841\":1}}],[\"369\",{\"1\":{\"87\":1}}],[\"3x3\",{\"1\":{\"1691\":1,\"1732\":1}}],[\"3d\",{\"1\":{\"1145\":5,\"1166\":1,\"1517\":1,\"1655\":1,\"1719\":2}}],[\"38\",{\"1\":{\"2444\":1,\"2500\":5,\"2617\":5,\"2635\":5}}],[\"3829\",{\"1\":{\"705\":1}}],[\"3825\",{\"1\":{\"705\":1}}],[\"384\",{\"1\":{\"21\":2,\"1505\":1,\"1669\":1,\"1778\":2,\"1788\":2,\"1850\":3,\"1851\":3,\"1852\":4,\"2003\":1,\"2087\":1,\"2090\":2,\"2243\":2,\"2244\":4,\"2255\":4,\"2265\":1,\"2279\":4,\"2290\":1}}],[\"3711\",{\"1\":{\"700\":2,\"1048\":1,\"1138\":2,\"1139\":2}}],[\"3773\",{\"1\":{\"17\":1}}],[\"3f\",{\"1\":{\"174\":2}}],[\"3rd\",{\"1\":{\"149\":1}}],[\"30\",{\"1\":{\"1398\":1,\"1862\":1,\"1880\":1,\"1914\":1,\"1940\":1,\"2095\":1,\"2194\":1,\"2292\":1,\"2372\":1,\"2373\":2,\"2375\":1,\"2384\":1,\"2387\":1,\"2394\":1,\"2428\":1,\"2530\":1,\"2551\":1,\"2571\":1,\"2584\":1}}],[\"3072\",{\"1\":{\"1115\":4,\"1269\":1}}],[\"3023\",{\"1\":{\"1400\":1}}],[\"302\",{\"1\":{\"130\":1}}],[\"3000\",{\"1\":{\"1905\":1,\"1921\":1,\"1922\":1,\"1938\":1}}],[\"300\",{\"1\":{\"76\":1,\"295\":8,\"730\":1,\"756\":2,\"1158\":2,\"1239\":2,\"1524\":1,\"1525\":1,\"1611\":2,\"1791\":2}}],[\"311\",{\"1\":{\"130\":1}}],[\"31\",{\"1\":{\"115\":1,\"987\":1,\"1140\":1,\"1148\":1,\"1149\":1,\"1169\":1,\"1203\":1,\"1778\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"2003\":1,\"2026\":1,\"2054\":1,\"2090\":1,\"2243\":1,\"2244\":1,\"2279\":1,\"2440\":1,\"2442\":1,\"2564\":1}}],[\"315\",{\"1\":{\"87\":1}}],[\"3nodes\",{\"0\":{\"41\":1}}],[\"32767\",{\"1\":{\"2592\":2,\"2596\":2}}],[\"32768\",{\"1\":{\"2177\":1,\"2188\":1,\"2208\":1}}],[\"320\",{\"1\":{\"22\":1,\"730\":1,\"756\":3,\"825\":1,\"1158\":3,\"1220\":3,\"1222\":2,\"1239\":3,\"1270\":1,\"1282\":2,\"1289\":2,\"1524\":2,\"1525\":1,\"1611\":3,\"1791\":3,\"2019\":1}}],[\"32\",{\"1\":{\"21\":4,\"26\":1,\"122\":1,\"172\":1,\"509\":1,\"512\":1,\"525\":1,\"585\":1,\"838\":1,\"1058\":1,\"1516\":1,\"1522\":1,\"1523\":1,\"1761\":1,\"1763\":2,\"1766\":2,\"1778\":2,\"1786\":2,\"1801\":2,\"1804\":1,\"1805\":3,\"1845\":1,\"1847\":1,\"1848\":1,\"1850\":3,\"1851\":2,\"1852\":4,\"1867\":1,\"1877\":2,\"1878\":1,\"1931\":1,\"2002\":2,\"2095\":3,\"2243\":2,\"2244\":2,\"2255\":2,\"2257\":2,\"2261\":2,\"2263\":3,\"2264\":2,\"2271\":1,\"2325\":1}}],[\"34\",{\"1\":{\"17\":1,\"51\":1,\"87\":3,\"130\":1,\"1385\":2,\"1387\":2,\"1418\":1}}],[\"35\",{\"1\":{\"17\":1}}],[\"3\",{\"0\":{\"136\":1,\"173\":1,\"186\":1,\"240\":1,\"2401\":1,\"2419\":1,\"2431\":1,\"2433\":1,\"2456\":1,\"2461\":1,\"2475\":1,\"2476\":1,\"2497\":1,\"2537\":1,\"2559\":1,\"2568\":1,\"2639\":1,\"2644\":1},\"1\":{\"3\":1,\"8\":1,\"19\":2,\"21\":3,\"29\":1,\"31\":1,\"39\":3,\"58\":1,\"63\":2,\"71\":1,\"98\":1,\"115\":1,\"119\":2,\"132\":1,\"135\":3,\"143\":2,\"149\":1,\"150\":7,\"171\":1,\"172\":1,\"175\":1,\"186\":1,\"200\":1,\"217\":1,\"235\":1,\"237\":1,\"238\":2,\"240\":5,\"242\":1,\"694\":1,\"700\":1,\"716\":1,\"730\":2,\"756\":3,\"765\":1,\"794\":7,\"798\":1,\"813\":1,\"900\":4,\"902\":4,\"904\":7,\"921\":1,\"928\":2,\"987\":1,\"1025\":2,\"1048\":2,\"1081\":2,\"1115\":2,\"1138\":1,\"1139\":1,\"1148\":1,\"1149\":1,\"1158\":3,\"1169\":1,\"1170\":1,\"1203\":1,\"1239\":3,\"1269\":4,\"1375\":1,\"1377\":2,\"1391\":1,\"1400\":1,\"1410\":1,\"1412\":1,\"1418\":2,\"1506\":2,\"1508\":2,\"1515\":1,\"1522\":3,\"1523\":2,\"1524\":2,\"1525\":2,\"1528\":1,\"1529\":1,\"1534\":2,\"1539\":1,\"1543\":2,\"1545\":1,\"1605\":2,\"1611\":3,\"1626\":2,\"1631\":2,\"1654\":2,\"1655\":2,\"1658\":2,\"1659\":2,\"1671\":1,\"1698\":1,\"1704\":1,\"1707\":1,\"1712\":1,\"1713\":2,\"1714\":1,\"1715\":2,\"1719\":2,\"1759\":1,\"1761\":42,\"1763\":42,\"1765\":4,\"1771\":1,\"1776\":1,\"1777\":1,\"1778\":12,\"1787\":1,\"1788\":2,\"1791\":3,\"1798\":1,\"1800\":5,\"1801\":7,\"1803\":4,\"1804\":4,\"1805\":53,\"1844\":4,\"1845\":6,\"1846\":2,\"1847\":8,\"1848\":6,\"1849\":1,\"1850\":14,\"1851\":8,\"1852\":14,\"1856\":4,\"1857\":2,\"1858\":2,\"1861\":1,\"1862\":2,\"1866\":2,\"1867\":1,\"1868\":2,\"1870\":1,\"1874\":1,\"1877\":13,\"1878\":6,\"1880\":2,\"1905\":2,\"1947\":1,\"1985\":1,\"2002\":1,\"2026\":1,\"2030\":1,\"2054\":1,\"2070\":3,\"2074\":2,\"2075\":2,\"2079\":1,\"2086\":3,\"2087\":4,\"2090\":1,\"2095\":3,\"2151\":1,\"2154\":1,\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2200\":1,\"2243\":2,\"2244\":4,\"2255\":4,\"2257\":1,\"2261\":1,\"2263\":3,\"2264\":2,\"2265\":1,\"2275\":1,\"2279\":3,\"2292\":1,\"2314\":1,\"2323\":1,\"2358\":1,\"2362\":1,\"2372\":1,\"2373\":3,\"2378\":1,\"2384\":1,\"2387\":1,\"2394\":2,\"2398\":3,\"2400\":2,\"2401\":1,\"2409\":1,\"2428\":1,\"2430\":3,\"2433\":3,\"2437\":1,\"2440\":3,\"2441\":1,\"2442\":1,\"2444\":1,\"2446\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2455\":4,\"2481\":3,\"2497\":1,\"2498\":7,\"2504\":1,\"2517\":1,\"2520\":1,\"2530\":2,\"2534\":2,\"2536\":2,\"2537\":1,\"2551\":1,\"2555\":3,\"2558\":2,\"2559\":2,\"2563\":1,\"2568\":2,\"2569\":1,\"2572\":4,\"2579\":1,\"2584\":4,\"2616\":7,\"2618\":3,\"2634\":7,\"2638\":1}}],[\"2|0\",{\"1\":{\"2572\":2}}],[\"2|1\",{\"1\":{\"2564\":1}}],[\"2|7\",{\"1\":{\"2564\":2}}],[\"2|\",{\"1\":{\"2441\":3}}],[\"2|38\",{\"1\":{\"2441\":1}}],[\"2|4\",{\"1\":{\"2440\":1}}],[\"2|8\",{\"1\":{\"2440\":1}}],[\"2mix\",{\"0\":{\"2372\":1,\"2496\":1,\"2614\":1,\"2632\":1},\"1\":{\"2371\":4,\"2494\":1,\"2612\":4,\"2630\":5,\"2638\":1,\"2640\":1}}],[\"2best\",{\"1\":{\"2193\":2}}],[\"2e6\",{\"1\":{\"2154\":1}}],[\"2epoch\",{\"1\":{\"87\":2}}],[\"2695\",{\"1\":{\"2446\":1}}],[\"267\",{\"1\":{\"1870\":1}}],[\"26\",{\"1\":{\"1696\":1,\"1698\":1,\"2441\":2}}],[\"2columns\",{\"0\":{\"1420\":1},\"1\":{\"1420\":1,\"1421\":1}}],[\"2ch\",{\"1\":{\"48\":1}}],[\"2nd\",{\"1\":{\"1010\":1}}],[\"2nodes\",{\"1\":{\"94\":1}}],[\"2^\",{\"1\":{\"835\":1}}],[\"27\",{\"1\":{\"239\":1,\"2442\":1}}],[\"278\",{\"1\":{\"87\":1}}],[\"288\",{\"1\":{\"1047\":1}}],[\"2816\",{\"1\":{\"771\":1,\"772\":1,\"809\":1,\"810\":1,\"1148\":1,\"1203\":1,\"2054\":1}}],[\"28\",{\"1\":{\"195\":1}}],[\"280e\",{\"1\":{\"87\":1}}],[\"2911179\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"29430212\",{\"1\":{\"1515\":1}}],[\"29\",{\"1\":{\"176\":1}}],[\"2985\",{\"1\":{\"17\":1}}],[\"2pass\",{\"1\":{\"140\":1,\"157\":1,\"244\":1,\"2474\":1,\"2649\":1}}],[\"2+\",{\"1\":{\"132\":1,\"899\":1,\"901\":1,\"1133\":2,\"1214\":1,\"1273\":1,\"2001\":1}}],[\"2111\",{\"1\":{\"2467\":1}}],[\"2110\",{\"1\":{\"130\":1,\"2040\":1}}],[\"21\",{\"1\":{\"1398\":1,\"1761\":4,\"1763\":4,\"1805\":4,\"2446\":1}}],[\"2107\",{\"1\":{\"1976\":1,\"2003\":1}}],[\"2108\",{\"1\":{\"1084\":1,\"1829\":1}}],[\"2106\",{\"1\":{\"770\":2,\"1371\":1,\"1932\":1,\"2372\":4,\"2497\":4,\"2614\":4,\"2632\":4}}],[\"21437\",{\"1\":{\"130\":2}}],[\"225\",{\"1\":{\"2500\":2,\"2617\":2,\"2635\":2}}],[\"2210\",{\"1\":{\"1056\":1}}],[\"2211\",{\"1\":{\"130\":2,\"1138\":1,\"1210\":1,\"1211\":1,\"1286\":2,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":2,\"1337\":2,\"1660\":1}}],[\"2222\",{\"1\":{\"1015\":2}}],[\"22\",{\"1\":{\"295\":12,\"2441\":2,\"2442\":1,\"2444\":2,\"2445\":1}}],[\"2203\",{\"1\":{\"1850\":1}}],[\"2206\",{\"1\":{\"1767\":1,\"1793\":1}}],[\"2209\",{\"1\":{\"1066\":1,\"1660\":1}}],[\"2201\",{\"1\":{\"1048\":1}}],[\"2202\",{\"1\":{\"758\":1,\"2078\":2,\"2081\":1,\"2083\":1,\"2095\":1}}],[\"22050\",{\"1\":{\"217\":1,\"1763\":1,\"1776\":1,\"1778\":2,\"1786\":1,\"1801\":1,\"1804\":1,\"1805\":2,\"1850\":2,\"1852\":2,\"1859\":1,\"1877\":2,\"2084\":1,\"2089\":1,\"2236\":1,\"2241\":1,\"2266\":1,\"2271\":1}}],[\"2207\",{\"1\":{\"130\":1,\"1049\":1,\"2260\":1,\"2467\":1,\"2646\":1}}],[\"2d\",{\"0\":{\"1688\":1,\"1693\":1,\"1729\":1,\"1730\":1,\"1755\":1,\"1756\":1},\"1\":{\"115\":1,\"684\":1,\"691\":1,\"706\":2,\"717\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"734\":1,\"773\":1,\"817\":1,\"1145\":1,\"1166\":1,\"1230\":1,\"1390\":1,\"1405\":1,\"1426\":1,\"1517\":1,\"1564\":1,\"1688\":2,\"1693\":3,\"1729\":1,\"1730\":1,\"1755\":3,\"1756\":2,\"1959\":1}}],[\"251\",{\"1\":{\"1132\":1}}],[\"25000\",{\"1\":{\"1057\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1}}],[\"257\",{\"1\":{\"730\":1,\"1525\":1}}],[\"25\",{\"1\":{\"115\":6,\"121\":1,\"233\":1,\"689\":1,\"1047\":1,\"1093\":1,\"1244\":1,\"1761\":1,\"1763\":1,\"1805\":1,\"2188\":1,\"2196\":1,\"2295\":1,\"2296\":1,\"2387\":1,\"2430\":1,\"2433\":1,\"2441\":1,\"2442\":1,\"2555\":1}}],[\"255m\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"255\",{\"1\":{\"49\":1,\"2500\":4,\"2617\":4,\"2635\":4}}],[\"2565\",{\"1\":{\"2445\":1}}],[\"256\",{\"1\":{\"21\":4,\"79\":1,\"115\":4,\"116\":6,\"117\":1,\"295\":15,\"1064\":1,\"1073\":2,\"1083\":1,\"1132\":1,\"1140\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1169\":1,\"1178\":1,\"1180\":1,\"1198\":1,\"1200\":1,\"1203\":1,\"1269\":1,\"1272\":1,\"1516\":4,\"1522\":1,\"1523\":1,\"1539\":1,\"1659\":1,\"1671\":2,\"1761\":18,\"1763\":19,\"1776\":1,\"1777\":1,\"1778\":1,\"1786\":2,\"1801\":1,\"1804\":1,\"1805\":19,\"1850\":2,\"1851\":1,\"1852\":1,\"1859\":1,\"1877\":1,\"1960\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"2026\":1,\"2029\":1,\"2054\":1,\"2086\":2,\"2087\":2,\"2095\":1,\"2196\":1,\"2211\":1,\"2236\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2255\":1,\"2260\":4,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":3,\"2266\":1,\"2440\":1,\"2558\":1,\"2564\":1,\"2584\":1}}],[\"2gpus\",{\"0\":{\"36\":1,\"39\":1,\"40\":1,\"42\":1},\"1\":{\"94\":1}}],[\"2hosts\",{\"0\":{\"39\":1,\"40\":1,\"42\":1}}],[\"2host\",{\"0\":{\"36\":1}}],[\"2444\",{\"1\":{\"700\":1,\"1138\":1,\"1139\":1}}],[\"24k\",{\"1\":{\"295\":11}}],[\"24000\",{\"1\":{\"224\":1,\"231\":1,\"1785\":1,\"1791\":1}}],[\"24\",{\"1\":{\"25\":1,\"509\":1,\"512\":1,\"525\":1,\"585\":1,\"1179\":1,\"2266\":1,\"2431\":1,\"2432\":2,\"2441\":1,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"2x\",{\"1\":{\"21\":1,\"115\":1,\"1452\":1}}],[\"2305\",{\"1\":{\"1075\":1}}],[\"2309\",{\"1\":{\"130\":1}}],[\"231\",{\"1\":{\"110\":1}}],[\"23\",{\"1\":{\"17\":1,\"2384\":1,\"2431\":1,\"2432\":1,\"2441\":2,\"2442\":2}}],[\"2079\",{\"1\":{\"1082\":1}}],[\"2015\",{\"1\":{\"2446\":1}}],[\"2014\",{\"1\":{\"1712\":1,\"1715\":1}}],[\"2016\",{\"1\":{\"1528\":2,\"1529\":1,\"1568\":1}}],[\"2011\",{\"1\":{\"1466\":1}}],[\"2010\",{\"1\":{\"885\":1,\"1198\":1,\"1706\":1,\"1707\":1,\"1842\":1,\"1917\":1,\"1986\":1}}],[\"2010u\",{\"1\":{\"197\":1}}],[\"2017\",{\"1\":{\"729\":1,\"1515\":1,\"1524\":1,\"1547\":1}}],[\"2018\",{\"1\":{\"130\":3,\"940\":1,\"1572\":1,\"1712\":1,\"1715\":1,\"2070\":1}}],[\"2012\",{\"1\":{\"119\":1}}],[\"2019\",{\"1\":{\"84\":1,\"176\":2,\"195\":1,\"233\":1,\"705\":1,\"1696\":2,\"1698\":2,\"2030\":1,\"2387\":1}}],[\"2048\",{\"1\":{\"116\":2,\"295\":8,\"1065\":1,\"1066\":1,\"1079\":1,\"1115\":2,\"1140\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1169\":2,\"1196\":1,\"1197\":1,\"1203\":1,\"1204\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1870\":1,\"2001\":1,\"2004\":1,\"2026\":1,\"2029\":1,\"2054\":1,\"2266\":2,\"2461\":1,\"2584\":2}}],[\"20ms\",{\"1\":{\"102\":1}}],[\"20batch\",{\"1\":{\"87\":1}}],[\"203\",{\"1\":{\"87\":1}}],[\"2004\",{\"1\":{\"1705\":1,\"1715\":2}}],[\"2007\",{\"1\":{\"1704\":1}}],[\"2003\",{\"1\":{\"1645\":1}}],[\"2008\",{\"1\":{\"1566\":1}}],[\"2000\",{\"1\":{\"1410\":1,\"1412\":1,\"2565\":2}}],[\"200000\",{\"1\":{\"78\":1}}],[\"2005\",{\"1\":{\"1371\":1}}],[\"2002\",{\"1\":{\"700\":1,\"1138\":1,\"1139\":1}}],[\"2006\",{\"1\":{\"692\":1,\"693\":1,\"706\":2,\"2586\":1}}],[\"200\",{\"1\":{\"72\":1,\"2260\":2,\"2440\":1,\"2585\":1}}],[\"202301\",{\"1\":{\"2442\":1}}],[\"202301`\",{\"1\":{\"2441\":1}}],[\"2023`\",{\"1\":{\"2441\":1}}],[\"2023spring\",{\"1\":{\"2409\":1}}],[\"2023\",{\"0\":{\"2380\":1,\"2388\":1,\"2406\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1},\"1\":{\"130\":1,\"152\":2,\"1462\":3,\"1463\":3,\"1661\":2,\"1662\":2,\"1670\":1,\"1671\":1,\"2384\":1,\"2441\":3,\"2442\":3}}],[\"2022fall\",{\"1\":{\"2394\":1,\"2529\":1,\"2530\":1}}],[\"2022\",{\"0\":{\"2421\":1,\"2524\":1,\"2544\":1},\"1\":{\"113\":1,\"130\":7,\"1604\":1,\"1655\":1,\"1660\":2,\"1719\":1,\"2054\":1,\"2055\":1,\"2064\":1,\"2380\":2,\"2388\":3,\"2421\":1,\"2429\":1,\"2438\":1,\"2524\":2,\"2544\":1,\"2554\":1,\"2564\":1,\"2583\":1}}],[\"2021\",{\"0\":{\"2354\":1},\"1\":{\"23\":1,\"24\":1,\"113\":1,\"119\":2,\"120\":1,\"130\":2,\"140\":1,\"156\":1,\"244\":1,\"1466\":1,\"1529\":2,\"1568\":2,\"2372\":1,\"2380\":1,\"2388\":1,\"2421\":1,\"2524\":1,\"2544\":1}}],[\"2020\",{\"1\":{\"23\":3,\"87\":3,\"119\":2,\"130\":4,\"1132\":1,\"1523\":1,\"1566\":1,\"1581\":1,\"1645\":1,\"2032\":1,\"2049\":1,\"2387\":1}}],[\"20\",{\"1\":{\"17\":1,\"72\":2,\"76\":2,\"115\":2,\"133\":1,\"760\":1,\"831\":1,\"932\":1,\"941\":1,\"948\":1,\"960\":1,\"1067\":2,\"1096\":1,\"1106\":1,\"1108\":1,\"1207\":1,\"1257\":1,\"1398\":1,\"1534\":1,\"1539\":1,\"1645\":2,\"1654\":1,\"1671\":1,\"1761\":4,\"1763\":4,\"1805\":4,\"1906\":1,\"1920\":1,\"1949\":1,\"2000\":1,\"2188\":1,\"2255\":1,\"2260\":2,\"2375\":1,\"2445\":1,\"2559\":1}}],[\"2\",{\"0\":{\"2\":1,\"135\":1,\"172\":1,\"180\":1,\"222\":1,\"239\":1,\"2400\":1,\"2418\":1,\"2455\":1,\"2457\":1,\"2459\":1,\"2460\":1,\"2472\":1,\"2473\":1,\"2474\":1,\"2490\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2536\":1,\"2543\":1,\"2556\":1,\"2565\":1,\"2567\":1,\"2638\":1,\"2643\":1,\"2646\":1},\"1\":{\"2\":1,\"3\":3,\"17\":1,\"18\":2,\"19\":1,\"21\":3,\"22\":2,\"28\":1,\"29\":1,\"36\":4,\"39\":1,\"40\":2,\"42\":2,\"49\":1,\"58\":3,\"71\":2,\"72\":2,\"80\":1,\"87\":1,\"90\":2,\"91\":1,\"94\":2,\"98\":1,\"115\":2,\"116\":2,\"118\":1,\"119\":4,\"133\":1,\"143\":2,\"144\":1,\"148\":1,\"150\":1,\"168\":2,\"180\":2,\"185\":1,\"194\":1,\"217\":1,\"235\":1,\"239\":2,\"240\":2,\"241\":2,\"249\":1,\"251\":1,\"629\":1,\"690\":1,\"694\":1,\"700\":3,\"704\":1,\"705\":1,\"712\":4,\"714\":2,\"715\":3,\"716\":2,\"718\":1,\"719\":3,\"727\":1,\"728\":1,\"741\":3,\"744\":11,\"756\":1,\"760\":1,\"765\":1,\"776\":3,\"778\":1,\"794\":10,\"798\":1,\"799\":1,\"808\":1,\"809\":2,\"813\":1,\"825\":2,\"876\":14,\"878\":3,\"899\":2,\"900\":4,\"901\":2,\"902\":4,\"904\":7,\"906\":1,\"907\":1,\"944\":1,\"950\":1,\"959\":2,\"968\":1,\"987\":1,\"1011\":3,\"1015\":1,\"1025\":1,\"1048\":3,\"1049\":7,\"1050\":7,\"1051\":1,\"1052\":9,\"1054\":1,\"1055\":1,\"1056\":7,\"1057\":1,\"1059\":1,\"1061\":1,\"1067\":1,\"1068\":4,\"1069\":1,\"1076\":19,\"1077\":1,\"1079\":2,\"1096\":1,\"1107\":2,\"1108\":1,\"1115\":10,\"1116\":1,\"1133\":4,\"1138\":3,\"1139\":3,\"1144\":1,\"1153\":2,\"1158\":1,\"1211\":2,\"1214\":2,\"1222\":2,\"1224\":1,\"1228\":1,\"1239\":1,\"1257\":2,\"1269\":8,\"1273\":2,\"1286\":1,\"1287\":1,\"1336\":2,\"1337\":1,\"1345\":1,\"1347\":1,\"1348\":1,\"1374\":1,\"1400\":1,\"1408\":1,\"1418\":2,\"1420\":1,\"1423\":4,\"1424\":1,\"1455\":1,\"1462\":3,\"1463\":2,\"1464\":1,\"1505\":1,\"1506\":1,\"1515\":2,\"1516\":3,\"1517\":1,\"1522\":3,\"1523\":5,\"1524\":1,\"1528\":2,\"1529\":3,\"1534\":1,\"1539\":1,\"1545\":1,\"1551\":1,\"1554\":1,\"1568\":1,\"1594\":2,\"1604\":1,\"1605\":5,\"1611\":1,\"1618\":1,\"1619\":2,\"1626\":1,\"1645\":2,\"1654\":1,\"1658\":1,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1665\":1,\"1669\":1,\"1671\":3,\"1688\":1,\"1693\":1,\"1710\":1,\"1712\":1,\"1713\":2,\"1715\":2,\"1719\":1,\"1735\":1,\"1741\":1,\"1755\":1,\"1756\":1,\"1761\":14,\"1763\":18,\"1765\":3,\"1766\":8,\"1771\":4,\"1772\":1,\"1778\":9,\"1785\":1,\"1786\":8,\"1787\":2,\"1788\":3,\"1791\":1,\"1798\":1,\"1800\":4,\"1801\":9,\"1803\":3,\"1804\":5,\"1805\":23,\"1844\":2,\"1845\":1,\"1846\":4,\"1847\":5,\"1849\":2,\"1850\":13,\"1851\":7,\"1852\":12,\"1856\":1,\"1857\":3,\"1858\":2,\"1859\":1,\"1861\":3,\"1862\":1,\"1867\":1,\"1870\":3,\"1871\":13,\"1872\":1,\"1873\":2,\"1874\":1,\"1877\":9,\"1878\":3,\"1880\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":3,\"1926\":1,\"1940\":1,\"1958\":1,\"1960\":1,\"2001\":2,\"2002\":2,\"2003\":5,\"2032\":2,\"2040\":1,\"2041\":1,\"2070\":2,\"2087\":1,\"2090\":1,\"2095\":3,\"2103\":1,\"2104\":1,\"2118\":1,\"2149\":1,\"2154\":1,\"2181\":1,\"2193\":1,\"2196\":1,\"2243\":2,\"2244\":5,\"2255\":4,\"2257\":1,\"2261\":1,\"2263\":3,\"2264\":4,\"2265\":2,\"2267\":1,\"2275\":1,\"2279\":3,\"2290\":1,\"2294\":1,\"2304\":1,\"2314\":1,\"2325\":1,\"2330\":1,\"2364\":2,\"2372\":4,\"2373\":3,\"2387\":1,\"2394\":1,\"2397\":1,\"2398\":1,\"2400\":2,\"2401\":1,\"2413\":1,\"2424\":1,\"2430\":3,\"2439\":1,\"2440\":2,\"2441\":1,\"2442\":1,\"2444\":4,\"2445\":2,\"2446\":2,\"2452\":1,\"2474\":1,\"2481\":5,\"2491\":1,\"2498\":10,\"2499\":1,\"2500\":8,\"2504\":1,\"2507\":2,\"2510\":4,\"2513\":2,\"2530\":1,\"2533\":1,\"2534\":1,\"2536\":2,\"2537\":1,\"2543\":1,\"2547\":1,\"2555\":3,\"2556\":1,\"2558\":1,\"2564\":1,\"2568\":2,\"2572\":1,\"2574\":1,\"2584\":6,\"2585\":1,\"2616\":11,\"2617\":8,\"2618\":6,\"2634\":11,\"2635\":8,\"2638\":2,\"2654\":2,\"2658\":2}}],[\"p=pyaudio\",{\"1\":{\"2596\":1}}],[\"pd\",{\"1\":{\"2359\":2,\"2456\":2,\"2460\":2,\"2521\":2,\"2580\":2}}],[\"pdf`\",{\"1\":{\"2078\":1}}],[\"pdf\",{\"1\":{\"130\":1,\"668\":3,\"681\":2,\"682\":2,\"700\":6,\"770\":4,\"940\":1,\"950\":2,\"968\":2,\"1037\":2,\"1048\":2,\"1049\":2,\"1056\":2,\"1066\":2,\"1072\":2,\"1075\":2,\"1080\":2,\"1138\":6,\"1139\":6,\"1210\":1,\"1211\":1,\"1286\":3,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":3,\"1337\":3,\"1371\":6,\"1400\":1,\"1523\":1,\"1524\":1,\"1782\":2,\"1793\":2,\"1932\":2,\"2003\":2,\"2019\":2,\"2032\":1,\"2040\":3,\"2078\":3,\"2095\":2,\"2260\":2,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"pdfs\",{\"1\":{\"130\":1,\"2032\":1}}],[\"ps\",{\"1\":{\"1879\":2,\"2091\":2,\"2245\":2,\"2256\":2,\"2280\":2}}],[\"pseudo\",{\"0\":{\"2304\":1},\"1\":{\"1860\":1,\"2294\":2,\"2304\":3}}],[\"psd\",{\"1\":{\"690\":2,\"885\":4,\"887\":2,\"962\":1,\"1455\":2,\"1524\":6,\"1680\":3,\"1696\":2,\"1697\":2,\"1698\":7,\"1704\":7,\"1705\":3,\"1706\":5,\"1707\":7,\"1708\":5,\"1711\":2,\"1712\":9,\"1713\":7,\"1714\":2,\"1715\":9,\"1739\":6}}],[\"pqmf\",{\"0\":{\"1860\":2,\"1883\":1},\"1\":{\"1761\":2,\"1763\":2,\"1767\":1,\"1778\":6,\"1805\":2,\"1852\":6,\"1860\":8,\"1870\":3,\"1883\":2}}],[\"pc\",{\"1\":{\"1646\":2,\"1647\":1}}],[\"pcm16\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"pcm\",{\"0\":{\"279\":1,\"525\":1},\"1\":{\"46\":1,\"48\":1,\"49\":3,\"52\":2,\"279\":6,\"525\":3,\"991\":1,\"994\":1,\"1015\":1,\"1927\":6,\"2521\":1}}],[\"p1\",{\"1\":{\"1331\":1}}],[\"pw\",{\"1\":{\"859\":4,\"862\":4,\"1771\":4}}],[\"pwd\",{\"1\":{\"38\":1,\"40\":1,\"41\":1,\"42\":1}}],[\"pp^\",{\"1\":{\"1248\":1}}],[\"pp\",{\"1\":{\"705\":1,\"1696\":1,\"1698\":1}}],[\"ppp\",{\"1\":{\"45\":2}}],[\"p808\",{\"1\":{\"363\":2,\"1526\":1}}],[\"phrase\",{\"1\":{\"2573\":1}}],[\"phrasing\",{\"1\":{\"2473\":1}}],[\"phi\",{\"0\":{\"1031\":1},\"1\":{\"1031\":3,\"1696\":4,\"1697\":2}}],[\"phonemize\",{\"1\":{\"2131\":1}}],[\"phonemizer\",{\"0\":{\"2131\":1},\"1\":{\"2131\":8}}],[\"phonemization\",{\"1\":{\"2125\":2}}],[\"phonemetokenizer\",{\"0\":{\"2130\":1},\"1\":{\"2130\":1}}],[\"phonemepredictor\",{\"0\":{\"1787\":1},\"1\":{\"1787\":2}}],[\"phonemes\",{\"1\":{\"826\":1,\"2090\":1}}],[\"phoneme\",{\"0\":{\"1787\":1,\"2121\":1,\"2122\":1,\"2125\":1,\"2126\":1,\"2130\":1,\"2131\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2303\":1},\"1\":{\"774\":2,\"1787\":3,\"1804\":3,\"1805\":4,\"1986\":2,\"2121\":2,\"2122\":2,\"2125\":2,\"2126\":2,\"2130\":1,\"2131\":2,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":3,\"2143\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2260\":1,\"2303\":2}}],[\"phones\",{\"1\":{\"1523\":1,\"1773\":3,\"2082\":3}}],[\"phone\",{\"0\":{\"2145\":1,\"2146\":1},\"1\":{\"461\":2,\"1423\":3,\"1773\":6,\"1798\":4,\"1927\":1,\"2082\":5,\"2086\":1,\"2087\":1,\"2131\":1,\"2145\":1,\"2146\":1,\"2523\":1}}],[\"phase\",{\"0\":{\"1717\":1,\"1929\":1},\"1\":{\"242\":1,\"1717\":4,\"1929\":5,\"1947\":1,\"2046\":1}}],[\"phn\",{\"1\":{\"210\":3,\"211\":3,\"212\":3,\"217\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"295\":4,\"461\":1,\"485\":1,\"579\":1,\"1773\":24,\"1778\":2,\"1805\":2,\"2082\":26,\"2087\":3,\"2090\":3,\"2095\":3,\"2196\":1,\"2414\":1,\"2507\":2,\"2510\":4,\"2513\":2}}],[\"png\",{\"1\":{\"240\":13,\"754\":1,\"820\":1,\"821\":1,\"826\":1,\"909\":2,\"2375\":2,\"2415\":1,\"2416\":1,\"2440\":3,\"2558\":2,\"2564\":3,\"2572\":2}}],[\"p9t\",{\"1\":{\"222\":1}}],[\"pkl\",{\"1\":{\"214\":1,\"215\":1,\"216\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"282\":1,\"2520\":1}}],[\"pt\",{\"0\":{\"2432\":1},\"1\":{\"174\":1,\"295\":2,\"1179\":1,\"2431\":3,\"2432\":3,\"2441\":4,\"2584\":1}}],[\"pth\",{\"1\":{\"65\":1,\"66\":6,\"1969\":2,\"2158\":5,\"2368\":1,\"2431\":1,\"2455\":1,\"2460\":1,\"2461\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2486\":1,\"2490\":1,\"2492\":1,\"2494\":1,\"2500\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":1,\"2584\":3,\"2585\":6,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2628\":1,\"2648\":1,\"2649\":1}}],[\"pbs\",{\"1\":{\"141\":1,\"142\":3,\"143\":1}}],[\"pueden\",{\"1\":{\"2457\":1}}],[\"puede\",{\"1\":{\"2457\":1}}],[\"push\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"pushing\",{\"1\":{\"2055\":1}}],[\"punctuation\",{\"1\":{\"2358\":1,\"2520\":1,\"2579\":1}}],[\"punc\",{\"1\":{\"2126\":1}}],[\"punkt\",{\"1\":{\"217\":1}}],[\"pulsegen\",{\"1\":{\"1797\":1}}],[\"pulse\",{\"1\":{\"1797\":2}}],[\"pulse=false\",{\"1\":{\"1797\":2}}],[\"pulls\",{\"1\":{\"2446\":1}}],[\"pull\",{\"1\":{\"771\":1,\"772\":1,\"809\":1,\"810\":1,\"1047\":1,\"1076\":1,\"1148\":1,\"1203\":1,\"2054\":1,\"2446\":1,\"2575\":1}}],[\"pubmed\",{\"1\":{\"1515\":1}}],[\"publication\",{\"1\":{\"2363\":3,\"2506\":3,\"2653\":3}}],[\"public\",{\"1\":{\"2019\":1,\"2567\":1}}],[\"publicly\",{\"1\":{\"130\":1}}],[\"publisher\",{\"1\":{\"130\":1}}],[\"put\",{\"0\":{\"1326\":1},\"1\":{\"135\":2,\"167\":1,\"198\":1,\"1326\":1,\"2639\":1}}],[\"pure\",{\"1\":{\"150\":1}}],[\"purely\",{\"1\":{\"112\":1}}],[\"purposes\",{\"1\":{\"56\":1,\"1251\":1}}],[\"purpose\",{\"1\":{\"5\":1,\"17\":1,\"59\":1,\"2415\":1,\"2419\":1,\"2450\":1,\"2482\":1}}],[\"picture\",{\"1\":{\"2452\":1,\"2468\":1}}],[\"pickalable\",{\"1\":{\"2121\":1,\"2122\":1}}],[\"pickable\",{\"1\":{\"999\":1}}],[\"pi\",{\"1\":{\"1797\":1}}],[\"pixels\",{\"1\":{\"1011\":1,\"1693\":2,\"1755\":2}}],[\"pixel\",{\"1\":{\"1011\":5}}],[\"pil\",{\"1\":{\"950\":1,\"956\":1,\"968\":2,\"973\":2}}],[\"pitcture\",{\"1\":{\"2452\":1,\"2473\":1}}],[\"pitch\",{\"0\":{\"1771\":1,\"1813\":1,\"1941\":1},\"1\":{\"171\":1,\"175\":1,\"294\":1,\"952\":1,\"1771\":2,\"1773\":17,\"1776\":1,\"1778\":5,\"1798\":1,\"1804\":4,\"1805\":7,\"1813\":1,\"1817\":1,\"1822\":1,\"1837\":14,\"1850\":10,\"1851\":32,\"1852\":7,\"1879\":3,\"1941\":6,\"1946\":1,\"1947\":1,\"2082\":17,\"2086\":7,\"2087\":7,\"2090\":10,\"2091\":1,\"2095\":6,\"2240\":17,\"2243\":1,\"2244\":35,\"2245\":3,\"2255\":33,\"2256\":3,\"2278\":17,\"2279\":33,\"2280\":3}}],[\"pits\",{\"0\":{\"1806\":1,\"1808\":1},\"1\":{\"1806\":2,\"1808\":1}}],[\"pitsolver\",{\"0\":{\"1622\":1},\"1\":{\"1622\":2}}],[\"pitloss\",{\"1\":{\"1218\":1}}],[\"pitlosswrapper\",{\"0\":{\"1218\":1},\"1\":{\"1218\":1}}],[\"pit\",{\"0\":{\"794\":1,\"1218\":1,\"1603\":1,\"1622\":1},\"1\":{\"794\":8,\"1218\":2,\"1371\":2,\"1552\":2,\"1600\":1,\"1603\":4,\"1622\":3}}],[\"pinyin\",{\"1\":{\"231\":2,\"295\":2}}],[\"pin\",{\"1\":{\"174\":2,\"1071\":1,\"1895\":1,\"1896\":1,\"1900\":1}}],[\"piecewiselinearwarmuplr\",{\"0\":{\"2020\":1},\"1\":{\"2020\":2}}],[\"piecewise\",{\"0\":{\"1886\":1,\"2020\":1},\"1\":{\"1886\":1,\"2020\":2}}],[\"pieces\",{\"1\":{\"80\":1}}],[\"piece\",{\"1\":{\"80\":1,\"271\":2,\"272\":2}}],[\"pipe\",{\"0\":{\"2386\":1},\"1\":{\"203\":1,\"237\":1,\"2385\":1,\"2430\":2,\"2555\":2}}],[\"pipefail\",{\"1\":{\"98\":1,\"2568\":1,\"2569\":1}}],[\"pipelines\",{\"1\":{\"2468\":1}}],[\"pipeline\",{\"1\":{\"98\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1253\":2,\"1254\":1,\"1279\":1,\"2429\":1,\"2554\":1,\"2565\":1,\"2568\":1,\"2569\":1}}],[\"pip\",{\"1\":{\"10\":2,\"17\":2,\"127\":1,\"136\":2,\"167\":1,\"178\":1,\"196\":1,\"200\":2,\"207\":2,\"220\":1,\"227\":1,\"234\":1,\"2355\":2,\"2362\":2,\"2384\":2,\"2387\":1,\"2396\":1,\"2409\":1,\"2450\":9,\"2466\":4,\"2482\":4,\"2499\":1,\"2504\":9,\"2517\":9,\"2576\":3,\"2585\":1,\"2598\":2,\"2602\":2,\"2619\":2,\"2635\":1,\"2646\":4,\"2651\":2}}],[\"p\",{\"1\":{\"53\":2,\"178\":1,\"196\":1,\"200\":1,\"202\":1,\"234\":1,\"698\":1,\"699\":1,\"734\":3,\"767\":3,\"817\":3,\"828\":3,\"1069\":2,\"1166\":1,\"1248\":2,\"1261\":1,\"1339\":3,\"1352\":2,\"1379\":2,\"1382\":1,\"1604\":1,\"1655\":1,\"1664\":2,\"1665\":2,\"1705\":1,\"1719\":1,\"1761\":1,\"1763\":1,\"1768\":1,\"1805\":1,\"1806\":1,\"1841\":2,\"1853\":6,\"1854\":4,\"1879\":2,\"1889\":3,\"2091\":2,\"2245\":2,\"2256\":2,\"2280\":2,\"2311\":1,\"2431\":1,\"2432\":1,\"2500\":4,\"2514\":3,\"2568\":2,\"2596\":1,\"2617\":4,\"2635\":4,\"2659\":3}}],[\"pleae\",{\"1\":{\"2377\":1,\"2436\":1,\"2562\":1}}],[\"please\",{\"1\":{\"14\":1,\"17\":1,\"19\":2,\"20\":1,\"33\":1,\"34\":1,\"47\":2,\"49\":1,\"83\":1,\"84\":1,\"91\":1,\"92\":1,\"95\":1,\"99\":1,\"102\":3,\"103\":1,\"104\":1,\"106\":1,\"112\":1,\"115\":1,\"124\":1,\"127\":2,\"133\":1,\"141\":1,\"146\":1,\"147\":1,\"148\":3,\"196\":1,\"198\":1,\"200\":1,\"209\":1,\"213\":1,\"234\":1,\"295\":2,\"700\":1,\"1138\":1,\"1139\":1,\"1180\":1,\"1269\":1,\"2355\":1,\"2357\":1,\"2363\":1,\"2368\":1,\"2371\":1,\"2372\":4,\"2373\":3,\"2375\":1,\"2384\":2,\"2387\":1,\"2389\":3,\"2393\":1,\"2394\":1,\"2398\":1,\"2400\":1,\"2401\":2,\"2403\":1,\"2408\":3,\"2411\":1,\"2414\":3,\"2418\":2,\"2419\":1,\"2420\":2,\"2422\":3,\"2424\":1,\"2427\":1,\"2429\":4,\"2430\":4,\"2431\":1,\"2437\":1,\"2446\":1,\"2449\":3,\"2457\":2,\"2459\":1,\"2461\":1,\"2462\":2,\"2465\":3,\"2481\":3,\"2486\":1,\"2487\":1,\"2490\":1,\"2491\":1,\"2497\":1,\"2500\":1,\"2501\":1,\"2503\":3,\"2508\":2,\"2510\":3,\"2514\":1,\"2525\":3,\"2529\":1,\"2530\":1,\"2534\":1,\"2536\":1,\"2537\":2,\"2539\":1,\"2542\":1,\"2545\":3,\"2547\":1,\"2550\":1,\"2552\":1,\"2554\":3,\"2555\":4,\"2558\":4,\"2560\":1,\"2563\":1,\"2564\":1,\"2565\":1,\"2567\":1,\"2572\":2,\"2574\":1,\"2575\":1,\"2578\":1,\"2584\":1,\"2600\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2642\":1,\"2653\":1,\"2659\":1}}],[\"plus\",{\"0\":{\"1331\":1},\"1\":{\"1331\":1,\"1339\":1,\"1791\":1,\"1804\":1}}],[\"pl|queue\",{\"1\":{\"294\":1}}],[\"pl|utils\",{\"1\":{\"275\":1,\"276\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1}}],[\"plt\",{\"1\":{\"171\":3,\"174\":5,\"175\":3,\"185\":3,\"193\":3,\"202\":2,\"238\":9,\"648\":2,\"2359\":2,\"2360\":2,\"2386\":2,\"2456\":2,\"2458\":2,\"2460\":2,\"2498\":25,\"2521\":3,\"2522\":3,\"2523\":3,\"2580\":2,\"2581\":2,\"2582\":2,\"2616\":25,\"2634\":25}}],[\"play\",{\"1\":{\"2355\":1,\"2369\":1,\"2487\":1,\"2523\":1,\"2606\":1,\"2623\":1}}],[\"plain\",{\"1\":{\"1715\":1}}],[\"planes\",{\"1\":{\"1134\":1,\"1305\":2,\"1487\":1,\"1491\":1,\"1592\":1,\"1627\":1,\"1690\":2,\"1691\":2,\"1731\":2,\"1732\":2,\"2042\":1,\"2047\":1,\"2059\":1}}],[\"planning\",{\"1\":{\"83\":1}}],[\"places\",{\"1\":{\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1}}],[\"placeholder\",{\"1\":{\"1154\":1,\"1255\":2,\"1660\":1,\"1661\":1,\"1662\":1}}],[\"placed\",{\"1\":{\"169\":1,\"181\":1}}],[\"place\",{\"1\":{\"112\":1,\"134\":1,\"135\":1,\"926\":1,\"1048\":1}}],[\"platform\",{\"1\":{\"99\":1}}],[\"platforms\",{\"1\":{\"99\":1}}],[\"plots\",{\"1\":{\"2440\":1,\"2558\":1}}],[\"plotfn\",{\"1\":{\"799\":1}}],[\"plotattentionreporter\",{\"1\":{\"996\":2}}],[\"plotattentionreport\",{\"0\":{\"629\":1,\"799\":1},\"1\":{\"629\":2,\"799\":2}}],[\"plot\",{\"0\":{\"648\":1,\"799\":1,\"909\":2,\"916\":1},\"1\":{\"88\":2,\"171\":1,\"174\":2,\"175\":1,\"185\":1,\"193\":1,\"629\":2,\"648\":3,\"676\":4,\"754\":6,\"781\":2,\"799\":1,\"820\":5,\"821\":3,\"826\":6,\"909\":3,\"916\":2,\"1198\":1,\"1850\":3,\"1877\":3,\"2185\":1,\"2193\":2,\"2201\":3,\"2203\":1,\"2416\":1,\"2440\":1,\"2498\":7,\"2508\":1,\"2515\":1,\"2558\":1,\"2616\":7,\"2634\":7}}],[\"pl\",{\"0\":{\"143\":1},\"1\":{\"47\":1,\"142\":5,\"143\":5,\"144\":2,\"275\":1,\"276\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"294\":1,\"297\":1,\"2568\":1}}],[\"p2\",{\"1\":{\"44\":1,\"1331\":1}}],[\"people\",{\"1\":{\"2400\":1,\"2401\":1,\"2523\":1,\"2536\":1,\"2537\":1}}],[\"ped\",{\"1\":{\"2367\":2,\"2485\":2,\"2604\":2,\"2621\":2}}],[\"peak\",{\"1\":{\"778\":1,\"1921\":2}}],[\"pesq\",{\"0\":{\"367\":1},\"1\":{\"363\":2,\"528\":2}}],[\"penalization\",{\"1\":{\"700\":1}}],[\"penalty=1\",{\"1\":{\"611\":1}}],[\"penalty=0\",{\"1\":{\"609\":1,\"2460\":2,\"2592\":1}}],[\"penalty\",{\"0\":{\"2302\":1,\"2305\":1},\"1\":{\"249\":2,\"251\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"307\":2,\"315\":2,\"327\":2,\"384\":2,\"391\":2,\"399\":4,\"408\":2,\"422\":2,\"443\":4,\"449\":4,\"485\":2,\"1139\":1,\"2040\":2,\"2302\":3,\"2305\":2,\"2461\":2}}],[\"peng2023reproducing\",{\"1\":{\"130\":1}}],[\"peng\",{\"1\":{\"130\":2,\"2421\":1,\"2438\":1,\"2544\":1,\"2564\":1}}],[\"pe\",{\"1\":{\"144\":1,\"770\":1,\"810\":1,\"818\":1,\"1077\":1}}],[\"peter\",{\"1\":{\"130\":2}}],[\"pem\",{\"1\":{\"70\":1}}],[\"peer\",{\"1\":{\"44\":2}}],[\"perspective\",{\"1\":{\"2400\":1,\"2536\":1}}],[\"personalized\",{\"1\":{\"1526\":1}}],[\"perfect\",{\"1\":{\"1860\":1}}],[\"performed\",{\"1\":{\"122\":1,\"238\":1,\"718\":1,\"752\":1,\"756\":1,\"760\":1,\"778\":1,\"831\":1,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1134\":1,\"1136\":1,\"1143\":1,\"1151\":1,\"1156\":1,\"1158\":1,\"1174\":1,\"1184\":1,\"1188\":1,\"1194\":1,\"1207\":1,\"1212\":1,\"1215\":1,\"1220\":1,\"1222\":1,\"1226\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1245\":1,\"1249\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":1,\"1518\":1,\"1520\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1543\":1,\"1547\":1,\"1549\":1,\"1555\":1,\"1561\":1,\"1564\":1,\"1573\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1596\":1,\"1598\":1,\"1603\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1613\":1,\"1620\":1,\"1622\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1641\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1656\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1688\":2,\"1713\":1,\"1719\":1,\"1756\":2,\"1761\":1,\"1763\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1801\":1,\"1806\":1,\"1890\":1,\"1902\":1,\"1907\":1,\"1912\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1976\":1,\"1978\":1,\"1981\":1,\"1984\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1994\":1,\"1997\":1,\"2024\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2050\":1,\"2052\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2079\":1,\"2080\":1,\"2149\":1,\"2168\":1,\"2233\":1,\"2237\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2275\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1,\"2441\":1}}],[\"performs\",{\"1\":{\"82\":1,\"238\":1,\"240\":1,\"241\":1,\"795\":1,\"814\":1,\"1187\":1,\"1198\":1,\"1202\":1,\"1917\":1,\"2201\":1}}],[\"performances\",{\"1\":{\"2420\":1,\"2461\":1}}],[\"performance\",{\"1\":{\"23\":3,\"31\":1,\"44\":1,\"100\":1,\"119\":3,\"1245\":1,\"2435\":1,\"2441\":4,\"2461\":1,\"2467\":1,\"2561\":1,\"2639\":1}}],[\"perform\",{\"0\":{\"1736\":1,\"1737\":1},\"1\":{\"16\":1,\"85\":1,\"122\":1,\"235\":1,\"242\":1,\"630\":1,\"631\":1,\"691\":2,\"692\":1,\"693\":2,\"697\":3,\"698\":1,\"699\":1,\"754\":2,\"797\":4,\"826\":2,\"857\":2,\"869\":1,\"877\":1,\"906\":1,\"1028\":1,\"1048\":1,\"1069\":1,\"1205\":1,\"1219\":1,\"1344\":1,\"1346\":1,\"1736\":2,\"1737\":1,\"1746\":1,\"1766\":1,\"1778\":1,\"1787\":1,\"1800\":2,\"1805\":1,\"1844\":1,\"1850\":1,\"1852\":1,\"1857\":1,\"1862\":1,\"1871\":1,\"1877\":1,\"1964\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2201\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2372\":1,\"2424\":1,\"2430\":2,\"2547\":1,\"2555\":2,\"2569\":1,\"2584\":1,\"2585\":1}}],[\"performing\",{\"1\":{\"16\":1,\"812\":1,\"1688\":1,\"1756\":1}}],[\"periods\",{\"1\":{\"1778\":1,\"1801\":3,\"1805\":1,\"1845\":3,\"1847\":3,\"1850\":1,\"1852\":1,\"1877\":1}}],[\"period\",{\"1\":{\"1778\":2,\"1801\":4,\"1805\":3,\"1845\":3,\"1847\":7,\"1848\":4,\"1850\":2,\"1852\":2,\"1877\":2}}],[\"perms\",{\"1\":{\"2500\":3,\"2617\":3,\"2635\":3}}],[\"perm=true\",{\"1\":{\"1603\":1,\"1622\":1}}],[\"permutate\",{\"1\":{\"1218\":1}}],[\"permutations\",{\"1\":{\"794\":2,\"2500\":2,\"2617\":2,\"2635\":2}}],[\"permutationdfs\",{\"1\":{\"794\":1}}],[\"permutation\",{\"1\":{\"528\":2,\"530\":1,\"794\":5,\"1218\":1,\"1530\":1,\"1552\":1,\"1553\":1,\"1563\":1,\"1600\":2,\"1603\":6,\"1622\":5}}],[\"permute\",{\"1\":{\"794\":1,\"1218\":2,\"1371\":1}}],[\"perm\",{\"0\":{\"530\":1},\"1\":{\"530\":2,\"1218\":1,\"1552\":1,\"1553\":3,\"1603\":1,\"1622\":1,\"2500\":5,\"2617\":5,\"2635\":5}}],[\"perplexity\",{\"0\":{\"380\":1,\"616\":1},\"1\":{\"380\":3,\"499\":1,\"616\":3,\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"1956\":1}}],[\"perturbed\",{\"1\":{\"1946\":1,\"1947\":1}}],[\"perturbation\",{\"1\":{\"294\":1,\"952\":2,\"1946\":1,\"2373\":2,\"2430\":3,\"2555\":3,\"2564\":1}}],[\"perturb\",{\"0\":{\"294\":1,\"940\":1,\"948\":1,\"949\":1,\"952\":1,\"961\":1,\"1946\":1},\"1\":{\"294\":2,\"940\":1,\"948\":2,\"949\":2,\"952\":3,\"961\":1,\"1905\":2,\"1946\":2,\"2373\":1,\"2430\":1,\"2555\":1}}],[\"perl\",{\"1\":{\"142\":1}}],[\"percentage\",{\"1\":{\"148\":1}}],[\"percent\",{\"1\":{\"121\":1,\"1925\":2}}],[\"per\",{\"0\":{\"1353\":1},\"1\":{\"41\":1,\"69\":2,\"429\":4,\"506\":2,\"614\":1,\"806\":1,\"807\":1,\"933\":1,\"997\":1,\"1011\":1,\"1130\":1,\"1133\":2,\"1214\":1,\"1228\":2,\"1244\":2,\"1245\":1,\"1248\":1,\"1252\":2,\"1254\":1,\"1270\":1,\"1273\":1,\"1345\":2,\"1347\":2,\"1353\":1,\"1398\":1,\"1406\":1,\"1551\":2,\"1553\":3,\"1895\":1,\"1896\":1,\"1897\":2,\"1900\":1,\"1926\":1,\"1927\":1,\"1941\":5,\"2001\":1,\"2023\":1,\"2099\":3,\"2102\":3,\"2106\":2,\"2183\":2,\"2190\":2,\"2596\":1}}],[\"paint16\",{\"1\":{\"2596\":1}}],[\"pairwise\",{\"1\":{\"1010\":2}}],[\"pairs\",{\"1\":{\"172\":1,\"272\":2,\"725\":2,\"806\":2,\"824\":2,\"1270\":2}}],[\"pair\",{\"0\":{\"560\":1},\"1\":{\"171\":2,\"182\":1,\"185\":1,\"560\":2,\"612\":1,\"710\":1,\"1398\":1}}],[\"page\",{\"1\":{\"528\":2,\"564\":1,\"2032\":1,\"2155\":1,\"2585\":1}}],[\"pages\",{\"1\":{\"130\":1,\"2585\":1}}],[\"pages=\",{\"1\":{\"130\":6}}],[\"pause\",{\"0\":{\"2140\":1},\"1\":{\"461\":1,\"2140\":1,\"2363\":3,\"2653\":3}}],[\"paola\",{\"1\":{\"130\":1}}],[\"pavel\",{\"1\":{\"130\":1}}],[\"papers\",{\"1\":{\"940\":1,\"1371\":1,\"1523\":1}}],[\"paper\",{\"1\":{\"103\":1,\"120\":1,\"771\":1,\"809\":1,\"940\":1,\"1337\":1,\"1564\":1,\"2030\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2064\":1,\"2070\":1,\"2411\":1,\"2646\":1}}],[\"package\",{\"1\":{\"2386\":1,\"2393\":1,\"2427\":1,\"2450\":1,\"2482\":1,\"2529\":1,\"2550\":1}}],[\"packages\",{\"0\":{\"2591\":1},\"1\":{\"127\":1,\"132\":1,\"135\":1,\"136\":1,\"2155\":1,\"2384\":1,\"2429\":1,\"2431\":1,\"2450\":1,\"2552\":1,\"2598\":1}}],[\"pack\",{\"0\":{\"285\":1,\"397\":1,\"1961\":1,\"1965\":1,\"1966\":1,\"1967\":2,\"1968\":1},\"1\":{\"285\":1,\"397\":3,\"398\":11,\"1961\":2,\"1965\":1,\"1966\":1,\"1967\":4,\"1968\":2}}],[\"packed\",{\"1\":{\"99\":2}}],[\"packing\",{\"0\":{\"99\":1},\"1\":{\"91\":2,\"99\":2}}],[\"pad=\",{\"1\":{\"1753\":1}}],[\"pad=0\",{\"1\":{\"1687\":1,\"1689\":2}}],[\"pad=nan\",{\"1\":{\"1390\":1,\"1392\":1}}],[\"pad2\",{\"1\":{\"1660\":1,\"1661\":1,\"1719\":1}}],[\"pads\",{\"1\":{\"750\":1}}],[\"pad\",{\"0\":{\"899\":1,\"901\":1,\"906\":1,\"908\":1},\"1\":{\"60\":4,\"174\":3,\"676\":1,\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"731\":1,\"750\":6,\"758\":2,\"768\":2,\"774\":2,\"852\":2,\"863\":3,\"865\":3,\"899\":1,\"900\":5,\"901\":1,\"902\":5,\"905\":2,\"906\":3,\"907\":1,\"908\":4,\"920\":2,\"924\":2,\"925\":4,\"943\":1,\"950\":1,\"953\":1,\"955\":1,\"965\":1,\"967\":1,\"968\":1,\"970\":1,\"972\":1,\"1066\":2,\"1073\":2,\"1075\":2,\"1083\":2,\"1117\":2,\"1119\":1,\"1133\":4,\"1136\":2,\"1140\":2,\"1145\":10,\"1148\":2,\"1149\":6,\"1150\":6,\"1169\":2,\"1171\":8,\"1178\":2,\"1179\":4,\"1180\":2,\"1181\":4,\"1182\":2,\"1186\":2,\"1190\":5,\"1200\":2,\"1203\":2,\"1204\":4,\"1206\":8,\"1210\":2,\"1214\":4,\"1215\":2,\"1216\":2,\"1220\":2,\"1221\":1,\"1222\":1,\"1244\":4,\"1269\":4,\"1270\":2,\"1272\":2,\"1273\":4,\"1282\":1,\"1354\":1,\"1392\":6,\"1517\":1,\"1552\":8,\"1560\":1,\"1606\":1,\"1741\":3,\"1754\":4,\"1781\":2,\"1856\":4,\"1857\":4,\"1858\":5,\"1867\":4,\"1870\":2,\"1892\":4,\"1893\":4,\"2027\":2,\"2029\":2,\"2177\":2,\"2188\":3,\"2208\":2,\"2283\":1,\"2285\":1,\"2287\":2,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":2,\"2600\":1}}],[\"paddings\",{\"1\":{\"2070\":1}}],[\"padding=1\",{\"1\":{\"1691\":1,\"1732\":1}}],[\"padding=0\",{\"1\":{\"1576\":1,\"1577\":1,\"1690\":1,\"1731\":1}}],[\"padding=false\",{\"1\":{\"1501\":1,\"1512\":1,\"1629\":1}}],[\"padding=\",{\"1\":{\"1478\":1,\"1480\":2,\"1506\":1,\"1518\":1,\"1522\":3,\"1545\":3,\"1577\":1}}],[\"padding=none\",{\"1\":{\"617\":1}}],[\"padding\",{\"0\":{\"1815\":1},\"1\":{\"60\":2,\"239\":1,\"617\":2,\"726\":3,\"749\":3,\"774\":1,\"825\":1,\"852\":1,\"858\":3,\"861\":3,\"863\":1,\"865\":1,\"905\":1,\"906\":2,\"908\":1,\"920\":2,\"924\":1,\"1048\":1,\"1057\":1,\"1066\":1,\"1073\":1,\"1075\":1,\"1099\":1,\"1116\":6,\"1140\":1,\"1148\":3,\"1149\":3,\"1150\":3,\"1154\":1,\"1155\":2,\"1169\":1,\"1182\":1,\"1200\":3,\"1203\":3,\"1255\":5,\"1272\":3,\"1370\":1,\"1378\":1,\"1478\":3,\"1505\":3,\"1518\":1,\"1520\":1,\"1522\":6,\"1523\":9,\"1545\":6,\"1546\":1,\"1560\":1,\"1576\":2,\"1577\":3,\"1663\":1,\"1688\":1,\"1741\":4,\"1752\":2,\"1756\":1,\"1776\":3,\"1777\":3,\"1778\":1,\"1781\":1,\"1801\":1,\"1805\":1,\"1815\":1,\"1830\":3,\"1831\":3,\"1832\":3,\"1846\":1,\"1847\":1,\"1850\":1,\"1852\":1,\"1856\":2,\"1857\":2,\"1858\":3,\"1867\":2,\"1877\":1,\"1917\":2,\"1929\":1,\"1941\":1,\"1947\":1,\"1996\":1,\"2008\":1,\"2009\":1,\"2012\":2,\"2026\":1,\"2029\":3,\"2054\":3,\"2081\":1,\"2083\":1,\"2283\":1,\"2290\":1,\"2292\":2,\"2295\":2,\"2296\":2,\"2305\":3,\"2600\":3}}],[\"padded\",{\"1\":{\"26\":1,\"676\":4,\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"701\":2,\"702\":1,\"732\":2,\"735\":2,\"737\":2,\"742\":3,\"747\":1,\"754\":5,\"755\":1,\"758\":2,\"781\":3,\"802\":2,\"821\":5,\"822\":2,\"826\":5,\"852\":3,\"899\":2,\"901\":2,\"905\":3,\"906\":1,\"908\":1,\"920\":1,\"924\":1,\"932\":1,\"1048\":1,\"1142\":4,\"1145\":2,\"1154\":1,\"1155\":1,\"1211\":1,\"1224\":1,\"1287\":1,\"1298\":3,\"1299\":3,\"1301\":3,\"1302\":3,\"1303\":3,\"1304\":3,\"1336\":1,\"1348\":1,\"1392\":2,\"1693\":1,\"1696\":1,\"1755\":1,\"1778\":13,\"1798\":1,\"1804\":18,\"1805\":14,\"1851\":3,\"1879\":1,\"1949\":1,\"2000\":1,\"2001\":2,\"2002\":2,\"2004\":2,\"2012\":1,\"2078\":2,\"2083\":2,\"2086\":20,\"2087\":20,\"2088\":2,\"2090\":18,\"2091\":1,\"2095\":18,\"2211\":1,\"2243\":4,\"2244\":6,\"2245\":1,\"2255\":6,\"2256\":1,\"2257\":1,\"2261\":1,\"2263\":3,\"2264\":3,\"2265\":1,\"2279\":5,\"2280\":1,\"2292\":1}}],[\"pandas\",{\"1\":{\"2359\":1,\"2456\":1,\"2460\":1,\"2521\":1,\"2580\":1}}],[\"pan\",{\"1\":{\"52\":2}}],[\"pan=\",{\"1\":{\"52\":2}}],[\"pattern\",{\"1\":{\"1951\":1,\"2030\":1,\"2154\":1,\"2168\":1,\"2170\":1}}],[\"patience5\",{\"1\":{\"2494\":4}}],[\"patience3\",{\"1\":{\"2494\":4}}],[\"patience=10\",{\"1\":{\"2022\":1}}],[\"patience\",{\"1\":{\"62\":1,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"2022\":1,\"2186\":1,\"2193\":1,\"2202\":2,\"2204\":1,\"2440\":1,\"2558\":1,\"2584\":1}}],[\"patent\",{\"1\":{\"49\":1}}],[\"pathlib\",{\"1\":{\"2202\":1}}],[\"path>\",{\"1\":{\"2157\":1}}],[\"pathways\",{\"1\":{\"1522\":3,\"1523\":3}}],[\"path=0\",{\"1\":{\"1252\":1,\"1254\":1}}],[\"path=none\",{\"1\":{\"641\":1,\"1961\":1}}],[\"paths\",{\"1\":{\"85\":1,\"124\":1,\"315\":2,\"485\":2,\"564\":1,\"1327\":1,\"1428\":1,\"1664\":1,\"1665\":1,\"2344\":4,\"2347\":2,\"2492\":1,\"2628\":1}}],[\"path\",{\"0\":{\"1965\":1},\"1\":{\"15\":1,\"28\":4,\"57\":16,\"58\":7,\"59\":5,\"60\":1,\"66\":1,\"74\":4,\"75\":4,\"76\":8,\"77\":4,\"78\":4,\"79\":2,\"85\":6,\"98\":3,\"108\":1,\"113\":1,\"135\":1,\"137\":1,\"144\":2,\"150\":1,\"187\":1,\"210\":4,\"211\":4,\"212\":4,\"214\":3,\"215\":3,\"216\":3,\"217\":6,\"222\":5,\"223\":5,\"224\":6,\"229\":5,\"230\":5,\"231\":6,\"235\":2,\"237\":2,\"238\":1,\"253\":2,\"272\":1,\"282\":1,\"286\":4,\"296\":3,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"375\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"619\":4,\"634\":4,\"641\":5,\"643\":3,\"645\":3,\"653\":4,\"654\":3,\"655\":3,\"754\":1,\"820\":1,\"868\":3,\"987\":1,\"1012\":2,\"1178\":2,\"1179\":4,\"1180\":2,\"1190\":2,\"1191\":1,\"1192\":1,\"1244\":2,\"1252\":2,\"1254\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1382\":1,\"1384\":1,\"1386\":1,\"1389\":5,\"1391\":9,\"1392\":1,\"1394\":1,\"1395\":5,\"1396\":2,\"1397\":5,\"1402\":5,\"1403\":2,\"1404\":5,\"1406\":9,\"1407\":2,\"1408\":1,\"1411\":1,\"1414\":5,\"1415\":2,\"1416\":5,\"1417\":2,\"1419\":2,\"1420\":2,\"1421\":4,\"1422\":2,\"1424\":2,\"1425\":12,\"1427\":5,\"1430\":2,\"1454\":2,\"1526\":2,\"1531\":1,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":2,\"1558\":1,\"1581\":1,\"1645\":1,\"1800\":1,\"1906\":1,\"1962\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":4,\"1968\":2,\"2028\":1,\"2099\":3,\"2106\":2,\"2109\":2,\"2113\":2,\"2115\":2,\"2116\":2,\"2120\":1,\"2123\":1,\"2124\":1,\"2130\":1,\"2132\":1,\"2135\":1,\"2136\":1,\"2137\":2,\"2178\":3,\"2179\":3,\"2182\":1,\"2186\":1,\"2187\":1,\"2189\":1,\"2191\":3,\"2193\":1,\"2194\":3,\"2195\":4,\"2196\":3,\"2197\":3,\"2201\":2,\"2202\":2,\"2204\":1,\"2222\":1,\"2223\":1,\"2225\":1,\"2228\":1,\"2229\":1,\"2231\":1,\"2254\":2,\"2294\":1,\"2307\":1,\"2308\":1,\"2343\":1,\"2344\":2,\"2347\":7,\"2349\":9,\"2359\":2,\"2372\":2,\"2385\":2,\"2386\":2,\"2394\":1,\"2429\":2,\"2430\":2,\"2431\":3,\"2432\":1,\"2456\":1,\"2460\":1,\"2514\":3,\"2521\":4,\"2530\":1,\"2554\":2,\"2555\":2,\"2566\":1,\"2567\":2,\"2568\":12,\"2572\":2,\"2573\":1,\"2580\":2,\"2584\":3,\"2638\":1,\"2659\":3}}],[\"parenthesis\",{\"0\":{\"2331\":1},\"1\":{\"2331\":2}}],[\"parent\",{\"1\":{\"1944\":1}}],[\"park\",{\"1\":{\"1257\":1}}],[\"pariente\",{\"1\":{\"1132\":1,\"2457\":1}}],[\"par\",{\"1\":{\"785\":3}}],[\"parsing\",{\"1\":{\"62\":1}}],[\"parsed\",{\"1\":{\"659\":1,\"660\":1,\"661\":1,\"662\":1}}],[\"parser\",{\"0\":{\"663\":1,\"664\":1,\"666\":1},\"1\":{\"56\":2,\"173\":6,\"175\":4,\"194\":4,\"203\":2,\"606\":2,\"650\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":3,\"664\":3,\"666\":3,\"672\":1,\"673\":1,\"676\":2,\"734\":2,\"742\":3,\"754\":2,\"767\":2,\"781\":2,\"817\":2,\"820\":2,\"821\":2,\"826\":2,\"828\":2,\"944\":1,\"957\":1,\"2096\":1,\"2097\":3,\"2098\":1,\"2099\":4,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2176\":4,\"2185\":1,\"2201\":2,\"2203\":1,\"2314\":6,\"2320\":6,\"2328\":6,\"2335\":3,\"2340\":6}}],[\"parse\",{\"0\":{\"90\":1,\"601\":1,\"647\":1,\"1029\":1,\"2326\":1},\"1\":{\"18\":2,\"84\":1,\"90\":2,\"173\":1,\"175\":1,\"194\":1,\"203\":1,\"601\":3,\"647\":3,\"889\":1,\"1029\":3,\"1030\":1,\"2176\":1,\"2275\":1,\"2309\":1,\"2314\":4,\"2320\":4,\"2326\":1,\"2328\":4,\"2335\":1,\"2340\":4,\"2387\":1,\"2568\":1}}],[\"para\",{\"1\":{\"2387\":2}}],[\"parase\",{\"1\":{\"90\":1}}],[\"parallelwaveganpretrainedvocoder\",{\"0\":{\"2254\":1},\"1\":{\"2254\":2}}],[\"parallelwavegangenerator\",{\"0\":{\"1862\":1},\"1\":{\"1862\":2}}],[\"parallelwavegandiscriminator\",{\"0\":{\"1861\":1},\"1\":{\"1861\":2}}],[\"parallelwavegan\",{\"1\":{\"206\":3,\"1860\":1,\"2361\":3,\"2450\":2,\"2517\":2,\"2650\":3}}],[\"parallelized\",{\"1\":{\"2372\":1,\"2429\":1,\"2554\":1}}],[\"parallelizes\",{\"1\":{\"2148\":1}}],[\"parallelize\",{\"1\":{\"172\":1,\"2148\":1}}],[\"parallelization\",{\"1\":{\"141\":1,\"143\":1}}],[\"parallelsentenceiterator\",{\"0\":{\"612\":1},\"1\":{\"612\":1}}],[\"parallels\",{\"1\":{\"93\":2}}],[\"parallel\",{\"0\":{\"93\":1,\"214\":1,\"1832\":1,\"1834\":1,\"1861\":2,\"1862\":2,\"1869\":1,\"1876\":1,\"2254\":1},\"1\":{\"32\":1,\"34\":1,\"46\":1,\"47\":1,\"84\":1,\"207\":1,\"214\":4,\"217\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"229\":1,\"230\":1,\"231\":1,\"238\":1,\"275\":1,\"276\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"294\":1,\"295\":9,\"297\":1,\"307\":2,\"408\":2,\"576\":1,\"709\":2,\"727\":4,\"734\":2,\"742\":2,\"1144\":1,\"1219\":1,\"1829\":1,\"1832\":1,\"1834\":1,\"1861\":3,\"1862\":3,\"1869\":1,\"1876\":1,\"2244\":1,\"2254\":2,\"2362\":1,\"2363\":17,\"2504\":2,\"2506\":3,\"2510\":3,\"2512\":10,\"2651\":1,\"2653\":17,\"2657\":10}}],[\"paramters\",{\"1\":{\"1864\":1,\"1865\":1}}],[\"parameers\",{\"1\":{\"1245\":1}}],[\"parameterization\",{\"1\":{\"1351\":1}}],[\"parameterizations\",{\"1\":{\"1245\":1}}],[\"parameterized\",{\"1\":{\"118\":1,\"1132\":1}}],[\"parameter\",{\"1\":{\"21\":2,\"23\":1,\"24\":1,\"109\":1,\"113\":3,\"115\":3,\"116\":1,\"117\":1,\"119\":2,\"121\":1,\"122\":1,\"672\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"688\":1,\"705\":1,\"743\":2,\"758\":1,\"770\":1,\"825\":1,\"1037\":1,\"1040\":1,\"1084\":1,\"1210\":1,\"1211\":1,\"1244\":1,\"1245\":1,\"1252\":1,\"1278\":1,\"1286\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":1,\"1528\":1,\"1618\":1,\"1712\":1,\"1715\":1,\"1765\":1,\"1778\":9,\"1800\":1,\"1803\":1,\"1804\":3,\"1805\":7,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":6,\"1851\":1,\"1852\":8,\"1856\":1,\"1857\":1,\"1858\":1,\"1861\":1,\"1862\":1,\"1866\":1,\"1867\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1877\":7,\"1878\":3,\"1880\":1,\"1919\":1,\"1948\":1,\"2019\":1,\"2584\":3,\"2585\":1}}],[\"parameters\",{\"0\":{\"67\":1,\"897\":1,\"931\":1,\"1093\":1,\"1097\":1},\"1\":{\"21\":4,\"23\":4,\"25\":1,\"28\":2,\"29\":3,\"30\":1,\"62\":3,\"64\":3,\"66\":6,\"82\":1,\"113\":1,\"114\":2,\"115\":5,\"116\":2,\"117\":2,\"119\":3,\"120\":1,\"121\":2,\"122\":1,\"124\":3,\"169\":1,\"174\":2,\"181\":1,\"197\":1,\"198\":1,\"240\":2,\"295\":1,\"429\":2,\"600\":1,\"601\":1,\"602\":1,\"603\":1,\"606\":1,\"610\":1,\"614\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":3,\"629\":2,\"630\":1,\"631\":1,\"632\":2,\"633\":1,\"634\":2,\"635\":1,\"636\":1,\"637\":1,\"639\":3,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"657\":1,\"658\":1,\"659\":2,\"660\":2,\"661\":4,\"662\":2,\"665\":1,\"668\":1,\"670\":1,\"672\":2,\"674\":1,\"676\":7,\"677\":2,\"678\":2,\"679\":2,\"680\":1,\"681\":2,\"682\":2,\"683\":1,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"690\":1,\"691\":8,\"692\":4,\"693\":7,\"695\":1,\"696\":2,\"697\":11,\"698\":2,\"699\":2,\"700\":8,\"701\":3,\"702\":2,\"703\":2,\"705\":2,\"706\":8,\"708\":1,\"710\":1,\"711\":3,\"712\":4,\"713\":2,\"714\":2,\"715\":2,\"716\":2,\"717\":2,\"718\":2,\"719\":2,\"720\":2,\"721\":2,\"722\":2,\"723\":2,\"724\":1,\"725\":9,\"726\":3,\"727\":3,\"728\":3,\"729\":1,\"730\":1,\"731\":3,\"732\":2,\"734\":5,\"735\":2,\"737\":3,\"738\":2,\"740\":2,\"741\":2,\"742\":5,\"743\":1,\"745\":3,\"746\":2,\"747\":3,\"748\":1,\"749\":3,\"750\":3,\"754\":7,\"755\":2,\"758\":1,\"759\":3,\"760\":1,\"762\":2,\"763\":3,\"764\":2,\"766\":2,\"767\":2,\"768\":2,\"770\":2,\"771\":3,\"772\":2,\"773\":3,\"774\":2,\"775\":2,\"776\":2,\"777\":2,\"778\":1,\"781\":5,\"782\":1,\"784\":2,\"785\":4,\"786\":2,\"787\":1,\"792\":1,\"793\":2,\"794\":4,\"796\":1,\"797\":8,\"800\":1,\"801\":1,\"802\":2,\"804\":2,\"805\":1,\"806\":9,\"807\":1,\"808\":1,\"809\":3,\"810\":2,\"812\":2,\"813\":4,\"815\":4,\"816\":1,\"817\":4,\"818\":2,\"820\":2,\"821\":4,\"822\":2,\"823\":1,\"824\":5,\"825\":8,\"826\":5,\"827\":2,\"828\":4,\"829\":2,\"830\":2,\"833\":1,\"834\":1,\"835\":3,\"836\":1,\"838\":2,\"852\":1,\"855\":1,\"856\":1,\"857\":1,\"858\":2,\"859\":2,\"860\":2,\"861\":1,\"862\":2,\"863\":1,\"865\":1,\"866\":1,\"867\":1,\"868\":2,\"869\":1,\"870\":1,\"871\":1,\"872\":1,\"873\":1,\"874\":1,\"875\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"889\":1,\"890\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":2,\"898\":1,\"899\":1,\"901\":1,\"903\":1,\"906\":1,\"908\":1,\"909\":1,\"910\":2,\"911\":3,\"912\":1,\"914\":1,\"915\":1,\"917\":1,\"918\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"931\":2,\"932\":1,\"933\":1,\"934\":2,\"935\":1,\"936\":1,\"937\":1,\"938\":1,\"943\":1,\"950\":1,\"955\":1,\"965\":1,\"968\":1,\"972\":1,\"975\":1,\"976\":1,\"981\":1,\"988\":1,\"996\":1,\"997\":1,\"998\":1,\"999\":1,\"1000\":1,\"1001\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1007\":1,\"1012\":1,\"1013\":1,\"1015\":1,\"1016\":1,\"1019\":1,\"1025\":1,\"1028\":1,\"1032\":1,\"1033\":1,\"1034\":1,\"1037\":1,\"1039\":1,\"1040\":1,\"1042\":1,\"1043\":1,\"1046\":7,\"1047\":2,\"1048\":9,\"1049\":4,\"1050\":4,\"1051\":2,\"1052\":6,\"1053\":2,\"1054\":2,\"1055\":2,\"1056\":4,\"1057\":4,\"1058\":4,\"1059\":4,\"1061\":1,\"1062\":5,\"1063\":1,\"1064\":5,\"1065\":6,\"1066\":10,\"1067\":1,\"1068\":4,\"1069\":9,\"1070\":5,\"1071\":6,\"1072\":2,\"1073\":9,\"1074\":2,\"1075\":9,\"1076\":7,\"1077\":3,\"1078\":5,\"1079\":7,\"1080\":2,\"1081\":6,\"1082\":1,\"1083\":8,\"1084\":1,\"1085\":1,\"1086\":2,\"1087\":2,\"1088\":2,\"1089\":2,\"1090\":1,\"1091\":2,\"1092\":1,\"1093\":7,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":3,\"1098\":2,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1106\":2,\"1107\":1,\"1108\":2,\"1113\":2,\"1114\":1,\"1116\":1,\"1132\":2,\"1133\":5,\"1138\":9,\"1139\":8,\"1140\":1,\"1141\":2,\"1142\":3,\"1144\":1,\"1145\":5,\"1148\":2,\"1149\":4,\"1150\":4,\"1153\":1,\"1154\":1,\"1155\":1,\"1169\":1,\"1170\":2,\"1171\":3,\"1172\":2,\"1173\":4,\"1178\":3,\"1179\":3,\"1180\":3,\"1181\":3,\"1182\":2,\"1186\":3,\"1190\":5,\"1191\":1,\"1192\":1,\"1198\":2,\"1200\":2,\"1203\":2,\"1204\":1,\"1206\":2,\"1209\":4,\"1210\":2,\"1211\":2,\"1214\":3,\"1217\":1,\"1218\":1,\"1221\":2,\"1222\":1,\"1224\":1,\"1228\":1,\"1240\":1,\"1244\":4,\"1245\":5,\"1248\":1,\"1252\":1,\"1254\":1,\"1255\":2,\"1256\":1,\"1269\":4,\"1270\":9,\"1272\":2,\"1273\":3,\"1282\":1,\"1286\":2,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1,\"1336\":1,\"1337\":1,\"1345\":1,\"1347\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1352\":1,\"1368\":2,\"1369\":1,\"1370\":1,\"1371\":2,\"1372\":2,\"1373\":1,\"1374\":1,\"1375\":2,\"1376\":1,\"1377\":2,\"1378\":1,\"1379\":2,\"1392\":1,\"1407\":1,\"1427\":1,\"1430\":2,\"1432\":1,\"1436\":1,\"1451\":1,\"1452\":1,\"1454\":2,\"1455\":1,\"1462\":2,\"1463\":2,\"1464\":1,\"1466\":1,\"1470\":2,\"1471\":2,\"1472\":2,\"1473\":1,\"1477\":1,\"1484\":1,\"1505\":2,\"1510\":2,\"1511\":2,\"1514\":1,\"1515\":2,\"1516\":5,\"1522\":2,\"1523\":2,\"1524\":3,\"1525\":2,\"1528\":2,\"1529\":2,\"1530\":1,\"1531\":2,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":2,\"1543\":1,\"1545\":2,\"1546\":1,\"1551\":2,\"1552\":5,\"1553\":3,\"1554\":1,\"1557\":1,\"1558\":2,\"1563\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1571\":1,\"1572\":2,\"1575\":2,\"1576\":2,\"1577\":2,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1585\":1,\"1594\":1,\"1595\":1,\"1598\":1,\"1600\":2,\"1601\":1,\"1602\":2,\"1603\":2,\"1604\":1,\"1609\":1,\"1611\":1,\"1612\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":3,\"1619\":3,\"1622\":2,\"1623\":1,\"1626\":2,\"1637\":1,\"1638\":5,\"1639\":1,\"1640\":1,\"1643\":3,\"1644\":3,\"1645\":2,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":2,\"1655\":2,\"1656\":1,\"1658\":2,\"1659\":3,\"1660\":2,\"1661\":2,\"1662\":2,\"1663\":1,\"1664\":2,\"1665\":2,\"1666\":1,\"1668\":1,\"1669\":2,\"1670\":2,\"1671\":2,\"1680\":1,\"1688\":1,\"1693\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1717\":1,\"1719\":5,\"1736\":1,\"1737\":1,\"1739\":1,\"1741\":1,\"1746\":1,\"1752\":1,\"1755\":1,\"1756\":1,\"1758\":1,\"1759\":1,\"1765\":4,\"1766\":2,\"1767\":1,\"1768\":1,\"1771\":2,\"1772\":2,\"1773\":3,\"1776\":2,\"1777\":2,\"1778\":3,\"1781\":4,\"1785\":2,\"1786\":2,\"1787\":2,\"1788\":2,\"1792\":1,\"1798\":2,\"1800\":6,\"1801\":5,\"1803\":4,\"1804\":3,\"1805\":3,\"1808\":4,\"1810\":1,\"1811\":1,\"1829\":2,\"1830\":2,\"1832\":2,\"1833\":2,\"1834\":2,\"1835\":2,\"1836\":2,\"1837\":2,\"1838\":2,\"1839\":2,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":2,\"1844\":5,\"1845\":3,\"1846\":4,\"1847\":5,\"1848\":2,\"1849\":2,\"1850\":3,\"1851\":4,\"1852\":3,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":2,\"1857\":5,\"1858\":5,\"1859\":2,\"1860\":4,\"1861\":3,\"1862\":4,\"1863\":3,\"1864\":2,\"1865\":2,\"1866\":2,\"1867\":2,\"1868\":2,\"1869\":2,\"1870\":6,\"1871\":5,\"1872\":2,\"1873\":2,\"1874\":2,\"1876\":2,\"1877\":3,\"1878\":3,\"1879\":2,\"1880\":2,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1889\":1,\"1890\":1,\"1892\":2,\"1893\":2,\"1904\":1,\"1905\":1,\"1906\":2,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":2,\"1918\":2,\"1919\":2,\"1920\":1,\"1921\":1,\"1922\":1,\"1923\":1,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1930\":1,\"1932\":1,\"1935\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1957\":4,\"1958\":1,\"1959\":1,\"1960\":3,\"1962\":1,\"1963\":1,\"1970\":2,\"1971\":2,\"1973\":4,\"1975\":2,\"1982\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1988\":2,\"1990\":2,\"1992\":2,\"1993\":1,\"2000\":1,\"2001\":4,\"2002\":3,\"2004\":2,\"2010\":1,\"2011\":1,\"2012\":1,\"2027\":2,\"2029\":2,\"2030\":1,\"2040\":1,\"2044\":1,\"2046\":1,\"2049\":2,\"2050\":1,\"2052\":1,\"2054\":2,\"2055\":1,\"2064\":1,\"2068\":1,\"2070\":1,\"2076\":2,\"2078\":2,\"2079\":1,\"2081\":2,\"2082\":3,\"2083\":3,\"2084\":3,\"2086\":4,\"2087\":4,\"2088\":2,\"2089\":2,\"2090\":4,\"2091\":2,\"2092\":1,\"2093\":1,\"2095\":3,\"2097\":6,\"2099\":1,\"2142\":1,\"2151\":2,\"2152\":1,\"2154\":1,\"2155\":1,\"2156\":2,\"2157\":1,\"2170\":1,\"2186\":1,\"2197\":1,\"2202\":2,\"2204\":1,\"2210\":1,\"2234\":1,\"2238\":1,\"2239\":1,\"2240\":3,\"2242\":1,\"2243\":4,\"2244\":4,\"2245\":2,\"2247\":2,\"2249\":2,\"2251\":2,\"2252\":1,\"2254\":1,\"2255\":4,\"2256\":2,\"2257\":2,\"2258\":2,\"2259\":3,\"2260\":5,\"2261\":2,\"2262\":2,\"2263\":3,\"2264\":4,\"2265\":2,\"2267\":4,\"2268\":1,\"2270\":1,\"2272\":1,\"2273\":1,\"2274\":1,\"2278\":3,\"2279\":4,\"2280\":2,\"2301\":1,\"2302\":1,\"2303\":1,\"2305\":1,\"2317\":1,\"2325\":1,\"2330\":1,\"2343\":1,\"2344\":1,\"2347\":1,\"2349\":1,\"2583\":1,\"2584\":6,\"2585\":1,\"2600\":1}}],[\"params=\",{\"1\":{\"1800\":1}}],[\"params\",{\"1\":{\"639\":1,\"888\":1,\"944\":1,\"1087\":2,\"1088\":2,\"1089\":2,\"1091\":2,\"1336\":1,\"1763\":1,\"1765\":2,\"1778\":22,\"1800\":1,\"1801\":10,\"1803\":2,\"1805\":17,\"1834\":2,\"1844\":2,\"1845\":3,\"1846\":5,\"1847\":8,\"1848\":2,\"1849\":2,\"1850\":18,\"1851\":2,\"1852\":22,\"1856\":4,\"1857\":4,\"1858\":6,\"1861\":2,\"1862\":2,\"1866\":2,\"1867\":4,\"1870\":6,\"1871\":2,\"1876\":2,\"1877\":17,\"1972\":1,\"1973\":1}}],[\"param\",{\"1\":{\"29\":2,\"66\":7,\"67\":1,\"88\":1,\"604\":5,\"605\":6,\"607\":3,\"621\":2,\"650\":1,\"705\":3,\"710\":8,\"758\":9,\"792\":2,\"801\":6,\"905\":4,\"944\":3,\"956\":4,\"973\":4,\"987\":9,\"989\":4,\"1008\":3,\"1060\":3,\"1116\":2,\"1171\":5,\"1206\":5,\"1336\":3,\"1517\":1,\"1552\":5,\"1688\":1,\"1693\":1,\"1752\":1,\"1755\":1,\"1756\":1,\"1785\":4,\"1953\":3,\"1954\":3,\"1955\":3,\"1956\":3,\"1972\":1,\"2157\":2,\"2271\":3,\"2357\":10,\"2363\":6,\"2371\":2,\"2506\":2,\"2510\":2,\"2512\":2,\"2578\":10,\"2612\":2,\"2630\":2,\"2653\":6,\"2657\":2}}],[\"partly\",{\"1\":{\"1245\":1}}],[\"participation\",{\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"particular\",{\"1\":{\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"2467\":1}}],[\"partition\",{\"1\":{\"144\":2}}],[\"partial=false\",{\"1\":{\"1327\":1}}],[\"partialscorerinterface\",{\"0\":{\"795\":1},\"1\":{\"695\":1,\"795\":1}}],[\"partial\",{\"0\":{\"642\":1},\"1\":{\"28\":1,\"115\":12,\"119\":1,\"307\":2,\"408\":2,\"642\":1,\"658\":2,\"691\":5,\"695\":2,\"697\":6,\"706\":2,\"795\":3,\"796\":1,\"797\":2,\"1072\":2,\"1093\":2,\"1098\":2}}],[\"partiallyarinference\",{\"0\":{\"1219\":1},\"1\":{\"1219\":1}}],[\"partiallyarhypothesis\",{\"0\":{\"798\":1},\"1\":{\"797\":5,\"798\":2}}],[\"partiallyarbeamsearch\",{\"0\":{\"797\":1},\"1\":{\"797\":1}}],[\"partially\",{\"0\":{\"314\":1,\"415\":1,\"765\":1,\"797\":1,\"798\":1,\"1219\":1},\"1\":{\"26\":1,\"765\":1,\"785\":2,\"797\":4,\"798\":2,\"1133\":2,\"1219\":2}}],[\"parts\",{\"1\":{\"30\":1,\"56\":1,\"83\":1,\"114\":1,\"116\":1,\"119\":1,\"124\":1,\"170\":1,\"576\":2,\"1019\":1,\"1037\":1,\"1039\":1,\"1522\":1,\"1671\":1,\"1778\":1,\"1852\":1,\"2403\":1,\"2539\":1,\"2564\":1,\"2584\":3}}],[\"part\",{\"1\":{\"21\":3,\"30\":4,\"113\":2,\"174\":1,\"186\":1,\"226\":1,\"249\":1,\"295\":1,\"691\":7,\"697\":17,\"702\":1,\"737\":2,\"754\":1,\"755\":1,\"771\":1,\"809\":1,\"821\":1,\"822\":1,\"826\":1,\"858\":4,\"862\":3,\"886\":3,\"899\":2,\"901\":2,\"903\":1,\"910\":4,\"911\":1,\"934\":3,\"1072\":1,\"1098\":1,\"1148\":1,\"1203\":1,\"1248\":3,\"1377\":1,\"1516\":2,\"1705\":1,\"1746\":1,\"1798\":1,\"1849\":1,\"1851\":1,\"1879\":1,\"2054\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2090\":1,\"2091\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2255\":1,\"2256\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2279\":1,\"2280\":1,\"2374\":1,\"2384\":1,\"2387\":1,\"2400\":1,\"2401\":1,\"2434\":1,\"2435\":1,\"2536\":1,\"2537\":1,\"2557\":1,\"2561\":1,\"2564\":1,\"2565\":1}}],[\"paste\",{\"1\":{\"2569\":1,\"2570\":1}}],[\"past\",{\"1\":{\"711\":5,\"1719\":3}}],[\"paser\",{\"1\":{\"18\":1}}],[\"passes\",{\"1\":{\"1917\":1,\"2050\":1}}],[\"passed\",{\"1\":{\"27\":1,\"115\":1,\"118\":1,\"652\":1,\"727\":1,\"728\":1,\"759\":1,\"1187\":1,\"1202\":1,\"1254\":1,\"1286\":1,\"1287\":1,\"1327\":1,\"1452\":1,\"1951\":1,\"2398\":1,\"2437\":1,\"2534\":1,\"2563\":1}}],[\"passing\",{\"1\":{\"115\":1,\"116\":1,\"1245\":1,\"1246\":1,\"2378\":1,\"2437\":1,\"2563\":1,\"2584\":1}}],[\"pass\",{\"0\":{\"405\":1,\"2473\":1,\"2646\":1,\"2648\":1,\"2649\":1},\"1\":{\"1\":1,\"2\":1,\"102\":1,\"124\":1,\"157\":1,\"718\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"823\":1,\"832\":1,\"1085\":1,\"1086\":2,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1160\":3,\"1161\":3,\"1162\":2,\"1163\":2,\"1164\":3,\"1165\":2,\"1166\":1,\"1175\":1,\"1177\":3,\"1185\":1,\"1187\":3,\"1189\":1,\"1202\":3,\"1208\":1,\"1209\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1243\":1,\"1246\":1,\"1247\":1,\"1248\":2,\"1250\":1,\"1251\":1,\"1252\":3,\"1253\":4,\"1254\":3,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1269\":1,\"1275\":1,\"1277\":1,\"1279\":2,\"1281\":1,\"1283\":1,\"1285\":1,\"1286\":2,\"1287\":2,\"1298\":2,\"1299\":2,\"1301\":1,\"1302\":2,\"1303\":2,\"1304\":1,\"1327\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1775\":1,\"1780\":1,\"1781\":1,\"1783\":1,\"1786\":1,\"1788\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1808\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2027\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2150\":1,\"2169\":1,\"2176\":2,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2323\":1,\"2467\":1,\"2472\":3,\"2473\":2,\"2474\":8,\"2569\":1,\"2570\":1,\"2573\":1,\"2646\":1,\"2648\":4,\"2649\":7}}],[\"pyaudio\",{\"0\":{\"2595\":1,\"2596\":1},\"1\":{\"2595\":1,\"2596\":1}}],[\"pysndfile\",{\"1\":{\"2360\":2,\"2450\":1,\"2458\":2,\"2517\":1,\"2523\":2,\"2582\":2}}],[\"pysize\",{\"1\":{\"2324\":1}}],[\"pyscripts\",{\"1\":{\"85\":1,\"2372\":1,\"2385\":1,\"2394\":1,\"2429\":1,\"2461\":1,\"2530\":1,\"2554\":1,\"2566\":1}}],[\"pyyaml\",{\"1\":{\"207\":1}}],[\"pyplot\",{\"1\":{\"171\":1,\"174\":1,\"175\":1,\"185\":1,\"193\":1,\"202\":1,\"238\":1,\"629\":2,\"648\":2,\"2359\":1,\"2360\":1,\"2386\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2498\":1,\"2521\":1,\"2522\":1,\"2523\":1,\"2580\":1,\"2581\":1,\"2582\":1,\"2616\":1,\"2634\":1}}],[\"pypinyin==0\",{\"1\":{\"2504\":1,\"2651\":1}}],[\"pypinyin\",{\"0\":{\"2144\":1,\"2145\":1,\"2146\":1},\"1\":{\"227\":1,\"231\":2,\"461\":3,\"2144\":1,\"2145\":1,\"2146\":1}}],[\"pypi\",{\"1\":{\"127\":1}}],[\"pyopenjtalk==0\",{\"1\":{\"2362\":1,\"2504\":1}}],[\"pyopenjtalk\",{\"0\":{\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1},\"1\":{\"136\":2,\"220\":1,\"224\":2,\"461\":5,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2143\":2}}],[\"pytz\",{\"1\":{\"2392\":2,\"2425\":2,\"2528\":2,\"2548\":2}}],[\"pytroch\",{\"1\":{\"240\":1}}],[\"pythonic\",{\"1\":{\"2452\":1}}],[\"python3\",{\"1\":{\"135\":2,\"137\":1,\"1950\":1,\"2155\":1,\"2372\":1,\"2431\":1,\"2432\":1,\"2554\":1,\"2568\":1}}],[\"python\",{\"0\":{\"192\":1,\"2384\":1,\"2394\":1,\"2428\":1,\"2530\":1,\"2551\":1},\"1\":{\"15\":1,\"34\":2,\"35\":1,\"36\":2,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"49\":1,\"56\":1,\"57\":1,\"58\":1,\"59\":3,\"60\":1,\"62\":4,\"63\":2,\"64\":2,\"65\":1,\"66\":6,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":2,\"72\":4,\"74\":1,\"75\":1,\"76\":2,\"77\":1,\"78\":1,\"79\":1,\"80\":1,\"81\":1,\"82\":1,\"85\":8,\"127\":1,\"128\":4,\"132\":1,\"135\":13,\"136\":6,\"137\":1,\"167\":2,\"170\":1,\"178\":2,\"179\":1,\"196\":2,\"234\":2,\"238\":2,\"661\":1,\"672\":1,\"676\":1,\"767\":1,\"781\":1,\"1008\":1,\"1323\":1,\"1355\":1,\"1735\":1,\"2371\":1,\"2372\":3,\"2384\":2,\"2385\":1,\"2387\":1,\"2409\":3,\"2429\":4,\"2441\":1,\"2442\":1,\"2466\":1,\"2481\":1,\"2552\":3,\"2554\":2,\"2568\":2,\"2612\":1,\"2618\":1,\"2630\":1,\"2642\":1,\"2646\":1}}],[\"pytorchlightning\",{\"1\":{\"2154\":1}}],[\"pytorchscheduler\",{\"0\":{\"671\":1},\"1\":{\"671\":2}}],[\"pytorch1\",{\"1\":{\"132\":1}}],[\"pytorch\",{\"0\":{\"31\":1,\"60\":1,\"593\":1,\"594\":1,\"595\":1,\"596\":1,\"598\":1,\"599\":1,\"602\":1,\"603\":1,\"615\":1,\"617\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"635\":1,\"636\":1,\"637\":1,\"639\":1,\"640\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"650\":1,\"658\":1,\"659\":1,\"660\":1,\"662\":1,\"671\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"701\":1,\"702\":1,\"708\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":1,\"726\":1,\"729\":1,\"730\":1,\"733\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"749\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"766\":1,\"770\":1,\"771\":1,\"772\":1,\"774\":1,\"775\":1,\"776\":1,\"778\":1,\"780\":1,\"782\":1,\"785\":1,\"786\":1,\"787\":1,\"792\":1,\"793\":1,\"794\":1,\"799\":1,\"802\":1,\"803\":1,\"806\":1,\"807\":1,\"809\":1,\"810\":1,\"811\":1,\"813\":1,\"816\":1,\"817\":1,\"818\":1,\"819\":1,\"821\":1,\"822\":1,\"823\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"830\":1,\"831\":1,\"835\":1,\"836\":1,\"837\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"853\":1,\"854\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":1,\"868\":1,\"869\":1,\"871\":1,\"877\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"897\":1,\"899\":1,\"901\":1,\"903\":1,\"905\":1,\"906\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"929\":1,\"930\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":1,\"936\":1,\"937\":1,\"938\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"1033\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1044\":1,\"1045\":1,\"2160\":2},\"1\":{\"8\":2,\"16\":3,\"20\":1,\"24\":2,\"28\":1,\"31\":1,\"38\":1,\"57\":1,\"74\":1,\"82\":2,\"135\":2,\"167\":2,\"173\":1,\"178\":2,\"191\":1,\"192\":1,\"194\":2,\"196\":2,\"210\":2,\"211\":2,\"212\":1,\"222\":2,\"223\":2,\"229\":2,\"230\":1,\"234\":2,\"240\":24,\"241\":6,\"242\":5,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"285\":1,\"286\":3,\"593\":1,\"594\":1,\"595\":1,\"596\":1,\"598\":1,\"599\":2,\"602\":2,\"603\":2,\"615\":2,\"617\":1,\"624\":2,\"625\":2,\"626\":2,\"627\":2,\"628\":1,\"629\":1,\"635\":1,\"636\":2,\"637\":1,\"639\":1,\"640\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"650\":3,\"654\":1,\"656\":1,\"658\":1,\"659\":3,\"660\":3,\"661\":2,\"662\":3,\"665\":1,\"671\":3,\"676\":5,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"701\":1,\"702\":1,\"706\":1,\"708\":1,\"710\":2,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":1,\"726\":1,\"729\":1,\"730\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":2,\"741\":2,\"749\":2,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"766\":1,\"770\":1,\"771\":1,\"772\":1,\"774\":1,\"775\":2,\"776\":2,\"778\":1,\"781\":4,\"782\":1,\"785\":1,\"786\":1,\"787\":1,\"792\":1,\"793\":2,\"794\":1,\"799\":2,\"802\":1,\"803\":1,\"806\":1,\"807\":2,\"809\":1,\"810\":1,\"811\":1,\"813\":1,\"815\":5,\"816\":1,\"817\":2,\"818\":1,\"819\":1,\"821\":1,\"822\":1,\"823\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"830\":1,\"831\":1,\"835\":1,\"836\":1,\"837\":2,\"838\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"853\":1,\"854\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":1,\"868\":1,\"869\":1,\"871\":1,\"872\":1,\"873\":1,\"874\":1,\"877\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"897\":1,\"899\":3,\"901\":3,\"903\":1,\"905\":1,\"906\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"929\":1,\"930\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":1,\"936\":2,\"937\":2,\"938\":2,\"974\":1,\"975\":2,\"976\":3,\"977\":2,\"978\":2,\"981\":1,\"1000\":1,\"1017\":1,\"1033\":2,\"1041\":1,\"1042\":2,\"1043\":4,\"1044\":2,\"1045\":2,\"1067\":1,\"1084\":1,\"1133\":5,\"1149\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1180\":1,\"1196\":1,\"1197\":1,\"1204\":1,\"1214\":2,\"1269\":1,\"1271\":1,\"1272\":1,\"1273\":3,\"1343\":1,\"1451\":4,\"1514\":4,\"1557\":4,\"1585\":4,\"1612\":4,\"1615\":4,\"1623\":4,\"1637\":4,\"1732\":1,\"1895\":1,\"1900\":1,\"1958\":1,\"1971\":1,\"1973\":1,\"2001\":3,\"2004\":1,\"2019\":2,\"2029\":1,\"2154\":2,\"2160\":2,\"2182\":1,\"2189\":1,\"2259\":1,\"2271\":1,\"2354\":1,\"2372\":2,\"2388\":1,\"2401\":4,\"2421\":1,\"2429\":2,\"2441\":1,\"2442\":2,\"2466\":1,\"2482\":1,\"2524\":1,\"2537\":4,\"2544\":1,\"2552\":2,\"2584\":1,\"2646\":1}}],[\"py\",{\"0\":{\"46\":1,\"187\":1,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"377\":1,\"380\":1,\"384\":1,\"391\":1,\"397\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"493\":1,\"496\":1,\"499\":1,\"501\":1,\"503\":1,\"506\":1,\"509\":1,\"512\":1,\"515\":1,\"517\":1,\"519\":1,\"522\":1,\"525\":1,\"528\":1,\"530\":1,\"533\":1,\"536\":1,\"538\":1,\"541\":1,\"544\":1,\"546\":1,\"549\":1,\"551\":1,\"554\":1,\"557\":1,\"560\":1,\"562\":1,\"564\":1,\"566\":1,\"568\":1,\"570\":1,\"572\":1,\"574\":1,\"576\":1,\"579\":1,\"582\":1,\"585\":1,\"588\":1,\"590\":1,\"2667\":1},\"1\":{\"11\":4,\"19\":1,\"24\":1,\"25\":4,\"27\":1,\"46\":5,\"49\":2,\"92\":1,\"107\":1,\"108\":1,\"110\":1,\"115\":1,\"124\":2,\"137\":1,\"167\":1,\"170\":1,\"173\":1,\"178\":1,\"187\":2,\"189\":1,\"196\":1,\"234\":1,\"240\":1,\"245\":2,\"247\":2,\"249\":2,\"251\":2,\"253\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2,\"299\":2,\"301\":2,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"363\":2,\"368\":2,\"375\":2,\"377\":2,\"380\":2,\"384\":2,\"391\":2,\"397\":2,\"398\":11,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":2,\"441\":2,\"443\":2,\"449\":2,\"455\":2,\"461\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"491\":2,\"493\":2,\"496\":2,\"499\":2,\"501\":2,\"503\":2,\"506\":2,\"509\":2,\"512\":2,\"515\":2,\"517\":2,\"519\":2,\"522\":2,\"525\":2,\"528\":2,\"530\":2,\"533\":2,\"536\":2,\"538\":2,\"541\":2,\"544\":2,\"546\":2,\"549\":2,\"551\":2,\"554\":2,\"557\":2,\"560\":2,\"562\":2,\"564\":2,\"566\":2,\"568\":2,\"570\":2,\"572\":2,\"574\":2,\"576\":2,\"579\":2,\"582\":2,\"585\":2,\"588\":2,\"590\":2,\"628\":1,\"650\":1,\"734\":1,\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"817\":2,\"828\":1,\"1093\":1,\"1101\":1,\"1102\":1,\"1171\":1,\"1172\":1,\"1180\":1,\"1198\":1,\"1218\":1,\"1371\":1,\"1452\":1,\"1454\":1,\"1551\":3,\"1552\":3,\"1553\":3,\"1554\":1,\"1695\":1,\"1717\":1,\"1719\":1,\"1735\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"1892\":1,\"1893\":1,\"1950\":1,\"1958\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":1,\"2086\":1,\"2087\":1,\"2131\":1,\"2154\":1,\"2155\":1,\"2267\":1,\"2372\":1,\"2387\":1,\"2394\":14,\"2400\":1,\"2401\":1,\"2403\":1,\"2429\":2,\"2461\":1,\"2530\":14,\"2536\":1,\"2537\":1,\"2539\":1,\"2543\":2,\"2554\":2,\"2568\":4,\"2641\":9,\"2642\":1,\"2643\":1,\"2644\":1}}],[\"potential\",{\"1\":{\"2479\":1,\"2501\":1}}],[\"potentially\",{\"1\":{\"1482\":1,\"1735\":1}}],[\"popular\",{\"1\":{\"2385\":1}}],[\"population\",{\"1\":{\"1950\":1}}],[\"poisson\",{\"1\":{\"1269\":2}}],[\"pointing\",{\"1\":{\"2639\":1}}],[\"pointwise\",{\"1\":{\"1198\":3,\"1564\":1}}],[\"pointer\",{\"1\":{\"678\":1,\"1154\":1,\"1186\":1}}],[\"points=0\",{\"1\":{\"1036\":1}}],[\"points\",{\"0\":{\"2471\":1,\"2474\":1},\"1\":{\"275\":1,\"282\":1,\"712\":1,\"1001\":7,\"1015\":1,\"1025\":5,\"1026\":2,\"1035\":1,\"1052\":1,\"1859\":1,\"1895\":1,\"1900\":1,\"2317\":3,\"2325\":3,\"2330\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"point\",{\"0\":{\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2414\":1,\"2418\":1,\"2419\":1,\"2420\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2462\":1,\"2476\":1,\"2478\":1,\"2479\":1,\"2485\":1,\"2490\":1,\"2497\":1,\"2500\":1,\"2501\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2542\":1,\"2543\":1,\"2554\":1,\"2556\":1,\"2559\":1,\"2560\":1,\"2564\":1,\"2565\":1,\"2569\":1},\"1\":{\"45\":2,\"85\":1,\"128\":1,\"275\":2,\"282\":1,\"297\":2,\"678\":1,\"794\":1,\"1036\":2,\"1255\":1,\"1543\":2,\"1662\":1,\"1808\":2,\"1927\":1,\"2372\":1,\"2385\":1,\"2394\":1,\"2429\":1,\"2508\":2,\"2530\":1,\"2542\":1,\"2543\":1,\"2554\":1,\"2564\":1,\"2569\":1,\"2638\":1}}],[\"pool=none\",{\"1\":{\"1244\":1,\"1252\":1,\"1254\":1}}],[\"pooling\",{\"0\":{\"2036\":2,\"2044\":2,\"2052\":2,\"2068\":2},\"1\":{\"1198\":1,\"1244\":2,\"1252\":2,\"1254\":1,\"1688\":1,\"1693\":1,\"1778\":2,\"1801\":6,\"1805\":2,\"1846\":6,\"1847\":6,\"1850\":2,\"1852\":2,\"1858\":6,\"1877\":2,\"2036\":4,\"2040\":1,\"2044\":3,\"2046\":4,\"2050\":1,\"2052\":2,\"2068\":3,\"2401\":1,\"2537\":1}}],[\"pool\",{\"0\":{\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1279\":1,\"1280\":1,\"1311\":1,\"1358\":1},\"1\":{\"1160\":2,\"1161\":1,\"1162\":2,\"1163\":2,\"1164\":2,\"1165\":1,\"1231\":1,\"1244\":1,\"1252\":1,\"1254\":1,\"1279\":2,\"1280\":2,\"1311\":2,\"1358\":2,\"2290\":1,\"2295\":2,\"2296\":2}}],[\"poorly\",{\"1\":{\"11\":1}}],[\"policy\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"polish\",{\"1\":{\"461\":1}}],[\"polarity\",{\"0\":{\"1942\":1},\"1\":{\"1942\":1}}],[\"pole\",{\"1\":{\"1922\":1}}],[\"poly1d\",{\"0\":{\"1738\":1},\"1\":{\"1738\":2}}],[\"polyfit\",{\"1\":{\"1526\":1}}],[\"polyharmonic\",{\"1\":{\"1001\":2,\"1031\":1}}],[\"powerful\",{\"1\":{\"2574\":1}}],[\"powers=none\",{\"1\":{\"1739\":1}}],[\"powers\",{\"1\":{\"1524\":2,\"1739\":2}}],[\"power=0\",{\"1\":{\"1156\":1}}],[\"power\",{\"0\":{\"887\":1,\"1341\":1,\"1710\":1,\"1711\":1},\"1\":{\"297\":1,\"585\":1,\"887\":2,\"1130\":2,\"1158\":1,\"1341\":2,\"1525\":1,\"1611\":1,\"1696\":2,\"1697\":1,\"1698\":2,\"1701\":2,\"1702\":3,\"1704\":5,\"1707\":1,\"1708\":1,\"1710\":4,\"1711\":2,\"1712\":1,\"1713\":6,\"1715\":1,\"1759\":4,\"2210\":1}}],[\"po\",{\"1\":{\"130\":1,\"2259\":1}}],[\"portable\",{\"0\":{\"2492\":1,\"2628\":1},\"1\":{\"1906\":1,\"2481\":1,\"2618\":1}}],[\"ported\",{\"1\":{\"1337\":1,\"1349\":1,\"1350\":1,\"1695\":1,\"1717\":1,\"1757\":1}}],[\"ports\",{\"1\":{\"44\":1}}],[\"port=none\",{\"1\":{\"596\":1}}],[\"port=\",{\"1\":{\"38\":1}}],[\"port\",{\"0\":{\"595\":1,\"2212\":1,\"2215\":1},\"1\":{\"36\":4,\"38\":4,\"39\":6,\"40\":1,\"377\":2,\"429\":2,\"595\":5,\"2180\":2,\"2212\":6,\"2215\":1}}],[\"positive\",{\"1\":{\"821\":1,\"822\":1,\"826\":1,\"1695\":1,\"2095\":1,\"2154\":1,\"2263\":1,\"2264\":1}}],[\"positionwise\",{\"0\":{\"801\":1},\"1\":{\"725\":6,\"726\":6,\"749\":5,\"786\":1,\"801\":2,\"858\":6,\"859\":2,\"862\":2,\"1148\":6,\"1149\":5,\"1150\":5,\"1169\":1,\"1203\":6,\"1272\":5,\"1505\":6,\"1669\":5,\"1778\":2,\"1787\":4,\"1788\":6,\"1798\":7,\"1804\":4,\"1805\":2,\"1850\":2,\"1851\":2,\"1852\":2,\"1874\":7,\"1877\":2,\"1878\":4,\"2026\":2,\"2029\":5,\"2054\":6,\"2090\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2264\":4,\"2279\":2}}],[\"positionwisefeedforward\",{\"0\":{\"801\":1},\"1\":{\"711\":2,\"801\":2,\"827\":1}}],[\"positions\",{\"1\":{\"116\":2,\"1065\":2,\"1066\":2,\"1078\":3,\"1079\":8,\"1115\":2,\"1149\":3,\"1150\":3,\"1200\":1,\"1272\":1,\"2029\":1}}],[\"position\",{\"0\":{\"1242\":1},\"1\":{\"115\":2,\"116\":2,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"749\":3,\"771\":1,\"809\":1,\"1065\":2,\"1066\":2,\"1078\":2,\"1079\":2,\"1093\":1,\"1133\":1,\"1148\":1,\"1149\":4,\"1150\":4,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1200\":2,\"1203\":1,\"1269\":1,\"1272\":2,\"1505\":1,\"1669\":1,\"1771\":3,\"1781\":3,\"1787\":3,\"1804\":2,\"1878\":2,\"2001\":1,\"2004\":1,\"2024\":1,\"2028\":1,\"2029\":2,\"2054\":1,\"2264\":2,\"2600\":1}}],[\"positionalencoding\",{\"0\":{\"800\":1},\"1\":{\"747\":1,\"749\":2,\"772\":1,\"800\":1,\"810\":1,\"813\":1,\"818\":1,\"1133\":2,\"1149\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1204\":1,\"1271\":1,\"1272\":2,\"1273\":1,\"1971\":2,\"2001\":2,\"2004\":2,\"2029\":2}}],[\"positional\",{\"0\":{\"378\":1,\"494\":1,\"497\":1,\"504\":1,\"507\":1,\"510\":1,\"513\":1,\"516\":1,\"518\":1,\"520\":1,\"523\":1,\"526\":1,\"531\":1,\"534\":1,\"537\":1,\"539\":1,\"542\":1,\"545\":1,\"547\":1,\"550\":1,\"552\":1,\"555\":1,\"558\":1,\"569\":1,\"577\":1,\"580\":1,\"583\":1,\"586\":1,\"589\":1,\"591\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1094\":1},\"1\":{\"21\":2,\"104\":1,\"115\":2,\"712\":3,\"725\":3,\"726\":3,\"747\":1,\"748\":1,\"749\":3,\"754\":7,\"770\":4,\"771\":3,\"772\":3,\"800\":3,\"809\":3,\"810\":3,\"813\":2,\"818\":3,\"826\":7,\"858\":3,\"861\":1,\"886\":3,\"935\":1,\"1049\":4,\"1050\":4,\"1052\":7,\"1056\":4,\"1068\":2,\"1076\":5,\"1077\":5,\"1078\":1,\"1079\":4,\"1093\":2,\"1094\":4,\"1115\":2,\"1133\":1,\"1140\":1,\"1148\":6,\"1149\":4,\"1150\":4,\"1167\":1,\"1168\":1,\"1169\":1,\"1196\":1,\"1197\":1,\"1203\":6,\"1204\":1,\"1269\":2,\"1271\":1,\"1272\":3,\"1273\":1,\"1505\":4,\"1669\":4,\"1771\":4,\"1778\":2,\"1787\":6,\"1788\":6,\"1798\":7,\"1804\":6,\"1805\":2,\"1850\":2,\"1851\":8,\"1852\":2,\"1874\":7,\"1877\":2,\"1878\":7,\"1960\":1,\"1971\":3,\"2001\":1,\"2004\":1,\"2026\":1,\"2029\":3,\"2054\":6,\"2090\":8,\"2243\":8,\"2244\":8,\"2255\":7,\"2264\":8,\"2279\":8,\"2440\":2,\"2558\":2,\"2564\":1,\"2568\":1}}],[\"posenc\",{\"0\":{\"722\":1},\"1\":{\"722\":1}}],[\"possibly\",{\"1\":{\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"2046\":1}}],[\"possible\",{\"1\":{\"5\":1,\"24\":1,\"77\":1,\"78\":1,\"109\":1,\"1142\":2,\"1186\":2,\"1210\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":2,\"1551\":1,\"1553\":1,\"2389\":1,\"2397\":1,\"2408\":1,\"2419\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2508\":1,\"2525\":1,\"2533\":1,\"2545\":1}}],[\"possibilities\",{\"1\":{\"82\":1}}],[\"postdecoder\",{\"0\":{\"2024\":2,\"2028\":2},\"1\":{\"2024\":2,\"2027\":1,\"2028\":2}}],[\"postfrontends=none\",{\"1\":{\"1774\":1}}],[\"postfrontend\",{\"1\":{\"1773\":1,\"1791\":1}}],[\"postfix\",{\"1\":{\"1190\":2}}],[\"postact\",{\"1\":{\"1242\":1}}],[\"postact=\",{\"1\":{\"1241\":1}}],[\"postnet\",{\"0\":{\"802\":1},\"1\":{\"802\":5,\"821\":6,\"826\":8,\"1778\":4,\"1852\":4,\"2002\":9,\"2003\":3,\"2078\":11,\"2086\":12,\"2087\":12,\"2090\":12,\"2095\":9,\"2243\":12,\"2244\":12,\"2255\":12,\"2263\":9,\"2264\":12,\"2279\":12}}],[\"postnets\",{\"1\":{\"755\":2,\"822\":2,\"2000\":2,\"2088\":2,\"2091\":2,\"2245\":2,\"2256\":2,\"2280\":2}}],[\"posteriorencoder\",{\"0\":{\"1863\":1},\"1\":{\"1863\":2}}],[\"posterior\",{\"0\":{\"929\":1,\"1863\":1},\"1\":{\"705\":2,\"929\":2,\"1804\":21,\"1805\":6,\"1853\":1,\"1854\":2,\"1863\":3,\"1877\":6,\"1878\":21}}],[\"postencoder\",{\"0\":{\"1123\":2,\"1192\":2,\"1195\":2,\"2026\":2,\"2029\":2},\"1\":{\"112\":1,\"1123\":2,\"1171\":1,\"1172\":1,\"1191\":1,\"1192\":3,\"1195\":3,\"1206\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2026\":3,\"2027\":2,\"2028\":1,\"2029\":2,\"2076\":1}}],[\"posting\",{\"1\":{\"127\":1}}],[\"post\",{\"0\":{\"1774\":1,\"1791\":1},\"1\":{\"44\":1,\"691\":2,\"697\":2,\"797\":3,\"1244\":1,\"1252\":1,\"1539\":1,\"1543\":2,\"1680\":1,\"1774\":1,\"1791\":1}}],[\"pos\",{\"0\":{\"886\":1,\"935\":1},\"1\":{\"21\":2,\"104\":1,\"115\":7,\"116\":2,\"712\":4,\"714\":2,\"715\":2,\"716\":2,\"718\":2,\"719\":2,\"720\":2,\"721\":2,\"725\":1,\"726\":4,\"747\":1,\"749\":2,\"754\":1,\"770\":2,\"771\":2,\"809\":2,\"821\":1,\"822\":2,\"826\":2,\"858\":4,\"861\":2,\"886\":4,\"911\":3,\"929\":1,\"935\":1,\"1049\":6,\"1050\":6,\"1052\":10,\"1056\":6,\"1065\":2,\"1066\":2,\"1068\":4,\"1076\":6,\"1077\":1,\"1093\":6,\"1115\":6,\"1133\":2,\"1140\":3,\"1141\":3,\"1148\":6,\"1149\":4,\"1150\":4,\"1167\":1,\"1168\":1,\"1169\":4,\"1170\":3,\"1196\":1,\"1197\":1,\"1203\":5,\"1204\":1,\"1269\":4,\"1271\":1,\"1272\":2,\"1273\":1,\"1505\":3,\"1669\":2,\"1771\":3,\"1778\":4,\"1787\":1,\"1788\":2,\"1798\":1,\"1804\":1,\"1805\":1,\"1850\":4,\"1851\":12,\"1852\":4,\"1874\":1,\"1877\":1,\"1878\":1,\"1960\":1,\"1971\":2,\"2000\":1,\"2001\":2,\"2003\":3,\"2004\":2,\"2026\":3,\"2029\":2,\"2054\":6,\"2090\":8,\"2095\":2,\"2243\":12,\"2244\":12,\"2255\":12,\"2263\":2,\"2264\":7,\"2279\":12,\"2440\":3,\"2564\":3}}],[\"pruning\",{\"1\":{\"705\":1}}],[\"pruned\",{\"1\":{\"113\":6,\"706\":1,\"795\":1,\"815\":1,\"1057\":7,\"1427\":1}}],[\"prune\",{\"1\":{\"23\":1,\"113\":1,\"119\":1,\"700\":1,\"917\":2,\"1048\":2,\"1138\":1,\"1139\":1}}],[\"practical\",{\"1\":{\"166\":1,\"177\":1}}],[\"private\",{\"1\":{\"2565\":1}}],[\"privilege\",{\"1\":{\"134\":2}}],[\"priority\",{\"1\":{\"2437\":1,\"2563\":1}}],[\"prior=none\",{\"1\":{\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1}}],[\"prior=true\",{\"1\":{\"1829\":1}}],[\"priordecoder\",{\"0\":{\"1788\":1},\"1\":{\"1788\":2}}],[\"prior\",{\"0\":{\"1788\":1},\"1\":{\"1618\":4,\"1619\":4,\"1638\":4,\"1639\":1,\"1640\":1,\"1788\":3,\"1829\":2}}],[\"priors\",{\"1\":{\"1566\":1}}],[\"primitives\",{\"1\":{\"1144\":2,\"1228\":2,\"1345\":2,\"1347\":2}}],[\"primer\",{\"1\":{\"691\":1,\"693\":1,\"697\":4,\"797\":3,\"1219\":2,\"2457\":1}}],[\"primary\",{\"1\":{\"363\":2,\"1526\":1}}],[\"printout\",{\"1\":{\"2457\":1}}],[\"print\",{\"0\":{\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1,\"2572\":1},\"1\":{\"49\":1,\"62\":5,\"98\":1,\"171\":1,\"174\":1,\"175\":2,\"185\":1,\"194\":2,\"203\":3,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":3,\"218\":2,\"222\":1,\"223\":1,\"224\":2,\"225\":2,\"229\":1,\"230\":1,\"231\":3,\"232\":2,\"238\":3,\"240\":5,\"301\":4,\"760\":1,\"778\":1,\"831\":1,\"1476\":1,\"1598\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2084\":1,\"2089\":1,\"2099\":3,\"2294\":1,\"2359\":4,\"2360\":3,\"2365\":2,\"2369\":2,\"2372\":4,\"2386\":1,\"2389\":2,\"2392\":6,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2408\":2,\"2422\":2,\"2425\":6,\"2432\":3,\"2440\":3,\"2449\":2,\"2456\":5,\"2458\":2,\"2459\":2,\"2460\":5,\"2465\":2,\"2472\":3,\"2474\":3,\"2476\":7,\"2478\":1,\"2481\":2,\"2487\":2,\"2491\":2,\"2497\":4,\"2500\":7,\"2501\":2,\"2503\":2,\"2508\":2,\"2510\":2,\"2514\":2,\"2515\":2,\"2521\":9,\"2522\":3,\"2523\":3,\"2525\":2,\"2528\":6,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2542\":1,\"2543\":1,\"2545\":2,\"2548\":6,\"2554\":2,\"2556\":2,\"2559\":2,\"2560\":3,\"2564\":2,\"2568\":2,\"2569\":1,\"2572\":2,\"2580\":4,\"2581\":3,\"2582\":3,\"2600\":4,\"2606\":2,\"2607\":2,\"2610\":2,\"2614\":4,\"2615\":4,\"2617\":7,\"2623\":2,\"2624\":2,\"2627\":2,\"2632\":4,\"2633\":4,\"2635\":7,\"2648\":3,\"2649\":3,\"2655\":2,\"2659\":2,\"2660\":2}}],[\"pr\",{\"1\":{\"13\":1,\"700\":1,\"1138\":1,\"1139\":1}}],[\"promise\",{\"1\":{\"2360\":3,\"2458\":3,\"2523\":3,\"2582\":3}}],[\"prompt\",{\"1\":{\"307\":6,\"2178\":2,\"2179\":2}}],[\"prosodic\",{\"1\":{\"2142\":1}}],[\"prosody\",{\"0\":{\"2142\":1,\"2146\":1},\"1\":{\"461\":2,\"2142\":2,\"2143\":2,\"2146\":1,\"2363\":8,\"2653\":8}}],[\"prosoody\",{\"1\":{\"2142\":1}}],[\"pronunciation\",{\"1\":{\"2125\":1}}],[\"prototype\",{\"0\":{\"1883\":1},\"1\":{\"1883\":4}}],[\"protocol\",{\"1\":{\"612\":1}}],[\"prodiffloss\",{\"0\":{\"2256\":1},\"1\":{\"2256\":3}}],[\"prodiff\",{\"0\":{\"2252\":1,\"2255\":3,\"2256\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2273\":1,\"2274\":1},\"1\":{\"2252\":2,\"2255\":10,\"2256\":2,\"2258\":2,\"2259\":2,\"2260\":1,\"2273\":2,\"2274\":1}}],[\"prod\",{\"1\":{\"1800\":1,\"1834\":1,\"1856\":1,\"1857\":2,\"1871\":2}}],[\"produces\",{\"1\":{\"1032\":1,\"1033\":1}}],[\"product\",{\"1\":{\"78\":1,\"680\":1,\"687\":1,\"771\":1,\"785\":1,\"809\":1,\"1069\":2,\"1076\":1,\"1209\":1}}],[\"projs\",{\"1\":{\"782\":1,\"1595\":1}}],[\"proj\",{\"1\":{\"701\":4,\"821\":2,\"1115\":2,\"1184\":1,\"1774\":1}}],[\"projects\",{\"1\":{\"2584\":1}}],[\"projector\",{\"0\":{\"2038\":2,\"2057\":2,\"2066\":2,\"2072\":2},\"1\":{\"2038\":2,\"2046\":4,\"2057\":2,\"2066\":2,\"2072\":2}}],[\"projected\",{\"1\":{\"1269\":1,\"1804\":4,\"1853\":3,\"1854\":4,\"1863\":2,\"1874\":2,\"1878\":4}}],[\"projection\",{\"0\":{\"1789\":1},\"1\":{\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"701\":2,\"758\":1,\"805\":1,\"808\":2,\"821\":2,\"892\":1,\"1064\":1,\"1145\":1,\"1184\":1,\"1201\":1,\"1222\":3,\"1269\":2,\"1282\":3,\"1470\":1,\"1609\":1,\"1761\":1,\"1763\":1,\"1765\":2,\"1774\":1,\"1789\":2,\"1800\":2,\"1804\":6,\"1805\":2,\"2584\":1,\"2585\":1}}],[\"project\",{\"1\":{\"429\":2,\"1269\":1,\"2046\":1,\"2618\":1,\"2635\":1}}],[\"propagated\",{\"1\":{\"2209\":1}}],[\"propagation\",{\"1\":{\"681\":1,\"682\":1,\"701\":1,\"702\":1,\"708\":1,\"711\":1,\"713\":1,\"735\":1,\"737\":1,\"738\":1,\"742\":1,\"745\":1,\"754\":1,\"755\":1,\"758\":1,\"762\":1,\"763\":1,\"764\":1,\"774\":1,\"786\":1,\"793\":1,\"802\":1,\"804\":1,\"820\":1,\"821\":1,\"822\":1,\"826\":1,\"830\":1,\"835\":1,\"838\":1,\"1140\":1,\"1148\":1,\"1169\":1,\"1203\":1,\"1765\":1,\"1787\":1,\"1798\":1,\"1800\":1,\"1803\":1,\"1804\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1838\":1,\"1840\":1,\"1841\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1851\":1,\"1855\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1876\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2044\":1,\"2049\":2,\"2054\":1,\"2078\":1,\"2081\":1,\"2083\":1,\"2086\":2,\"2087\":2,\"2088\":1,\"2090\":1,\"2091\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2252\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2279\":1,\"2280\":1,\"2467\":1}}],[\"proposing\",{\"1\":{\"2090\":1}}],[\"proposed\",{\"1\":{\"120\":2,\"668\":1,\"1581\":1,\"1829\":1,\"2019\":1,\"2044\":1,\"2068\":1,\"2468\":1,\"2473\":1}}],[\"propose\",{\"1\":{\"115\":1,\"117\":1}}],[\"proper\",{\"1\":{\"2584\":1}}],[\"properly\",{\"1\":{\"944\":1}}],[\"property\",{\"1\":{\"593\":1,\"594\":2,\"612\":2,\"676\":2,\"754\":2,\"781\":1,\"792\":1,\"820\":2,\"821\":1,\"826\":2,\"944\":1,\"981\":1,\"997\":1,\"998\":1,\"1110\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1175\":1,\"1233\":1,\"1243\":1,\"1252\":2,\"1253\":4,\"1254\":3,\"1279\":1,\"1280\":1,\"1363\":1,\"1367\":1,\"1374\":1,\"1375\":1,\"1377\":2,\"1436\":1,\"1438\":2,\"1446\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1558\":1,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1570\":6,\"1571\":2,\"1600\":1,\"1604\":1,\"1611\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1626\":1,\"1638\":1,\"1644\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1667\":4,\"1669\":1,\"1671\":1,\"1719\":1,\"1778\":2,\"1805\":2,\"1850\":2,\"1852\":2,\"1862\":1,\"1877\":2,\"1880\":1,\"1977\":1,\"1979\":1,\"1980\":2,\"1985\":1,\"2022\":1,\"2077\":2,\"2235\":2,\"2277\":2,\"2289\":1,\"2294\":1,\"2642\":1}}],[\"profile\",{\"1\":{\"135\":1}}],[\"proc\",{\"1\":{\"130\":1,\"1132\":1,\"1462\":2,\"1463\":2,\"1543\":2,\"1670\":1,\"1671\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2064\":1,\"2070\":1}}],[\"proceed\",{\"1\":{\"2393\":1,\"2394\":1,\"2529\":1,\"2530\":1}}],[\"proceedings\",{\"1\":{\"130\":5,\"1524\":1,\"2030\":1}}],[\"procedures\",{\"1\":{\"91\":1,\"170\":1}}],[\"procedure\",{\"0\":{\"2429\":1},\"1\":{\"16\":1,\"113\":1,\"120\":1,\"238\":1,\"295\":1,\"745\":1,\"746\":1,\"2380\":1,\"2387\":1,\"2406\":1,\"2407\":1,\"2459\":2,\"2463\":1}}],[\"processor\",{\"0\":{\"1071\":1,\"1373\":1},\"1\":{\"1071\":1,\"1373\":1}}],[\"processed\",{\"1\":{\"1058\":2,\"1427\":1,\"1432\":2,\"1510\":2,\"1598\":1,\"1643\":2,\"1652\":1,\"1654\":1,\"1715\":1,\"2086\":1,\"2087\":1,\"2564\":1}}],[\"processerror\",{\"1\":{\"593\":1,\"594\":1}}],[\"processes=none\",{\"1\":{\"997\":1}}],[\"processes\",{\"1\":{\"37\":1,\"127\":1,\"148\":2,\"251\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"596\":1,\"628\":1,\"997\":3,\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"2121\":1,\"2122\":1}}],[\"processe\",{\"1\":{\"37\":1}}],[\"processing\",{\"0\":{\"2725\":1},\"1\":{\"32\":2,\"46\":1,\"104\":3,\"130\":4,\"152\":1,\"235\":2,\"237\":1,\"238\":1,\"576\":1,\"691\":1,\"692\":1,\"693\":1,\"697\":1,\"705\":1,\"754\":1,\"797\":1,\"987\":1,\"1149\":3,\"1150\":4,\"1462\":1,\"1463\":1,\"1543\":1,\"1670\":5,\"1671\":5,\"1696\":1,\"1698\":1,\"1705\":1,\"2097\":2,\"2243\":1,\"2354\":4,\"2372\":1,\"2373\":1,\"2380\":1,\"2387\":2,\"2388\":4,\"2421\":4,\"2433\":1,\"2468\":3,\"2473\":1,\"2524\":4,\"2544\":4,\"2555\":1,\"2574\":1}}],[\"process\",{\"0\":{\"65\":1,\"644\":1},\"1\":{\"5\":1,\"17\":1,\"65\":2,\"69\":1,\"99\":1,\"113\":1,\"122\":2,\"134\":1,\"149\":1,\"377\":1,\"593\":2,\"594\":2,\"595\":1,\"644\":1,\"691\":1,\"692\":2,\"697\":1,\"727\":1,\"728\":1,\"731\":1,\"794\":2,\"797\":3,\"959\":1,\"999\":1,\"1127\":1,\"1198\":1,\"1430\":2,\"1462\":1,\"1464\":1,\"1528\":1,\"1538\":2,\"1895\":1,\"1900\":1,\"2201\":1,\"2212\":1,\"2260\":1,\"2368\":1,\"2371\":1,\"2372\":1,\"2398\":1,\"2401\":1,\"2412\":1,\"2429\":1,\"2430\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2534\":1,\"2537\":1,\"2554\":1,\"2555\":1,\"2600\":3,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1}}],[\"probably\",{\"1\":{\"2435\":1,\"2441\":1,\"2561\":1}}],[\"probablities\",{\"1\":{\"704\":1,\"705\":1}}],[\"probabilistic\",{\"1\":{\"2302\":1}}],[\"probabiliy\",{\"1\":{\"1638\":1}}],[\"probabilities\",{\"1\":{\"605\":2,\"620\":1,\"676\":1,\"692\":1,\"693\":1,\"710\":3,\"759\":1,\"820\":1,\"821\":1,\"826\":1,\"917\":2,\"1048\":2,\"1060\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"2002\":1,\"2079\":1,\"2095\":1,\"2263\":1,\"2264\":1}}],[\"probability\",{\"1\":{\"115\":1,\"249\":2,\"307\":2,\"321\":2,\"408\":2,\"605\":1,\"676\":1,\"787\":1,\"838\":1,\"914\":1,\"1063\":1,\"1068\":1,\"1093\":1,\"1107\":1,\"1141\":3,\"1142\":1,\"1170\":1,\"1186\":1,\"1198\":3,\"1205\":1,\"1210\":1,\"1219\":1,\"1220\":1,\"1222\":1,\"1256\":3,\"1269\":7,\"1282\":1,\"1334\":1,\"1352\":1,\"1557\":1,\"1618\":2,\"1619\":2,\"1623\":1,\"1637\":1,\"1638\":5,\"1829\":1,\"1841\":2,\"1851\":1,\"1889\":2,\"1932\":1,\"2197\":3,\"2403\":1,\"2539\":1}}],[\"probs\",{\"1\":{\"676\":1,\"706\":1,\"759\":2,\"929\":1,\"1142\":7,\"1155\":5,\"1211\":1,\"1224\":1,\"1287\":1,\"1293\":1}}],[\"prob\",{\"1\":{\"605\":1,\"705\":2,\"706\":1,\"710\":1,\"1115\":6,\"1180\":2,\"1269\":4,\"1371\":1,\"1376\":1,\"1552\":1,\"1618\":2,\"1619\":2,\"1638\":1,\"1841\":2,\"2002\":1,\"2095\":1,\"2178\":3,\"2179\":3,\"2184\":3,\"2191\":3,\"2194\":4,\"2195\":3,\"2197\":6,\"2200\":3,\"2263\":1,\"2264\":1}}],[\"problem\",{\"0\":{\"128\":1},\"1\":{\"127\":1,\"148\":1,\"2372\":1}}],[\"proxy=\",{\"1\":{\"70\":1}}],[\"proxy\",{\"1\":{\"70\":2}}],[\"provides\",{\"1\":{\"32\":1,\"57\":1,\"98\":1,\"206\":1,\"745\":1,\"746\":1,\"1132\":1,\"1255\":2,\"2361\":1,\"2468\":1,\"2516\":1,\"2576\":1,\"2586\":1,\"2597\":1,\"2601\":1,\"2618\":1,\"2646\":1,\"2650\":1}}],[\"provided\",{\"1\":{\"25\":1,\"35\":1,\"124\":1,\"295\":1,\"506\":2,\"858\":1,\"861\":1,\"866\":1,\"918\":1,\"933\":1,\"1046\":1,\"1524\":1,\"1618\":1,\"1619\":1,\"1639\":2,\"1804\":3,\"1851\":3,\"1859\":1,\"1878\":4,\"2001\":3,\"2002\":3,\"2004\":3,\"2086\":3,\"2087\":3,\"2090\":3,\"2095\":3,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2279\":3,\"2565\":1,\"2568\":1,\"2584\":1}}],[\"provide\",{\"1\":{\"17\":1,\"25\":1,\"56\":1,\"60\":1,\"64\":1,\"204\":1,\"1274\":1,\"1639\":1,\"2019\":1,\"2354\":1,\"2388\":1,\"2394\":1,\"2421\":1,\"2452\":1,\"2461\":1,\"2508\":1,\"2512\":1,\"2514\":1,\"2524\":1,\"2530\":1,\"2544\":1,\"2593\":1,\"2600\":1,\"2635\":1,\"2640\":2,\"2657\":1,\"2659\":1}}],[\"progressive\",{\"1\":{\"1605\":2,\"2255\":1}}],[\"progressive=\",{\"1\":{\"1605\":1}}],[\"progressbar=true\",{\"1\":{\"2368\":1,\"2371\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1}}],[\"progressbar\",{\"1\":{\"343\":2,\"350\":2,\"368\":2}}],[\"progress\",{\"1\":{\"17\":1,\"197\":1,\"2373\":1,\"2385\":1,\"2430\":1,\"2555\":1,\"2592\":4,\"2596\":3}}],[\"programming\",{\"1\":{\"44\":1}}],[\"program\",{\"1\":{\"1\":1,\"602\":1,\"603\":1,\"622\":1,\"636\":1,\"649\":1,\"650\":1,\"657\":1,\"855\":1,\"867\":1,\"870\":1,\"937\":1,\"938\":1,\"1032\":2,\"1033\":2,\"1034\":2}}],[\"pretained\",{\"1\":{\"2490\":1,\"2609\":1,\"2626\":1}}],[\"pretraining\",{\"1\":{\"1181\":2}}],[\"pretrain\",{\"1\":{\"97\":1,\"1116\":2,\"1179\":1,\"1181\":2,\"1269\":3,\"1892\":1,\"1893\":1,\"2308\":1,\"2431\":3}}],[\"pretrained\",{\"0\":{\"66\":1,\"97\":1,\"195\":1,\"197\":1,\"198\":1,\"209\":1,\"213\":1,\"221\":1,\"228\":1,\"1957\":1,\"2152\":1,\"2157\":2,\"2254\":1,\"2368\":1,\"2472\":1,\"2483\":1,\"2486\":1,\"2489\":1,\"2499\":1,\"2589\":1,\"2590\":1,\"2605\":1,\"2609\":1,\"2617\":1,\"2620\":1,\"2622\":1,\"2626\":1,\"2635\":1,\"2648\":1,\"2649\":1},\"1\":{\"97\":1,\"110\":1,\"139\":1,\"164\":1,\"165\":1,\"201\":1,\"202\":1,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"243\":1,\"295\":2,\"735\":1,\"820\":2,\"1178\":3,\"1179\":2,\"1180\":4,\"1181\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1239\":1,\"1240\":1,\"1269\":1,\"1454\":4,\"1791\":2,\"1792\":1,\"1957\":2,\"2152\":1,\"2157\":2,\"2158\":5,\"2254\":2,\"2364\":1,\"2431\":5,\"2432\":3,\"2454\":1,\"2467\":2,\"2468\":1,\"2472\":1,\"2473\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2481\":3,\"2485\":1,\"2490\":1,\"2494\":1,\"2497\":1,\"2507\":1,\"2508\":1,\"2510\":2,\"2512\":2,\"2513\":1,\"2515\":1,\"2519\":2,\"2520\":1,\"2573\":1,\"2584\":6,\"2585\":2,\"2599\":3,\"2600\":3,\"2618\":3,\"2630\":1,\"2648\":1,\"2649\":1,\"2654\":1,\"2657\":2,\"2658\":1}}],[\"preinstalled\",{\"1\":{\"2362\":1,\"2504\":1,\"2576\":1,\"2651\":1}}],[\"presenters\",{\"1\":{\"2618\":1}}],[\"presented\",{\"1\":{\"2387\":1}}],[\"preservation`\",{\"1\":{\"2003\":2}}],[\"preserving\",{\"1\":{\"837\":1,\"1619\":1}}],[\"press\",{\"1\":{\"1524\":1}}],[\"prenorm=true\",{\"1\":{\"1252\":1,\"1254\":1}}],[\"prenorm\",{\"1\":{\"1244\":2,\"1252\":1}}],[\"prenet\",{\"0\":{\"803\":1,\"1994\":1},\"1\":{\"803\":4,\"804\":3,\"821\":4,\"826\":8,\"1851\":1,\"1994\":2,\"2002\":6,\"2003\":3,\"2078\":6,\"2086\":4,\"2087\":4,\"2090\":1,\"2095\":6,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":6,\"2264\":8,\"2279\":1}}],[\"preemp\",{\"1\":{\"1207\":1}}],[\"preempahsis\",{\"1\":{\"1132\":1}}],[\"preemphasis\",{\"0\":{\"1943\":1},\"1\":{\"1943\":2}}],[\"preemph\",{\"1\":{\"1132\":2}}],[\"preencoder\",{\"0\":{\"1125\":2,\"1198\":1,\"1201\":1,\"1256\":1},\"1\":{\"102\":5,\"112\":1,\"1113\":1,\"1125\":2,\"1132\":1,\"1171\":1,\"1172\":1,\"1198\":2,\"1201\":2,\"1206\":1,\"1256\":2,\"1892\":1,\"1893\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":1,\"2439\":1,\"2440\":2}}],[\"prelu\",{\"1\":{\"1115\":2,\"1578\":1,\"1579\":1,\"1580\":1,\"1658\":1,\"1659\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1665\":1}}],[\"precomputed\",{\"1\":{\"1198\":1}}],[\"precalculated\",{\"1\":{\"754\":2}}],[\"precision=true\",{\"1\":{\"1339\":1}}],[\"precision\",{\"0\":{\"81\":1},\"1\":{\"48\":1,\"1181\":1,\"1524\":1,\"1525\":1,\"2440\":1,\"2558\":1}}],[\"prevent\",{\"1\":{\"1132\":1,\"1180\":1,\"2584\":1}}],[\"prev\",{\"1\":{\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":4,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"691\":2,\"697\":2,\"758\":2,\"797\":2,\"1069\":1,\"1119\":1,\"1140\":2,\"1148\":2,\"1149\":6,\"1150\":6,\"1169\":2,\"1178\":2,\"1179\":2,\"1180\":2,\"1181\":2,\"1200\":2,\"1203\":2,\"1215\":1,\"1221\":2,\"1222\":1,\"1269\":2,\"1272\":2,\"1282\":1,\"1975\":6,\"2029\":2,\"2194\":3,\"2592\":4}}],[\"previously\",{\"1\":{\"29\":1,\"99\":1,\"2584\":3}}],[\"previous\",{\"1\":{\"5\":1,\"38\":1,\"122\":1,\"161\":1,\"612\":1,\"650\":1,\"677\":1,\"678\":2,\"679\":2,\"681\":2,\"682\":3,\"684\":2,\"685\":2,\"686\":1,\"687\":1,\"688\":2,\"689\":2,\"697\":1,\"711\":2,\"758\":2,\"821\":1,\"827\":1,\"836\":2,\"1049\":2,\"1050\":2,\"1052\":2,\"1056\":2,\"1058\":2,\"1068\":2,\"1069\":2,\"1076\":4,\"1077\":2,\"1104\":3,\"1429\":1,\"1670\":1,\"1671\":1,\"2002\":1,\"2078\":1,\"2095\":1,\"2263\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2409\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2450\":1,\"2459\":2,\"2466\":1,\"2482\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2542\":1,\"2568\":1,\"2570\":1,\"2585\":1}}],[\"prediciton\",{\"1\":{\"2401\":1,\"2537\":1}}],[\"predicting\",{\"1\":{\"2275\":1,\"2412\":1,\"2419\":1}}],[\"predictive\",{\"1\":{\"1890\":2}}],[\"predictions\",{\"1\":{\"235\":1,\"802\":1,\"803\":1,\"804\":1,\"2263\":1}}],[\"prediction\",{\"1\":{\"108\":3,\"109\":1,\"175\":1,\"194\":1,\"501\":1,\"738\":1,\"750\":2,\"802\":2,\"803\":2,\"925\":1,\"1059\":4,\"1106\":1,\"1107\":1,\"1108\":1,\"1173\":4,\"1252\":1,\"1253\":1,\"1254\":1,\"1808\":1,\"1850\":1,\"1877\":1,\"1878\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2078\":3,\"2081\":2,\"2083\":2,\"2087\":1,\"2090\":1,\"2095\":1,\"2244\":1,\"2263\":1,\"2403\":1,\"2414\":1,\"2473\":1,\"2539\":1}}],[\"predicted\",{\"1\":{\"737\":2,\"750\":1,\"768\":1,\"774\":1,\"802\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1375\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":2,\"1529\":1,\"1534\":1,\"1539\":1,\"1558\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1669\":1,\"1671\":1,\"1767\":1,\"1772\":1,\"1781\":4,\"1787\":1,\"1808\":2,\"1850\":2,\"1851\":3,\"1877\":2,\"1980\":1,\"2001\":1,\"2077\":1,\"2235\":1,\"2265\":1,\"2277\":1,\"2301\":2,\"2303\":1,\"2467\":1}}],[\"predicts\",{\"1\":{\"736\":1,\"802\":1}}],[\"predict\",{\"1\":{\"408\":2,\"605\":3,\"710\":5,\"1505\":2,\"1523\":2,\"1524\":2,\"1525\":2,\"1528\":2,\"1534\":2,\"1539\":2,\"1551\":1,\"1553\":1,\"1558\":2,\"1626\":2,\"1654\":1,\"1658\":2,\"1669\":2,\"2086\":1,\"2087\":1,\"2412\":1,\"2467\":1,\"2472\":1,\"2521\":2,\"2522\":1,\"2523\":1,\"2648\":1}}],[\"predictor=\",{\"1\":{\"1646\":1}}],[\"predictors\",{\"0\":{\"1557\":1,\"1615\":1,\"1623\":1,\"1637\":1},\"1\":{\"1557\":1,\"1615\":1,\"1623\":1,\"1637\":1}}],[\"predictor\",{\"0\":{\"736\":1,\"738\":1,\"1623\":1,\"1771\":1,\"1772\":1,\"1787\":1,\"1868\":1,\"2265\":1},\"1\":{\"24\":1,\"605\":4,\"710\":5,\"736\":4,\"737\":1,\"738\":3,\"754\":7,\"755\":2,\"1451\":1,\"1557\":2,\"1615\":3,\"1623\":3,\"1637\":2,\"1646\":3,\"1647\":1,\"1771\":1,\"1772\":3,\"1778\":5,\"1787\":2,\"1804\":4,\"1805\":2,\"1850\":14,\"1851\":42,\"1852\":14,\"1868\":3,\"1877\":5,\"1878\":13,\"1879\":6,\"1983\":2,\"2003\":3,\"2087\":12,\"2090\":12,\"2091\":4,\"2243\":13,\"2244\":42,\"2245\":6,\"2255\":42,\"2256\":6,\"2265\":4,\"2279\":42,\"2280\":6}}],[\"predcited\",{\"1\":{\"1611\":1}}],[\"pred\",{\"1\":{\"1059\":7,\"1107\":3,\"1110\":1,\"1145\":1,\"1173\":7,\"1371\":3,\"1850\":4,\"1877\":4,\"1890\":4,\"1892\":2}}],[\"predefined\",{\"1\":{\"173\":1,\"872\":1,\"873\":1,\"874\":1}}],[\"prerequisite\",{\"0\":{\"101\":1}}],[\"prepred\",{\"1\":{\"2405\":1,\"2541\":1}}],[\"preprint\",{\"1\":{\"130\":3,\"1660\":2}}],[\"preprocessing\",{\"0\":{\"2413\":1},\"1\":{\"84\":2,\"522\":1,\"987\":1,\"2413\":1,\"2414\":1,\"2424\":1,\"2547\":1}}],[\"preprocessor\",{\"0\":{\"2171\":1,\"2178\":1,\"2179\":1,\"2181\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2200\":1,\"2207\":1,\"2210\":1,\"2211\":1},\"1\":{\"79\":2,\"429\":2,\"2171\":2,\"2178\":1,\"2179\":1,\"2181\":1,\"2184\":3,\"2191\":1,\"2194\":2,\"2195\":2,\"2196\":3,\"2197\":3,\"2200\":3,\"2207\":2,\"2210\":1,\"2211\":2}}],[\"preprocess\",{\"0\":{\"2347\":1,\"2348\":1,\"2349\":1,\"2720\":1},\"1\":{\"56\":1,\"245\":2,\"247\":2,\"249\":2,\"251\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2,\"276\":2,\"281\":2,\"506\":2,\"522\":2,\"525\":2,\"533\":2,\"987\":5,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":2,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":2,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2182\":1,\"2189\":1,\"2347\":1,\"2348\":2,\"2349\":1,\"2372\":1,\"2474\":5,\"2649\":5}}],[\"prep\",{\"1\":{\"91\":1,\"96\":2,\"110\":1,\"140\":1,\"161\":1,\"244\":1,\"2373\":1,\"2382\":1,\"2384\":1,\"2387\":1,\"2394\":1,\"2423\":1,\"2430\":1,\"2431\":1,\"2530\":1,\"2546\":1,\"2555\":1,\"2568\":4,\"2569\":1,\"2585\":1}}],[\"preparation\",{\"0\":{\"168\":1,\"180\":1,\"182\":1,\"237\":1,\"239\":1,\"2373\":1,\"2380\":1,\"2385\":1,\"2387\":1,\"2395\":1,\"2412\":1,\"2430\":1,\"2531\":1,\"2555\":1,\"2568\":1,\"2638\":1},\"1\":{\"16\":2,\"85\":1,\"91\":2,\"93\":1,\"149\":1,\"168\":1,\"169\":1,\"179\":1,\"181\":1,\"235\":2,\"237\":1,\"2372\":1,\"2373\":1,\"2384\":1,\"2385\":3,\"2387\":2,\"2397\":2,\"2412\":2,\"2429\":1,\"2430\":1,\"2452\":1,\"2468\":1,\"2533\":2,\"2554\":1,\"2555\":1,\"2565\":1,\"2568\":1,\"2584\":1}}],[\"prepared\",{\"1\":{\"48\":1,\"169\":1,\"181\":1,\"2377\":1,\"2436\":1,\"2543\":1,\"2562\":1,\"2568\":1}}],[\"prepare\",{\"0\":{\"910\":1,\"911\":1,\"1739\":1,\"2347\":1,\"2397\":1,\"2533\":1,\"2592\":1,\"2639\":1,\"2640\":1},\"1\":{\"16\":1,\"47\":1,\"74\":2,\"133\":1,\"161\":1,\"235\":2,\"237\":1,\"731\":1,\"884\":1,\"898\":1,\"910\":2,\"911\":2,\"1739\":2,\"1897\":1,\"2347\":1,\"2372\":2,\"2380\":1,\"2424\":1,\"2430\":1,\"2431\":1,\"2547\":1,\"2555\":1,\"2564\":1,\"2568\":2,\"2600\":2,\"2638\":1,\"2639\":1}}],[\"pre\",{\"0\":{\"2359\":1,\"2415\":1,\"2522\":1,\"2573\":1,\"2574\":1,\"2581\":1,\"2584\":1,\"2588\":1,\"2607\":1,\"2624\":1},\"1\":{\"24\":2,\"28\":1,\"30\":2,\"155\":3,\"156\":1,\"157\":1,\"158\":1,\"164\":2,\"200\":1,\"245\":1,\"635\":5,\"637\":1,\"640\":1,\"642\":1,\"643\":1,\"646\":2,\"658\":1,\"677\":1,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"691\":16,\"692\":2,\"693\":7,\"697\":16,\"698\":5,\"699\":5,\"758\":1,\"795\":1,\"797\":5,\"857\":7,\"987\":1,\"1179\":2,\"1198\":2,\"1244\":1,\"1252\":1,\"1255\":1,\"1543\":2,\"1553\":2,\"1554\":2,\"1658\":2,\"1659\":2,\"1664\":2,\"1665\":2,\"1715\":1,\"1739\":1,\"1834\":2,\"1943\":3,\"2027\":1,\"2097\":2,\"2287\":1,\"2295\":1,\"2296\":1,\"2355\":2,\"2377\":2,\"2410\":1,\"2415\":1,\"2429\":1,\"2446\":1,\"2479\":1,\"2500\":1,\"2501\":1,\"2522\":1,\"2552\":1,\"2573\":2,\"2574\":3,\"2575\":1,\"2581\":1,\"2583\":1,\"2584\":9,\"2585\":5,\"2588\":1}}],[\"prebuilt\",{\"1\":{\"5\":2,\"8\":3,\"134\":1}}],[\"pref\",{\"1\":{\"895\":4}}],[\"preforms\",{\"1\":{\"803\":1}}],[\"prefetch\",{\"1\":{\"997\":2}}],[\"prefetch=1\",{\"1\":{\"997\":1}}],[\"prefetching\",{\"1\":{\"148\":2}}],[\"preferable\",{\"1\":{\"5\":1}}],[\"prefixes\",{\"1\":{\"1896\":1,\"2344\":1}}],[\"prefixparser\",{\"0\":{\"673\":1},\"1\":{\"673\":2}}],[\"prefix=\",{\"1\":{\"610\":1}}],[\"prefix\",{\"0\":{\"704\":1,\"705\":1,\"895\":1,\"2232\":1},\"1\":{\"1\":1,\"23\":7,\"27\":1,\"28\":1,\"45\":2,\"119\":1,\"135\":2,\"150\":1,\"249\":2,\"377\":2,\"610\":2,\"672\":1,\"673\":1,\"691\":2,\"692\":1,\"695\":2,\"696\":2,\"697\":4,\"700\":6,\"704\":1,\"705\":3,\"706\":5,\"734\":5,\"773\":4,\"796\":3,\"797\":1,\"815\":4,\"817\":2,\"828\":4,\"884\":1,\"895\":4,\"913\":4,\"1048\":1,\"1133\":2,\"1138\":6,\"1139\":6,\"1190\":6,\"1214\":2,\"1221\":2,\"1244\":4,\"1273\":2,\"1957\":4,\"1958\":2,\"1959\":2,\"1960\":4,\"2001\":2,\"2181\":1,\"2184\":3,\"2200\":3,\"2232\":2,\"2344\":2,\"2492\":1,\"2628\":1}}],[\"lb7po2rppr4fqydtjbz7jmiinx\",{\"1\":{\"2372\":1,\"2497\":1,\"2614\":1,\"2632\":1}}],[\"lv2\",{\"1\":{\"1761\":1,\"1763\":1,\"1805\":1}}],[\"lv1\",{\"1\":{\"1761\":1,\"1763\":1,\"1805\":1}}],[\"l^\",{\"1\":{\"1695\":3}}],[\"l^h\",{\"1\":{\"1695\":1}}],[\"lyric\",{\"1\":{\"1393\":1}}],[\"l=16\",{\"1\":{\"2063\":1,\"2074\":1,\"2075\":1}}],[\"l=none\",{\"1\":{\"1245\":2,\"1247\":1,\"1248\":2}}],[\"l=sequence\",{\"1\":{\"1243\":1}}],[\"lf\",{\"1\":{\"1203\":1}}],[\"lfs\",{\"1\":{\"99\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2648\":1,\"2649\":1}}],[\"ld\",{\"1\":{\"950\":1,\"968\":1}}],[\"ldc2004t12\",{\"1\":{\"1400\":1}}],[\"ldc94s13b\",{\"1\":{\"3\":2}}],[\"ldc93s6b\",{\"1\":{\"3\":2}}],[\"ldc\",{\"1\":{\"3\":5,\"1400\":1}}],[\"lcmv\",{\"0\":{\"1705\":1},\"1\":{\"1705\":2}}],[\"lc\",{\"1\":{\"294\":2,\"2568\":1,\"2638\":1}}],[\"lj\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"ljy8npvtkmb\",{\"1\":{\"216\":1}}],[\"ljspeech\",{\"1\":{\"198\":3,\"214\":1,\"215\":1,\"216\":1,\"243\":1,\"282\":1,\"295\":28,\"2363\":13,\"2506\":7,\"2510\":5,\"2653\":15}}],[\"ln\",{\"1\":{\"135\":1,\"2255\":1,\"2432\":1,\"2500\":1,\"2510\":2}}],[\"lugar\",{\"1\":{\"2457\":1}}],[\"lug\",{\"1\":{\"1695\":1}}],[\"luo\",{\"1\":{\"1462\":2,\"1463\":2}}],[\"lun\",{\"1\":{\"1214\":1}}],[\"lu\",{\"1\":{\"130\":1,\"1604\":1,\"1655\":1,\"1719\":1,\"2618\":2}}],[\"lu22c\",{\"1\":{\"130\":1}}],[\"l\",{\"1\":{\"76\":2,\"118\":5,\"144\":7,\"169\":1,\"181\":1,\"235\":1,\"236\":1,\"237\":1,\"242\":1,\"744\":3,\"754\":1,\"769\":1,\"806\":1,\"820\":3,\"821\":3,\"824\":3,\"825\":6,\"826\":3,\"884\":1,\"987\":2,\"1057\":2,\"1059\":1,\"1065\":12,\"1066\":2,\"1069\":4,\"1070\":2,\"1073\":1,\"1074\":2,\"1075\":2,\"1078\":2,\"1079\":4,\"1083\":1,\"1099\":1,\"1116\":5,\"1140\":2,\"1148\":2,\"1149\":3,\"1150\":3,\"1160\":2,\"1161\":2,\"1164\":2,\"1165\":3,\"1169\":2,\"1173\":1,\"1177\":2,\"1178\":1,\"1179\":2,\"1180\":1,\"1181\":1,\"1200\":1,\"1203\":2,\"1209\":2,\"1241\":4,\"1243\":4,\"1245\":1,\"1246\":1,\"1247\":4,\"1248\":7,\"1251\":2,\"1252\":2,\"1253\":4,\"1254\":2,\"1269\":1,\"1270\":1,\"1272\":1,\"1274\":2,\"1276\":2,\"1341\":2,\"1342\":2,\"1429\":2,\"1560\":2,\"1695\":2,\"1705\":1,\"1773\":1,\"1906\":1,\"1920\":1,\"2029\":1,\"2049\":2,\"2054\":2,\"2079\":3,\"2082\":1,\"2155\":4,\"2387\":3,\"2411\":1,\"2568\":1}}],[\"lsp\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"lsc\",{\"1\":{\"1198\":1}}],[\"lsm\",{\"1\":{\"251\":2,\"255\":2,\"259\":2,\"896\":2,\"1171\":1,\"1172\":1,\"1206\":1,\"1892\":1,\"1955\":1,\"1970\":1,\"1975\":1,\"2027\":1,\"2076\":1,\"2440\":1,\"2558\":1,\"2584\":1}}],[\"ls\",{\"1\":{\"63\":1,\"169\":1,\"179\":1,\"192\":1,\"236\":1,\"237\":1,\"238\":5,\"239\":2,\"240\":1,\"241\":2,\"242\":1,\"2372\":1,\"2373\":2,\"2385\":3,\"2387\":4,\"2411\":1,\"2429\":3,\"2430\":2,\"2554\":1,\"2555\":2,\"2566\":1,\"2567\":1}}],[\"lstms\",{\"1\":{\"1572\":2}}],[\"lstmcell\",{\"1\":{\"838\":2}}],[\"lstmp\",{\"1\":{\"251\":2}}],[\"lstm\",{\"1\":{\"21\":2,\"116\":2,\"251\":2,\"614\":1,\"685\":2,\"803\":1,\"805\":1,\"807\":1,\"821\":3,\"838\":3,\"1073\":1,\"1220\":1,\"1222\":2,\"1270\":1,\"1282\":2,\"1430\":1,\"1462\":1,\"1463\":1,\"1515\":1,\"1516\":2,\"1522\":5,\"1523\":5,\"1528\":1,\"1529\":1,\"1531\":1,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":2,\"1572\":8,\"1581\":1,\"1598\":2,\"1602\":1,\"1626\":1,\"1648\":2,\"1650\":1,\"1660\":3,\"1661\":3,\"1662\":3,\"1670\":1,\"1671\":2,\"1958\":1,\"1983\":1,\"2002\":3,\"2078\":3,\"2086\":2,\"2087\":2,\"2095\":3,\"2125\":1,\"2263\":3,\"2377\":1,\"2436\":1,\"2558\":1,\"2562\":1}}],[\"lr1e\",{\"1\":{\"2454\":1,\"2455\":2}}],[\"lrscheduler\",{\"1\":{\"2018\":1,\"2019\":2,\"2020\":1,\"2021\":1,\"2023\":1}}],[\"lrngth\",{\"1\":{\"1773\":4,\"2082\":4}}],[\"lr=none\",{\"1\":{\"1217\":1,\"1245\":1,\"1247\":1,\"1248\":1}}],[\"lr=0\",{\"1\":{\"64\":1,\"92\":2,\"174\":1,\"2022\":1,\"2378\":1,\"2437\":1,\"2563\":1}}],[\"lrate\",{\"1\":{\"792\":1}}],[\"lr\",{\"0\":{\"631\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2023\":1},\"1\":{\"62\":2,\"64\":1,\"87\":2,\"255\":4,\"259\":4,\"265\":2,\"269\":2,\"631\":4,\"668\":1,\"669\":1,\"670\":1,\"754\":1,\"826\":1,\"1243\":1,\"1245\":1,\"1248\":2,\"1781\":1,\"1972\":3,\"2018\":6,\"2019\":6,\"2020\":3,\"2021\":9,\"2022\":9,\"2023\":10,\"2264\":1,\"2440\":1,\"2494\":4,\"2558\":2,\"2584\":1}}],[\"le\",{\"1\":{\"2568\":1}}],[\"lecture\",{\"1\":{\"2410\":1}}],[\"lecun\",{\"0\":{\"897\":1},\"1\":{\"897\":2}}],[\"leq\",{\"0\":{\"1429\":1},\"1\":{\"1429\":1}}],[\"less\",{\"1\":{\"940\":1,\"1735\":1,\"2398\":1,\"2400\":1,\"2534\":1,\"2536\":1}}],[\"leyered\",{\"1\":{\"786\":1}}],[\"legnths\",{\"1\":{\"1889\":1}}],[\"legt\",{\"1\":{\"1245\":1}}],[\"legs\",{\"1\":{\"1245\":3}}],[\"legacy\",{\"1\":{\"1148\":3,\"1203\":3,\"1851\":1,\"2003\":1,\"2026\":1,\"2054\":3,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2279\":1}}],[\"legacyrelpositionalencoding\",{\"0\":{\"772\":1},\"1\":{\"772\":1}}],[\"legacyrelpositionmultiheadedattention\",{\"0\":{\"771\":1},\"1\":{\"771\":1}}],[\"legend\",{\"1\":{\"174\":1}}],[\"lexical\",{\"0\":{\"620\":1},\"1\":{\"620\":3}}],[\"lee\",{\"1\":{\"130\":1,\"1660\":2,\"1661\":2,\"1662\":2}}],[\"leibny\",{\"1\":{\"130\":1}}],[\"lead\",{\"1\":{\"2584\":1}}],[\"leads\",{\"1\":{\"1712\":1,\"1715\":1,\"2126\":1}}],[\"leaving\",{\"1\":{\"940\":1}}],[\"leave\",{\"1\":{\"778\":1,\"2584\":1}}],[\"least\",{\"1\":{\"745\":1,\"746\":1,\"940\":1,\"1735\":1,\"2201\":1,\"2638\":1}}],[\"leaky\",{\"1\":{\"1518\":1,\"1520\":1}}],[\"leakyrelu\",{\"1\":{\"115\":2,\"1096\":3,\"1198\":1,\"1765\":1,\"1778\":3,\"1800\":1,\"1801\":2,\"1803\":1,\"1805\":2,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":1,\"1849\":1,\"1850\":3,\"1851\":1,\"1852\":3,\"1856\":1,\"1857\":1,\"1858\":1,\"1861\":1,\"1866\":1,\"1867\":1,\"1870\":1,\"1871\":1,\"1877\":2}}],[\"leak\",{\"1\":{\"734\":1}}],[\"learned\",{\"1\":{\"1115\":2,\"1141\":2,\"1198\":1,\"1529\":1,\"1530\":1,\"2440\":1,\"2564\":1}}],[\"learn\",{\"1\":{\"803\":1,\"2095\":1,\"2473\":1}}],[\"learning\",{\"0\":{\"22\":1,\"28\":1,\"66\":1,\"100\":1,\"118\":1,\"2583\":1},\"1\":{\"22\":1,\"27\":1,\"28\":2,\"30\":1,\"84\":1,\"92\":1,\"100\":1,\"118\":1,\"140\":1,\"155\":1,\"244\":1,\"635\":1,\"658\":1,\"672\":1,\"754\":1,\"812\":1,\"826\":1,\"1217\":1,\"1245\":1,\"1523\":1,\"1572\":1,\"1600\":1,\"1603\":1,\"1622\":1,\"1798\":1,\"1805\":1,\"1829\":1,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"2011\":1,\"2018\":3,\"2264\":1,\"2354\":1,\"2388\":1,\"2421\":1,\"2524\":1,\"2543\":1,\"2544\":1,\"2564\":1,\"2573\":4,\"2574\":1,\"2584\":2,\"2585\":4}}],[\"learnablefourierposenc\",{\"0\":{\"770\":1},\"1\":{\"770\":1}}],[\"learnable\",{\"1\":{\"21\":2,\"115\":1,\"712\":1,\"770\":1,\"1052\":1,\"1130\":1}}],[\"letters\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"let\",{\"1\":{\"113\":1,\"170\":1,\"186\":1,\"192\":1,\"196\":1,\"197\":1,\"198\":2,\"201\":1,\"202\":1,\"203\":1,\"234\":1,\"235\":3,\"237\":1,\"240\":3,\"1695\":1,\"2359\":1,\"2365\":1,\"2372\":1,\"2373\":2,\"2375\":1,\"2377\":1,\"2385\":2,\"2387\":1,\"2411\":1,\"2417\":1,\"2418\":1,\"2426\":1,\"2429\":2,\"2430\":1,\"2435\":1,\"2436\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2521\":1,\"2549\":1,\"2553\":1,\"2554\":2,\"2555\":2,\"2559\":1,\"2561\":1,\"2562\":1,\"2569\":1,\"2580\":1,\"2600\":3,\"2655\":1,\"2660\":1}}],[\"left=0\",{\"1\":{\"1887\":1}}],[\"left=true\",{\"1\":{\"648\":1}}],[\"left\",{\"1\":{\"111\":1,\"121\":4,\"122\":1,\"333\":2,\"648\":1,\"863\":1,\"865\":1,\"908\":1,\"1048\":1,\"1049\":4,\"1050\":4,\"1052\":4,\"1056\":4,\"1058\":4,\"1068\":4,\"1076\":8,\"1077\":4,\"1093\":3,\"1101\":3,\"1143\":1,\"1478\":1,\"2099\":1,\"2102\":1}}],[\"levels\",{\"1\":{\"1573\":1,\"2197\":1}}],[\"level=20\",{\"1\":{\"1323\":1}}],[\"level\",{\"1\":{\"85\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"620\":1,\"661\":1,\"672\":1,\"676\":1,\"750\":4,\"767\":1,\"774\":2,\"781\":1,\"825\":2,\"866\":1,\"869\":1,\"877\":1,\"918\":1,\"1015\":1,\"1059\":4,\"1132\":1,\"1142\":1,\"1144\":1,\"1173\":4,\"1186\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1228\":1,\"1287\":1,\"1301\":1,\"1304\":1,\"1337\":1,\"1345\":1,\"1347\":1,\"1349\":1,\"1350\":1,\"1409\":1,\"1432\":1,\"1436\":1,\"1510\":1,\"1511\":1,\"1643\":1,\"1644\":1,\"1660\":1,\"1661\":1,\"1799\":2,\"1881\":2,\"2044\":3,\"2046\":5,\"2049\":1,\"2052\":3,\"2055\":1,\"2064\":1,\"2068\":3,\"2070\":1,\"2271\":1,\"2410\":1}}],[\"lenghts\",{\"1\":{\"1058\":1,\"1701\":1}}],[\"length160\",{\"1\":{\"2436\":2,\"2562\":2}}],[\"lengthbatchsampler\",{\"0\":{\"2008\":1},\"1\":{\"2008\":1}}],[\"lengthbonus\",{\"0\":{\"773\":1},\"1\":{\"773\":1,\"815\":1}}],[\"lengthadaptorpostencoder\",{\"0\":{\"1195\":1},\"1\":{\"1195\":1}}],[\"lengthofquery\",{\"1\":{\"809\":1}}],[\"lengthregulator\",{\"0\":{\"774\":1,\"1781\":1},\"1\":{\"774\":1,\"1781\":1}}],[\"length=128\",{\"1\":{\"2498\":1,\"2616\":1,\"2634\":1}}],[\"length=160\",{\"1\":{\"1526\":1}}],[\"length=512\",{\"1\":{\"1466\":1,\"1639\":1}}],[\"length=5000\",{\"1\":{\"800\":1}}],[\"length=true\",{\"1\":{\"952\":1,\"987\":1}}],[\"length=none\",{\"1\":{\"945\":1,\"947\":1,\"951\":1,\"953\":1,\"966\":1,\"967\":1,\"969\":1,\"970\":1,\"1539\":1,\"1826\":1,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"length=false\",{\"1\":{\"768\":1}}],[\"length=0\",{\"1\":{\"612\":1}}],[\"length256\",{\"1\":{\"2357\":3,\"2578\":3}}],[\"length2\",{\"1\":{\"76\":2,\"77\":2}}],[\"lengths=\",{\"1\":{\"1786\":1}}],[\"lengths=none\",{\"1\":{\"1243\":1,\"1257\":1,\"1983\":1,\"1986\":1}}],[\"lengths\",{\"1\":{\"60\":3,\"108\":3,\"109\":1,\"150\":1,\"501\":1,\"676\":3,\"692\":1,\"693\":1,\"697\":1,\"701\":2,\"702\":1,\"705\":1,\"735\":2,\"742\":1,\"747\":1,\"754\":4,\"755\":2,\"762\":2,\"763\":2,\"781\":2,\"797\":1,\"821\":4,\"822\":1,\"825\":14,\"826\":4,\"857\":1,\"899\":3,\"900\":7,\"901\":3,\"902\":7,\"903\":3,\"904\":2,\"1057\":19,\"1058\":2,\"1066\":1,\"1099\":3,\"1102\":5,\"1113\":4,\"1114\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1133\":3,\"1142\":4,\"1145\":2,\"1158\":1,\"1166\":2,\"1171\":8,\"1172\":6,\"1184\":1,\"1186\":10,\"1191\":1,\"1192\":1,\"1195\":1,\"1198\":1,\"1201\":1,\"1206\":4,\"1210\":10,\"1211\":1,\"1224\":1,\"1239\":1,\"1255\":4,\"1269\":1,\"1284\":1,\"1293\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1336\":1,\"1337\":6,\"1348\":1,\"1349\":6,\"1350\":6,\"1371\":10,\"1374\":1,\"1376\":1,\"1377\":1,\"1392\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1510\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1551\":5,\"1552\":14,\"1553\":6,\"1554\":7,\"1558\":1,\"1611\":2,\"1616\":1,\"1617\":1,\"1626\":1,\"1643\":2,\"1644\":2,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":1,\"1671\":1,\"1719\":1,\"1771\":2,\"1773\":36,\"1774\":1,\"1778\":11,\"1786\":2,\"1788\":2,\"1791\":1,\"1798\":2,\"1804\":18,\"1805\":13,\"1829\":4,\"1837\":20,\"1838\":1,\"1841\":2,\"1850\":6,\"1851\":14,\"1852\":6,\"1863\":2,\"1864\":1,\"1865\":1,\"1874\":2,\"1877\":6,\"1878\":8,\"1879\":1,\"1881\":4,\"1884\":2,\"1889\":3,\"1892\":8,\"1893\":8,\"1902\":1,\"1909\":1,\"1914\":1,\"1915\":1,\"1919\":2,\"1940\":3,\"1946\":2,\"1953\":4,\"1954\":3,\"1955\":4,\"1956\":3,\"1970\":8,\"1971\":4,\"1975\":14,\"1980\":2,\"1981\":1,\"1983\":1,\"1984\":8,\"1985\":2,\"1986\":3,\"1987\":1,\"1989\":1,\"1991\":1,\"2000\":1,\"2001\":6,\"2002\":6,\"2004\":6,\"2007\":1,\"2012\":2,\"2026\":1,\"2027\":10,\"2046\":3,\"2076\":11,\"2077\":2,\"2078\":1,\"2081\":1,\"2082\":43,\"2083\":2,\"2084\":8,\"2086\":21,\"2087\":20,\"2088\":1,\"2089\":9,\"2090\":20,\"2091\":2,\"2095\":20,\"2168\":1,\"2170\":1,\"2233\":1,\"2235\":2,\"2237\":3,\"2240\":20,\"2241\":3,\"2243\":9,\"2244\":15,\"2245\":2,\"2246\":1,\"2248\":1,\"2250\":1,\"2255\":15,\"2256\":2,\"2263\":6,\"2264\":6,\"2266\":3,\"2275\":1,\"2277\":2,\"2278\":24,\"2279\":14,\"2280\":2,\"2281\":1,\"2294\":7,\"2375\":1,\"2558\":1}}],[\"length\",{\"0\":{\"773\":1,\"774\":1,\"903\":1,\"1195\":1,\"1781\":1,\"1842\":1,\"2008\":1},\"1\":{\"23\":5,\"26\":1,\"60\":5,\"73\":4,\"74\":2,\"75\":9,\"76\":13,\"77\":5,\"108\":3,\"110\":1,\"115\":1,\"116\":2,\"119\":3,\"121\":1,\"122\":1,\"150\":2,\"245\":2,\"247\":4,\"274\":2,\"275\":3,\"297\":7,\"301\":2,\"307\":2,\"327\":8,\"391\":2,\"399\":2,\"408\":2,\"422\":2,\"429\":4,\"443\":4,\"449\":8,\"501\":1,\"509\":2,\"512\":2,\"519\":2,\"525\":2,\"585\":4,\"629\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"691\":6,\"692\":6,\"693\":7,\"694\":3,\"697\":12,\"700\":6,\"715\":1,\"716\":1,\"717\":1,\"719\":1,\"720\":1,\"721\":1,\"729\":1,\"730\":1,\"740\":1,\"741\":1,\"742\":1,\"745\":1,\"746\":1,\"747\":1,\"758\":1,\"768\":2,\"770\":1,\"772\":1,\"773\":2,\"774\":5,\"775\":1,\"776\":1,\"797\":10,\"798\":3,\"800\":2,\"810\":1,\"813\":1,\"815\":1,\"818\":2,\"821\":2,\"826\":2,\"857\":4,\"863\":1,\"865\":1,\"870\":1,\"899\":2,\"901\":2,\"903\":2,\"904\":1,\"905\":1,\"909\":2,\"987\":2,\"1005\":6,\"1028\":6,\"1048\":5,\"1065\":3,\"1066\":3,\"1069\":6,\"1071\":7,\"1077\":1,\"1078\":3,\"1079\":3,\"1093\":1,\"1113\":2,\"1115\":6,\"1132\":4,\"1138\":7,\"1139\":7,\"1140\":2,\"1142\":6,\"1148\":2,\"1149\":3,\"1150\":3,\"1154\":2,\"1155\":2,\"1158\":2,\"1160\":2,\"1161\":2,\"1162\":2,\"1163\":2,\"1164\":2,\"1167\":1,\"1168\":1,\"1169\":2,\"1171\":8,\"1172\":4,\"1177\":2,\"1178\":1,\"1179\":1,\"1180\":3,\"1181\":2,\"1186\":5,\"1192\":1,\"1195\":3,\"1196\":1,\"1197\":1,\"1200\":1,\"1203\":2,\"1206\":7,\"1207\":3,\"1208\":1,\"1210\":5,\"1211\":1,\"1216\":1,\"1224\":1,\"1241\":1,\"1242\":1,\"1243\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":3,\"1252\":5,\"1253\":2,\"1254\":5,\"1255\":10,\"1269\":8,\"1272\":1,\"1279\":2,\"1287\":1,\"1298\":4,\"1299\":4,\"1301\":4,\"1302\":4,\"1303\":4,\"1304\":4,\"1334\":2,\"1336\":1,\"1337\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1354\":1,\"1368\":1,\"1369\":1,\"1371\":6,\"1372\":1,\"1373\":2,\"1381\":1,\"1392\":3,\"1466\":1,\"1472\":1,\"1473\":1,\"1517\":1,\"1522\":1,\"1523\":1,\"1524\":1,\"1525\":1,\"1552\":10,\"1558\":2,\"1575\":1,\"1639\":2,\"1643\":3,\"1644\":3,\"1652\":1,\"1683\":1,\"1701\":1,\"1735\":5,\"1741\":8,\"1752\":2,\"1763\":1,\"1766\":2,\"1771\":1,\"1773\":11,\"1776\":1,\"1777\":6,\"1778\":4,\"1781\":7,\"1787\":3,\"1788\":1,\"1797\":3,\"1798\":1,\"1801\":1,\"1803\":1,\"1804\":6,\"1805\":5,\"1811\":4,\"1826\":1,\"1829\":2,\"1837\":10,\"1838\":1,\"1842\":1,\"1850\":5,\"1851\":4,\"1852\":5,\"1859\":6,\"1863\":1,\"1864\":1,\"1865\":1,\"1874\":1,\"1877\":6,\"1878\":5,\"1881\":2,\"1884\":1,\"1889\":2,\"1892\":7,\"1893\":6,\"1896\":2,\"1897\":3,\"1910\":2,\"1914\":1,\"1915\":1,\"1918\":2,\"1929\":6,\"1940\":2,\"1941\":6,\"1947\":6,\"1953\":1,\"1954\":2,\"1955\":2,\"1956\":2,\"1970\":4,\"1971\":1,\"1975\":6,\"1983\":3,\"1984\":1,\"1986\":4,\"1987\":2,\"1989\":2,\"1991\":2,\"1996\":1,\"2002\":2,\"2008\":1,\"2010\":1,\"2011\":1,\"2012\":2,\"2024\":1,\"2027\":4,\"2028\":1,\"2029\":1,\"2076\":5,\"2079\":6,\"2082\":16,\"2084\":2,\"2087\":1,\"2089\":2,\"2090\":2,\"2095\":3,\"2194\":1,\"2196\":1,\"2210\":1,\"2211\":1,\"2236\":1,\"2240\":10,\"2241\":2,\"2246\":2,\"2248\":2,\"2250\":2,\"2260\":3,\"2263\":2,\"2264\":2,\"2268\":1,\"2270\":1,\"2272\":1,\"2278\":12,\"2279\":1,\"2317\":3,\"2325\":3,\"2410\":1,\"2440\":1,\"2461\":1,\"2521\":2,\"2522\":2,\"2523\":2,\"2558\":1,\"2584\":1,\"2592\":8,\"2600\":6}}],[\"lens\",{\"1\":{\"1057\":1,\"1099\":2,\"1117\":1,\"1133\":2,\"1145\":2,\"1171\":8,\"1190\":3,\"1204\":2,\"1206\":8,\"1211\":4,\"1214\":2,\"1218\":5,\"1220\":1,\"1224\":4,\"1244\":2,\"1273\":3,\"1286\":2,\"1287\":4,\"1336\":4,\"1348\":4,\"1552\":8,\"2027\":1}}],[\"len=none\",{\"1\":{\"1986\":1}}],[\"len=16\",{\"1\":{\"1560\":2}}],[\"len=2\",{\"1\":{\"794\":1}}],[\"len=5000\",{\"1\":{\"770\":1,\"772\":1,\"810\":1,\"813\":1,\"818\":1}}],[\"len\",{\"1\":{\"77\":1,\"115\":1,\"172\":1,\"174\":3,\"218\":1,\"225\":1,\"231\":1,\"232\":1,\"272\":4,\"307\":2,\"408\":2,\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"697\":1,\"706\":2,\"734\":2,\"745\":1,\"746\":1,\"758\":2,\"760\":1,\"767\":2,\"770\":1,\"772\":1,\"796\":1,\"797\":1,\"810\":1,\"813\":1,\"817\":2,\"818\":1,\"825\":24,\"828\":2,\"863\":3,\"865\":3,\"1058\":5,\"1077\":2,\"1093\":2,\"1099\":2,\"1102\":1,\"1148\":1,\"1169\":1,\"1219\":1,\"1371\":1,\"1464\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1553\":1,\"1558\":4,\"1602\":1,\"1648\":1,\"1650\":1,\"1660\":2,\"1661\":2,\"1662\":1,\"1719\":2,\"1778\":2,\"1800\":1,\"1804\":2,\"1805\":2,\"1811\":2,\"1818\":2,\"1877\":2,\"1878\":2,\"1915\":1,\"1957\":2,\"1960\":2,\"2054\":1,\"2081\":1,\"2094\":2,\"2365\":1,\"2498\":6,\"2508\":1,\"2510\":1,\"2514\":2,\"2515\":1,\"2568\":1,\"2592\":6,\"2596\":2,\"2600\":3,\"2616\":7,\"2634\":7,\"2655\":1,\"2659\":2,\"2660\":1}}],[\"l5\",{\"1\":{\"22\":1}}],[\"l464\",{\"1\":{\"1695\":1}}],[\"l4\",{\"1\":{\"22\":1}}],[\"l32\",{\"1\":{\"2131\":1}}],[\"l3das22\",{\"1\":{\"1604\":1,\"1655\":1,\"1719\":1}}],[\"l3\",{\"1\":{\"22\":1,\"676\":2,\"742\":1,\"781\":1}}],[\"l2=true\",{\"1\":{\"1156\":1}}],[\"l2\",{\"1\":{\"22\":1,\"676\":2,\"742\":1,\"781\":1,\"2086\":1,\"2090\":1,\"2091\":1,\"2095\":1,\"2263\":1}}],[\"l1+l2\",{\"1\":{\"2000\":1,\"2086\":1,\"2090\":1,\"2091\":1,\"2095\":1,\"2263\":2}}],[\"l169\",{\"1\":{\"1717\":1}}],[\"l1\",{\"1\":{\"22\":1,\"240\":2,\"676\":2,\"702\":1,\"742\":1,\"755\":1,\"781\":1,\"822\":1,\"1569\":1,\"1604\":1,\"1666\":1,\"1778\":1,\"2000\":1,\"2086\":2,\"2088\":1,\"2090\":2,\"2091\":2,\"2095\":2,\"2245\":1,\"2256\":1,\"2263\":1,\"2264\":1}}],[\"lmtask\",{\"0\":{\"2107\":1},\"1\":{\"2107\":2}}],[\"lm=lm\",{\"1\":{\"1951\":1}}],[\"lm=false\",{\"1\":{\"1034\":1,\"2040\":1}}],[\"lmspc\",{\"1\":{\"2330\":2}}],[\"lms\",{\"1\":{\"873\":1,\"2479\":1}}],[\"lmax\",{\"1\":{\"629\":3,\"676\":2,\"702\":2,\"732\":1,\"735\":1,\"742\":2,\"754\":2,\"755\":3,\"781\":2,\"799\":3,\"820\":1,\"821\":4,\"822\":5,\"826\":2,\"852\":3,\"905\":3,\"920\":3,\"924\":3,\"925\":2,\"1145\":1,\"1778\":1,\"1804\":2,\"2000\":5,\"2078\":5,\"2086\":2,\"2087\":2,\"2088\":3,\"2257\":1,\"2261\":1,\"2264\":1}}],[\"lmevaluator\",{\"0\":{\"607\":1},\"1\":{\"607\":2}}],[\"lminterface\",{\"0\":{\"767\":1},\"1\":{\"606\":1,\"672\":2,\"733\":2,\"767\":4,\"817\":1,\"828\":1}}],[\"lm\",{\"0\":{\"253\":1,\"380\":1,\"384\":1,\"604\":2,\"605\":2,\"606\":2,\"607\":2,\"609\":1,\"610\":2,\"611\":1,\"612\":2,\"614\":2,\"615\":2,\"616\":2,\"617\":2,\"618\":2,\"619\":2,\"620\":2,\"621\":2,\"622\":2,\"640\":1,\"710\":1,\"733\":1,\"767\":1,\"807\":1,\"817\":1,\"828\":1,\"866\":1,\"873\":2,\"891\":1,\"918\":1,\"1320\":1,\"1427\":2,\"1428\":2,\"1429\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":2,\"1958\":2,\"1960\":2,\"2107\":1,\"2672\":1,\"2695\":1},\"1\":{\"22\":8,\"24\":3,\"30\":1,\"56\":1,\"92\":2,\"96\":4,\"110\":6,\"113\":2,\"118\":6,\"191\":1,\"249\":4,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"285\":3,\"307\":12,\"315\":10,\"327\":10,\"333\":6,\"380\":2,\"384\":13,\"391\":10,\"398\":12,\"408\":12,\"422\":12,\"443\":20,\"449\":10,\"478\":12,\"485\":10,\"604\":5,\"605\":2,\"606\":4,\"607\":5,\"609\":1,\"610\":4,\"611\":1,\"612\":2,\"614\":4,\"615\":4,\"616\":4,\"617\":2,\"618\":4,\"619\":5,\"620\":4,\"621\":4,\"622\":4,\"640\":5,\"668\":2,\"691\":1,\"693\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":6,\"707\":1,\"710\":2,\"725\":7,\"733\":1,\"734\":1,\"742\":2,\"751\":3,\"767\":4,\"797\":1,\"806\":6,\"807\":1,\"815\":3,\"817\":2,\"824\":7,\"825\":15,\"828\":2,\"857\":1,\"866\":11,\"873\":6,\"891\":7,\"918\":12,\"1034\":2,\"1048\":8,\"1057\":6,\"1060\":5,\"1063\":5,\"1138\":6,\"1139\":6,\"1176\":3,\"1190\":1,\"1193\":2,\"1206\":1,\"1270\":6,\"1320\":1,\"1427\":6,\"1428\":4,\"1429\":1,\"1951\":5,\"1953\":2,\"1955\":2,\"1957\":3,\"1958\":4,\"1959\":1,\"1960\":5,\"2040\":1,\"2107\":2,\"2375\":3,\"2377\":2,\"2436\":2,\"2440\":1,\"2559\":3,\"2562\":2,\"2564\":1,\"2569\":2,\"2574\":1,\"2592\":1}}],[\"laacl\",{\"1\":{\"2462\":1}}],[\"laal\",{\"1\":{\"2461\":1}}],[\"la\",{\"1\":{\"2457\":2}}],[\"layout\",{\"1\":{\"2394\":3,\"2395\":1,\"2530\":3,\"2531\":1}}],[\"layerdrop\",{\"1\":{\"1115\":4,\"1179\":1,\"1180\":1}}],[\"layeri\",{\"1\":{\"821\":1}}],[\"layered\",{\"1\":{\"786\":1}}],[\"layernormalization4dcf\",{\"0\":{\"1590\":1},\"1\":{\"1590\":1}}],[\"layernormalization4d\",{\"0\":{\"1588\":1},\"1\":{\"1588\":1}}],[\"layernormalization\",{\"0\":{\"1586\":1},\"1\":{\"769\":2,\"1586\":2}}],[\"layernorm\",{\"0\":{\"769\":1,\"1779\":1},\"1\":{\"769\":2,\"1049\":1,\"1050\":1,\"1054\":1,\"1056\":1,\"1065\":1,\"1068\":1,\"1070\":1,\"1074\":1,\"1274\":2,\"1779\":1,\"1798\":1,\"1851\":2,\"1874\":1,\"1932\":1,\"2090\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2264\":2,\"2279\":2}}],[\"layer=6\",{\"1\":{\"1462\":1}}],[\"layer=4\",{\"1\":{\"1460\":1}}],[\"layer=none\",{\"1\":{\"1116\":2,\"1244\":1,\"1252\":1,\"1254\":2}}],[\"layer=embed\",{\"1\":{\"749\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1200\":1,\"1203\":1,\"1272\":1,\"1505\":1,\"2029\":1,\"2054\":1}}],[\"layer=\",{\"1\":{\"747\":1,\"749\":1,\"2083\":1}}],[\"layer\",{\"0\":{\"711\":1,\"713\":1,\"732\":1,\"748\":1,\"769\":1,\"786\":1,\"827\":1,\"861\":1,\"911\":1,\"1725\":1,\"1907\":1,\"2155\":2},\"1\":{\"21\":1,\"22\":1,\"28\":2,\"29\":4,\"102\":2,\"115\":3,\"116\":5,\"614\":1,\"683\":2,\"701\":2,\"703\":1,\"711\":13,\"713\":2,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"725\":9,\"726\":14,\"731\":1,\"732\":6,\"740\":3,\"741\":3,\"743\":1,\"747\":4,\"748\":5,\"749\":7,\"754\":4,\"769\":1,\"770\":1,\"771\":1,\"775\":3,\"776\":3,\"784\":3,\"785\":1,\"786\":1,\"787\":4,\"791\":3,\"801\":1,\"806\":3,\"807\":1,\"808\":1,\"809\":1,\"821\":1,\"826\":5,\"827\":2,\"830\":1,\"855\":1,\"858\":16,\"859\":3,\"861\":5,\"862\":3,\"867\":1,\"870\":1,\"911\":7,\"914\":3,\"933\":5,\"1066\":4,\"1073\":1,\"1074\":2,\"1075\":4,\"1083\":1,\"1093\":2,\"1115\":2,\"1130\":1,\"1132\":2,\"1133\":11,\"1140\":3,\"1141\":1,\"1148\":17,\"1149\":9,\"1150\":8,\"1167\":2,\"1168\":2,\"1169\":9,\"1170\":1,\"1178\":2,\"1179\":1,\"1180\":2,\"1181\":1,\"1190\":1,\"1195\":1,\"1196\":2,\"1197\":2,\"1198\":3,\"1200\":4,\"1203\":18,\"1204\":4,\"1209\":1,\"1212\":1,\"1214\":2,\"1222\":1,\"1233\":1,\"1239\":1,\"1244\":13,\"1252\":8,\"1254\":7,\"1269\":18,\"1270\":2,\"1271\":3,\"1272\":10,\"1273\":4,\"1282\":1,\"1368\":1,\"1372\":1,\"1375\":1,\"1376\":1,\"1377\":2,\"1430\":3,\"1462\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1482\":2,\"1505\":15,\"1508\":1,\"1515\":2,\"1516\":3,\"1522\":8,\"1523\":8,\"1528\":2,\"1529\":2,\"1531\":1,\"1534\":2,\"1535\":1,\"1538\":2,\"1539\":2,\"1542\":1,\"1545\":5,\"1558\":2,\"1560\":1,\"1561\":1,\"1572\":3,\"1575\":1,\"1602\":1,\"1603\":5,\"1626\":2,\"1645\":1,\"1650\":1,\"1654\":2,\"1658\":2,\"1659\":8,\"1665\":7,\"1669\":5,\"1670\":3,\"1671\":1,\"1716\":1,\"1725\":1,\"1765\":1,\"1766\":4,\"1767\":1,\"1768\":1,\"1771\":13,\"1778\":3,\"1786\":2,\"1787\":10,\"1788\":11,\"1791\":1,\"1798\":10,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":13,\"1805\":3,\"1834\":2,\"1835\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":2,\"1849\":3,\"1850\":3,\"1851\":14,\"1852\":3,\"1856\":5,\"1857\":4,\"1858\":4,\"1862\":1,\"1865\":1,\"1866\":2,\"1867\":2,\"1871\":2,\"1872\":1,\"1874\":10,\"1877\":3,\"1878\":13,\"1880\":1,\"1907\":1,\"1932\":1,\"1960\":1,\"2001\":11,\"2002\":2,\"2003\":2,\"2004\":11,\"2026\":4,\"2029\":8,\"2044\":1,\"2046\":1,\"2054\":15,\"2068\":1,\"2083\":2,\"2086\":2,\"2087\":2,\"2090\":9,\"2095\":2,\"2155\":6,\"2243\":13,\"2244\":13,\"2255\":13,\"2260\":1,\"2262\":3,\"2263\":2,\"2264\":9,\"2279\":13,\"2401\":1,\"2431\":3,\"2432\":3,\"2439\":1,\"2440\":3,\"2537\":1,\"2558\":1,\"2564\":3,\"2584\":3,\"2585\":1}}],[\"layerspp\",{\"0\":{\"1458\":1,\"1474\":1,\"1573\":1,\"1631\":1,\"1635\":1},\"1\":{\"1458\":1,\"1474\":1,\"1573\":1,\"1631\":1,\"1635\":1}}],[\"layers=3\",{\"1\":{\"2083\":1}}],[\"layers=6\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"layers=1\",{\"1\":{\"1252\":1,\"1518\":1,\"1520\":1,\"1532\":1,\"1535\":1,\"1537\":1}}],[\"layers=5\",{\"1\":{\"802\":1,\"1522\":1,\"1545\":1,\"2078\":1}}],[\"layers=2\",{\"1\":{\"736\":1,\"803\":1,\"1522\":1,\"1572\":1,\"1994\":1,\"2078\":1}}],[\"layers=4\",{\"1\":{\"701\":1}}],[\"layers=8\",{\"1\":{\"701\":1}}],[\"layers\",{\"0\":{\"933\":1,\"1151\":1,\"1153\":1,\"1182\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1372\":1,\"1375\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":1,\"1430\":1,\"1452\":1,\"1455\":1,\"1456\":2,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1465\":1,\"1468\":2,\"1470\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1484\":2,\"1485\":2,\"1487\":2,\"1489\":2,\"1491\":2,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":2,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":2,\"1517\":1,\"1518\":1,\"1520\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1542\":2,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1549\":2,\"1559\":1,\"1560\":1,\"1561\":1,\"1564\":1,\"1572\":1,\"1573\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1592\":2,\"1594\":1,\"1595\":1,\"1596\":2,\"1598\":1,\"1601\":2,\"1602\":1,\"1605\":1,\"1607\":2,\"1609\":1,\"1613\":1,\"1620\":1,\"1624\":2,\"1627\":2,\"1629\":2,\"1631\":1,\"1633\":2,\"1635\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1655\":1,\"1656\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1670\":1,\"1672\":2,\"1674\":2,\"1676\":1,\"1678\":1,\"1680\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":2,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":2,\"1691\":2,\"1692\":2,\"1693\":1,\"1694\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":2,\"1700\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1720\":2,\"1721\":2,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":2,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":2,\"1732\":2,\"1733\":1,\"1734\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":2,\"1758\":1,\"1759\":1,\"1902\":1,\"1904\":1,\"1905\":1,\"1906\":1,\"1907\":1,\"1909\":1,\"1910\":1,\"1911\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1922\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1926\":1,\"1928\":1,\"1929\":1,\"1930\":1,\"1931\":1,\"1932\":1,\"1933\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1944\":1,\"1945\":1,\"1946\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1950\":1,\"2032\":1,\"2047\":1,\"2694\":1},\"1\":{\"21\":2,\"22\":1,\"30\":1,\"80\":1,\"102\":1,\"115\":5,\"116\":3,\"117\":1,\"614\":3,\"701\":4,\"711\":1,\"718\":1,\"723\":2,\"726\":2,\"731\":2,\"737\":4,\"747\":2,\"754\":4,\"782\":1,\"785\":2,\"802\":2,\"804\":2,\"805\":1,\"806\":2,\"807\":3,\"808\":1,\"821\":11,\"826\":11,\"833\":1,\"836\":2,\"866\":3,\"893\":1,\"918\":3,\"933\":5,\"1049\":1,\"1056\":1,\"1073\":5,\"1115\":8,\"1151\":1,\"1153\":1,\"1160\":2,\"1161\":2,\"1162\":2,\"1163\":2,\"1164\":2,\"1177\":2,\"1178\":2,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1192\":1,\"1195\":1,\"1220\":1,\"1222\":3,\"1231\":1,\"1244\":3,\"1252\":4,\"1253\":2,\"1254\":2,\"1269\":6,\"1270\":6,\"1279\":2,\"1282\":3,\"1366\":2,\"1368\":1,\"1369\":2,\"1370\":1,\"1372\":1,\"1375\":2,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":2,\"1430\":3,\"1452\":1,\"1455\":1,\"1456\":2,\"1458\":1,\"1460\":2,\"1462\":3,\"1463\":3,\"1464\":2,\"1465\":2,\"1468\":2,\"1470\":2,\"1471\":2,\"1472\":1,\"1473\":2,\"1474\":1,\"1476\":1,\"1478\":2,\"1480\":1,\"1482\":2,\"1484\":2,\"1485\":2,\"1487\":2,\"1489\":2,\"1491\":2,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":3,\"1503\":1,\"1505\":2,\"1506\":1,\"1508\":1,\"1512\":2,\"1515\":2,\"1516\":1,\"1517\":4,\"1518\":1,\"1520\":1,\"1522\":10,\"1523\":10,\"1524\":1,\"1525\":2,\"1526\":2,\"1527\":2,\"1528\":2,\"1529\":2,\"1531\":5,\"1532\":5,\"1534\":2,\"1535\":5,\"1537\":5,\"1539\":2,\"1542\":2,\"1543\":3,\"1545\":6,\"1546\":1,\"1547\":1,\"1549\":2,\"1559\":2,\"1560\":2,\"1561\":2,\"1564\":1,\"1572\":4,\"1573\":1,\"1575\":2,\"1576\":3,\"1577\":2,\"1581\":1,\"1583\":1,\"1586\":2,\"1592\":2,\"1594\":2,\"1595\":2,\"1596\":2,\"1598\":3,\"1601\":2,\"1602\":3,\"1603\":1,\"1605\":2,\"1607\":4,\"1609\":1,\"1613\":1,\"1620\":2,\"1624\":2,\"1626\":2,\"1627\":2,\"1629\":2,\"1631\":1,\"1633\":2,\"1635\":1,\"1645\":4,\"1648\":3,\"1650\":3,\"1652\":3,\"1655\":4,\"1656\":1,\"1658\":1,\"1659\":3,\"1660\":2,\"1661\":2,\"1662\":3,\"1663\":2,\"1664\":2,\"1665\":5,\"1669\":2,\"1670\":3,\"1671\":1,\"1672\":2,\"1674\":2,\"1676\":1,\"1678\":1,\"1680\":1,\"1681\":2,\"1682\":2,\"1683\":2,\"1684\":2,\"1685\":1,\"1686\":2,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":2,\"1691\":2,\"1692\":2,\"1693\":1,\"1694\":2,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":2,\"1700\":2,\"1701\":2,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":2,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":2,\"1719\":3,\"1720\":2,\"1721\":4,\"1722\":2,\"1723\":1,\"1724\":1,\"1725\":2,\"1726\":1,\"1727\":2,\"1728\":2,\"1729\":1,\"1730\":1,\"1731\":2,\"1732\":2,\"1733\":1,\"1734\":2,\"1736\":1,\"1737\":1,\"1738\":2,\"1739\":1,\"1740\":2,\"1741\":1,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1747\":1,\"1748\":2,\"1749\":2,\"1750\":1,\"1751\":2,\"1752\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":2,\"1758\":2,\"1759\":2,\"1765\":6,\"1766\":3,\"1769\":1,\"1776\":1,\"1778\":3,\"1786\":1,\"1798\":3,\"1800\":6,\"1803\":6,\"1804\":8,\"1805\":3,\"1806\":1,\"1808\":4,\"1820\":1,\"1833\":3,\"1835\":3,\"1839\":3,\"1844\":6,\"1848\":7,\"1849\":9,\"1850\":6,\"1851\":24,\"1852\":7,\"1856\":4,\"1857\":5,\"1858\":5,\"1861\":7,\"1862\":6,\"1863\":3,\"1864\":3,\"1865\":3,\"1866\":2,\"1867\":2,\"1868\":3,\"1870\":1,\"1871\":6,\"1874\":3,\"1877\":4,\"1878\":11,\"1880\":8,\"1902\":2,\"1904\":2,\"1905\":1,\"1906\":2,\"1907\":1,\"1909\":1,\"1910\":1,\"1911\":2,\"1912\":2,\"1914\":1,\"1915\":1,\"1916\":2,\"1917\":2,\"1918\":2,\"1919\":2,\"1920\":2,\"1921\":1,\"1922\":1,\"1923\":1,\"1924\":1,\"1925\":2,\"1926\":2,\"1928\":2,\"1929\":2,\"1930\":1,\"1931\":2,\"1932\":3,\"1933\":1,\"1934\":1,\"1935\":2,\"1936\":1,\"1937\":1,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":2,\"1942\":1,\"1943\":2,\"1944\":1,\"1945\":2,\"1946\":2,\"1947\":2,\"1948\":2,\"1949\":2,\"1950\":1,\"1963\":1,\"2002\":7,\"2003\":4,\"2032\":2,\"2046\":1,\"2047\":2,\"2078\":5,\"2083\":1,\"2086\":8,\"2087\":11,\"2090\":8,\"2095\":20,\"2239\":1,\"2243\":17,\"2244\":23,\"2255\":24,\"2257\":9,\"2260\":3,\"2261\":9,\"2263\":20,\"2264\":24,\"2265\":5,\"2279\":14,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"lack\",{\"1\":{\"2309\":1}}],[\"lagging\",{\"1\":{\"2462\":1}}],[\"lag\",{\"1\":{\"2267\":4}}],[\"lage\",{\"1\":{\"2084\":1}}],[\"laborotv\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"lab\",{\"1\":{\"1778\":7,\"1805\":7,\"2086\":9,\"2087\":9,\"2090\":9,\"2095\":9}}],[\"labeling\",{\"1\":{\"2467\":1}}],[\"labelaggregate\",{\"0\":{\"1910\":1},\"1\":{\"1910\":2}}],[\"labelprocessor\",{\"0\":{\"1373\":1},\"1\":{\"1373\":1}}],[\"labellength\",{\"1\":{\"1211\":1,\"1224\":1,\"1287\":1,\"1336\":1,\"1348\":1}}],[\"labelleft\",{\"1\":{\"648\":1}}],[\"labelleft=true\",{\"1\":{\"648\":1}}],[\"labeldist\",{\"1\":{\"870\":2}}],[\"labeltop\",{\"1\":{\"648\":1}}],[\"labeltop=false\",{\"1\":{\"648\":1}}],[\"labelright\",{\"1\":{\"648\":1}}],[\"labelright=true\",{\"1\":{\"648\":1}}],[\"labelsmoothingloss\",{\"0\":{\"768\":1},\"1\":{\"768\":1}}],[\"labels\",{\"1\":{\"648\":1,\"704\":1,\"710\":3,\"725\":1,\"768\":2,\"806\":3,\"821\":1,\"822\":3,\"824\":1,\"825\":3,\"826\":1,\"852\":1,\"884\":2,\"908\":2,\"1046\":2,\"1066\":4,\"1073\":2,\"1075\":4,\"1083\":2,\"1099\":2,\"1113\":1,\"1142\":5,\"1155\":4,\"1186\":5,\"1210\":5,\"1211\":2,\"1224\":2,\"1269\":1,\"1270\":3,\"1286\":1,\"1287\":2,\"1293\":1,\"1336\":2,\"1337\":3,\"1348\":2,\"1349\":3,\"1350\":3,\"1371\":5,\"2000\":3,\"2002\":1,\"2046\":4,\"2142\":1,\"2294\":2,\"2304\":1,\"2310\":1,\"2341\":1}}],[\"labelbottom=false\",{\"1\":{\"2498\":3,\"2616\":3,\"2634\":3}}],[\"labelbottom=true\",{\"1\":{\"648\":1}}],[\"labelbottom\",{\"1\":{\"648\":1}}],[\"label=1\",{\"1\":{\"876\":1}}],[\"label=none\",{\"1\":{\"743\":1,\"745\":1,\"746\":1,\"875\":1,\"2030\":1,\"2034\":1}}],[\"label=\",{\"1\":{\"174\":2}}],[\"label\",{\"0\":{\"768\":1,\"896\":1,\"1373\":1,\"1422\":1,\"1910\":1,\"2223\":1,\"2304\":1},\"1\":{\"22\":2,\"118\":2,\"150\":1,\"175\":1,\"194\":1,\"249\":2,\"253\":6,\"257\":2,\"261\":2,\"605\":2,\"619\":3,\"621\":3,\"704\":1,\"705\":4,\"706\":1,\"710\":4,\"725\":10,\"743\":2,\"744\":1,\"768\":2,\"806\":9,\"815\":1,\"824\":4,\"825\":15,\"875\":2,\"884\":2,\"895\":2,\"896\":2,\"905\":2,\"908\":3,\"912\":1,\"922\":1,\"925\":4,\"1046\":5,\"1048\":1,\"1057\":5,\"1059\":3,\"1060\":1,\"1063\":1,\"1066\":5,\"1073\":6,\"1075\":5,\"1083\":5,\"1099\":3,\"1106\":3,\"1107\":3,\"1108\":3,\"1113\":1,\"1115\":2,\"1142\":3,\"1173\":3,\"1181\":4,\"1186\":4,\"1210\":4,\"1211\":4,\"1224\":4,\"1270\":9,\"1286\":1,\"1287\":3,\"1293\":1,\"1336\":4,\"1337\":4,\"1348\":4,\"1349\":4,\"1350\":4,\"1371\":5,\"1373\":4,\"1400\":2,\"1422\":2,\"1423\":1,\"1570\":1,\"1773\":24,\"1778\":9,\"1804\":12,\"1805\":9,\"1910\":3,\"1996\":1,\"2040\":1,\"2082\":24,\"2084\":7,\"2086\":11,\"2087\":11,\"2089\":6,\"2090\":10,\"2094\":1,\"2095\":10,\"2188\":1,\"2196\":2,\"2223\":2,\"2304\":2,\"2440\":1,\"2558\":1}}],[\"lapack\",{\"1\":{\"1695\":1}}],[\"lamdba\",{\"1\":{\"821\":1}}],[\"lamb\",{\"1\":{\"269\":1}}],[\"lambda\",{\"1\":{\"22\":1,\"24\":1,\"113\":2,\"118\":1,\"217\":1,\"762\":1,\"763\":2,\"821\":1,\"825\":2,\"826\":2,\"999\":2,\"1057\":3,\"1142\":2,\"1186\":2,\"1210\":2,\"1211\":2,\"1224\":2,\"1286\":1,\"1287\":2,\"1300\":1,\"1301\":2,\"1304\":2,\"1336\":1,\"1337\":2,\"1348\":1,\"1349\":2,\"1350\":2,\"1695\":1,\"1704\":1,\"1778\":12,\"1805\":16,\"1850\":10,\"1852\":8,\"1877\":10,\"2090\":8,\"2095\":3,\"2263\":3,\"2264\":3,\"2482\":1}}],[\"launch\",{\"0\":{\"377\":1,\"593\":1,\"594\":1,\"595\":1,\"596\":2,\"598\":1},\"1\":{\"377\":3,\"593\":1,\"594\":1,\"595\":1,\"596\":3,\"598\":1,\"1142\":1,\"1186\":3,\"1210\":3,\"2401\":1,\"2440\":1,\"2537\":1,\"2558\":1,\"2571\":2}}],[\"launcher\",{\"1\":{\"40\":1,\"41\":1,\"42\":1,\"429\":1,\"2180\":2,\"2213\":1,\"2214\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1}}],[\"last10\",{\"1\":{\"285\":1}}],[\"last1\",{\"1\":{\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1}}],[\"last\",{\"1\":{\"99\":1,\"137\":1,\"195\":1,\"233\":1,\"238\":1,\"416\":2,\"429\":2,\"681\":3,\"682\":3,\"734\":1,\"758\":3,\"760\":2,\"767\":1,\"817\":1,\"828\":1,\"1097\":3,\"1133\":1,\"1138\":1,\"1142\":1,\"1178\":2,\"1186\":1,\"1210\":1,\"1276\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1304\":1,\"1336\":1,\"1517\":1,\"1522\":18,\"1523\":18,\"1531\":1,\"1545\":16,\"1603\":1,\"1622\":1,\"1670\":1,\"1671\":1,\"1849\":1,\"1856\":2,\"1858\":1,\"1880\":4,\"2004\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":2,\"2018\":3,\"2019\":1,\"2020\":1,\"2021\":1,\"2023\":1,\"2194\":1,\"2568\":1,\"2584\":1,\"2585\":1}}],[\"lats\",{\"1\":{\"1427\":3,\"1428\":1}}],[\"lat\",{\"1\":{\"1136\":1}}],[\"latest33\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"latest\",{\"1\":{\"1140\":1,\"1148\":1,\"1169\":1,\"1203\":1,\"1778\":1,\"1850\":1,\"1852\":1,\"2054\":1,\"2372\":1,\"2440\":1,\"2448\":1,\"2451\":1,\"2464\":1,\"2467\":1,\"2564\":1,\"2591\":1}}],[\"latent\",{\"1\":{\"1115\":2,\"1618\":1,\"1619\":1,\"1638\":1}}],[\"latency\",{\"0\":{\"107\":1},\"1\":{\"107\":2,\"110\":2,\"111\":1,\"1142\":1,\"1186\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1287\":1,\"1301\":1,\"1304\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"2461\":1}}],[\"later\",{\"1\":{\"75\":1,\"96\":1,\"2355\":1}}],[\"latter\",{\"1\":{\"25\":1,\"745\":1,\"746\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1}}],[\"largest\",{\"1\":{\"1618\":1,\"1619\":1}}],[\"larger\",{\"1\":{\"80\":3,\"1143\":1,\"1241\":1,\"1712\":1,\"1715\":1}}],[\"large\",{\"1\":{\"49\":1,\"69\":1,\"84\":2,\"98\":1,\"102\":1,\"243\":2,\"785\":1,\"1398\":1,\"2040\":3,\"2377\":2,\"2431\":4,\"2432\":7,\"2441\":1,\"2558\":1,\"2564\":1,\"2573\":1,\"2574\":3,\"2600\":1}}],[\"lawor\",{\"1\":{\"48\":1}}],[\"law\",{\"0\":{\"869\":1,\"877\":1},\"1\":{\"48\":1,\"869\":2,\"877\":2,\"1927\":2}}],[\"lang=\",{\"1\":{\"2455\":1,\"2460\":1}}],[\"langevincorrector\",{\"0\":{\"1585\":1},\"1\":{\"1585\":1}}],[\"langevin\",{\"1\":{\"1451\":1,\"1646\":1}}],[\"langs\",{\"1\":{\"274\":1,\"294\":1,\"1778\":1,\"1804\":2,\"1805\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"1877\":1,\"1878\":2,\"2001\":2,\"2002\":2,\"2003\":1,\"2004\":2,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2279\":2}}],[\"lang\",{\"0\":{\"572\":1},\"1\":{\"175\":1,\"194\":1,\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"239\":2,\"257\":2,\"261\":2,\"276\":1,\"285\":1,\"286\":2,\"295\":2,\"298\":1,\"307\":2,\"408\":2,\"416\":4,\"572\":2,\"579\":2,\"1171\":1,\"1191\":1,\"1192\":1,\"2076\":1,\"2178\":1,\"2179\":1,\"2357\":5,\"2359\":3,\"2363\":3,\"2365\":1,\"2456\":3,\"2460\":3,\"2506\":1,\"2508\":1,\"2510\":2,\"2512\":1,\"2515\":1,\"2520\":1,\"2521\":3,\"2569\":1,\"2578\":5,\"2580\":3,\"2653\":3,\"2655\":1,\"2657\":1,\"2660\":1}}],[\"languageespnetmodel\",{\"1\":{\"1951\":1}}],[\"languagemodel\",{\"1\":{\"1951\":1}}],[\"languages\",{\"1\":{\"294\":1,\"1804\":1,\"1851\":1,\"1878\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2131\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2506\":1,\"2585\":2}}],[\"language\",{\"0\":{\"157\":1,\"416\":1,\"2374\":1,\"2434\":1,\"2463\":1,\"2467\":1,\"2557\":1},\"1\":{\"98\":1,\"130\":2,\"150\":2,\"157\":1,\"161\":1,\"179\":1,\"197\":1,\"253\":1,\"286\":1,\"416\":2,\"491\":2,\"572\":1,\"676\":2,\"781\":2,\"812\":2,\"815\":1,\"817\":1,\"828\":1,\"1429\":1,\"1773\":3,\"1778\":2,\"1804\":2,\"1805\":2,\"1837\":2,\"1850\":1,\"1851\":2,\"1877\":2,\"1878\":2,\"1953\":1,\"1955\":1,\"1958\":1,\"2001\":1,\"2002\":2,\"2004\":1,\"2082\":3,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2128\":1,\"2129\":1,\"2137\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2240\":3,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2278\":3,\"2279\":2,\"2374\":2,\"2375\":1,\"2387\":2,\"2434\":2,\"2451\":2,\"2452\":2,\"2463\":1,\"2464\":1,\"2467\":4,\"2468\":1,\"2473\":1,\"2557\":2,\"2559\":1,\"2584\":2,\"2585\":5,\"2646\":1}}],[\"lanuage\",{\"1\":{\"168\":1}}],[\"lan\",{\"1\":{\"45\":1}}],[\"llbackward\",{\"1\":{\"1299\":3,\"1303\":3}}],[\"llforward\",{\"1\":{\"1298\":3,\"1302\":3}}],[\"ll60k\",{\"1\":{\"102\":1,\"2431\":4,\"2432\":4}}],[\"ll\",{\"1\":{\"19\":1,\"96\":1,\"132\":1,\"134\":1,\"135\":2}}],[\"license\",{\"1\":{\"2411\":1}}],[\"lichenda1996\",{\"1\":{\"2618\":1}}],[\"lichenda\",{\"1\":{\"2366\":1,\"2601\":1,\"2618\":1}}],[\"live\",{\"0\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"li52\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"light\",{\"1\":{\"2450\":1,\"2482\":1}}],[\"lightweightsincconvs\",{\"0\":{\"1198\":1},\"1\":{\"1198\":2}}],[\"lightweight\",{\"1\":{\"775\":3,\"776\":3,\"1198\":4,\"2450\":1,\"2482\":1}}],[\"lightweightconvolutiontransformerdecoder\",{\"0\":{\"1197\":1},\"1\":{\"1197\":1}}],[\"lightweightconvolution2dtransformerdecoder\",{\"0\":{\"1196\":1},\"1\":{\"1196\":1}}],[\"lightweightconvolution2d\",{\"0\":{\"776\":1},\"1\":{\"776\":1}}],[\"lightweightconvolution\",{\"0\":{\"775\":1},\"1\":{\"775\":1}}],[\"lightconv2d\",{\"0\":{\"776\":1},\"1\":{\"776\":1}}],[\"lightconv\",{\"0\":{\"775\":1},\"1\":{\"775\":1}}],[\"lightning\",{\"1\":{\"57\":1,\"1343\":1,\"2154\":2}}],[\"lids\",{\"1\":{\"1773\":6,\"1778\":4,\"1804\":5,\"1805\":4,\"1837\":4,\"1850\":2,\"1851\":5,\"1877\":4,\"1878\":5,\"1984\":1,\"1985\":1,\"2001\":3,\"2002\":5,\"2004\":3,\"2082\":6,\"2086\":5,\"2087\":5,\"2090\":5,\"2095\":5,\"2240\":6,\"2243\":5,\"2244\":5,\"2255\":5,\"2263\":5,\"2264\":5,\"2278\":6,\"2279\":5}}],[\"lid\",{\"1\":{\"698\":1}}],[\"lien\",{\"1\":{\"237\":1}}],[\"limits\",{\"1\":{\"2055\":1}}],[\"limited\",{\"1\":{\"1923\":1}}],[\"limitation\",{\"0\":{\"1923\":1},\"1\":{\"1923\":2}}],[\"limitations\",{\"0\":{\"111\":1},\"1\":{\"107\":1}}],[\"limit=0\",{\"1\":{\"692\":2,\"2592\":2}}],[\"limit\",{\"1\":{\"327\":4,\"449\":4,\"823\":3,\"1085\":3,\"1095\":1,\"2592\":1,\"2596\":1}}],[\"lim\",{\"0\":{\"2317\":1,\"2325\":2,\"2330\":1},\"1\":{\"235\":1,\"242\":3,\"275\":2,\"519\":1,\"2317\":3,\"2325\":5,\"2330\":2,\"2506\":1,\"2510\":2}}],[\"li\",{\"1\":{\"130\":4,\"1566\":1,\"1604\":1,\"1655\":1,\"1719\":1,\"2366\":1,\"2371\":3,\"2601\":1,\"2612\":3,\"2618\":2,\"2630\":4}}],[\"li2020espnet\",{\"1\":{\"130\":1}}],[\"lib\",{\"1\":{\"1950\":1,\"2155\":1,\"2431\":1}}],[\"libatlas\",{\"1\":{\"134\":1}}],[\"librosa\",{\"1\":{\"778\":1,\"1912\":1,\"2359\":2,\"2360\":4,\"2384\":1,\"2386\":2,\"2456\":2,\"2458\":4,\"2460\":2,\"2521\":3,\"2522\":3,\"2523\":5,\"2580\":2,\"2581\":2,\"2582\":4,\"2600\":4}}],[\"libritts\",{\"1\":{\"243\":1,\"295\":6,\"2512\":11,\"2657\":11}}],[\"librispeech\",{\"1\":{\"85\":1,\"110\":6,\"286\":2,\"950\":1,\"968\":1,\"2357\":1,\"2377\":3,\"2411\":5,\"2431\":2,\"2436\":2,\"2520\":1,\"2562\":2,\"2564\":1,\"2574\":1,\"2578\":1,\"2593\":1}}],[\"libraries\",{\"1\":{\"7\":1,\"136\":2,\"2362\":1,\"2384\":1,\"2504\":1,\"2576\":1,\"2651\":1}}],[\"library=<module\",{\"1\":{\"2155\":1}}],[\"library\",{\"0\":{\"166\":1,\"170\":1},\"1\":{\"5\":1,\"134\":1,\"139\":1,\"164\":2,\"165\":1,\"166\":1,\"170\":1,\"177\":1,\"192\":1,\"2155\":4,\"2394\":2,\"2530\":2}}],[\"libsndfile1\",{\"1\":{\"2372\":1}}],[\"libsndfile\",{\"1\":{\"49\":1}}],[\"little\",{\"1\":{\"48\":1}}],[\"linspace\",{\"1\":{\"2498\":1,\"2616\":3,\"2634\":3}}],[\"linguists\",{\"1\":{\"2387\":1}}],[\"linguistic\",{\"1\":{\"461\":4,\"2120\":2,\"2130\":2,\"2136\":2,\"2137\":2,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2467\":1}}],[\"linguistics\",{\"1\":{\"130\":2}}],[\"lin\",{\"1\":{\"1064\":1,\"1245\":2}}],[\"link|dict\",{\"1\":{\"626\":1}}],[\"links\",{\"0\":{\"2355\":1,\"2382\":1,\"2390\":1,\"2424\":1,\"2526\":1,\"2547\":1},\"1\":{\"134\":1,\"606\":1,\"615\":1,\"626\":1,\"709\":1,\"811\":1}}],[\"link\",{\"1\":{\"97\":1,\"133\":1,\"605\":1,\"606\":1,\"610\":3,\"615\":1,\"626\":4,\"709\":1,\"743\":3,\"811\":1,\"1179\":1,\"2432\":1,\"2639\":1,\"2646\":1}}],[\"lint\",{\"1\":{\"57\":1}}],[\"linux\",{\"0\":{\"133\":1},\"1\":{\"45\":4,\"133\":2}}],[\"lineno\",{\"1\":{\"2568\":1}}],[\"linear3072\",{\"1\":{\"2436\":1,\"2562\":1}}],[\"linearspectrogram\",{\"0\":{\"1987\":1,\"2246\":1},\"1\":{\"1987\":1,\"2246\":1}}],[\"linearsampling\",{\"0\":{\"777\":1},\"1\":{\"777\":2}}],[\"linearly\",{\"1\":{\"1705\":1,\"2440\":1,\"2558\":1}}],[\"linearprojection\",{\"0\":{\"1201\":1},\"1\":{\"1201\":1}}],[\"linearencoder\",{\"0\":{\"1200\":1},\"1\":{\"1200\":1}}],[\"linearactivation\",{\"0\":{\"1199\":1},\"1\":{\"1199\":1}}],[\"lineardecoder\",{\"0\":{\"1114\":1,\"1374\":1},\"1\":{\"1114\":1,\"1374\":1}}],[\"linear\",{\"0\":{\"1114\":1,\"1200\":1,\"1201\":1,\"1374\":1,\"1987\":1,\"2020\":1,\"2246\":1},\"1\":{\"46\":1,\"48\":1,\"49\":2,\"102\":1,\"115\":9,\"116\":2,\"117\":1,\"648\":1,\"701\":2,\"711\":3,\"713\":2,\"725\":1,\"726\":2,\"737\":2,\"738\":1,\"739\":1,\"743\":1,\"747\":2,\"749\":7,\"777\":1,\"785\":2,\"858\":1,\"875\":1,\"885\":1,\"1011\":1,\"1049\":3,\"1056\":3,\"1066\":2,\"1067\":1,\"1074\":2,\"1075\":2,\"1081\":1,\"1096\":3,\"1114\":2,\"1133\":4,\"1140\":2,\"1148\":8,\"1149\":7,\"1150\":7,\"1151\":2,\"1153\":1,\"1156\":1,\"1167\":1,\"1168\":1,\"1169\":4,\"1181\":2,\"1184\":1,\"1196\":1,\"1197\":1,\"1199\":1,\"1200\":3,\"1201\":2,\"1203\":8,\"1204\":1,\"1271\":1,\"1272\":7,\"1273\":1,\"1276\":2,\"1374\":2,\"1375\":2,\"1430\":1,\"1470\":1,\"1482\":2,\"1505\":9,\"1542\":1,\"1576\":1,\"1577\":1,\"1658\":2,\"1659\":2,\"1664\":3,\"1665\":2,\"1669\":8,\"1670\":1,\"1706\":1,\"1707\":1,\"1742\":1,\"1771\":3,\"1774\":1,\"1787\":3,\"1788\":3,\"1798\":3,\"1799\":1,\"1804\":1,\"1859\":1,\"1874\":3,\"1878\":1,\"1888\":1,\"1927\":2,\"1973\":1,\"1987\":2,\"2001\":5,\"2004\":5,\"2018\":1,\"2020\":2,\"2026\":3,\"2029\":8,\"2054\":5,\"2246\":2,\"2290\":2,\"2325\":2,\"2330\":2,\"2394\":2,\"2401\":3,\"2440\":4,\"2530\":2,\"2537\":3,\"2558\":2,\"2564\":2,\"2584\":2}}],[\"lines=\",{\"1\":{\"2592\":1}}],[\"lines\",{\"1\":{\"19\":1,\"167\":1,\"217\":4,\"224\":4,\"231\":4,\"2400\":1,\"2510\":2,\"2514\":2,\"2536\":1,\"2558\":1,\"2568\":1,\"2592\":9,\"2659\":2}}],[\"line\",{\"0\":{\"90\":1,\"179\":1,\"190\":1,\"2378\":1,\"2437\":1,\"2563\":1},\"1\":{\"1\":1,\"2\":1,\"4\":1,\"18\":1,\"57\":1,\"58\":1,\"59\":1,\"62\":2,\"63\":1,\"90\":1,\"143\":1,\"144\":2,\"166\":1,\"173\":1,\"177\":1,\"217\":2,\"224\":2,\"231\":2,\"237\":3,\"606\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"734\":1,\"760\":2,\"767\":1,\"778\":2,\"817\":1,\"828\":1,\"831\":2,\"1406\":2,\"1476\":2,\"1598\":2,\"1906\":2,\"1910\":2,\"1912\":2,\"1914\":2,\"1915\":2,\"1918\":2,\"1919\":2,\"1920\":2,\"1927\":1,\"2084\":2,\"2089\":2,\"2119\":1,\"2120\":1,\"2124\":1,\"2129\":1,\"2130\":1,\"2132\":1,\"2136\":1,\"2315\":1,\"2378\":1,\"2385\":3,\"2386\":2,\"2387\":2,\"2400\":1,\"2437\":2,\"2514\":5,\"2536\":1,\"2558\":1,\"2563\":2,\"2569\":1,\"2570\":1,\"2585\":1,\"2592\":2,\"2659\":5}}],[\"likelihood\",{\"1\":{\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"1171\":2,\"1186\":1,\"1206\":2,\"1210\":1,\"1298\":2,\"1299\":2,\"1301\":3,\"1302\":2,\"1303\":2,\"1304\":3,\"1552\":2,\"1618\":1,\"1619\":1,\"1638\":1,\"1804\":1,\"1868\":1,\"1878\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1}}],[\"likely\",{\"1\":{\"149\":1}}],[\"like\",{\"0\":{\"1733\":1},\"1\":{\"26\":1,\"31\":1,\"49\":1,\"54\":1,\"57\":1,\"69\":1,\"80\":1,\"99\":1,\"111\":1,\"135\":1,\"142\":1,\"612\":1,\"981\":1,\"1053\":3,\"1097\":1,\"1244\":1,\"1252\":1,\"1254\":2,\"1382\":1,\"1391\":1,\"1406\":1,\"1543\":1,\"1564\":1,\"1733\":1,\"1917\":1,\"2148\":1,\"2185\":1,\"2201\":1,\"2203\":1,\"2386\":1,\"2388\":1,\"2430\":1,\"2441\":1,\"2467\":1,\"2468\":3,\"2492\":1,\"2506\":1,\"2510\":1,\"2524\":1,\"2555\":1,\"2558\":2,\"2572\":1,\"2628\":1,\"2635\":2}}],[\"listing\",{\"1\":{\"2398\":1,\"2534\":1}}],[\"listed\",{\"1\":{\"2387\":1,\"2394\":1,\"2429\":1,\"2530\":1,\"2552\":1,\"2584\":1}}],[\"listen\",{\"1\":{\"198\":1,\"2359\":1,\"2365\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2521\":1,\"2580\":1,\"2655\":1,\"2660\":1}}],[\"listconfig\",{\"1\":{\"1356\":1}}],[\"liststotensor\",{\"0\":{\"2085\":1},\"1\":{\"2085\":1}}],[\"lists\",{\"1\":{\"759\":1,\"1355\":1,\"1427\":1}}],[\"listofchainer\",{\"1\":{\"703\":1}}],[\"list=<class\",{\"1\":{\"698\":1,\"699\":1}}],[\"list=none\",{\"1\":{\"676\":2,\"781\":2,\"812\":2,\"1767\":1}}],[\"list=\",{\"1\":{\"11\":1}}],[\"list\",{\"0\":{\"183\":1,\"184\":1,\"906\":1,\"1289\":1,\"1329\":1,\"1356\":1},\"1\":{\"11\":5,\"22\":2,\"28\":1,\"29\":1,\"45\":4,\"60\":3,\"115\":2,\"173\":1,\"175\":6,\"185\":1,\"193\":1,\"194\":5,\"217\":1,\"237\":4,\"485\":2,\"600\":6,\"601\":5,\"618\":2,\"619\":1,\"621\":2,\"625\":2,\"629\":2,\"633\":6,\"637\":3,\"639\":2,\"641\":1,\"647\":5,\"658\":1,\"667\":1,\"671\":1,\"676\":19,\"677\":1,\"678\":7,\"679\":7,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":3,\"687\":3,\"688\":4,\"689\":4,\"691\":11,\"692\":2,\"693\":8,\"694\":2,\"695\":2,\"696\":4,\"697\":16,\"698\":1,\"699\":1,\"700\":3,\"710\":1,\"722\":2,\"725\":11,\"726\":2,\"734\":4,\"742\":9,\"749\":2,\"750\":3,\"751\":5,\"752\":1,\"754\":2,\"756\":1,\"758\":1,\"759\":3,\"773\":4,\"781\":15,\"794\":2,\"797\":8,\"806\":5,\"808\":1,\"812\":8,\"820\":1,\"821\":2,\"824\":11,\"825\":2,\"826\":4,\"828\":4,\"856\":1,\"857\":6,\"858\":1,\"865\":1,\"866\":1,\"889\":1,\"895\":2,\"906\":4,\"907\":1,\"908\":1,\"909\":3,\"910\":1,\"911\":1,\"912\":1,\"915\":1,\"917\":2,\"918\":1,\"922\":2,\"933\":4,\"948\":1,\"949\":1,\"987\":1,\"988\":1,\"1000\":1,\"1003\":3,\"1004\":3,\"1005\":3,\"1028\":3,\"1046\":11,\"1048\":6,\"1057\":4,\"1058\":1,\"1059\":4,\"1060\":2,\"1062\":1,\"1063\":4,\"1066\":12,\"1068\":3,\"1073\":3,\"1075\":7,\"1081\":1,\"1083\":3,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1103\":1,\"1133\":9,\"1138\":8,\"1139\":5,\"1140\":3,\"1148\":2,\"1171\":3,\"1172\":4,\"1173\":4,\"1176\":5,\"1190\":4,\"1193\":6,\"1203\":5,\"1205\":1,\"1206\":2,\"1211\":1,\"1214\":6,\"1219\":3,\"1221\":2,\"1244\":4,\"1256\":2,\"1269\":3,\"1270\":5,\"1272\":1,\"1273\":6,\"1286\":1,\"1289\":1,\"1329\":2,\"1336\":1,\"1337\":2,\"1356\":6,\"1375\":1,\"1389\":1,\"1392\":2,\"1406\":1,\"1412\":2,\"1414\":1,\"1426\":1,\"1427\":2,\"1429\":1,\"1432\":1,\"1436\":1,\"1443\":2,\"1454\":1,\"1463\":1,\"1484\":4,\"1505\":1,\"1510\":1,\"1511\":1,\"1515\":3,\"1516\":5,\"1522\":2,\"1523\":3,\"1524\":4,\"1525\":4,\"1528\":1,\"1529\":1,\"1530\":3,\"1531\":3,\"1534\":1,\"1539\":1,\"1551\":4,\"1552\":3,\"1553\":15,\"1554\":1,\"1558\":1,\"1563\":2,\"1568\":1,\"1600\":6,\"1601\":4,\"1603\":5,\"1604\":4,\"1611\":1,\"1622\":2,\"1626\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":1,\"1671\":3,\"1719\":2,\"1721\":1,\"1739\":5,\"1741\":1,\"1752\":2,\"1761\":1,\"1763\":1,\"1765\":19,\"1766\":8,\"1767\":12,\"1768\":8,\"1786\":10,\"1800\":11,\"1801\":3,\"1803\":15,\"1804\":27,\"1834\":3,\"1836\":18,\"1839\":18,\"1843\":9,\"1844\":15,\"1845\":6,\"1846\":4,\"1847\":7,\"1848\":7,\"1849\":8,\"1850\":1,\"1851\":18,\"1852\":1,\"1856\":8,\"1857\":3,\"1858\":10,\"1866\":3,\"1870\":12,\"1871\":6,\"1876\":3,\"1878\":15,\"1892\":2,\"1893\":2,\"1896\":1,\"1897\":1,\"1905\":7,\"1924\":1,\"1931\":1,\"1932\":3,\"1955\":3,\"1957\":4,\"1958\":3,\"1960\":4,\"1963\":1,\"1964\":2,\"1970\":4,\"1975\":2,\"1984\":6,\"2001\":6,\"2007\":1,\"2008\":1,\"2009\":1,\"2012\":1,\"2020\":4,\"2024\":1,\"2027\":4,\"2054\":1,\"2070\":3,\"2076\":4,\"2095\":3,\"2096\":2,\"2097\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2106\":4,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2135\":1,\"2142\":2,\"2153\":1,\"2178\":3,\"2179\":4,\"2181\":1,\"2182\":1,\"2184\":3,\"2185\":1,\"2186\":1,\"2189\":1,\"2191\":10,\"2194\":1,\"2195\":4,\"2196\":1,\"2197\":3,\"2200\":3,\"2201\":2,\"2202\":2,\"2203\":1,\"2204\":1,\"2208\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2257\":3,\"2261\":3,\"2263\":3,\"2264\":4,\"2267\":1,\"2268\":1,\"2270\":1,\"2272\":1,\"2294\":2,\"2343\":3,\"2344\":7,\"2349\":3,\"2373\":3,\"2384\":1,\"2387\":1,\"2394\":2,\"2429\":2,\"2433\":1,\"2476\":3,\"2500\":1,\"2514\":1,\"2530\":2,\"2552\":1,\"2554\":1,\"2555\":1,\"2556\":2,\"2568\":3,\"2572\":1,\"2600\":2,\"2617\":1,\"2635\":1,\"2659\":1}}],[\"lt\",{\"1\":{\"3\":4,\"10\":1,\"12\":1,\"36\":2,\"38\":6,\"39\":3,\"44\":3,\"51\":4,\"52\":6,\"57\":3,\"66\":4,\"80\":1,\"85\":2,\"88\":2,\"96\":3,\"97\":1,\"99\":5,\"127\":2,\"128\":1,\"134\":7,\"135\":13,\"136\":6,\"137\":1,\"175\":5,\"194\":5,\"203\":4,\"217\":3,\"224\":3,\"231\":2,\"237\":11,\"238\":3,\"239\":3,\"274\":2,\"275\":17,\"276\":11,\"277\":5,\"278\":4,\"279\":11,\"280\":3,\"281\":8,\"282\":13,\"283\":10,\"284\":10,\"285\":12,\"286\":10,\"290\":2,\"292\":1,\"294\":5,\"295\":1,\"296\":8,\"297\":10,\"298\":6,\"2373\":6,\"2375\":1,\"2430\":3,\"2431\":1,\"2555\":6,\"2559\":1,\"2567\":1,\"2584\":5,\"2637\":2,\"2638\":3,\"2643\":6}}],[\"louder\",{\"1\":{\"1928\":1}}],[\"loudness\",{\"0\":{\"1812\":1,\"1819\":1},\"1\":{\"1812\":1,\"1819\":1}}],[\"lora\",{\"0\":{\"1932\":1,\"1934\":1},\"1\":{\"429\":6,\"1930\":1,\"1932\":10,\"1934\":2}}],[\"lots\",{\"1\":{\"2585\":1}}],[\"lot\",{\"1\":{\"148\":1}}],[\"los\",{\"1\":{\"87\":1}}],[\"loss2\",{\"1\":{\"2201\":2}}],[\"loss1\",{\"1\":{\"2201\":2}}],[\"losswrapper\",{\"1\":{\"1603\":1,\"1622\":1}}],[\"lossfun=crossentropyloss\",{\"1\":{\"710\":1}}],[\"lossfun=<function\",{\"1\":{\"605\":1}}],[\"lossfun\",{\"1\":{\"605\":1,\"710\":1}}],[\"loss=0\",{\"1\":{\"2193\":1}}],[\"loss=loss\",{\"1\":{\"2170\":2}}],[\"loss=false\",{\"1\":{\"1466\":2,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1570\":2,\"1571\":2,\"1604\":2,\"1639\":2,\"1640\":2,\"1641\":2,\"1666\":2,\"1667\":2,\"1668\":2}}],[\"loss=44\",{\"1\":{\"87\":1}}],[\"loss=50\",{\"1\":{\"87\":1}}],[\"lossy\",{\"1\":{\"49\":3}}],[\"lossless\",{\"1\":{\"49\":1}}],[\"losses\",{\"0\":{\"1978\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1},\"1\":{\"22\":3,\"118\":1,\"794\":5,\"825\":2,\"1113\":1,\"1145\":1,\"1211\":1,\"1224\":1,\"1269\":2,\"1336\":1,\"1348\":1,\"1551\":2,\"1552\":2,\"1553\":2,\"1773\":1,\"1778\":1,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1978\":2,\"1984\":1,\"1996\":1,\"1997\":2,\"1999\":1,\"2000\":1,\"2082\":1,\"2170\":1,\"2240\":1,\"2278\":1,\"2294\":1}}],[\"loss\",{\"0\":{\"768\":1,\"1106\":2,\"1107\":2,\"1108\":2,\"1109\":2,\"1336\":1,\"1337\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1437\":2,\"1443\":1,\"1466\":1,\"1530\":1,\"1563\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1600\":1,\"1603\":1,\"1604\":1,\"1622\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1836\":1,\"1839\":1,\"1841\":1,\"1843\":1,\"1853\":1,\"1854\":1,\"1859\":1,\"1879\":1,\"1890\":1,\"1978\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2030\":1,\"2034\":2,\"2040\":1,\"2091\":1,\"2245\":1,\"2256\":1,\"2259\":1,\"2273\":1,\"2280\":1,\"2288\":2,\"2301\":2,\"2302\":1,\"2303\":2,\"2304\":2,\"2305\":1,\"2644\":1},\"1\":{\"17\":7,\"20\":3,\"22\":39,\"24\":2,\"28\":2,\"49\":1,\"56\":4,\"80\":2,\"87\":3,\"110\":2,\"112\":1,\"113\":13,\"118\":12,\"150\":2,\"173\":1,\"174\":2,\"194\":1,\"240\":9,\"241\":1,\"242\":2,\"251\":2,\"255\":1,\"259\":1,\"499\":1,\"605\":1,\"606\":1,\"615\":1,\"676\":2,\"702\":5,\"710\":3,\"734\":2,\"738\":5,\"742\":5,\"754\":9,\"755\":6,\"759\":1,\"762\":4,\"763\":3,\"767\":2,\"768\":6,\"781\":2,\"794\":5,\"811\":3,\"817\":2,\"820\":7,\"821\":14,\"822\":7,\"825\":58,\"826\":20,\"828\":2,\"896\":1,\"933\":3,\"1057\":20,\"1099\":1,\"1106\":3,\"1107\":3,\"1108\":3,\"1109\":3,\"1113\":1,\"1137\":1,\"1142\":1,\"1145\":3,\"1148\":1,\"1155\":1,\"1171\":2,\"1172\":2,\"1186\":2,\"1203\":1,\"1206\":2,\"1210\":2,\"1211\":1,\"1218\":1,\"1224\":1,\"1272\":1,\"1286\":1,\"1298\":1,\"1299\":1,\"1302\":2,\"1303\":2,\"1304\":1,\"1336\":2,\"1337\":2,\"1348\":2,\"1349\":2,\"1350\":2,\"1371\":4,\"1437\":3,\"1443\":2,\"1454\":3,\"1466\":3,\"1530\":3,\"1551\":14,\"1552\":5,\"1553\":16,\"1554\":3,\"1563\":3,\"1566\":3,\"1567\":3,\"1568\":4,\"1569\":3,\"1570\":4,\"1571\":3,\"1600\":4,\"1603\":7,\"1604\":5,\"1611\":1,\"1622\":5,\"1639\":5,\"1640\":5,\"1641\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1666\":3,\"1667\":4,\"1668\":3,\"1760\":1,\"1773\":3,\"1778\":32,\"1805\":32,\"1828\":1,\"1829\":1,\"1836\":9,\"1837\":3,\"1839\":7,\"1841\":5,\"1843\":8,\"1850\":26,\"1851\":3,\"1852\":30,\"1853\":5,\"1854\":3,\"1859\":5,\"1877\":26,\"1879\":8,\"1889\":1,\"1890\":6,\"1892\":3,\"1893\":1,\"1951\":1,\"1955\":1,\"1957\":1,\"1960\":1,\"1962\":1,\"1970\":2,\"1975\":2,\"1978\":3,\"1980\":1,\"1996\":2,\"1997\":3,\"1999\":3,\"2000\":6,\"2027\":2,\"2030\":3,\"2034\":4,\"2040\":1,\"2046\":1,\"2076\":2,\"2077\":1,\"2082\":2,\"2086\":6,\"2087\":3,\"2088\":6,\"2090\":19,\"2091\":13,\"2095\":15,\"2168\":3,\"2170\":13,\"2193\":2,\"2235\":1,\"2240\":2,\"2243\":3,\"2244\":3,\"2245\":9,\"2255\":3,\"2256\":10,\"2259\":7,\"2263\":15,\"2264\":17,\"2273\":2,\"2277\":1,\"2278\":2,\"2279\":3,\"2280\":9,\"2288\":5,\"2294\":1,\"2301\":3,\"2302\":1,\"2303\":3,\"2304\":3,\"2305\":1,\"2375\":2,\"2394\":5,\"2431\":1,\"2440\":1,\"2492\":1,\"2494\":2,\"2519\":1,\"2530\":5,\"2543\":4,\"2558\":3,\"2584\":1,\"2628\":1,\"2644\":9}}],[\"loops\",{\"1\":{\"1427\":7}}],[\"loop\",{\"1\":{\"607\":3,\"627\":1,\"2375\":1,\"2558\":1}}],[\"loopback\",{\"1\":{\"45\":1}}],[\"looks\",{\"1\":{\"1391\":1,\"1406\":1,\"2572\":1}}],[\"lookaheadwordlm\",{\"0\":{\"609\":1},\"1\":{\"609\":1}}],[\"look\",{\"1\":{\"62\":1,\"104\":2,\"144\":1,\"692\":1,\"693\":5,\"1149\":3,\"1150\":3,\"2558\":1,\"2600\":2}}],[\"longest\",{\"1\":{\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1}}],[\"longer\",{\"1\":{\"6\":1,\"612\":1,\"2387\":2,\"2398\":1,\"2534\":1}}],[\"longformerencoder\",{\"0\":{\"1203\":1},\"1\":{\"1203\":1}}],[\"longformerattention\",{\"0\":{\"780\":1}}],[\"longformer\",{\"0\":{\"780\":1,\"1203\":1},\"1\":{\"1203\":6}}],[\"longshortdata\",{\"0\":{\"288\":1},\"1\":{\"288\":1}}],[\"longtensororlist\",{\"1\":{\"899\":1,\"901\":1,\"903\":1}}],[\"longtensor\",{\"1\":{\"217\":1,\"224\":1,\"231\":1,\"690\":1,\"701\":2,\"702\":1,\"729\":1,\"730\":1,\"737\":1,\"738\":1,\"752\":1,\"754\":4,\"755\":2,\"756\":1,\"760\":1,\"762\":2,\"763\":2,\"774\":1,\"778\":1,\"782\":1,\"793\":1,\"794\":1,\"821\":4,\"822\":2,\"826\":4,\"831\":1,\"835\":2,\"925\":1,\"932\":1,\"1455\":1,\"1524\":2,\"1525\":2,\"1595\":1,\"1778\":13,\"1804\":16,\"1805\":13,\"1851\":2,\"1879\":3,\"2000\":2,\"2001\":3,\"2002\":4,\"2004\":3,\"2024\":4,\"2028\":4,\"2078\":1,\"2081\":1,\"2083\":2,\"2086\":18,\"2087\":18,\"2088\":1,\"2090\":19,\"2091\":4,\"2095\":18,\"2239\":1,\"2243\":7,\"2244\":8,\"2245\":4,\"2255\":8,\"2256\":4,\"2263\":4,\"2264\":4,\"2279\":8,\"2280\":4,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"long\",{\"0\":{\"51\":1},\"1\":{\"197\":1,\"215\":1,\"793\":1,\"1436\":1,\"1511\":1,\"1639\":1,\"1644\":1,\"1652\":1,\"1654\":1,\"1670\":1,\"1671\":1,\"2182\":1,\"2189\":1,\"2368\":1,\"2371\":1,\"2373\":2,\"2400\":1,\"2433\":2,\"2440\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2512\":2,\"2521\":1,\"2522\":1,\"2523\":1,\"2536\":1,\"2555\":2,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2657\":2}}],[\"loadinputsandtargets\",{\"0\":{\"987\":1},\"1\":{\"987\":3}}],[\"loading\",{\"0\":{\"347\":1,\"355\":1,\"361\":1,\"373\":1},\"1\":{\"49\":1,\"627\":1,\"1180\":1,\"1390\":1,\"1398\":1,\"1524\":1,\"1525\":1,\"1611\":1,\"1696\":2,\"1697\":2,\"1698\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1712\":2,\"1714\":1,\"1715\":2,\"1719\":2}}],[\"loaded\",{\"1\":{\"238\":3,\"634\":1,\"641\":1,\"653\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1}}],[\"loaders\",{\"1\":{\"2398\":1,\"2429\":1,\"2534\":1}}],[\"loader\",{\"0\":{\"60\":1,\"2222\":1,\"2223\":1,\"2225\":1,\"2226\":1,\"2228\":1,\"2229\":1,\"2231\":1},\"1\":{\"60\":4,\"174\":4,\"1384\":1,\"1386\":1,\"1417\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2222\":2,\"2223\":2,\"2225\":1,\"2226\":3,\"2228\":2,\"2229\":2,\"2231\":1,\"2398\":2,\"2534\":2}}],[\"load\",{\"0\":{\"171\":1,\"193\":1,\"194\":1,\"619\":1,\"634\":1,\"645\":1,\"646\":1,\"653\":1,\"868\":1,\"1100\":1,\"1417\":1,\"1419\":1,\"2152\":1,\"2157\":2,\"2224\":1,\"2368\":1,\"2472\":1,\"2486\":1,\"2489\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2648\":1,\"2649\":1},\"1\":{\"49\":1,\"66\":5,\"170\":1,\"171\":4,\"174\":1,\"175\":4,\"185\":3,\"193\":3,\"194\":6,\"202\":1,\"217\":4,\"224\":4,\"231\":4,\"238\":8,\"240\":1,\"619\":3,\"634\":3,\"645\":2,\"646\":2,\"651\":1,\"653\":3,\"734\":2,\"792\":2,\"820\":2,\"868\":2,\"987\":6,\"999\":3,\"1100\":2,\"1214\":1,\"1417\":1,\"1418\":1,\"1419\":2,\"1423\":1,\"1639\":2,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2022\":1,\"2152\":1,\"2157\":3,\"2158\":5,\"2193\":1,\"2200\":2,\"2224\":2,\"2254\":1,\"2360\":1,\"2368\":1,\"2386\":1,\"2398\":1,\"2401\":1,\"2440\":2,\"2458\":1,\"2486\":1,\"2490\":1,\"2514\":2,\"2520\":1,\"2523\":1,\"2534\":1,\"2537\":1,\"2558\":2,\"2571\":2,\"2573\":1,\"2582\":1,\"2600\":4,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2659\":2}}],[\"lo\",{\"1\":{\"45\":4}}],[\"lowpass\",{\"0\":{\"1939\":1},\"1\":{\"1905\":1,\"1939\":2}}],[\"low=0\",{\"1\":{\"1387\":1}}],[\"lowest\",{\"1\":{\"778\":1,\"1912\":1}}],[\"lower=\",{\"1\":{\"948\":1,\"961\":1}}],[\"lower=0\",{\"1\":{\"940\":1,\"952\":1}}],[\"lower\",{\"1\":{\"202\":1,\"217\":1,\"623\":1,\"1695\":1,\"1904\":1,\"1925\":1}}],[\"low\",{\"1\":{\"23\":1,\"119\":1,\"148\":1,\"1142\":1,\"1186\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1245\":1,\"1248\":3,\"1287\":1,\"1301\":1,\"1304\":1,\"1337\":1,\"1339\":1,\"1342\":1,\"1349\":1,\"1350\":1,\"1386\":1,\"1604\":1,\"1655\":1,\"1712\":5,\"1715\":5,\"1719\":1,\"2574\":1,\"2584\":1,\"2585\":1}}],[\"logcompression\",{\"0\":{\"1911\":1},\"1\":{\"1911\":2}}],[\"logflow\",{\"0\":{\"1855\":1},\"1\":{\"1855\":2}}],[\"logfile\",{\"1\":{\"108\":3}}],[\"logll\",{\"1\":{\"1142\":2,\"1301\":3,\"1304\":3}}],[\"loglikelihood\",{\"1\":{\"1142\":4}}],[\"logit\",{\"1\":{\"1115\":2,\"1210\":1,\"1211\":1,\"1286\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":1,\"1337\":1,\"2287\":1,\"2295\":1,\"2296\":1}}],[\"logits\",{\"1\":{\"822\":3,\"835\":1,\"1892\":1,\"2000\":3,\"2002\":1,\"2078\":1,\"2295\":1,\"2301\":2,\"2303\":1,\"2305\":4}}],[\"login\",{\"1\":{\"1\":1,\"70\":1}}],[\"logmel2linear\",{\"0\":{\"2330\":1},\"1\":{\"2330\":2}}],[\"logmelfbank\",{\"0\":{\"1989\":1,\"2248\":1},\"1\":{\"1989\":1,\"2248\":1}}],[\"logmelspectrogram\",{\"0\":{\"947\":1,\"967\":1},\"1\":{\"947\":1,\"967\":1}}],[\"logmel\",{\"0\":{\"778\":1,\"1912\":1},\"1\":{\"778\":1,\"1912\":2}}],[\"logreport\",{\"1\":{\"616\":1}}],[\"logzero\",{\"1\":{\"609\":1,\"611\":1}}],[\"logspectrogram\",{\"0\":{\"1991\":1,\"2250\":1},\"1\":{\"1991\":1,\"2250\":1}}],[\"logsoftmaxgradmodification\",{\"0\":{\"1202\":1},\"1\":{\"1202\":1}}],[\"logs\",{\"1\":{\"109\":1,\"1853\":4,\"1854\":4}}],[\"logprobs\",{\"1\":{\"1142\":2,\"1186\":2,\"1210\":2,\"1298\":4,\"1299\":4,\"1301\":4,\"1302\":4,\"1303\":4,\"1304\":4,\"1334\":5}}],[\"logps\",{\"1\":{\"917\":2}}],[\"logp\",{\"0\":{\"1334\":1},\"1\":{\"23\":1,\"119\":1,\"700\":1,\"917\":1,\"1048\":3,\"1138\":1,\"1139\":1,\"1334\":1,\"1618\":1,\"1619\":1,\"1638\":1}}],[\"logdir\",{\"1\":{\"17\":1,\"89\":1,\"110\":2,\"240\":1,\"278\":1,\"2375\":1,\"2401\":1,\"2440\":1,\"2537\":1,\"2558\":1,\"2559\":1,\"2571\":1}}],[\"logger=none\",{\"1\":{\"2193\":1}}],[\"logger\",{\"0\":{\"996\":1,\"1323\":1},\"1\":{\"629\":1,\"799\":1,\"996\":6,\"1323\":3}}],[\"logged\",{\"1\":{\"17\":1,\"109\":1}}],[\"logging\",{\"0\":{\"17\":1,\"68\":1},\"1\":{\"56\":1,\"108\":1,\"1551\":1,\"1553\":1}}],[\"log\",{\"0\":{\"87\":1,\"203\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1824\":1,\"1912\":1,\"1989\":1,\"1991\":1,\"2248\":1,\"2250\":1},\"1\":{\"17\":1,\"68\":1,\"87\":1,\"107\":1,\"108\":6,\"109\":1,\"110\":3,\"143\":7,\"169\":1,\"181\":1,\"197\":2,\"203\":8,\"240\":4,\"275\":3,\"279\":3,\"281\":2,\"282\":3,\"283\":3,\"284\":3,\"297\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"377\":4,\"380\":3,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":3,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"499\":2,\"501\":3,\"605\":4,\"629\":1,\"691\":1,\"693\":1,\"697\":1,\"701\":2,\"703\":2,\"707\":2,\"710\":4,\"734\":3,\"736\":1,\"737\":3,\"738\":3,\"739\":1,\"759\":3,\"767\":3,\"797\":1,\"799\":1,\"817\":3,\"828\":3,\"835\":1,\"857\":1,\"917\":1,\"1048\":1,\"1060\":1,\"1063\":1,\"1082\":1,\"1132\":3,\"1142\":7,\"1145\":3,\"1155\":4,\"1158\":1,\"1171\":2,\"1186\":4,\"1206\":2,\"1207\":1,\"1210\":2,\"1211\":1,\"1216\":2,\"1224\":1,\"1244\":1,\"1247\":1,\"1248\":1,\"1252\":1,\"1285\":1,\"1287\":1,\"1293\":1,\"1298\":2,\"1299\":2,\"1301\":3,\"1302\":2,\"1303\":2,\"1304\":3,\"1331\":1,\"1332\":2,\"1333\":1,\"1334\":1,\"1552\":2,\"1618\":4,\"1619\":3,\"1638\":3,\"1778\":1,\"1804\":1,\"1805\":1,\"1824\":2,\"1829\":1,\"1833\":1,\"1838\":1,\"1840\":1,\"1841\":3,\"1850\":1,\"1851\":1,\"1852\":1,\"1855\":3,\"1859\":3,\"1865\":1,\"1868\":2,\"1877\":1,\"1878\":1,\"1889\":4,\"1911\":3,\"1912\":3,\"1917\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1,\"1964\":1,\"1989\":3,\"1991\":2,\"2091\":2,\"2186\":2,\"2193\":2,\"2199\":2,\"2202\":4,\"2204\":2,\"2236\":1,\"2248\":3,\"2250\":2,\"2330\":2,\"2375\":4,\"2403\":1,\"2539\":1,\"2558\":3,\"2559\":4,\"2568\":6,\"2574\":1}}],[\"locations\",{\"0\":{\"1018\":1,\"1021\":1,\"1022\":1},\"1\":{\"1011\":3,\"1018\":2,\"1021\":1,\"1022\":1,\"1036\":2}}],[\"location\",{\"1\":{\"31\":1,\"286\":4,\"296\":3,\"679\":2,\"683\":1,\"684\":2,\"685\":2,\"688\":2,\"689\":2,\"1017\":1,\"1220\":1,\"1289\":1,\"2157\":1,\"2263\":1}}],[\"located\",{\"1\":{\"1\":1,\"926\":1,\"2415\":1,\"2543\":1}}],[\"locale\",{\"1\":{\"2482\":2}}],[\"localrank\",{\"1\":{\"644\":1}}],[\"localhost\",{\"1\":{\"17\":1,\"596\":1}}],[\"local\",{\"0\":{\"5\":1,\"1526\":1,\"2213\":1},\"1\":{\"5\":11,\"12\":1,\"85\":2,\"134\":1,\"136\":1,\"141\":1,\"142\":1,\"169\":1,\"181\":1,\"235\":1,\"363\":1,\"429\":2,\"1517\":1,\"1526\":2,\"1862\":2,\"1880\":2,\"2180\":2,\"2213\":1,\"2216\":1,\"2217\":1,\"2372\":1,\"2385\":4,\"2387\":3,\"2394\":1,\"2415\":2,\"2416\":2,\"2429\":6,\"2431\":1,\"2432\":3,\"2530\":1,\"2554\":1,\"2566\":1,\"2568\":8,\"2583\":1,\"2586\":1,\"2638\":4}}],[\"fkai\",{\"1\":{\"2432\":1}}],[\"ftshijt\",{\"1\":{\"2356\":1,\"2357\":4,\"2359\":1,\"2450\":1,\"2456\":1,\"2516\":1,\"2517\":1,\"2521\":1,\"2576\":1,\"2578\":4,\"2580\":1}}],[\"ftswish\",{\"0\":{\"1061\":1},\"1\":{\"115\":4,\"1061\":6,\"1096\":6}}],[\"fprs\",{\"1\":{\"2311\":1}}],[\"fp16\",{\"1\":{\"98\":1}}],[\"fwskattention\",{\"0\":{\"2075\":1},\"1\":{\"2075\":1}}],[\"fbaipublicfiles\",{\"1\":{\"2431\":2,\"2432\":2}}],[\"fbankdir\",{\"1\":{\"294\":1}}],[\"fbanks\",{\"1\":{\"174\":4}}],[\"fbank\",{\"0\":{\"275\":1,\"283\":1,\"509\":1,\"519\":1,\"541\":1,\"1989\":1,\"2248\":1},\"1\":{\"169\":1,\"171\":3,\"175\":4,\"181\":1,\"185\":2,\"193\":2,\"194\":1,\"202\":2,\"238\":12,\"251\":6,\"259\":6,\"275\":4,\"282\":2,\"283\":5,\"294\":1,\"509\":3,\"519\":3,\"541\":3,\"778\":1,\"959\":1,\"1158\":1,\"1912\":1,\"1989\":2,\"2248\":2,\"2519\":1,\"2520\":2}}],[\"fb\",{\"0\":{\"1810\":1},\"1\":{\"1785\":2,\"1810\":3}}],[\"f0=none\",{\"1\":{\"1800\":2}}],[\"f0\",{\"0\":{\"1811\":2},\"1\":{\"1773\":2,\"1776\":4,\"1778\":2,\"1797\":5,\"1800\":1,\"1804\":5,\"1805\":3,\"1811\":8,\"2082\":2,\"2086\":3,\"2087\":3,\"2090\":5,\"2091\":2,\"2095\":3,\"2236\":5}}],[\"f0max\",{\"1\":{\"562\":2,\"2236\":1}}],[\"f0min\",{\"1\":{\"562\":2,\"2236\":1}}],[\"fgnt\",{\"1\":{\"1717\":2}}],[\"f2146bdc7abf293186de9449bfa2272775e39e1d\",{\"1\":{\"2466\":1,\"2646\":1}}],[\"f2\",{\"1\":{\"1452\":3}}],[\"f1\",{\"1\":{\"1452\":3}}],[\"f=30\",{\"1\":{\"965\":1,\"1019\":1,\"1037\":1}}],[\"fd\",{\"1\":{\"429\":2,\"2106\":2,\"2182\":1,\"2222\":1}}],[\"fmax=none\",{\"1\":{\"947\":1,\"954\":1,\"967\":1,\"971\":1}}],[\"fmax\",{\"1\":{\"251\":2,\"259\":2,\"275\":2,\"509\":2,\"519\":2,\"752\":1,\"778\":3,\"1158\":1,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1859\":2,\"1877\":1,\"1912\":3,\"1989\":1,\"2248\":1,\"2317\":1,\"2330\":1}}],[\"fmin=none\",{\"1\":{\"947\":1,\"954\":1,\"967\":1,\"971\":1}}],[\"fmin\",{\"1\":{\"251\":2,\"259\":2,\"275\":2,\"509\":2,\"519\":2,\"752\":1,\"778\":2,\"1158\":1,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1859\":2,\"1877\":1,\"1912\":2,\"1989\":1,\"2248\":1,\"2317\":1,\"2330\":1}}],[\"fnrs\",{\"1\":{\"2311\":1}}],[\"fname=$\",{\"1\":{\"2568\":1}}],[\"fname\",{\"1\":{\"1388\":1,\"1390\":1,\"1394\":1,\"1399\":1,\"1401\":1,\"1405\":1,\"1409\":1,\"1413\":1,\"2568\":1}}],[\"fn=slutask\",{\"1\":{\"2474\":1,\"2649\":1}}],[\"fn=cls\",{\"1\":{\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1}}],[\"fn=collate\",{\"1\":{\"174\":2}}],[\"fn=collcate\",{\"1\":{\"60\":1}}],[\"fn=none\",{\"1\":{\"651\":1,\"1340\":1,\"1895\":1,\"1896\":1,\"1900\":1,\"2078\":1,\"2350\":1}}],[\"fn\",{\"0\":{\"1901\":1,\"1931\":1,\"1932\":1,\"1933\":1,\"1934\":1,\"2177\":1,\"2188\":1,\"2208\":2},\"1\":{\"56\":2,\"60\":12,\"623\":2,\"629\":2,\"787\":1,\"799\":1,\"914\":3,\"1017\":1,\"1115\":2,\"1145\":1,\"1153\":1,\"1182\":1,\"1198\":1,\"1207\":1,\"1218\":1,\"1340\":1,\"1343\":1,\"1451\":2,\"1514\":2,\"1557\":2,\"1585\":2,\"1612\":1,\"1615\":1,\"1623\":3,\"1637\":2,\"1647\":1,\"1901\":1,\"1931\":1,\"1932\":1,\"1933\":1,\"1934\":1,\"2078\":1,\"2096\":5,\"2097\":2,\"2098\":5,\"2099\":7,\"2100\":5,\"2101\":5,\"2102\":5,\"2103\":5,\"2104\":5,\"2105\":5,\"2106\":4,\"2107\":5,\"2108\":5,\"2109\":5,\"2110\":5,\"2111\":1,\"2112\":5,\"2113\":5,\"2114\":5,\"2115\":5,\"2116\":5,\"2117\":5,\"2118\":5,\"2156\":1,\"2177\":3,\"2188\":3,\"2208\":4,\"2209\":1,\"2474\":4,\"2649\":4}}],[\"fsbd\",{\"1\":{\"1761\":1,\"1763\":1,\"1805\":1}}],[\"fst\",{\"0\":{\"1427\":1,\"1428\":1,\"1429\":1,\"2688\":1},\"1\":{\"1427\":1,\"1428\":2,\"1429\":3}}],[\"fsavec\",{\"1\":{\"1427\":2}}],[\"fsas\",{\"1\":{\"1427\":7,\"1429\":1}}],[\"fsa\",{\"1\":{\"1047\":1,\"1076\":1,\"1101\":1,\"1102\":1,\"1136\":1,\"1427\":3,\"1428\":1}}],[\"fs=sr\",{\"1\":{\"2372\":1,\"2497\":1,\"2498\":3,\"2614\":1,\"2615\":1,\"2616\":3,\"2632\":1,\"2633\":1,\"2634\":3}}],[\"fs=24000\",{\"1\":{\"1774\":1}}],[\"fs=48000\",{\"1\":{\"1462\":1,\"1464\":1}}],[\"fs=16000\",{\"1\":{\"1184\":1,\"1774\":1}}],[\"fs=16k\",{\"1\":{\"47\":1}}],[\"fs=none\",{\"1\":{\"648\":1,\"1255\":1,\"1462\":1,\"1464\":1}}],[\"fs\",{\"1\":{\"47\":2,\"217\":1,\"218\":1,\"224\":1,\"225\":1,\"231\":1,\"232\":1,\"247\":2,\"251\":2,\"259\":2,\"275\":2,\"282\":2,\"295\":2,\"297\":2,\"301\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"509\":2,\"512\":2,\"519\":2,\"541\":2,\"585\":2,\"648\":1,\"752\":1,\"778\":3,\"947\":1,\"954\":1,\"959\":1,\"967\":1,\"971\":1,\"992\":2,\"995\":2,\"1158\":1,\"1198\":2,\"1239\":1,\"1255\":1,\"1284\":1,\"1431\":1,\"1435\":1,\"1462\":3,\"1463\":3,\"1464\":3,\"1510\":2,\"1511\":2,\"1553\":2,\"1616\":2,\"1617\":2,\"1643\":3,\"1644\":3,\"1778\":1,\"1791\":2,\"1804\":2,\"1805\":1,\"1850\":1,\"1852\":1,\"1859\":2,\"1877\":1,\"1896\":1,\"1904\":2,\"1912\":3,\"1916\":2,\"1917\":2,\"1923\":1,\"1989\":1,\"2084\":1,\"2089\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2236\":1,\"2241\":1,\"2248\":1,\"2266\":1,\"2267\":3,\"2317\":2,\"2330\":2,\"2357\":5,\"2359\":1,\"2365\":2,\"2371\":1,\"2455\":1,\"2456\":1,\"2460\":2,\"2492\":1,\"2501\":3,\"2508\":2,\"2510\":2,\"2514\":1,\"2515\":2,\"2520\":1,\"2521\":2,\"2522\":1,\"2578\":5,\"2580\":1,\"2581\":1,\"2612\":1,\"2628\":1,\"2630\":1,\"2655\":2,\"2659\":1,\"2660\":2}}],[\"fues\",{\"1\":{\"2063\":1,\"2075\":1}}],[\"fuction\",{\"1\":{\"1171\":1,\"1206\":1,\"1429\":1,\"1552\":1,\"1953\":1,\"1955\":1}}],[\"future\",{\"1\":{\"1005\":1,\"1028\":1,\"1148\":1,\"1203\":1,\"1719\":3,\"2046\":2,\"2054\":1,\"2201\":1}}],[\"fuse\",{\"1\":{\"1115\":2,\"2074\":1}}],[\"fusedpostfrontends\",{\"0\":{\"1774\":1},\"1\":{\"1774\":1}}],[\"fusedfrontends\",{\"0\":{\"1184\":1},\"1\":{\"1184\":2}}],[\"fused\",{\"0\":{\"1184\":1,\"1774\":1},\"1\":{\"745\":1,\"746\":1,\"1116\":1,\"1184\":2,\"1688\":2,\"1756\":2,\"1774\":1,\"1807\":1}}],[\"fusion\",{\"1\":{\"700\":1,\"1048\":1,\"1116\":1,\"1138\":1,\"1139\":1,\"1179\":1}}],[\"furthermore\",{\"1\":{\"689\":1}}],[\"further\",{\"1\":{\"608\":1,\"700\":1,\"1138\":1,\"1139\":1}}],[\"funcname\",{\"1\":{\"2568\":1}}],[\"funcs\",{\"0\":{\"1961\":2,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":2,\"1966\":2,\"1967\":2,\"1968\":2,\"2153\":1,\"2166\":1,\"2696\":1},\"1\":{\"1898\":1,\"1961\":4,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":2,\"1966\":2,\"1967\":4,\"1968\":4,\"2153\":1,\"2166\":2}}],[\"func=none\",{\"1\":{\"979\":1}}],[\"functor\",{\"1\":{\"2177\":1,\"2188\":1}}],[\"functrans\",{\"0\":{\"944\":1},\"1\":{\"943\":1,\"944\":3,\"950\":1,\"955\":1,\"956\":1}}],[\"functioning\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"function`\",{\"1\":{\"2252\":1}}],[\"functionality\",{\"1\":{\"2635\":1}}],[\"functional\",{\"0\":{\"944\":1},\"1\":{\"944\":3,\"1257\":1,\"1336\":1,\"1348\":1,\"1452\":1,\"1917\":3}}],[\"functionaloptimizeradaptor\",{\"1\":{\"665\":1}}],[\"functionnode\",{\"1\":{\"745\":3,\"746\":3}}],[\"functionnodes\",{\"1\":{\"608\":1}}],[\"functions\",{\"0\":{\"2644\":1},\"1\":{\"117\":1,\"162\":1,\"944\":1,\"958\":1,\"1093\":1,\"1327\":1,\"1638\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1699\":1,\"2090\":1,\"2267\":1,\"2406\":1,\"2407\":1}}],[\"function\",{\"0\":{\"1825\":1,\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1},\"1\":{\"46\":1,\"60\":1,\"106\":1,\"115\":1,\"116\":2,\"117\":1,\"596\":1,\"605\":2,\"607\":2,\"608\":1,\"623\":1,\"629\":2,\"630\":1,\"631\":1,\"651\":1,\"652\":1,\"656\":1,\"690\":1,\"692\":1,\"693\":1,\"697\":1,\"700\":1,\"702\":1,\"710\":2,\"727\":2,\"728\":2,\"729\":1,\"730\":1,\"731\":1,\"738\":1,\"740\":1,\"741\":1,\"743\":1,\"745\":4,\"746\":4,\"753\":1,\"755\":1,\"757\":1,\"759\":1,\"761\":1,\"762\":1,\"763\":1,\"775\":1,\"776\":1,\"779\":1,\"782\":1,\"784\":1,\"797\":2,\"819\":1,\"821\":1,\"822\":1,\"832\":1,\"857\":1,\"859\":2,\"860\":2,\"862\":2,\"875\":3,\"883\":1,\"909\":1,\"914\":1,\"946\":1,\"952\":1,\"976\":1,\"981\":5,\"999\":2,\"1000\":2,\"1001\":1,\"1008\":2,\"1013\":1,\"1016\":2,\"1025\":1,\"1043\":1,\"1051\":1,\"1064\":1,\"1065\":1,\"1066\":2,\"1067\":2,\"1069\":1,\"1070\":1,\"1084\":1,\"1086\":4,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1096\":4,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1132\":3,\"1135\":1,\"1137\":1,\"1148\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1171\":1,\"1175\":1,\"1180\":1,\"1185\":1,\"1187\":5,\"1189\":1,\"1198\":3,\"1202\":5,\"1203\":1,\"1206\":1,\"1208\":1,\"1209\":1,\"1213\":1,\"1216\":1,\"1218\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1248\":1,\"1250\":1,\"1252\":1,\"1253\":1,\"1254\":2,\"1255\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1286\":4,\"1287\":4,\"1343\":3,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1375\":2,\"1430\":1,\"1432\":2,\"1434\":1,\"1436\":3,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1466\":1,\"1467\":1,\"1469\":1,\"1470\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1505\":2,\"1507\":1,\"1509\":1,\"1510\":1,\"1511\":2,\"1513\":1,\"1515\":1,\"1519\":1,\"1521\":1,\"1524\":1,\"1525\":1,\"1528\":1,\"1529\":1,\"1533\":1,\"1534\":1,\"1536\":1,\"1537\":1,\"1538\":1,\"1539\":2,\"1541\":1,\"1543\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1552\":1,\"1556\":1,\"1559\":1,\"1560\":2,\"1562\":1,\"1565\":1,\"1574\":1,\"1581\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1595\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1626\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1638\":1,\"1642\":1,\"1643\":1,\"1644\":2,\"1646\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1657\":1,\"1658\":2,\"1659\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":2,\"1665\":2,\"1669\":1,\"1670\":1,\"1671\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1713\":1,\"1714\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1762\":1,\"1764\":1,\"1765\":2,\"1770\":1,\"1771\":1,\"1775\":1,\"1778\":1,\"1780\":1,\"1783\":1,\"1787\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1798\":1,\"1800\":2,\"1802\":1,\"1803\":2,\"1804\":1,\"1807\":1,\"1825\":1,\"1834\":2,\"1844\":2,\"1848\":2,\"1849\":2,\"1851\":3,\"1856\":4,\"1857\":5,\"1858\":4,\"1861\":2,\"1866\":2,\"1867\":4,\"1871\":5,\"1873\":3,\"1874\":1,\"1876\":2,\"1878\":1,\"1891\":1,\"1903\":1,\"1906\":1,\"1908\":1,\"1910\":1,\"1911\":2,\"1913\":1,\"1914\":1,\"1915\":1,\"1917\":8,\"1918\":1,\"1919\":1,\"1920\":1,\"1926\":1,\"1927\":1,\"1929\":1,\"1941\":2,\"1946\":1,\"1947\":2,\"1952\":1,\"1954\":2,\"1956\":2,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2002\":1,\"2012\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2054\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2063\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2078\":1,\"2084\":2,\"2086\":1,\"2088\":1,\"2089\":1,\"2090\":2,\"2091\":1,\"2095\":2,\"2096\":2,\"2097\":4,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2106\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2137\":1,\"2150\":1,\"2156\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2252\":1,\"2255\":1,\"2256\":1,\"2263\":2,\"2267\":2,\"2268\":3,\"2270\":2,\"2272\":2,\"2276\":1,\"2279\":1,\"2280\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2317\":1,\"2318\":1,\"2322\":1,\"2325\":1,\"2349\":1,\"2392\":1,\"2425\":1,\"2459\":1,\"2528\":1,\"2548\":1,\"2600\":1,\"2644\":1}}],[\"func\",{\"0\":{\"2321\":1},\"1\":{\"596\":1,\"608\":2,\"650\":1,\"944\":2,\"1008\":5,\"1644\":1,\"1917\":2,\"2086\":1,\"2087\":1,\"2321\":2,\"2322\":1,\"2323\":2,\"2329\":1}}],[\"full\",{\"1\":{\"16\":1,\"49\":1,\"75\":1,\"116\":1,\"121\":1,\"249\":1,\"691\":11,\"692\":4,\"697\":15,\"745\":1,\"746\":1,\"797\":6,\"857\":1,\"962\":1,\"1065\":1,\"1066\":1,\"1093\":1,\"1101\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1245\":1,\"1253\":2,\"1254\":1,\"1256\":1,\"1279\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"2002\":2,\"2142\":1,\"2355\":1,\"2363\":7,\"2372\":1,\"2384\":1,\"2424\":1,\"2433\":1,\"2440\":1,\"2450\":2,\"2482\":2,\"2512\":2,\"2513\":2,\"2521\":1,\"2522\":1,\"2523\":1,\"2547\":1,\"2555\":1,\"2653\":7,\"2657\":2}}],[\"fully\",{\"1\":{\"5\":1,\"1132\":1,\"1561\":1,\"1716\":1,\"2099\":1,\"2102\":1}}],[\"feb\",{\"1\":{\"2441\":2,\"2442\":2}}],[\"fed\",{\"1\":{\"1269\":1,\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"fetch\",{\"1\":{\"207\":1}}],[\"fewer\",{\"1\":{\"272\":1}}],[\"few\",{\"1\":{\"167\":1,\"178\":1,\"234\":1,\"2372\":1,\"2383\":1,\"2385\":1,\"2393\":1,\"2396\":1,\"2427\":1,\"2430\":1,\"2437\":1,\"2529\":1,\"2532\":1,\"2550\":1,\"2555\":1,\"2563\":1,\"2568\":1,\"2584\":1}}],[\"feat=\",{\"1\":{\"987\":1}}],[\"feat2\",{\"1\":{\"276\":1}}],[\"feat1\",{\"1\":{\"276\":1}}],[\"featbin\",{\"1\":{\"167\":5,\"178\":5,\"196\":4,\"234\":4}}],[\"feat\",{\"0\":{\"281\":1,\"533\":1},\"1\":{\"77\":2,\"78\":2,\"171\":1,\"174\":1,\"175\":1,\"185\":1,\"193\":1,\"274\":1,\"276\":3,\"281\":2,\"327\":2,\"449\":2,\"533\":2,\"564\":2,\"676\":1,\"692\":1,\"695\":1,\"696\":1,\"731\":1,\"734\":1,\"740\":2,\"741\":2,\"771\":2,\"773\":1,\"775\":2,\"776\":2,\"778\":1,\"785\":2,\"809\":2,\"828\":1,\"964\":1,\"987\":1,\"1115\":2,\"1133\":4,\"1179\":1,\"1190\":2,\"1204\":1,\"1209\":2,\"1214\":3,\"1244\":2,\"1273\":4,\"1366\":1,\"1375\":2,\"1397\":1,\"1408\":1,\"1555\":1,\"1778\":10,\"1805\":6,\"1842\":1,\"1850\":6,\"1852\":10,\"1877\":6,\"1912\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1993\":1,\"2001\":2,\"2002\":1,\"2046\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2253\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2432\":3,\"2592\":1}}],[\"feats2npy\",{\"0\":{\"536\":1},\"1\":{\"536\":2}}],[\"feats2\",{\"1\":{\"75\":2,\"76\":4,\"77\":2,\"78\":2,\"564\":1}}],[\"feats\",{\"0\":{\"509\":1,\"512\":1,\"522\":1,\"1981\":2,\"1987\":1,\"1989\":1,\"1991\":1,\"2084\":2,\"2085\":2,\"2089\":2,\"2094\":2,\"2233\":2,\"2236\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2275\":2,\"2281\":1,\"2433\":1},\"1\":{\"74\":2,\"75\":2,\"76\":4,\"77\":4,\"78\":4,\"79\":2,\"102\":1,\"202\":1,\"238\":2,\"276\":2,\"279\":1,\"281\":3,\"283\":1,\"284\":1,\"509\":2,\"512\":2,\"522\":2,\"564\":1,\"726\":4,\"778\":1,\"911\":2,\"1013\":2,\"1014\":2,\"1053\":1,\"1057\":7,\"1071\":13,\"1113\":2,\"1171\":2,\"1172\":1,\"1206\":1,\"1371\":4,\"1377\":2,\"1551\":2,\"1552\":1,\"1553\":3,\"1554\":2,\"1773\":5,\"1778\":7,\"1804\":16,\"1805\":19,\"1829\":7,\"1834\":2,\"1837\":2,\"1839\":4,\"1841\":1,\"1842\":1,\"1850\":10,\"1851\":13,\"1852\":5,\"1853\":5,\"1854\":4,\"1862\":2,\"1863\":5,\"1876\":1,\"1877\":9,\"1878\":19,\"1881\":3,\"1889\":3,\"1892\":1,\"1893\":1,\"1912\":1,\"1953\":1,\"1955\":1,\"1964\":1,\"1970\":2,\"1975\":2,\"1980\":2,\"1981\":2,\"1984\":3,\"1987\":1,\"1989\":1,\"1991\":1,\"2001\":5,\"2002\":10,\"2004\":5,\"2027\":2,\"2046\":4,\"2076\":2,\"2077\":2,\"2082\":5,\"2084\":2,\"2085\":2,\"2086\":7,\"2087\":7,\"2089\":2,\"2090\":8,\"2091\":3,\"2094\":2,\"2095\":10,\"2168\":1,\"2170\":1,\"2233\":2,\"2235\":2,\"2236\":2,\"2237\":1,\"2239\":2,\"2240\":2,\"2241\":3,\"2243\":8,\"2244\":8,\"2245\":3,\"2246\":1,\"2248\":1,\"2250\":1,\"2254\":3,\"2255\":8,\"2256\":3,\"2263\":10,\"2264\":10,\"2266\":3,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2275\":2,\"2277\":2,\"2278\":2,\"2279\":5,\"2280\":3,\"2281\":1,\"2292\":2,\"2294\":2,\"2429\":2,\"2431\":3,\"2432\":12,\"2433\":4,\"2436\":2,\"2440\":5,\"2520\":2}}],[\"featurematchloss\",{\"0\":{\"1839\":1},\"1\":{\"1839\":2}}],[\"featuremapdense\",{\"0\":{\"1561\":1},\"1\":{\"1561\":2}}],[\"featuretransform\",{\"0\":{\"752\":1},\"1\":{\"752\":1}}],[\"feature=true\",{\"1\":{\"102\":1}}],[\"features\",{\"0\":{\"2431\":1},\"1\":{\"35\":1,\"60\":1,\"73\":5,\"84\":2,\"100\":1,\"112\":3,\"238\":1,\"241\":1,\"536\":1,\"692\":1,\"693\":1,\"710\":2,\"711\":2,\"732\":1,\"735\":1,\"740\":1,\"741\":1,\"742\":1,\"754\":4,\"755\":1,\"760\":2,\"770\":1,\"771\":1,\"774\":1,\"775\":1,\"776\":1,\"784\":1,\"785\":1,\"809\":1,\"820\":2,\"821\":9,\"822\":1,\"826\":4,\"911\":1,\"1057\":4,\"1058\":4,\"1071\":9,\"1115\":2,\"1116\":4,\"1141\":1,\"1170\":1,\"1179\":1,\"1198\":1,\"1209\":1,\"1222\":3,\"1269\":4,\"1282\":3,\"1465\":2,\"1468\":1,\"1476\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1503\":1,\"1522\":1,\"1583\":1,\"1592\":1,\"1613\":1,\"1624\":1,\"1627\":1,\"1652\":1,\"1654\":1,\"1676\":1,\"1773\":3,\"1778\":1,\"1804\":2,\"1805\":1,\"1837\":2,\"1862\":4,\"1880\":2,\"1881\":1,\"1895\":1,\"1900\":1,\"2000\":1,\"2001\":1,\"2002\":3,\"2004\":1,\"2012\":1,\"2024\":1,\"2028\":1,\"2040\":2,\"2044\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2064\":1,\"2068\":1,\"2070\":1,\"2078\":2,\"2079\":2,\"2081\":1,\"2082\":3,\"2083\":1,\"2086\":5,\"2087\":5,\"2088\":1,\"2090\":4,\"2091\":1,\"2095\":3,\"2142\":1,\"2240\":3,\"2243\":3,\"2244\":3,\"2245\":1,\"2255\":3,\"2256\":1,\"2257\":1,\"2261\":1,\"2263\":3,\"2264\":3,\"2266\":1,\"2278\":3,\"2279\":2,\"2280\":1,\"2309\":1,\"2399\":1,\"2431\":2,\"2432\":1,\"2440\":1,\"2441\":1,\"2535\":1,\"2574\":2}}],[\"feature\",{\"0\":{\"100\":1,\"209\":1,\"238\":1,\"476\":1,\"752\":1,\"760\":1,\"778\":1,\"831\":1,\"881\":2,\"932\":1,\"1728\":1,\"1743\":1,\"2432\":1},\"1\":{\"16\":1,\"21\":1,\"59\":2,\"60\":1,\"62\":1,\"74\":3,\"75\":3,\"76\":3,\"78\":1,\"84\":2,\"102\":1,\"109\":2,\"134\":1,\"169\":2,\"171\":2,\"175\":1,\"181\":2,\"185\":1,\"193\":1,\"235\":2,\"238\":2,\"295\":2,\"476\":2,\"509\":1,\"512\":1,\"522\":1,\"533\":1,\"676\":5,\"691\":7,\"692\":4,\"693\":2,\"695\":1,\"696\":2,\"697\":8,\"706\":5,\"726\":3,\"730\":1,\"734\":2,\"752\":1,\"760\":1,\"773\":2,\"774\":1,\"778\":1,\"781\":2,\"796\":1,\"797\":4,\"812\":1,\"815\":2,\"817\":2,\"828\":2,\"829\":2,\"831\":1,\"835\":3,\"857\":1,\"881\":2,\"932\":1,\"1071\":4,\"1115\":4,\"1116\":1,\"1132\":2,\"1133\":1,\"1179\":1,\"1180\":1,\"1190\":2,\"1198\":1,\"1208\":1,\"1214\":1,\"1221\":2,\"1239\":1,\"1244\":2,\"1245\":1,\"1248\":1,\"1254\":1,\"1255\":1,\"1269\":7,\"1273\":1,\"1377\":2,\"1379\":1,\"1430\":2,\"1454\":1,\"1460\":1,\"1463\":1,\"1470\":3,\"1471\":3,\"1505\":2,\"1511\":2,\"1515\":3,\"1516\":1,\"1522\":1,\"1523\":2,\"1525\":1,\"1528\":3,\"1529\":3,\"1531\":2,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":4,\"1551\":5,\"1553\":6,\"1554\":2,\"1558\":3,\"1560\":1,\"1561\":1,\"1581\":1,\"1602\":5,\"1626\":2,\"1645\":1,\"1646\":4,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":2,\"1656\":2,\"1658\":2,\"1659\":3,\"1669\":2,\"1670\":4,\"1671\":3,\"1728\":2,\"1743\":2,\"1767\":1,\"1768\":1,\"1778\":4,\"1786\":1,\"1791\":1,\"1804\":3,\"1805\":6,\"1829\":2,\"1839\":3,\"1850\":5,\"1851\":4,\"1852\":5,\"1862\":2,\"1877\":5,\"1878\":7,\"1880\":2,\"1881\":3,\"1889\":1,\"1895\":1,\"1900\":1,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"1971\":1,\"2001\":1,\"2002\":1,\"2032\":1,\"2040\":1,\"2044\":1,\"2046\":1,\"2049\":1,\"2050\":2,\"2052\":1,\"2054\":1,\"2055\":1,\"2064\":1,\"2068\":1,\"2070\":1,\"2081\":1,\"2083\":2,\"2090\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2254\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2292\":1,\"2354\":1,\"2388\":1,\"2415\":1,\"2421\":1,\"2429\":3,\"2431\":4,\"2432\":8,\"2433\":1,\"2440\":1,\"2441\":2,\"2468\":1,\"2524\":1,\"2544\":1}}],[\"feel\",{\"1\":{\"59\":1,\"74\":1}}],[\"feedforwardtransformerloss\",{\"0\":{\"755\":1},\"1\":{\"755\":1}}],[\"feedforwardtransformer\",{\"0\":{\"754\":1},\"1\":{\"754\":1}}],[\"feedforward\",{\"0\":{\"1062\":1,\"1183\":1,\"1242\":1},\"1\":{\"732\":2,\"747\":2,\"748\":2,\"1062\":5,\"1075\":1,\"1181\":1,\"1183\":1,\"1269\":1}}],[\"feeds\",{\"1\":{\"710\":1}}],[\"feed\",{\"0\":{\"801\":1,\"1062\":1,\"1070\":1},\"1\":{\"21\":2,\"46\":1,\"115\":4,\"116\":2,\"711\":6,\"749\":1,\"754\":3,\"755\":2,\"774\":1,\"786\":1,\"801\":2,\"827\":3,\"1050\":6,\"1056\":6,\"1062\":1,\"1066\":1,\"1070\":2,\"1074\":2,\"1075\":1,\"1093\":1,\"1133\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1170\":6,\"1200\":1,\"1203\":1,\"1269\":2,\"1272\":1,\"1505\":1,\"1669\":1,\"2001\":1,\"2004\":1,\"2029\":1,\"2046\":1,\"2054\":1,\"2091\":1,\"2243\":1,\"2245\":1,\"2256\":1,\"2280\":1}}],[\"fa1b865352475b744c37f70440de1cc6b257ba70\",{\"1\":{\"2585\":1}}],[\"familiar\",{\"1\":{\"2401\":1,\"2537\":1}}],[\"fa\",{\"1\":{\"2311\":1,\"2341\":1}}],[\"fatory\",{\"1\":{\"2099\":1,\"2102\":1}}],[\"fatal\",{\"1\":{\"83\":1}}],[\"fake\",{\"1\":{\"1108\":1,\"1767\":2,\"1836\":1,\"2302\":2,\"2395\":1,\"2531\":1,\"2543\":1}}],[\"fasnetseparator\",{\"0\":{\"1558\":1},\"1\":{\"1558\":1}}],[\"fasnet\",{\"0\":{\"1460\":1,\"1558\":1,\"1559\":2,\"1560\":2},\"1\":{\"1460\":3,\"1558\":7,\"1559\":5,\"1560\":4,\"1718\":1}}],[\"fash\",{\"1\":{\"240\":1}}],[\"fastpitch\",{\"1\":{\"2244\":1}}],[\"fastselfattention\",{\"0\":{\"1182\":1},\"1\":{\"1182\":1}}],[\"fastspeech1\",{\"1\":{\"2090\":1}}],[\"fastspeech2discrete\",{\"0\":{\"2279\":1},\"1\":{\"2279\":1}}],[\"fastspeech2lossdiscrete\",{\"0\":{\"2280\":1},\"1\":{\"2280\":1}}],[\"fastspeech2loss\",{\"0\":{\"2245\":1},\"1\":{\"2245\":1}}],[\"fastspeech2\",{\"0\":{\"2244\":3,\"2245\":1,\"2265\":1,\"2279\":2,\"2280\":1},\"1\":{\"1850\":1,\"1852\":1,\"2091\":1,\"2244\":6,\"2245\":2,\"2265\":1,\"2279\":6,\"2280\":2,\"2363\":10,\"2364\":1,\"2506\":1,\"2507\":1,\"2510\":2,\"2512\":4,\"2513\":1,\"2653\":12,\"2654\":1,\"2657\":4,\"2658\":1}}],[\"fastspeech\",{\"0\":{\"212\":1,\"230\":1,\"735\":1,\"736\":1,\"738\":1,\"754\":1,\"755\":1,\"774\":1,\"2243\":3},\"1\":{\"212\":5,\"217\":2,\"228\":1,\"230\":7,\"235\":2,\"263\":2,\"295\":8,\"735\":2,\"736\":2,\"738\":1,\"754\":4,\"755\":1,\"774\":2,\"786\":1,\"2090\":1,\"2243\":7,\"2244\":1,\"2265\":1,\"2363\":4,\"2364\":1,\"2506\":2,\"2507\":1,\"2510\":14,\"2513\":1,\"2653\":4,\"2654\":1,\"2658\":1}}],[\"fastformer\",{\"0\":{\"1182\":1},\"1\":{\"1182\":2}}],[\"faster\",{\"1\":{\"26\":2,\"203\":1,\"217\":1,\"2450\":1,\"2482\":1}}],[\"fastemit\",{\"1\":{\"24\":2,\"113\":4,\"825\":3,\"1057\":3,\"1142\":4,\"1186\":4,\"1210\":4,\"1211\":4,\"1224\":4,\"1286\":1,\"1287\":4,\"1300\":1,\"1301\":4,\"1304\":4,\"1336\":1,\"1337\":4,\"1348\":1,\"1349\":4,\"1350\":4}}],[\"fast\",{\"1\":{\"19\":1,\"235\":1,\"736\":1,\"754\":2,\"774\":1,\"786\":1,\"835\":2,\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"1182\":1,\"1640\":1,\"2243\":2,\"2244\":1,\"2255\":1,\"2265\":1}}],[\"favorite\",{\"1\":{\"218\":1,\"2365\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2655\":1,\"2660\":1}}],[\"fall\",{\"0\":{\"2421\":1,\"2524\":1,\"2544\":1},\"1\":{\"2380\":2,\"2388\":3,\"2421\":1,\"2524\":2,\"2544\":1}}],[\"fall2021\",{\"0\":{\"163\":1,\"2723\":1}}],[\"fall2022\",{\"0\":{\"162\":1,\"2726\":1}}],[\"false\",{\"1\":{\"21\":2,\"32\":2,\"34\":2,\"39\":3,\"41\":1,\"56\":2,\"59\":2,\"62\":2,\"82\":1,\"91\":3,\"98\":1,\"99\":2,\"110\":1,\"115\":2,\"121\":1,\"691\":2,\"693\":2,\"697\":2,\"698\":1,\"699\":5,\"700\":1,\"711\":1,\"712\":1,\"730\":1,\"749\":1,\"752\":1,\"756\":2,\"766\":2,\"778\":1,\"797\":1,\"825\":5,\"831\":1,\"932\":1,\"987\":2,\"1013\":1,\"1015\":1,\"1019\":1,\"1037\":1,\"1039\":1,\"1048\":2,\"1051\":1,\"1052\":2,\"1054\":1,\"1055\":1,\"1057\":3,\"1059\":2,\"1064\":1,\"1067\":1,\"1076\":1,\"1084\":1,\"1093\":2,\"1115\":26,\"1133\":5,\"1138\":1,\"1139\":1,\"1140\":2,\"1148\":7,\"1149\":3,\"1150\":2,\"1158\":4,\"1167\":2,\"1168\":2,\"1169\":5,\"1171\":1,\"1172\":3,\"1173\":2,\"1178\":1,\"1179\":4,\"1180\":1,\"1181\":2,\"1190\":1,\"1195\":1,\"1196\":2,\"1197\":2,\"1203\":6,\"1204\":1,\"1206\":1,\"1207\":1,\"1215\":2,\"1220\":3,\"1239\":3,\"1243\":1,\"1244\":2,\"1269\":7,\"1271\":1,\"1272\":4,\"1273\":3,\"1289\":1,\"1377\":2,\"1390\":1,\"1405\":2,\"1407\":1,\"1424\":1,\"1426\":2,\"1505\":4,\"1516\":6,\"1517\":1,\"1523\":3,\"1524\":2,\"1525\":2,\"1528\":1,\"1531\":2,\"1534\":1,\"1535\":1,\"1539\":1,\"1545\":1,\"1551\":1,\"1553\":6,\"1554\":2,\"1558\":1,\"1598\":1,\"1603\":2,\"1611\":4,\"1622\":2,\"1626\":1,\"1643\":1,\"1644\":1,\"1645\":2,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":3,\"1658\":3,\"1659\":3,\"1669\":4,\"1702\":1,\"1712\":1,\"1715\":1,\"1741\":1,\"1761\":3,\"1763\":3,\"1771\":2,\"1778\":15,\"1787\":2,\"1788\":4,\"1791\":3,\"1797\":1,\"1798\":2,\"1801\":1,\"1804\":2,\"1805\":12,\"1833\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":16,\"1851\":9,\"1852\":16,\"1855\":1,\"1858\":1,\"1859\":1,\"1864\":1,\"1865\":1,\"1868\":1,\"1874\":2,\"1877\":9,\"1878\":1,\"1879\":1,\"1880\":4,\"1892\":3,\"1895\":2,\"1896\":2,\"1898\":1,\"1900\":3,\"1912\":1,\"1918\":1,\"1920\":1,\"1921\":2,\"1949\":1,\"1955\":1,\"1958\":1,\"1970\":3,\"1972\":1,\"1975\":1,\"1984\":1,\"1985\":2,\"1987\":1,\"1989\":2,\"1991\":1,\"1996\":1,\"2000\":1,\"2001\":4,\"2002\":3,\"2003\":3,\"2004\":4,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2026\":3,\"2027\":3,\"2029\":2,\"2046\":1,\"2054\":2,\"2076\":2,\"2086\":3,\"2087\":5,\"2090\":8,\"2091\":1,\"2095\":7,\"2096\":2,\"2097\":2,\"2098\":2,\"2099\":5,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2120\":1,\"2121\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2136\":1,\"2137\":2,\"2161\":1,\"2163\":1,\"2170\":1,\"2176\":1,\"2178\":2,\"2179\":2,\"2180\":2,\"2182\":1,\"2184\":4,\"2188\":1,\"2198\":1,\"2200\":6,\"2241\":1,\"2243\":8,\"2244\":10,\"2245\":1,\"2246\":1,\"2248\":2,\"2250\":1,\"2255\":9,\"2256\":1,\"2260\":1,\"2263\":6,\"2264\":7,\"2266\":1,\"2279\":9,\"2280\":1,\"2290\":5,\"2292\":1,\"2294\":5,\"2295\":2,\"2296\":2,\"2301\":1,\"2302\":1,\"2316\":1,\"2352\":1,\"2375\":3,\"2377\":2,\"2436\":2,\"2440\":3,\"2461\":1,\"2474\":1,\"2558\":1,\"2559\":3,\"2562\":2,\"2564\":2,\"2569\":1,\"2584\":1,\"2585\":1,\"2649\":1}}],[\"fact\",{\"1\":{\"2543\":1}}],[\"factor=2\",{\"1\":{\"1688\":1,\"1693\":1,\"1729\":1,\"1730\":1,\"1755\":1,\"1756\":1}}],[\"factor=4\",{\"1\":{\"909\":1}}],[\"factor=0\",{\"1\":{\"632\":1,\"742\":1,\"835\":1,\"1136\":1,\"2022\":1}}],[\"factor=1\",{\"1\":{\"624\":1,\"799\":1,\"936\":1,\"2078\":1}}],[\"factors\",{\"1\":{\"625\":2,\"889\":2,\"1763\":1,\"1801\":1,\"1866\":1,\"2373\":1,\"2430\":1,\"2555\":1}}],[\"factors=\",{\"1\":{\"625\":1}}],[\"factory\",{\"0\":{\"661\":1,\"665\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1},\"1\":{\"69\":3,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"665\":1,\"1371\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":2,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"2099\":11,\"2102\":4,\"2201\":3}}],[\"factor\",{\"0\":{\"107\":1},\"1\":{\"62\":1,\"107\":2,\"108\":1,\"113\":2,\"115\":3,\"203\":3,\"245\":2,\"501\":1,\"624\":2,\"632\":1,\"676\":2,\"709\":2,\"724\":2,\"742\":2,\"754\":2,\"792\":1,\"821\":2,\"826\":2,\"830\":4,\"835\":2,\"858\":2,\"861\":2,\"888\":1,\"909\":2,\"936\":2,\"1053\":3,\"1069\":3,\"1071\":3,\"1095\":4,\"1097\":3,\"1137\":1,\"1142\":1,\"1145\":1,\"1186\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1247\":1,\"1248\":1,\"1269\":2,\"1287\":1,\"1301\":1,\"1304\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1619\":1,\"1643\":1,\"1644\":1,\"1688\":8,\"1693\":7,\"1701\":2,\"1702\":1,\"1739\":1,\"1746\":1,\"1755\":7,\"1756\":8,\"1778\":1,\"1804\":1,\"1823\":1,\"1827\":1,\"1844\":1,\"1850\":1,\"1851\":4,\"1852\":1,\"1861\":4,\"1863\":1,\"1864\":1,\"1865\":1,\"1867\":1,\"1869\":2,\"1871\":1,\"1872\":5,\"1873\":5,\"1878\":1,\"1880\":1,\"1905\":2,\"1921\":1,\"1922\":1,\"1929\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1946\":3,\"1947\":3,\"2002\":3,\"2022\":1,\"2078\":2,\"2083\":2,\"2086\":3,\"2087\":3,\"2090\":3,\"2095\":3,\"2151\":2,\"2236\":1,\"2241\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2279\":3}}],[\"facilate\",{\"1\":{\"2387\":1}}],[\"facebookresearch\",{\"1\":{\"2294\":1,\"2450\":1,\"2517\":1}}],[\"faces\",{\"1\":{\"2040\":1}}],[\"face\",{\"0\":{\"375\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1320\":2,\"1321\":2,\"2028\":1,\"2123\":1,\"2124\":1},\"1\":{\"97\":1,\"99\":2,\"307\":4,\"375\":3,\"443\":4,\"449\":2,\"1190\":3,\"1191\":2,\"1192\":2,\"1320\":2,\"1321\":2,\"2026\":1,\"2028\":2,\"2030\":1,\"2040\":1,\"2123\":1,\"2124\":1,\"2446\":1,\"2575\":1}}],[\"far\",{\"1\":{\"57\":1}}],[\"fault\",{\"1\":{\"49\":1}}],[\"faq\",{\"0\":{\"106\":1,\"123\":1,\"125\":1},\"1\":{\"45\":1}}],[\"fair\",{\"1\":{\"2433\":1,\"2555\":1}}],[\"fairseqhubertpretrainencoder\",{\"0\":{\"1181\":1},\"1\":{\"1181\":1}}],[\"fairseqhubertencoder\",{\"0\":{\"1180\":1},\"1\":{\"1180\":1}}],[\"fairseqavhubertencoder\",{\"0\":{\"1179\":1},\"1\":{\"1179\":1}}],[\"fairseqwav2vec2\",{\"1\":{\"1178\":1}}],[\"fairseqwav2vec2encoder\",{\"0\":{\"1178\":1},\"1\":{\"1178\":1}}],[\"fairseq\",{\"1\":{\"56\":1,\"57\":1,\"101\":2,\"740\":2,\"741\":2,\"775\":2,\"776\":2,\"1178\":1,\"1179\":1,\"1180\":3,\"1181\":1,\"2294\":2,\"2429\":2,\"2431\":3,\"2432\":2,\"2466\":2,\"2552\":2,\"2646\":2}}],[\"fairscale\",{\"1\":{\"35\":2}}],[\"fails\",{\"1\":{\"2472\":1,\"2558\":1,\"2648\":1}}],[\"fail\",{\"1\":{\"2422\":1,\"2545\":1,\"2585\":1,\"2638\":1,\"2642\":1}}],[\"failure\",{\"1\":{\"1186\":1,\"1210\":1,\"2508\":3}}],[\"failed\",{\"1\":{\"38\":1,\"44\":1,\"149\":1}}],[\"fft512\",{\"1\":{\"2357\":3,\"2578\":3}}],[\"fft=2048\",{\"1\":{\"1812\":1}}],[\"fft=512\",{\"1\":{\"1719\":1,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"fft=128\",{\"1\":{\"1660\":1,\"1661\":1}}],[\"fftl\",{\"1\":{\"562\":2}}],[\"fft\",{\"0\":{\"1814\":1,\"1821\":1},\"1\":{\"275\":3,\"282\":3,\"295\":2,\"509\":2,\"512\":2,\"519\":2,\"541\":2,\"752\":1,\"778\":3,\"881\":1,\"947\":1,\"951\":1,\"953\":1,\"954\":1,\"967\":1,\"969\":1,\"970\":1,\"971\":1,\"1158\":1,\"1207\":1,\"1643\":1,\"1644\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"1777\":4,\"1778\":1,\"1785\":1,\"1799\":1,\"1804\":3,\"1805\":1,\"1814\":1,\"1818\":1,\"1821\":1,\"1850\":1,\"1852\":1,\"1859\":4,\"1877\":1,\"1912\":3,\"1918\":1,\"1929\":4,\"1941\":4,\"1947\":4,\"1987\":1,\"1989\":1,\"1991\":1,\"2084\":1,\"2089\":1,\"2236\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2272\":1,\"2317\":3,\"2325\":4,\"2330\":4,\"2600\":2}}],[\"ffn\",{\"1\":{\"116\":2,\"1066\":2,\"1074\":2,\"1075\":2,\"1115\":4,\"1169\":3,\"1179\":1,\"1804\":4,\"1805\":1,\"1851\":1,\"1877\":1,\"1878\":4,\"2243\":1,\"2244\":1,\"2255\":1,\"2279\":1}}],[\"ffmpeg\",{\"1\":{\"52\":3,\"2385\":1}}],[\"ff\",{\"0\":{\"1177\":2},\"1\":{\"21\":4,\"1177\":4,\"1242\":2,\"1269\":4}}],[\"flush\",{\"1\":{\"2592\":1}}],[\"flush=true\",{\"1\":{\"2369\":2,\"2372\":4,\"2487\":2,\"2491\":2,\"2497\":4,\"2500\":4,\"2501\":2,\"2606\":2,\"2607\":2,\"2610\":2,\"2614\":4,\"2615\":4,\"2617\":4,\"2623\":2,\"2624\":2,\"2627\":2,\"2632\":4,\"2633\":4,\"2635\":4}}],[\"flipped\",{\"1\":{\"1840\":1}}],[\"flip\",{\"1\":{\"1840\":1}}],[\"flipflow\",{\"0\":{\"1840\":1},\"1\":{\"1840\":2}}],[\"flens\",{\"1\":{\"1511\":1,\"1644\":1}}],[\"flexible\",{\"1\":{\"363\":2,\"1375\":1,\"1551\":1,\"1553\":2,\"1554\":1,\"1622\":3,\"2184\":1,\"2200\":1}}],[\"flexibly\",{\"1\":{\"57\":1}}],[\"flexibility\",{\"1\":{\"21\":1,\"2046\":1}}],[\"flooring\",{\"1\":{\"1524\":2,\"1525\":2,\"1611\":3}}],[\"floor=1e\",{\"1\":{\"941\":1,\"960\":1}}],[\"floor\",{\"1\":{\"760\":1}}],[\"flo\",{\"1\":{\"700\":1,\"1138\":1,\"1139\":1}}],[\"flow=false\",{\"1\":{\"1557\":1,\"1623\":1,\"1637\":1,\"1638\":1}}],[\"flows\",{\"0\":{\"1009\":1},\"1\":{\"1009\":2,\"1804\":3,\"1805\":1,\"1864\":3,\"1868\":3,\"1877\":2,\"1878\":6}}],[\"flow\",{\"0\":{\"1833\":1,\"1835\":1,\"1838\":1,\"1840\":1,\"1855\":1,\"1875\":1},\"1\":{\"235\":2,\"1011\":9,\"1127\":1,\"1198\":1,\"1618\":1,\"1619\":1,\"1638\":4,\"1778\":1,\"1804\":23,\"1805\":8,\"1833\":4,\"1835\":1,\"1838\":3,\"1840\":4,\"1853\":1,\"1854\":2,\"1855\":4,\"1864\":2,\"1865\":1,\"1868\":1,\"1875\":2,\"1877\":8,\"1878\":23,\"2315\":1}}],[\"floating\",{\"1\":{\"1927\":1}}],[\"floatornone\",{\"1\":{\"1927\":1,\"1929\":1,\"1941\":1,\"1947\":1}}],[\"floatortorch\",{\"1\":{\"1921\":1,\"1922\":1,\"1936\":2,\"1938\":1,\"1939\":1}}],[\"floattensor\",{\"1\":{\"1778\":3,\"1804\":2,\"1805\":3,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2}}],[\"floatrandomgeneratedataset\",{\"0\":{\"1384\":1},\"1\":{\"1384\":1,\"1385\":1}}],[\"float=\",{\"1\":{\"1115\":19}}],[\"float64\",{\"1\":{\"245\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":2,\"437\":1,\"443\":1,\"449\":1,\"455\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"1011\":1}}],[\"float16\",{\"1\":{\"245\":1,\"249\":2,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"301\":1,\"307\":2,\"315\":1,\"321\":1,\"327\":1,\"333\":2,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"380\":1,\"384\":2,\"391\":1,\"399\":1,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":1,\"443\":1,\"449\":1,\"455\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":2,\"485\":1,\"2592\":1,\"2596\":1}}],[\"float32\",{\"1\":{\"48\":1,\"245\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":2,\"437\":1,\"443\":1,\"449\":1,\"455\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"624\":1,\"625\":1,\"731\":1,\"734\":1,\"773\":1,\"817\":1,\"828\":1,\"875\":1,\"928\":1,\"936\":1,\"959\":1,\"1011\":1,\"1133\":3,\"1190\":1,\"1204\":1,\"1214\":2,\"1244\":1,\"1273\":3,\"1314\":1,\"1339\":1,\"1342\":1,\"1384\":1,\"1409\":1,\"1757\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"2001\":1,\"2099\":1,\"2182\":1,\"2189\":1,\"2386\":1}}],[\"float20\",{\"1\":{\"48\":1}}],[\"float\",{\"0\":{\"1749\":1,\"2319\":1},\"1\":{\"21\":8,\"22\":8,\"23\":1,\"60\":3,\"82\":1,\"113\":4,\"115\":30,\"116\":2,\"118\":5,\"119\":1,\"203\":1,\"623\":2,\"627\":1,\"630\":1,\"631\":1,\"632\":2,\"647\":1,\"672\":1,\"676\":2,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"688\":1,\"690\":2,\"691\":5,\"692\":4,\"693\":8,\"697\":11,\"698\":5,\"699\":4,\"700\":2,\"703\":2,\"707\":2,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"725\":1,\"726\":2,\"730\":1,\"731\":1,\"732\":1,\"734\":1,\"737\":2,\"738\":1,\"740\":1,\"741\":1,\"742\":5,\"747\":1,\"748\":1,\"749\":3,\"750\":2,\"751\":1,\"752\":2,\"754\":9,\"756\":1,\"758\":1,\"760\":2,\"762\":2,\"763\":4,\"765\":4,\"768\":2,\"770\":2,\"771\":1,\"772\":1,\"774\":2,\"775\":1,\"776\":1,\"778\":4,\"781\":1,\"784\":2,\"785\":1,\"786\":1,\"787\":1,\"793\":1,\"797\":4,\"798\":2,\"799\":1,\"800\":1,\"801\":2,\"802\":1,\"805\":1,\"806\":2,\"808\":1,\"809\":1,\"810\":1,\"813\":1,\"815\":1,\"818\":1,\"821\":8,\"822\":1,\"825\":9,\"826\":18,\"827\":1,\"831\":1,\"834\":3,\"838\":1,\"857\":8,\"858\":2,\"885\":2,\"887\":2,\"906\":1,\"911\":2,\"914\":1,\"917\":1,\"923\":1,\"925\":1,\"932\":1,\"976\":1,\"1010\":3,\"1011\":3,\"1025\":2,\"1043\":1,\"1047\":1,\"1048\":2,\"1049\":1,\"1050\":1,\"1052\":1,\"1054\":1,\"1056\":1,\"1057\":6,\"1060\":1,\"1061\":2,\"1063\":2,\"1065\":3,\"1066\":5,\"1067\":1,\"1069\":3,\"1070\":3,\"1072\":2,\"1073\":2,\"1074\":2,\"1075\":3,\"1076\":1,\"1077\":1,\"1078\":2,\"1079\":2,\"1080\":1,\"1082\":2,\"1083\":1,\"1084\":1,\"1093\":5,\"1096\":7,\"1098\":2,\"1106\":3,\"1107\":1,\"1108\":4,\"1115\":25,\"1132\":2,\"1133\":2,\"1138\":1,\"1139\":2,\"1140\":9,\"1141\":8,\"1142\":4,\"1144\":1,\"1145\":2,\"1148\":9,\"1149\":3,\"1150\":3,\"1151\":1,\"1153\":1,\"1166\":1,\"1167\":4,\"1168\":4,\"1169\":4,\"1170\":2,\"1171\":4,\"1172\":3,\"1176\":1,\"1179\":10,\"1180\":7,\"1181\":3,\"1186\":4,\"1193\":2,\"1195\":1,\"1196\":4,\"1197\":4,\"1198\":2,\"1200\":1,\"1201\":1,\"1203\":6,\"1204\":4,\"1205\":1,\"1206\":3,\"1209\":1,\"1210\":5,\"1211\":5,\"1214\":1,\"1215\":1,\"1219\":2,\"1220\":2,\"1222\":1,\"1224\":4,\"1244\":3,\"1248\":1,\"1256\":1,\"1257\":2,\"1261\":1,\"1269\":11,\"1270\":2,\"1271\":5,\"1272\":4,\"1273\":4,\"1282\":1,\"1287\":1,\"1300\":1,\"1301\":4,\"1302\":1,\"1303\":1,\"1304\":5,\"1331\":2,\"1333\":2,\"1336\":2,\"1337\":5,\"1348\":2,\"1349\":4,\"1350\":4,\"1352\":2,\"1371\":2,\"1376\":1,\"1384\":1,\"1386\":1,\"1392\":1,\"1430\":2,\"1455\":2,\"1465\":4,\"1470\":1,\"1471\":1,\"1505\":6,\"1515\":2,\"1524\":5,\"1525\":4,\"1528\":6,\"1529\":2,\"1531\":2,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":2,\"1552\":1,\"1558\":1,\"1581\":1,\"1598\":1,\"1600\":2,\"1602\":2,\"1603\":2,\"1604\":2,\"1611\":6,\"1622\":1,\"1626\":2,\"1638\":1,\"1639\":1,\"1640\":2,\"1643\":2,\"1644\":2,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":2,\"1669\":6,\"1670\":2,\"1671\":4,\"1680\":1,\"1696\":4,\"1697\":4,\"1698\":4,\"1703\":1,\"1704\":4,\"1705\":4,\"1706\":4,\"1707\":4,\"1708\":4,\"1711\":2,\"1712\":6,\"1714\":2,\"1715\":6,\"1719\":1,\"1746\":4,\"1749\":2,\"1759\":1,\"1771\":6,\"1772\":1,\"1773\":1,\"1776\":2,\"1777\":2,\"1778\":15,\"1781\":1,\"1785\":3,\"1787\":6,\"1788\":6,\"1798\":6,\"1804\":16,\"1805\":23,\"1810\":4,\"1833\":2,\"1835\":4,\"1837\":1,\"1841\":2,\"1850\":11,\"1851\":28,\"1852\":9,\"1855\":2,\"1859\":2,\"1860\":4,\"1862\":2,\"1863\":2,\"1864\":2,\"1865\":2,\"1868\":4,\"1874\":6,\"1877\":17,\"1878\":18,\"1880\":2,\"1883\":4,\"1890\":3,\"1892\":4,\"1896\":1,\"1904\":1,\"1905\":5,\"1906\":1,\"1912\":5,\"1915\":2,\"1916\":1,\"1917\":2,\"1920\":1,\"1921\":1,\"1922\":1,\"1925\":4,\"1926\":1,\"1928\":2,\"1929\":7,\"1932\":2,\"1934\":1,\"1935\":2,\"1936\":2,\"1938\":1,\"1939\":1,\"1941\":5,\"1943\":2,\"1946\":2,\"1947\":7,\"1949\":1,\"1955\":1,\"1958\":1,\"1960\":3,\"1970\":1,\"1971\":1,\"1972\":4,\"1975\":3,\"1985\":3,\"1989\":1,\"1993\":1,\"1996\":2,\"1997\":1,\"1999\":3,\"2000\":2,\"2001\":5,\"2002\":10,\"2003\":4,\"2004\":5,\"2018\":8,\"2019\":3,\"2020\":2,\"2021\":1,\"2022\":1,\"2023\":2,\"2026\":3,\"2027\":3,\"2029\":3,\"2054\":9,\"2076\":6,\"2078\":2,\"2079\":2,\"2081\":1,\"2082\":1,\"2083\":1,\"2086\":8,\"2087\":10,\"2090\":29,\"2095\":16,\"2106\":2,\"2151\":3,\"2152\":2,\"2153\":1,\"2170\":1,\"2175\":2,\"2177\":2,\"2178\":5,\"2179\":5,\"2181\":1,\"2182\":2,\"2184\":5,\"2186\":2,\"2188\":5,\"2189\":1,\"2191\":5,\"2194\":9,\"2195\":5,\"2196\":1,\"2197\":17,\"2199\":3,\"2200\":5,\"2202\":4,\"2204\":2,\"2205\":6,\"2208\":2,\"2210\":1,\"2222\":1,\"2225\":1,\"2229\":1,\"2230\":2,\"2231\":1,\"2240\":1,\"2243\":22,\"2244\":32,\"2248\":1,\"2255\":30,\"2259\":2,\"2260\":4,\"2262\":2,\"2263\":16,\"2264\":37,\"2265\":2,\"2267\":1,\"2273\":2,\"2274\":6,\"2278\":1,\"2279\":32,\"2290\":1,\"2292\":2,\"2294\":3,\"2295\":1,\"2296\":1,\"2301\":2,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2319\":3,\"2349\":2,\"2403\":1,\"2514\":1,\"2521\":1,\"2522\":1,\"2523\":1,\"2539\":1,\"2592\":1,\"2596\":1,\"2659\":1}}],[\"fly\",{\"1\":{\"84\":1}}],[\"flatened\",{\"1\":{\"1228\":2,\"1345\":2,\"1347\":2}}],[\"flat\",{\"0\":{\"1021\":1},\"1\":{\"1021\":1,\"1142\":2,\"1186\":2,\"1210\":1}}],[\"flattented\",{\"1\":{\"1142\":1,\"1155\":1,\"1186\":1,\"1210\":1}}],[\"flatten\",{\"0\":{\"1018\":1,\"1318\":1},\"1\":{\"1018\":1,\"1061\":1,\"1318\":1,\"1516\":1,\"1609\":1}}],[\"flattened\",{\"1\":{\"1009\":1,\"1142\":2,\"1154\":1,\"1186\":2,\"1210\":2,\"1228\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":2,\"1345\":1,\"1347\":1}}],[\"flaot\",{\"1\":{\"717\":1,\"777\":1}}],[\"flac\",{\"1\":{\"48\":1,\"49\":5,\"58\":1,\"132\":3,\"989\":2,\"1927\":1,\"2373\":1,\"2385\":1,\"2430\":1,\"2555\":1}}],[\"flake8\",{\"0\":{\"11\":1},\"1\":{\"10\":1,\"11\":10,\"2642\":1}}],[\"flags\",{\"1\":{\"99\":1}}],[\"flag\",{\"1\":{\"1\":1,\"2\":2,\"4\":1,\"5\":1,\"6\":1,\"626\":1,\"627\":2,\"677\":1,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"742\":2,\"758\":1,\"892\":1,\"1154\":1,\"1228\":1,\"1345\":1,\"1347\":1,\"1788\":3,\"1797\":4,\"2046\":1,\"2082\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1}}],[\"f\",{\"1\":{\"17\":1,\"52\":2,\"53\":2,\"76\":2,\"87\":1,\"135\":1,\"171\":4,\"174\":1,\"175\":4,\"185\":2,\"193\":2,\"194\":4,\"203\":2,\"217\":4,\"218\":1,\"224\":3,\"225\":1,\"231\":4,\"232\":1,\"690\":1,\"726\":2,\"729\":3,\"730\":3,\"744\":1,\"782\":3,\"876\":3,\"885\":3,\"887\":3,\"950\":2,\"968\":2,\"984\":2,\"989\":3,\"992\":2,\"995\":2,\"1015\":4,\"1019\":1,\"1037\":1,\"1058\":2,\"1071\":1,\"1114\":1,\"1116\":1,\"1198\":1,\"1207\":2,\"1374\":1,\"1376\":3,\"1452\":5,\"1455\":1,\"1462\":2,\"1463\":2,\"1464\":1,\"1510\":1,\"1515\":2,\"1516\":7,\"1517\":2,\"1522\":2,\"1523\":3,\"1524\":18,\"1525\":7,\"1528\":1,\"1529\":3,\"1530\":2,\"1545\":2,\"1566\":4,\"1568\":2,\"1569\":4,\"1571\":4,\"1594\":2,\"1595\":3,\"1638\":2,\"1643\":2,\"1644\":2,\"1646\":2,\"1655\":2,\"1659\":2,\"1662\":1,\"1671\":3,\"1680\":3,\"1696\":4,\"1697\":3,\"1698\":3,\"1701\":4,\"1702\":4,\"1703\":3,\"1704\":3,\"1705\":3,\"1706\":3,\"1707\":4,\"1708\":3,\"1710\":2,\"1711\":3,\"1712\":3,\"1713\":3,\"1715\":3,\"1717\":1,\"1719\":5,\"1736\":3,\"1737\":1,\"1739\":4,\"1758\":2,\"1761\":1,\"1763\":1,\"1768\":1,\"1785\":6,\"1805\":1,\"1810\":4,\"1869\":2,\"1904\":1,\"1916\":1,\"1918\":2,\"1927\":1,\"2074\":4,\"2075\":5,\"2317\":2,\"2330\":2,\"2359\":3,\"2360\":4,\"2365\":2,\"2372\":1,\"2386\":2,\"2392\":1,\"2425\":1,\"2456\":4,\"2458\":3,\"2460\":4,\"2472\":3,\"2474\":3,\"2476\":3,\"2478\":1,\"2497\":1,\"2498\":1,\"2500\":5,\"2501\":2,\"2508\":2,\"2510\":2,\"2514\":6,\"2515\":2,\"2521\":7,\"2522\":2,\"2523\":4,\"2528\":1,\"2548\":1,\"2568\":11,\"2580\":3,\"2581\":2,\"2582\":4,\"2584\":2,\"2600\":3,\"2607\":2,\"2614\":1,\"2615\":1,\"2616\":1,\"2617\":5,\"2624\":2,\"2632\":1,\"2633\":1,\"2634\":1,\"2635\":5,\"2648\":3,\"2649\":3,\"2655\":2,\"2659\":6,\"2660\":2}}],[\"fr=none\",{\"1\":{\"2341\":1}}],[\"fristly\",{\"1\":{\"1429\":1}}],[\"friendly\",{\"1\":{\"179\":1,\"765\":1,\"798\":1,\"1323\":1}}],[\"fr\",{\"1\":{\"296\":3,\"1712\":1,\"1715\":1}}],[\"fractional\",{\"1\":{\"1941\":1}}],[\"framing\",{\"0\":{\"1741\":1,\"2211\":1},\"1\":{\"1741\":1,\"2211\":2}}],[\"framed\",{\"1\":{\"1702\":1,\"1735\":1}}],[\"framewise\",{\"1\":{\"606\":1}}],[\"framework\",{\"1\":{\"162\":1,\"1829\":1,\"2468\":1,\"2479\":1}}],[\"framescorefeats\",{\"0\":{\"2084\":1},\"1\":{\"2084\":2}}],[\"frames=none\",{\"1\":{\"983\":1,\"986\":1,\"991\":1,\"994\":1}}],[\"frameshift\",{\"1\":{\"102\":2}}],[\"frames\",{\"0\":{\"1023\":1},\"1\":{\"26\":7,\"109\":1,\"121\":1,\"122\":1,\"150\":1,\"238\":4,\"239\":1,\"251\":6,\"255\":6,\"259\":6,\"265\":6,\"269\":6,\"279\":2,\"294\":2,\"496\":2,\"509\":2,\"512\":2,\"522\":2,\"525\":2,\"684\":1,\"929\":1,\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"1003\":1,\"1004\":10,\"1015\":3,\"1023\":2,\"1024\":2,\"1028\":10,\"1049\":2,\"1050\":2,\"1052\":2,\"1056\":2,\"1058\":5,\"1068\":2,\"1076\":4,\"1077\":2,\"1093\":1,\"1101\":1,\"1138\":1,\"1269\":2,\"1354\":1,\"1373\":1,\"1375\":3,\"1463\":3,\"1505\":3,\"1515\":3,\"1516\":3,\"1523\":3,\"1528\":3,\"1534\":3,\"1539\":3,\"1543\":1,\"1558\":3,\"1564\":1,\"1611\":7,\"1626\":3,\"1645\":3,\"1654\":3,\"1655\":2,\"1658\":3,\"1659\":1,\"1669\":3,\"1670\":1,\"1671\":5,\"1719\":7,\"1735\":7,\"1741\":2,\"1804\":1,\"1808\":1,\"1890\":2,\"1910\":1,\"1918\":2,\"2084\":2,\"2089\":1,\"2596\":1,\"2616\":1,\"2634\":1}}],[\"frame\",{\"0\":{\"1004\":1,\"2094\":1},\"1\":{\"26\":1,\"77\":1,\"109\":2,\"113\":1,\"150\":1,\"245\":2,\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"274\":1,\"648\":3,\"684\":1,\"703\":1,\"736\":1,\"749\":1,\"774\":2,\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"1004\":2,\"1132\":1,\"1145\":3,\"1181\":1,\"1255\":2,\"1432\":5,\"1436\":4,\"1446\":1,\"1510\":5,\"1511\":4,\"1526\":1,\"1626\":1,\"1643\":5,\"1644\":5,\"1653\":1,\"1654\":1,\"1658\":1,\"1660\":1,\"1661\":1,\"1719\":1,\"1735\":10,\"1741\":9,\"1781\":3,\"1800\":1,\"1811\":3,\"1881\":1,\"1986\":1,\"2044\":2,\"2046\":3,\"2049\":1,\"2052\":2,\"2055\":1,\"2064\":1,\"2068\":2,\"2070\":1,\"2094\":1,\"2210\":2,\"2211\":2,\"2498\":3,\"2616\":3,\"2634\":3}}],[\"frank\",{\"1\":{\"130\":1}}],[\"front\",{\"0\":{\"2574\":1},\"1\":{\"109\":1,\"226\":1,\"2558\":1}}],[\"frontends=none\",{\"1\":{\"1184\":1}}],[\"frontends\",{\"0\":{\"690\":1,\"729\":1,\"730\":1,\"752\":1,\"756\":1,\"760\":1,\"778\":1,\"782\":1,\"831\":1,\"854\":1,\"881\":1,\"882\":1,\"885\":1,\"887\":1,\"932\":1},\"1\":{\"690\":1,\"729\":1,\"730\":1,\"752\":1,\"756\":1,\"760\":1,\"778\":1,\"782\":1,\"831\":1,\"854\":1,\"881\":1,\"882\":1,\"885\":1,\"887\":1,\"932\":1}}],[\"frontend\",{\"0\":{\"756\":2,\"882\":2,\"1071\":1,\"1121\":2,\"1132\":2,\"1158\":1,\"1184\":1,\"1207\":1,\"1239\":1,\"1255\":1,\"1284\":1,\"1774\":1,\"1791\":1,\"1971\":1},\"1\":{\"102\":4,\"207\":1,\"217\":2,\"218\":1,\"224\":3,\"225\":1,\"231\":2,\"232\":1,\"251\":2,\"295\":2,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"756\":2,\"858\":1,\"882\":2,\"890\":1,\"1057\":3,\"1071\":4,\"1113\":3,\"1116\":1,\"1121\":2,\"1127\":1,\"1132\":5,\"1158\":3,\"1171\":3,\"1172\":3,\"1184\":2,\"1198\":5,\"1206\":2,\"1207\":1,\"1239\":4,\"1255\":1,\"1284\":1,\"1371\":3,\"1551\":2,\"1552\":3,\"1553\":2,\"1554\":2,\"1774\":1,\"1791\":1,\"1892\":3,\"1893\":3,\"1970\":3,\"1971\":3,\"1975\":3,\"1984\":2,\"1989\":1,\"1991\":1,\"2027\":3,\"2046\":1,\"2050\":2,\"2076\":3,\"2248\":1,\"2250\":1,\"2294\":2,\"2439\":1,\"2440\":2}}],[\"frombuffer\",{\"1\":{\"2592\":1,\"2596\":1}}],[\"from=\",{\"1\":{\"1719\":1}}],[\"from\",{\"0\":{\"52\":1,\"84\":1,\"91\":1,\"149\":1,\"192\":1,\"193\":1,\"204\":1,\"277\":1,\"541\":1,\"1317\":1,\"1750\":1,\"1966\":1,\"2155\":1,\"2307\":1,\"2416\":1,\"2585\":1,\"2594\":1,\"2599\":1,\"2600\":1},\"1\":{\"4\":1,\"5\":3,\"11\":1,\"15\":2,\"21\":2,\"23\":1,\"25\":1,\"26\":1,\"33\":1,\"46\":1,\"48\":1,\"54\":2,\"56\":2,\"60\":4,\"65\":2,\"72\":1,\"74\":2,\"82\":1,\"84\":2,\"85\":3,\"95\":1,\"102\":1,\"110\":1,\"113\":1,\"115\":1,\"121\":1,\"126\":1,\"134\":1,\"135\":2,\"143\":1,\"148\":2,\"149\":3,\"150\":2,\"170\":1,\"172\":1,\"173\":2,\"174\":3,\"175\":1,\"194\":3,\"198\":1,\"200\":1,\"201\":1,\"202\":2,\"203\":3,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":7,\"218\":1,\"222\":2,\"223\":2,\"224\":5,\"225\":1,\"229\":2,\"230\":2,\"231\":7,\"232\":1,\"235\":2,\"237\":1,\"239\":2,\"240\":3,\"241\":1,\"249\":1,\"257\":1,\"261\":1,\"263\":1,\"277\":2,\"286\":1,\"295\":1,\"296\":1,\"499\":1,\"509\":1,\"512\":1,\"525\":1,\"541\":3,\"544\":1,\"582\":1,\"593\":1,\"594\":1,\"628\":1,\"629\":1,\"632\":1,\"635\":1,\"640\":1,\"641\":1,\"654\":1,\"659\":2,\"660\":2,\"661\":2,\"662\":2,\"680\":1,\"683\":2,\"692\":1,\"693\":1,\"698\":1,\"699\":1,\"700\":3,\"703\":2,\"710\":1,\"712\":1,\"725\":1,\"734\":1,\"736\":1,\"745\":1,\"746\":1,\"754\":1,\"760\":1,\"767\":1,\"784\":1,\"799\":1,\"806\":1,\"812\":1,\"817\":1,\"824\":1,\"828\":1,\"837\":1,\"838\":1,\"869\":2,\"877\":2,\"889\":1,\"905\":1,\"909\":1,\"917\":1,\"918\":1,\"923\":1,\"927\":1,\"940\":1,\"942\":1,\"987\":1,\"997\":1,\"998\":1,\"1000\":1,\"1003\":2,\"1004\":2,\"1005\":3,\"1014\":2,\"1017\":2,\"1028\":3,\"1037\":1,\"1046\":1,\"1048\":2,\"1052\":1,\"1066\":1,\"1071\":1,\"1073\":1,\"1075\":1,\"1083\":1,\"1115\":1,\"1138\":3,\"1139\":3,\"1143\":1,\"1171\":2,\"1198\":2,\"1206\":2,\"1211\":2,\"1214\":1,\"1215\":1,\"1224\":2,\"1241\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1253\":1,\"1270\":1,\"1284\":1,\"1287\":2,\"1317\":1,\"1334\":1,\"1336\":2,\"1337\":1,\"1343\":1,\"1348\":2,\"1349\":1,\"1350\":1,\"1352\":2,\"1377\":1,\"1384\":1,\"1386\":1,\"1398\":1,\"1409\":1,\"1429\":1,\"1430\":1,\"1452\":2,\"1454\":2,\"1458\":1,\"1474\":1,\"1484\":1,\"1505\":1,\"1515\":2,\"1528\":2,\"1529\":2,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":2,\"1552\":2,\"1558\":1,\"1581\":1,\"1601\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1618\":1,\"1619\":1,\"1622\":2,\"1626\":2,\"1635\":1,\"1638\":2,\"1640\":1,\"1650\":1,\"1654\":1,\"1655\":1,\"1658\":2,\"1659\":3,\"1669\":1,\"1670\":3,\"1671\":3,\"1695\":1,\"1699\":1,\"1709\":1,\"1717\":1,\"1719\":2,\"1735\":1,\"1750\":1,\"1757\":1,\"1765\":2,\"1766\":1,\"1767\":2,\"1776\":1,\"1782\":1,\"1785\":1,\"1793\":1,\"1800\":2,\"1803\":2,\"1836\":2,\"1839\":2,\"1844\":2,\"1848\":2,\"1849\":4,\"1850\":2,\"1851\":7,\"1852\":2,\"1857\":2,\"1858\":2,\"1861\":2,\"1862\":2,\"1870\":1,\"1871\":2,\"1880\":2,\"1889\":2,\"1895\":1,\"1896\":1,\"1900\":1,\"1950\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"1962\":1,\"1963\":1,\"1964\":1,\"1966\":1,\"1986\":2,\"2001\":1,\"2004\":1,\"2019\":1,\"2049\":1,\"2055\":1,\"2064\":1,\"2070\":1,\"2078\":1,\"2086\":3,\"2087\":3,\"2090\":1,\"2096\":3,\"2098\":3,\"2099\":5,\"2100\":3,\"2101\":3,\"2102\":3,\"2103\":3,\"2104\":3,\"2105\":3,\"2107\":3,\"2108\":3,\"2109\":4,\"2110\":3,\"2111\":2,\"2112\":3,\"2113\":4,\"2114\":3,\"2115\":4,\"2116\":4,\"2117\":3,\"2118\":3,\"2142\":1,\"2143\":1,\"2151\":1,\"2153\":1,\"2154\":1,\"2155\":4,\"2168\":1,\"2170\":1,\"2209\":3,\"2244\":6,\"2255\":6,\"2259\":1,\"2267\":4,\"2279\":6,\"2294\":1,\"2302\":1,\"2307\":2,\"2309\":1,\"2318\":1,\"2324\":1,\"2344\":1,\"2347\":2,\"2358\":2,\"2359\":1,\"2360\":5,\"2364\":3,\"2365\":1,\"2367\":2,\"2368\":2,\"2371\":2,\"2372\":3,\"2373\":2,\"2385\":3,\"2386\":3,\"2387\":1,\"2391\":1,\"2392\":1,\"2395\":1,\"2396\":1,\"2415\":4,\"2416\":3,\"2417\":1,\"2421\":1,\"2423\":1,\"2425\":1,\"2429\":2,\"2430\":1,\"2431\":3,\"2432\":1,\"2433\":2,\"2440\":3,\"2450\":2,\"2452\":1,\"2455\":1,\"2456\":2,\"2458\":5,\"2460\":3,\"2462\":1,\"2467\":6,\"2470\":1,\"2472\":2,\"2473\":3,\"2474\":4,\"2476\":3,\"2478\":3,\"2480\":1,\"2482\":1,\"2485\":2,\"2486\":2,\"2490\":2,\"2494\":4,\"2497\":1,\"2498\":3,\"2500\":3,\"2501\":3,\"2502\":1,\"2507\":3,\"2508\":1,\"2510\":7,\"2513\":3,\"2514\":3,\"2515\":1,\"2518\":2,\"2520\":3,\"2521\":2,\"2522\":2,\"2523\":5,\"2527\":1,\"2528\":1,\"2531\":1,\"2532\":1,\"2543\":1,\"2546\":1,\"2548\":1,\"2554\":2,\"2555\":2,\"2564\":1,\"2565\":1,\"2567\":1,\"2568\":1,\"2570\":1,\"2571\":1,\"2572\":1,\"2574\":1,\"2579\":2,\"2580\":1,\"2581\":2,\"2582\":5,\"2583\":1,\"2584\":5,\"2585\":6,\"2588\":2,\"2591\":2,\"2593\":3,\"2599\":3,\"2600\":13,\"2604\":2,\"2605\":2,\"2607\":2,\"2609\":2,\"2612\":2,\"2614\":1,\"2615\":2,\"2616\":3,\"2617\":3,\"2621\":2,\"2622\":2,\"2624\":2,\"2626\":2,\"2630\":4,\"2632\":1,\"2633\":2,\"2634\":3,\"2635\":3,\"2637\":1,\"2647\":1,\"2648\":2,\"2649\":4,\"2654\":3,\"2655\":1,\"2658\":3,\"2659\":3,\"2660\":1}}],[\"freq=40\",{\"1\":{\"2074\":1,\"2075\":1}}],[\"freqwiseblock\",{\"0\":{\"1564\":1},\"1\":{\"1564\":2}}],[\"frequencíes\",{\"1\":{\"1904\":2,\"1916\":1}}],[\"frequence\",{\"1\":{\"1570\":1,\"1766\":1}}],[\"frequencies\",{\"1\":{\"1543\":1,\"1655\":3,\"1719\":5,\"1810\":2,\"1904\":1,\"1916\":1}}],[\"frequencydomainmse\",{\"0\":{\"1571\":1},\"1\":{\"1571\":1}}],[\"frequencydomainl1\",{\"0\":{\"1569\":1},\"1\":{\"1569\":1}}],[\"frequencydomainloss\",{\"0\":{\"1570\":1},\"1\":{\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1}}],[\"frequencydomaindpcl\",{\"0\":{\"1568\":1},\"1\":{\"1568\":1}}],[\"frequencydomaincrossentropy\",{\"0\":{\"1567\":1},\"1\":{\"1567\":1}}],[\"frequencydomainabscoherence\",{\"0\":{\"1566\":1},\"1\":{\"1566\":1}}],[\"frequency\",{\"1\":{\"47\":1,\"48\":2,\"49\":1,\"275\":2,\"297\":1,\"648\":1,\"778\":2,\"885\":1,\"940\":2,\"1019\":1,\"1037\":1,\"1430\":1,\"1462\":6,\"1463\":2,\"1464\":2,\"1517\":1,\"1543\":3,\"1551\":1,\"1553\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1571\":1,\"1660\":1,\"1661\":1,\"1662\":3,\"1671\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1766\":1,\"1785\":4,\"1786\":4,\"1801\":1,\"1810\":3,\"1834\":1,\"1859\":2,\"1860\":1,\"1869\":1,\"1876\":1,\"1883\":1,\"1904\":1,\"1912\":2,\"1916\":1,\"1921\":1,\"1922\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"2064\":1,\"2317\":3,\"2330\":3,\"2641\":1}}],[\"freqs=257\",{\"1\":{\"1655\":1}}],[\"freqs\",{\"1\":{\"1543\":3,\"1564\":1,\"1578\":1,\"1579\":1,\"1655\":1,\"1810\":4,\"2616\":1,\"2634\":1}}],[\"freqmask\",{\"0\":{\"943\":1},\"1\":{\"943\":2}}],[\"freq\",{\"0\":{\"965\":1,\"1019\":1},\"1\":{\"648\":1,\"729\":1,\"730\":1,\"943\":2,\"950\":6,\"955\":2,\"956\":2,\"965\":4,\"968\":8,\"972\":2,\"973\":2,\"1019\":2,\"1037\":3,\"1257\":3,\"1375\":3,\"1430\":3,\"1463\":3,\"1470\":2,\"1471\":2,\"1505\":3,\"1515\":3,\"1516\":3,\"1523\":3,\"1524\":1,\"1525\":1,\"1528\":3,\"1534\":3,\"1539\":3,\"1543\":2,\"1558\":3,\"1564\":1,\"1594\":1,\"1611\":7,\"1626\":3,\"1645\":3,\"1654\":3,\"1658\":3,\"1659\":1,\"1669\":3,\"1670\":2,\"1671\":3,\"1703\":1,\"1763\":1,\"1766\":1,\"1785\":1,\"1801\":2,\"1834\":2,\"1876\":2,\"1905\":3,\"1914\":1,\"1915\":1,\"1918\":2,\"1919\":1,\"1921\":4,\"1922\":4,\"1936\":2,\"1938\":2,\"1939\":2,\"1940\":1,\"1948\":1,\"2059\":1}}],[\"french\",{\"1\":{\"461\":1}}],[\"free=$0\",{\"1\":{\"144\":2}}],[\"free\",{\"0\":{\"530\":1,\"595\":1,\"2212\":1},\"1\":{\"36\":2,\"38\":3,\"39\":3,\"40\":1,\"44\":1,\"84\":2,\"530\":3,\"595\":2,\"2212\":3,\"2411\":1}}],[\"freeze\",{\"0\":{\"67\":1,\"639\":1},\"1\":{\"29\":5,\"67\":1,\"251\":2,\"265\":2,\"269\":2,\"639\":3,\"1178\":2,\"1179\":1,\"1180\":3,\"1269\":3,\"1284\":1,\"1719\":2}}],[\"freezing\",{\"0\":{\"29\":1},\"1\":{\"27\":1,\"29\":1,\"1719\":1}}],[\"freely\",{\"1\":{\"11\":1,\"15\":1,\"85\":2}}],[\"fi\",{\"1\":{\"2568\":4}}],[\"five\",{\"1\":{\"2457\":1}}],[\"fit\",{\"1\":{\"2387\":1}}],[\"fidelity\",{\"1\":{\"1462\":1,\"1463\":1}}],[\"figsize=\",{\"1\":{\"2498\":1,\"2616\":1,\"2634\":1}}],[\"fig\",{\"1\":{\"909\":1,\"1522\":1}}],[\"figure\",{\"1\":{\"754\":1,\"820\":1,\"821\":1,\"826\":1,\"2468\":1,\"2498\":1,\"2572\":1,\"2616\":1,\"2634\":1}}],[\"figures\",{\"1\":{\"240\":2,\"629\":1}}],[\"figdir\",{\"1\":{\"585\":2}}],[\"fixme\",{\"1\":{\"2019\":1}}],[\"fix\",{\"1\":{\"399\":2,\"464\":2,\"470\":2,\"1517\":3,\"2086\":1,\"2087\":1}}],[\"fixedordersolver\",{\"0\":{\"1563\":1},\"1\":{\"1563\":1}}],[\"fixed\",{\"0\":{\"1563\":1},\"1\":{\"69\":1,\"82\":1,\"109\":1,\"301\":1,\"1141\":2,\"1559\":1,\"1560\":1,\"1563\":2,\"1643\":1,\"1644\":1,\"1655\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1718\":1,\"1719\":1,\"1842\":1,\"2440\":1,\"2564\":1}}],[\"fisher\",{\"1\":{\"201\":3,\"296\":1}}],[\"fist\",{\"1\":{\"75\":1}}],[\"fields\",{\"1\":{\"2387\":1,\"2584\":1}}],[\"field\",{\"1\":{\"116\":1,\"461\":2,\"694\":6,\"765\":4,\"798\":5,\"1011\":1,\"1400\":1,\"1862\":2,\"1880\":2,\"2046\":1,\"2168\":1,\"2170\":1,\"2451\":1,\"2467\":1}}],[\"filts=15\",{\"1\":{\"1017\":1}}],[\"filts=5\",{\"1\":{\"802\":1,\"2078\":1,\"2083\":1}}],[\"filts=3\",{\"1\":{\"701\":1}}],[\"filts\",{\"1\":{\"679\":2,\"681\":2,\"682\":2,\"683\":2,\"684\":2,\"685\":2,\"688\":2,\"689\":2,\"701\":1,\"758\":2,\"802\":1,\"821\":4,\"826\":2,\"892\":2,\"1220\":2,\"1289\":2,\"1778\":1,\"1852\":1,\"2002\":4,\"2078\":1,\"2083\":1,\"2086\":4,\"2087\":4,\"2090\":2,\"2095\":6,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":6,\"2264\":4,\"2279\":2}}],[\"filt\",{\"0\":{\"538\":1},\"1\":{\"538\":3}}],[\"filter=true\",{\"1\":{\"2079\":1}}],[\"filter=asteroid\",{\"1\":{\"1454\":1}}],[\"filterw\",{\"1\":{\"1688\":1,\"1756\":1}}],[\"filterh\",{\"1\":{\"1688\":1,\"1756\":1}}],[\"filtering\",{\"0\":{\"1736\":1,\"1921\":1,\"1922\":1,\"1936\":1,\"1938\":1,\"1939\":1},\"1\":{\"885\":1,\"1466\":1,\"1680\":1,\"1706\":1,\"1707\":1,\"1715\":1,\"1736\":2,\"1917\":1,\"1921\":1,\"1922\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"2152\":2}}],[\"filterbank\",{\"1\":{\"802\":1,\"1132\":4,\"1198\":1,\"1810\":1,\"1917\":1,\"2049\":1,\"2330\":2,\"2574\":1}}],[\"filterbanks\",{\"1\":{\"238\":1,\"701\":2,\"826\":1,\"1785\":1,\"1810\":2,\"1883\":1,\"2002\":1,\"2263\":1,\"2264\":1}}],[\"filters\",{\"1\":{\"683\":1,\"778\":1,\"1132\":2,\"1198\":3,\"1375\":1,\"1379\":1,\"1639\":1,\"1664\":1,\"1665\":1,\"1761\":2,\"1763\":2,\"1765\":1,\"1795\":1,\"1800\":1,\"1804\":2,\"1805\":2,\"1883\":1,\"1912\":1,\"1917\":5}}],[\"filter\",{\"0\":{\"637\":1,\"1340\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1703\":1,\"1737\":1,\"1883\":1,\"2152\":1},\"1\":{\"52\":1,\"217\":1,\"272\":3,\"538\":1,\"637\":2,\"639\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"688\":1,\"689\":3,\"758\":2,\"802\":2,\"821\":7,\"826\":2,\"835\":3,\"892\":1,\"1132\":1,\"1198\":1,\"1340\":1,\"1466\":3,\"1558\":1,\"1639\":5,\"1688\":1,\"1693\":5,\"1696\":2,\"1697\":3,\"1698\":1,\"1703\":3,\"1708\":1,\"1712\":3,\"1715\":3,\"1719\":1,\"1736\":3,\"1737\":3,\"1739\":1,\"1755\":5,\"1756\":1,\"1758\":1,\"1759\":1,\"1772\":3,\"1785\":2,\"1804\":1,\"1810\":1,\"1860\":1,\"1883\":4,\"1904\":2,\"1916\":2,\"1917\":4,\"1921\":2,\"1922\":2,\"1936\":2,\"1938\":2,\"1939\":2,\"2002\":4,\"2078\":3,\"2079\":2,\"2083\":2,\"2086\":4,\"2087\":4,\"2095\":13,\"2152\":2,\"2263\":6,\"2264\":2}}],[\"filtered\",{\"1\":{\"45\":1,\"637\":1,\"639\":1,\"1921\":1,\"1922\":1,\"1928\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":1}}],[\"filling\",{\"0\":{\"2475\":1},\"1\":{\"2476\":1}}],[\"fill=0\",{\"1\":{\"903\":1}}],[\"filled\",{\"1\":{\"743\":1,\"875\":1,\"1016\":1,\"1019\":2,\"1037\":2,\"1039\":2,\"1959\":1}}],[\"fill\",{\"0\":{\"1016\":2},\"1\":{\"97\":1,\"903\":2,\"1016\":3,\"1017\":1,\"1741\":1,\"2401\":1,\"2403\":1,\"2457\":1,\"2476\":2,\"2521\":1,\"2522\":1,\"2523\":1,\"2537\":1,\"2539\":1,\"2568\":1,\"2584\":1}}],[\"file=cfg\",{\"1\":{\"2371\":1,\"2612\":1,\"2630\":1}}],[\"file=\",{\"1\":{\"2368\":1,\"2455\":1,\"2460\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":2,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2648\":1,\"2649\":1}}],[\"file=<\",{\"1\":{\"2099\":1}}],[\"filereader\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"fileobj\",{\"1\":{\"1961\":1}}],[\"file1\",{\"1\":{\"1400\":5}}],[\"fileio\",{\"0\":{\"1382\":1,\"1384\":1,\"1386\":1,\"1388\":1,\"1390\":1,\"1393\":1,\"1394\":1,\"1396\":1,\"1398\":1,\"1399\":1,\"1401\":1,\"1403\":1,\"1405\":1,\"1407\":1,\"1409\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"2687\":1},\"1\":{\"1382\":1,\"1384\":1,\"1386\":1,\"1388\":2,\"1390\":1,\"1393\":2,\"1394\":2,\"1396\":2,\"1398\":2,\"1399\":2,\"1401\":1,\"1403\":1,\"1405\":2,\"1407\":2,\"1409\":2,\"1411\":2,\"1413\":2,\"1415\":2,\"1417\":1,\"1419\":2,\"1420\":1,\"1422\":2,\"1424\":1,\"1426\":2}}],[\"filepath\",{\"1\":{\"989\":2,\"2226\":1}}],[\"filetype=\",{\"1\":{\"941\":1,\"948\":1,\"949\":1,\"987\":1}}],[\"filetype\",{\"1\":{\"247\":1,\"276\":1,\"279\":1,\"281\":1,\"283\":1,\"284\":1,\"496\":3,\"506\":2,\"509\":1,\"512\":1,\"519\":1,\"522\":2,\"525\":1,\"533\":1,\"541\":1,\"1013\":2,\"1015\":2}}],[\"filename=\",{\"1\":{\"656\":1,\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"filename\",{\"1\":{\"38\":1,\"621\":2,\"652\":2,\"909\":1,\"916\":1,\"1961\":1,\"2360\":1,\"2458\":1,\"2523\":1,\"2568\":3,\"2582\":1}}],[\"files\",{\"0\":{\"53\":1,\"182\":1},\"1\":{\"3\":1,\"11\":5,\"15\":1,\"46\":2,\"47\":4,\"49\":4,\"54\":3,\"63\":1,\"76\":1,\"85\":1,\"107\":1,\"124\":1,\"169\":3,\"171\":1,\"181\":3,\"182\":1,\"198\":2,\"235\":3,\"237\":2,\"238\":2,\"242\":1,\"272\":1,\"274\":2,\"377\":2,\"397\":1,\"441\":1,\"496\":1,\"515\":1,\"517\":1,\"525\":1,\"560\":1,\"566\":1,\"568\":1,\"582\":3,\"610\":1,\"629\":1,\"799\":1,\"989\":1,\"1406\":1,\"1407\":3,\"1962\":1,\"1967\":2,\"1968\":2,\"2007\":1,\"2008\":1,\"2009\":1,\"2012\":3,\"2099\":1,\"2106\":2,\"2309\":1,\"2343\":1,\"2344\":1,\"2371\":1,\"2372\":1,\"2375\":1,\"2377\":1,\"2385\":3,\"2386\":1,\"2387\":2,\"2389\":1,\"2394\":8,\"2408\":1,\"2412\":1,\"2422\":1,\"2429\":1,\"2432\":1,\"2436\":1,\"2449\":1,\"2465\":1,\"2481\":2,\"2497\":1,\"2500\":1,\"2501\":2,\"2503\":1,\"2522\":2,\"2525\":1,\"2530\":8,\"2545\":1,\"2554\":1,\"2558\":2,\"2562\":1,\"2566\":1,\"2568\":2,\"2581\":2,\"2593\":1,\"2600\":4,\"2607\":2,\"2612\":1,\"2615\":2,\"2617\":1,\"2624\":2,\"2630\":1,\"2633\":2,\"2635\":1,\"2638\":2,\"2639\":2}}],[\"file\",{\"0\":{\"46\":1,\"48\":1,\"49\":1,\"58\":1,\"63\":1,\"87\":1,\"88\":1,\"183\":1,\"1013\":1,\"1015\":1,\"2343\":1,\"2377\":1,\"2386\":1,\"2436\":1,\"2470\":1,\"2562\":1,\"2588\":1,\"2593\":1,\"2647\":1},\"1\":{\"2\":1,\"5\":1,\"25\":6,\"38\":5,\"40\":2,\"41\":1,\"42\":1,\"46\":1,\"47\":2,\"48\":1,\"49\":1,\"54\":2,\"57\":7,\"58\":4,\"66\":1,\"74\":5,\"75\":4,\"76\":8,\"77\":4,\"78\":4,\"79\":2,\"84\":3,\"85\":1,\"92\":1,\"98\":1,\"99\":1,\"102\":1,\"104\":1,\"110\":1,\"142\":2,\"143\":1,\"144\":1,\"235\":2,\"237\":3,\"238\":7,\"239\":5,\"240\":4,\"251\":2,\"259\":2,\"271\":1,\"276\":1,\"277\":5,\"279\":2,\"281\":1,\"283\":1,\"284\":1,\"286\":4,\"295\":1,\"296\":4,\"301\":2,\"307\":12,\"315\":8,\"321\":4,\"327\":8,\"333\":6,\"339\":4,\"343\":4,\"350\":4,\"357\":4,\"363\":2,\"368\":4,\"377\":2,\"380\":4,\"384\":8,\"391\":10,\"398\":28,\"399\":6,\"408\":10,\"416\":4,\"422\":10,\"429\":4,\"437\":4,\"443\":16,\"449\":8,\"455\":4,\"464\":6,\"470\":6,\"476\":4,\"478\":10,\"485\":8,\"491\":2,\"503\":1,\"525\":1,\"536\":1,\"538\":1,\"544\":1,\"551\":1,\"557\":1,\"560\":1,\"564\":1,\"570\":1,\"576\":1,\"582\":1,\"585\":1,\"619\":1,\"621\":1,\"634\":1,\"641\":2,\"652\":1,\"653\":1,\"654\":1,\"693\":2,\"742\":1,\"752\":1,\"760\":4,\"909\":1,\"987\":1,\"989\":1,\"1013\":1,\"1014\":4,\"1015\":6,\"1198\":1,\"1384\":1,\"1386\":1,\"1391\":1,\"1394\":2,\"1396\":2,\"1398\":1,\"1406\":2,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1699\":1,\"1709\":1,\"1800\":1,\"1906\":3,\"1927\":1,\"1961\":1,\"1962\":2,\"1968\":1,\"1969\":3,\"2006\":1,\"2007\":1,\"2010\":2,\"2011\":3,\"2012\":1,\"2099\":8,\"2109\":3,\"2113\":3,\"2115\":3,\"2116\":3,\"2189\":1,\"2197\":3,\"2254\":2,\"2309\":1,\"2343\":3,\"2344\":2,\"2347\":4,\"2349\":1,\"2371\":1,\"2373\":2,\"2375\":1,\"2377\":1,\"2378\":1,\"2385\":4,\"2387\":4,\"2389\":1,\"2395\":1,\"2398\":2,\"2408\":1,\"2422\":1,\"2430\":5,\"2431\":2,\"2436\":1,\"2437\":1,\"2440\":1,\"2449\":1,\"2461\":1,\"2465\":1,\"2470\":2,\"2476\":2,\"2478\":2,\"2481\":1,\"2487\":1,\"2491\":1,\"2501\":4,\"2503\":1,\"2522\":3,\"2525\":1,\"2531\":1,\"2534\":2,\"2545\":1,\"2555\":5,\"2558\":3,\"2559\":1,\"2562\":1,\"2563\":1,\"2564\":1,\"2567\":1,\"2568\":1,\"2570\":1,\"2581\":3,\"2584\":6,\"2585\":2,\"2586\":1,\"2593\":5,\"2600\":11,\"2607\":4,\"2612\":1,\"2615\":2,\"2624\":4,\"2630\":1,\"2633\":2,\"2639\":1,\"2647\":3}}],[\"firn\",{\"1\":{\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1}}],[\"firw\",{\"1\":{\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1}}],[\"firh\",{\"1\":{\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1}}],[\"fir=false\",{\"1\":{\"1631\":1}}],[\"fir=true\",{\"1\":{\"1605\":1}}],[\"fir\",{\"1\":{\"1605\":1,\"1631\":1,\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1}}],[\"firewall\",{\"1\":{\"44\":1}}],[\"first=false\",{\"1\":{\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1,\"1609\":1}}],[\"first=true\",{\"1\":{\"174\":2}}],[\"firstly\",{\"1\":{\"127\":1,\"2400\":1,\"2536\":1}}],[\"first\",{\"0\":{\"2429\":1,\"2648\":1},\"1\":{\"3\":1,\"28\":1,\"29\":1,\"45\":2,\"47\":1,\"57\":1,\"58\":1,\"60\":1,\"75\":2,\"76\":1,\"99\":1,\"112\":2,\"115\":1,\"146\":1,\"147\":1,\"148\":1,\"150\":1,\"171\":2,\"234\":1,\"242\":1,\"295\":1,\"416\":2,\"691\":2,\"697\":3,\"711\":1,\"749\":1,\"760\":1,\"785\":1,\"797\":1,\"834\":1,\"981\":2,\"1003\":1,\"1004\":1,\"1005\":1,\"1006\":1,\"1028\":1,\"1081\":3,\"1086\":5,\"1097\":1,\"1105\":3,\"1115\":2,\"1132\":1,\"1133\":1,\"1142\":4,\"1148\":1,\"1149\":1,\"1150\":1,\"1154\":3,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1186\":1,\"1187\":3,\"1200\":1,\"1202\":3,\"1203\":1,\"1210\":1,\"1269\":2,\"1272\":1,\"1286\":2,\"1287\":2,\"1298\":1,\"1299\":1,\"1301\":1,\"1304\":1,\"1352\":1,\"1505\":1,\"1531\":1,\"1669\":1,\"1670\":1,\"1671\":2,\"1766\":2,\"1785\":1,\"1797\":1,\"1846\":1,\"1847\":1,\"1849\":2,\"1856\":3,\"1858\":2,\"1880\":4,\"1890\":1,\"1897\":1,\"1905\":1,\"2001\":1,\"2004\":1,\"2018\":4,\"2029\":1,\"2054\":1,\"2186\":2,\"2194\":1,\"2204\":2,\"2372\":2,\"2377\":1,\"2385\":1,\"2387\":1,\"2389\":1,\"2392\":1,\"2395\":2,\"2396\":1,\"2399\":1,\"2400\":1,\"2403\":1,\"2411\":1,\"2417\":1,\"2425\":1,\"2429\":1,\"2431\":1,\"2436\":1,\"2467\":1,\"2472\":3,\"2525\":1,\"2528\":1,\"2531\":2,\"2532\":1,\"2535\":1,\"2536\":1,\"2539\":1,\"2548\":1,\"2554\":1,\"2562\":1,\"2568\":1,\"2584\":1,\"2585\":1,\"2637\":1,\"2648\":4}}],[\"finnish\",{\"1\":{\"461\":1}}],[\"finetuned\",{\"1\":{\"1178\":1}}],[\"finetune\",{\"0\":{\"2308\":1},\"1\":{\"1116\":1,\"1178\":3,\"1179\":1,\"1180\":2,\"2040\":1,\"2308\":2,\"2653\":1}}],[\"finetuning\",{\"0\":{\"27\":1},\"1\":{\"27\":1,\"1180\":1,\"1269\":4}}],[\"fine\",{\"0\":{\"66\":1,\"2573\":1},\"1\":{\"155\":1,\"734\":1,\"2099\":1,\"2377\":2,\"2573\":3}}],[\"finds\",{\"1\":{\"1025\":1,\"2324\":1}}],[\"finding\",{\"1\":{\"595\":1,\"2212\":1}}],[\"find\",{\"0\":{\"1965\":1},\"1\":{\"47\":1,\"63\":1,\"84\":1,\"85\":1,\"203\":1,\"238\":1,\"295\":1,\"595\":1,\"692\":1,\"693\":1,\"697\":1,\"797\":1,\"857\":1,\"1136\":2,\"1603\":1,\"1622\":1,\"1965\":1,\"2212\":1,\"2355\":1,\"2375\":1,\"2377\":1,\"2419\":1,\"2420\":1,\"2423\":1,\"2436\":1,\"2501\":1,\"2523\":1,\"2546\":1,\"2559\":1,\"2562\":1,\"2568\":1}}],[\"final=false\",{\"1\":{\"2592\":1,\"2596\":1}}],[\"final=true\",{\"1\":{\"1149\":1,\"1150\":1,\"2592\":2,\"2596\":1}}],[\"finalize\",{\"1\":{\"981\":2}}],[\"finals\",{\"1\":{\"231\":2}}],[\"finally\",{\"1\":{\"194\":1,\"242\":1,\"2401\":2,\"2432\":1,\"2537\":2,\"2572\":1,\"2638\":1,\"2645\":1}}],[\"finaly\",{\"1\":{\"174\":1}}],[\"final\",{\"1\":{\"23\":1,\"102\":1,\"115\":2,\"119\":1,\"231\":2,\"605\":2,\"609\":1,\"611\":1,\"692\":3,\"699\":1,\"700\":2,\"710\":3,\"734\":2,\"794\":1,\"815\":2,\"826\":1,\"863\":1,\"865\":1,\"908\":1,\"912\":1,\"922\":1,\"1048\":3,\"1071\":9,\"1093\":2,\"1115\":4,\"1138\":2,\"1139\":2,\"1149\":1,\"1150\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1269\":4,\"1375\":1,\"1765\":1,\"1778\":1,\"1800\":1,\"1803\":1,\"1805\":1,\"1839\":3,\"1844\":1,\"1848\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1857\":4,\"1877\":1,\"2373\":1,\"2385\":1,\"2412\":1,\"2430\":1,\"2518\":1,\"2555\":1,\"2600\":3}}],[\"finished\",{\"1\":{\"107\":1,\"137\":1,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"795\":1,\"2199\":1,\"2373\":1,\"2385\":1,\"2401\":1,\"2403\":1,\"2430\":1,\"2537\":1,\"2539\":1,\"2555\":1,\"2568\":1}}],[\"finish\",{\"0\":{\"2568\":1},\"1\":{\"17\":1,\"87\":1,\"204\":1,\"2193\":1,\"2403\":1,\"2539\":1,\"2565\":1,\"2568\":2}}],[\"fomulation\",{\"1\":{\"115\":1,\"1082\":1,\"1096\":1}}],[\"fout\",{\"1\":{\"1245\":1}}],[\"foundation\",{\"1\":{\"2422\":1,\"2545\":1}}],[\"found\",{\"1\":{\"44\":3,\"45\":1,\"110\":1,\"273\":1,\"289\":1,\"291\":1,\"293\":1,\"635\":1,\"771\":1,\"772\":1,\"809\":1,\"810\":1,\"1148\":1,\"1203\":1,\"1719\":1,\"2054\":1,\"2153\":1,\"2384\":1,\"2431\":2,\"2584\":1,\"2661\":1}}],[\"fourier\",{\"1\":{\"770\":1,\"1573\":1,\"1605\":2}}],[\"four\",{\"1\":{\"22\":1,\"116\":1,\"237\":1,\"1849\":1,\"2154\":1,\"2385\":1,\"2387\":1,\"2560\":1}}],[\"focusing\",{\"1\":{\"2400\":1,\"2536\":1}}],[\"focused\",{\"1\":{\"2354\":1,\"2441\":1,\"2468\":1}}],[\"focuses\",{\"1\":{\"79\":1}}],[\"focus\",{\"1\":{\"23\":1,\"56\":1,\"119\":1,\"263\":2,\"267\":2,\"1409\":2,\"2239\":1,\"2384\":1,\"2400\":1,\"2410\":1,\"2467\":1,\"2536\":1}}],[\"fooiterfactory\",{\"1\":{\"2099\":1}}],[\"foobar\",{\"1\":{\"944\":1}}],[\"foo=none\",{\"1\":{\"2320\":3,\"2328\":3,\"2340\":3}}],[\"foo=456\",{\"1\":{\"2328\":1}}],[\"foo=4\",{\"1\":{\"2176\":1,\"2320\":1}}],[\"foo=3\",{\"1\":{\"2176\":1}}],[\"foo=\",{\"1\":{\"144\":1,\"2335\":1,\"2340\":1}}],[\"foo=path\",{\"1\":{\"1\":1}}],[\"foo\",{\"1\":{\"3\":1,\"60\":2,\"63\":2,\"92\":1,\"944\":4,\"2149\":2,\"2176\":1,\"2320\":5,\"2328\":5,\"2335\":2,\"2340\":5}}],[\"fortunately\",{\"1\":{\"2398\":1,\"2534\":1}}],[\"forum\",{\"1\":{\"952\":1,\"1841\":1}}],[\"forces\",{\"1\":{\"762\":1}}],[\"force\",{\"0\":{\"2153\":1},\"1\":{\"698\":1,\"2153\":1,\"2184\":1,\"2200\":1,\"2638\":1}}],[\"forcing\",{\"1\":{\"399\":2,\"455\":2,\"464\":2,\"470\":2,\"1778\":3,\"1804\":3,\"1805\":3,\"1850\":3,\"1851\":3,\"1877\":3,\"1878\":4,\"1985\":1,\"2002\":3,\"2079\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":3,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2279\":3}}],[\"forget\",{\"0\":{\"919\":1},\"1\":{\"106\":1,\"919\":2}}],[\"formam\",{\"1\":{\"1407\":2}}],[\"formatted\",{\"1\":{\"1198\":1,\"2154\":1}}],[\"formatting\",{\"0\":{\"48\":1},\"1\":{\"48\":1}}],[\"format=format\",{\"1\":{\"2596\":1}}],[\"format=flac\",{\"1\":{\"47\":1}}],[\"format=pyaudio\",{\"1\":{\"2596\":1}}],[\"format=\",{\"1\":{\"991\":1,\"994\":1,\"1407\":1,\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"format=none\",{\"1\":{\"989\":1}}],[\"format=npy\",{\"1\":{\"58\":1}}],[\"format=text\",{\"1\":{\"58\":2}}],[\"format=kaldi\",{\"1\":{\"58\":1}}],[\"format=sound\",{\"1\":{\"58\":1}}],[\"formats\",{\"0\":{\"46\":1,\"49\":1},\"1\":{\"48\":1,\"49\":2,\"58\":1,\"1927\":3,\"2309\":1,\"2385\":1}}],[\"format\",{\"0\":{\"46\":1,\"52\":1,\"638\":1,\"2433\":1},\"1\":{\"16\":1,\"46\":14,\"47\":3,\"48\":2,\"49\":11,\"51\":3,\"54\":2,\"57\":3,\"58\":1,\"62\":1,\"63\":1,\"64\":1,\"84\":2,\"159\":1,\"161\":1,\"238\":1,\"240\":1,\"241\":1,\"271\":2,\"272\":2,\"276\":1,\"279\":1,\"281\":1,\"283\":1,\"284\":1,\"397\":1,\"525\":2,\"564\":1,\"638\":3,\"652\":2,\"989\":1,\"1015\":2,\"1400\":2,\"1407\":8,\"1773\":1,\"1837\":1,\"1926\":1,\"1927\":2,\"2275\":1,\"2354\":1,\"2373\":3,\"2381\":1,\"2382\":2,\"2384\":4,\"2385\":3,\"2386\":1,\"2387\":8,\"2388\":1,\"2395\":2,\"2397\":1,\"2398\":2,\"2412\":2,\"2421\":1,\"2430\":8,\"2431\":1,\"2500\":1,\"2524\":1,\"2531\":2,\"2533\":1,\"2534\":2,\"2544\":1,\"2555\":8,\"2564\":1,\"2568\":3,\"2584\":2,\"2597\":1,\"2617\":1,\"2635\":1,\"2642\":1}}],[\"form\",{\"1\":{\"1336\":1,\"1348\":1,\"1638\":1,\"1917\":2}}],[\"formulate\",{\"1\":{\"2395\":1,\"2531\":1}}],[\"formulation\",{\"1\":{\"115\":6,\"1061\":2,\"1067\":1,\"1082\":1,\"1096\":6}}],[\"formula\",{\"1\":{\"627\":1,\"778\":1,\"1011\":1,\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1,\"1452\":1,\"1912\":1,\"1915\":1}}],[\"former\",{\"1\":{\"25\":1,\"115\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1093\":3,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1}}],[\"forwardadaptor\",{\"0\":{\"2148\":1},\"1\":{\"2148\":1,\"2149\":1}}],[\"forwardsum\",{\"1\":{\"1841\":3}}],[\"forwardsumloss\",{\"0\":{\"1841\":1},\"1\":{\"1841\":2}}],[\"forwarding\",{\"1\":{\"80\":1}}],[\"forward\",{\"0\":{\"801\":1,\"1062\":1,\"1070\":1,\"2148\":1},\"1\":{\"21\":2,\"56\":1,\"57\":1,\"80\":1,\"87\":2,\"106\":1,\"115\":4,\"116\":2,\"124\":2,\"217\":1,\"263\":2,\"267\":2,\"399\":2,\"429\":2,\"464\":2,\"470\":2,\"627\":1,\"676\":1,\"677\":2,\"678\":2,\"679\":2,\"681\":7,\"682\":7,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"690\":2,\"692\":1,\"693\":1,\"697\":1,\"698\":1,\"699\":1,\"701\":2,\"702\":2,\"708\":2,\"710\":1,\"711\":10,\"712\":2,\"713\":2,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":3,\"726\":1,\"729\":2,\"730\":2,\"731\":2,\"732\":1,\"734\":1,\"735\":2,\"737\":4,\"738\":2,\"740\":2,\"741\":2,\"742\":2,\"745\":8,\"746\":4,\"747\":1,\"748\":2,\"749\":3,\"752\":1,\"753\":1,\"754\":5,\"755\":4,\"756\":1,\"757\":1,\"758\":7,\"759\":2,\"760\":1,\"761\":1,\"762\":2,\"763\":2,\"764\":2,\"766\":1,\"767\":1,\"768\":2,\"770\":1,\"771\":1,\"772\":1,\"774\":3,\"775\":2,\"776\":2,\"777\":1,\"778\":1,\"779\":1,\"781\":1,\"782\":2,\"784\":1,\"785\":3,\"786\":3,\"787\":1,\"793\":2,\"797\":1,\"800\":2,\"801\":2,\"802\":2,\"804\":2,\"806\":4,\"807\":2,\"809\":1,\"810\":1,\"813\":1,\"817\":1,\"818\":1,\"819\":1,\"820\":2,\"821\":2,\"822\":2,\"824\":2,\"825\":3,\"826\":2,\"827\":4,\"828\":1,\"830\":2,\"831\":1,\"832\":1,\"835\":2,\"838\":2,\"1046\":3,\"1047\":1,\"1049\":2,\"1050\":8,\"1051\":1,\"1052\":2,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":8,\"1057\":2,\"1058\":2,\"1061\":2,\"1062\":2,\"1064\":1,\"1065\":1,\"1066\":4,\"1067\":2,\"1068\":4,\"1069\":1,\"1070\":3,\"1072\":1,\"1073\":4,\"1074\":3,\"1075\":4,\"1076\":3,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":2,\"1083\":3,\"1084\":2,\"1086\":2,\"1093\":1,\"1106\":2,\"1107\":2,\"1108\":2,\"1109\":1,\"1110\":1,\"1111\":1,\"1112\":1,\"1113\":1,\"1114\":2,\"1116\":7,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":7,\"1134\":1,\"1135\":1,\"1136\":1,\"1137\":2,\"1140\":2,\"1141\":1,\"1142\":6,\"1145\":1,\"1148\":3,\"1149\":6,\"1150\":6,\"1151\":1,\"1152\":1,\"1153\":2,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1160\":3,\"1161\":3,\"1162\":2,\"1163\":2,\"1164\":3,\"1165\":3,\"1166\":2,\"1169\":2,\"1170\":7,\"1171\":1,\"1172\":1,\"1174\":1,\"1175\":1,\"1177\":3,\"1178\":2,\"1179\":3,\"1180\":2,\"1181\":2,\"1182\":2,\"1184\":1,\"1185\":1,\"1186\":2,\"1187\":7,\"1188\":1,\"1189\":1,\"1190\":2,\"1191\":2,\"1192\":2,\"1195\":2,\"1198\":1,\"1200\":2,\"1201\":2,\"1202\":7,\"1203\":3,\"1204\":2,\"1205\":1,\"1206\":1,\"1207\":1,\"1208\":1,\"1209\":4,\"1210\":2,\"1211\":2,\"1212\":1,\"1213\":1,\"1214\":4,\"1215\":1,\"1216\":1,\"1218\":1,\"1219\":1,\"1220\":1,\"1221\":2,\"1222\":1,\"1223\":1,\"1224\":2,\"1229\":1,\"1230\":1,\"1231\":1,\"1232\":1,\"1233\":1,\"1234\":1,\"1235\":1,\"1236\":1,\"1237\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1243\":2,\"1244\":2,\"1245\":2,\"1246\":3,\"1247\":3,\"1248\":3,\"1249\":1,\"1250\":1,\"1251\":2,\"1252\":3,\"1253\":5,\"1254\":3,\"1255\":1,\"1256\":2,\"1257\":1,\"1258\":1,\"1259\":1,\"1260\":1,\"1261\":1,\"1262\":1,\"1263\":1,\"1264\":1,\"1265\":1,\"1266\":1,\"1267\":1,\"1268\":1,\"1269\":5,\"1270\":4,\"1272\":2,\"1273\":4,\"1274\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":2,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1285\":1,\"1286\":7,\"1287\":7,\"1298\":6,\"1301\":3,\"1302\":6,\"1304\":3,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":2,\"1366\":1,\"1367\":1,\"1368\":2,\"1369\":2,\"1370\":2,\"1371\":1,\"1372\":2,\"1373\":2,\"1374\":2,\"1375\":1,\"1376\":2,\"1377\":2,\"1378\":2,\"1379\":1,\"1430\":2,\"1431\":1,\"1432\":2,\"1433\":1,\"1434\":1,\"1435\":1,\"1436\":2,\"1437\":1,\"1438\":1,\"1439\":1,\"1440\":2,\"1441\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1445\":1,\"1446\":2,\"1447\":1,\"1448\":1,\"1449\":1,\"1450\":1,\"1452\":2,\"1453\":1,\"1454\":3,\"1455\":2,\"1456\":1,\"1457\":1,\"1458\":1,\"1459\":1,\"1460\":1,\"1461\":1,\"1462\":2,\"1463\":2,\"1464\":2,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1470\":2,\"1471\":2,\"1472\":2,\"1473\":2,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":2,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":3,\"1506\":1,\"1507\":1,\"1508\":1,\"1509\":1,\"1510\":3,\"1511\":3,\"1512\":1,\"1513\":1,\"1515\":2,\"1516\":2,\"1517\":1,\"1518\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":2,\"1523\":2,\"1524\":2,\"1525\":2,\"1528\":2,\"1529\":2,\"1530\":1,\"1531\":1,\"1532\":1,\"1533\":1,\"1534\":2,\"1535\":1,\"1536\":1,\"1537\":1,\"1538\":1,\"1539\":2,\"1540\":1,\"1541\":1,\"1543\":1,\"1544\":1,\"1545\":2,\"1546\":2,\"1547\":1,\"1548\":1,\"1549\":1,\"1550\":1,\"1551\":6,\"1552\":1,\"1553\":8,\"1554\":3,\"1555\":1,\"1556\":1,\"1558\":2,\"1559\":2,\"1560\":2,\"1561\":1,\"1562\":1,\"1563\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1571\":1,\"1572\":2,\"1573\":1,\"1574\":1,\"1575\":2,\"1576\":2,\"1577\":2,\"1578\":2,\"1579\":2,\"1580\":2,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":2,\"1595\":2,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1601\":2,\"1602\":1,\"1603\":2,\"1604\":2,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":2,\"1613\":1,\"1614\":1,\"1616\":2,\"1617\":2,\"1620\":1,\"1621\":1,\"1622\":3,\"1624\":1,\"1625\":1,\"1626\":3,\"1627\":1,\"1628\":1,\"1629\":1,\"1630\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1639\":2,\"1640\":2,\"1641\":1,\"1642\":1,\"1643\":4,\"1644\":4,\"1645\":2,\"1646\":1,\"1647\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"1652\":1,\"1653\":2,\"1654\":3,\"1655\":2,\"1656\":1,\"1657\":1,\"1658\":3,\"1659\":2,\"1660\":2,\"1661\":2,\"1662\":2,\"1663\":2,\"1664\":1,\"1665\":2,\"1666\":2,\"1668\":2,\"1669\":3,\"1670\":2,\"1671\":2,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1718\":2,\"1719\":2,\"1760\":2,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1765\":2,\"1766\":2,\"1767\":2,\"1768\":2,\"1769\":1,\"1770\":1,\"1771\":2,\"1772\":2,\"1773\":4,\"1774\":1,\"1775\":1,\"1776\":1,\"1777\":2,\"1778\":5,\"1779\":1,\"1780\":1,\"1781\":2,\"1782\":1,\"1783\":1,\"1785\":2,\"1786\":2,\"1787\":2,\"1788\":2,\"1789\":1,\"1790\":1,\"1791\":1,\"1792\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":3,\"1798\":2,\"1800\":2,\"1801\":1,\"1802\":1,\"1803\":2,\"1804\":2,\"1805\":5,\"1806\":1,\"1807\":1,\"1808\":2,\"1828\":2,\"1829\":1,\"1833\":2,\"1834\":2,\"1835\":2,\"1836\":1,\"1837\":4,\"1838\":2,\"1839\":1,\"1840\":2,\"1841\":2,\"1842\":1,\"1843\":1,\"1844\":2,\"1845\":2,\"1846\":2,\"1847\":2,\"1848\":2,\"1849\":2,\"1850\":5,\"1851\":2,\"1852\":5,\"1853\":1,\"1854\":1,\"1855\":2,\"1856\":2,\"1857\":2,\"1858\":2,\"1859\":1,\"1861\":2,\"1862\":2,\"1863\":2,\"1864\":2,\"1865\":2,\"1866\":2,\"1867\":2,\"1868\":2,\"1869\":2,\"1870\":2,\"1871\":2,\"1872\":2,\"1873\":2,\"1874\":2,\"1875\":1,\"1876\":2,\"1877\":5,\"1878\":2,\"1879\":2,\"1880\":2,\"1890\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1902\":1,\"1903\":1,\"1906\":2,\"1907\":1,\"1908\":1,\"1910\":2,\"1911\":2,\"1912\":1,\"1913\":1,\"1914\":2,\"1915\":2,\"1917\":2,\"1918\":2,\"1919\":2,\"1920\":2,\"1951\":1,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1,\"1957\":1,\"1958\":1,\"1959\":1,\"1960\":1,\"1963\":1,\"1970\":1,\"1971\":1,\"1975\":1,\"1976\":1,\"1977\":1,\"1978\":1,\"1979\":1,\"1980\":1,\"1981\":1,\"1982\":1,\"1983\":2,\"1984\":1,\"1985\":2,\"1986\":1,\"1987\":1,\"1988\":1,\"1989\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1993\":2,\"1994\":1,\"1995\":1,\"1996\":2,\"1997\":1,\"1998\":1,\"1999\":2,\"2000\":2,\"2001\":5,\"2002\":5,\"2004\":3,\"2019\":1,\"2024\":1,\"2025\":1,\"2026\":2,\"2027\":1,\"2028\":2,\"2029\":2,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2039\":1,\"2040\":1,\"2041\":1,\"2042\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2046\":2,\"2047\":1,\"2048\":1,\"2049\":2,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":3,\"2055\":1,\"2056\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2063\":2,\"2064\":1,\"2065\":1,\"2066\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":2,\"2075\":2,\"2076\":1,\"2077\":1,\"2078\":2,\"2079\":3,\"2081\":2,\"2082\":1,\"2083\":2,\"2084\":2,\"2086\":4,\"2087\":4,\"2088\":2,\"2089\":2,\"2090\":2,\"2091\":3,\"2095\":5,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2148\":2,\"2149\":1,\"2150\":1,\"2168\":3,\"2169\":1,\"2170\":12,\"2186\":1,\"2202\":2,\"2204\":1,\"2233\":1,\"2234\":1,\"2235\":1,\"2237\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2242\":1,\"2243\":3,\"2244\":2,\"2245\":3,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":2,\"2254\":1,\"2255\":2,\"2256\":3,\"2257\":2,\"2258\":2,\"2259\":2,\"2260\":5,\"2261\":2,\"2262\":2,\"2263\":5,\"2264\":2,\"2265\":2,\"2266\":1,\"2267\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":2,\"2280\":3,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2288\":1,\"2289\":1,\"2290\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2294\":1,\"2297\":1,\"2298\":1,\"2299\":1,\"2300\":1,\"2301\":2,\"2302\":2,\"2303\":2,\"2304\":2,\"2305\":2,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"forked\",{\"1\":{\"112\":1}}],[\"fork\",{\"1\":{\"20\":1,\"113\":1,\"2393\":1,\"2450\":1,\"2529\":1}}],[\"for\",{\"0\":{\"36\":1,\"40\":1,\"42\":1,\"44\":1,\"54\":1,\"55\":1,\"60\":1,\"61\":1,\"64\":1,\"92\":1,\"96\":1,\"855\":1,\"867\":1,\"870\":1,\"878\":1,\"881\":1,\"882\":1,\"2385\":1,\"2387\":1,\"2417\":1,\"2475\":1,\"2477\":1,\"2492\":1,\"2568\":1,\"2583\":1,\"2588\":1,\"2589\":1,\"2590\":1,\"2592\":1,\"2628\":1,\"2638\":1,\"2645\":1},\"1\":{\"1\":1,\"3\":2,\"5\":3,\"7\":1,\"11\":2,\"15\":5,\"17\":2,\"19\":3,\"21\":12,\"22\":4,\"23\":5,\"24\":2,\"25\":1,\"26\":1,\"28\":3,\"29\":1,\"30\":7,\"32\":3,\"33\":1,\"36\":1,\"45\":2,\"47\":1,\"48\":1,\"49\":3,\"56\":3,\"57\":3,\"58\":1,\"59\":3,\"60\":6,\"62\":4,\"63\":2,\"69\":3,\"72\":3,\"74\":1,\"75\":1,\"76\":2,\"77\":2,\"78\":2,\"80\":4,\"82\":3,\"84\":2,\"85\":9,\"92\":5,\"93\":2,\"95\":1,\"96\":2,\"97\":1,\"98\":2,\"99\":2,\"102\":2,\"103\":1,\"104\":4,\"107\":2,\"109\":3,\"110\":1,\"112\":3,\"113\":4,\"114\":1,\"115\":44,\"116\":14,\"118\":2,\"119\":4,\"120\":2,\"121\":1,\"124\":3,\"130\":5,\"132\":7,\"133\":1,\"135\":4,\"136\":1,\"138\":1,\"140\":1,\"144\":3,\"148\":2,\"149\":2,\"150\":6,\"155\":1,\"156\":1,\"161\":1,\"164\":1,\"168\":3,\"169\":2,\"170\":1,\"171\":1,\"173\":2,\"174\":8,\"175\":2,\"179\":1,\"180\":1,\"181\":1,\"194\":2,\"196\":2,\"197\":1,\"198\":1,\"200\":1,\"202\":1,\"203\":3,\"204\":1,\"217\":5,\"224\":3,\"231\":5,\"234\":1,\"235\":1,\"237\":1,\"238\":6,\"239\":2,\"240\":1,\"241\":1,\"244\":1,\"247\":1,\"251\":2,\"271\":1,\"272\":1,\"274\":2,\"285\":1,\"295\":3,\"515\":1,\"576\":1,\"599\":1,\"604\":1,\"605\":3,\"607\":1,\"608\":1,\"619\":1,\"624\":1,\"625\":2,\"626\":2,\"627\":3,\"633\":1,\"635\":2,\"638\":1,\"643\":1,\"645\":2,\"646\":1,\"650\":1,\"652\":1,\"654\":1,\"656\":1,\"658\":1,\"659\":2,\"660\":2,\"661\":4,\"662\":2,\"672\":1,\"676\":15,\"680\":2,\"681\":1,\"682\":1,\"683\":2,\"686\":1,\"688\":1,\"689\":2,\"691\":9,\"692\":1,\"693\":7,\"694\":6,\"695\":4,\"696\":4,\"697\":14,\"698\":2,\"699\":2,\"700\":7,\"701\":1,\"702\":1,\"704\":1,\"705\":4,\"706\":15,\"707\":1,\"709\":1,\"710\":4,\"711\":2,\"712\":1,\"713\":1,\"725\":9,\"726\":7,\"727\":4,\"728\":4,\"733\":1,\"734\":7,\"735\":1,\"738\":1,\"740\":2,\"741\":2,\"742\":2,\"743\":1,\"745\":1,\"749\":1,\"750\":2,\"751\":1,\"753\":1,\"754\":7,\"755\":2,\"756\":1,\"757\":1,\"758\":2,\"761\":1,\"763\":1,\"765\":4,\"766\":1,\"767\":1,\"770\":2,\"773\":6,\"774\":2,\"775\":2,\"776\":2,\"778\":1,\"779\":1,\"781\":12,\"785\":5,\"786\":1,\"794\":4,\"795\":1,\"796\":4,\"797\":7,\"798\":6,\"802\":1,\"803\":1,\"806\":9,\"812\":3,\"814\":1,\"815\":6,\"816\":1,\"817\":4,\"820\":1,\"821\":4,\"822\":2,\"823\":4,\"824\":8,\"825\":8,\"826\":4,\"827\":1,\"828\":6,\"829\":3,\"832\":1,\"835\":2,\"836\":1,\"840\":1,\"841\":1,\"842\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"853\":1,\"855\":1,\"857\":2,\"858\":4,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"864\":1,\"867\":2,\"870\":1,\"875\":1,\"878\":1,\"880\":1,\"881\":1,\"882\":1,\"885\":1,\"889\":1,\"896\":1,\"906\":2,\"910\":1,\"911\":3,\"917\":3,\"918\":1,\"921\":1,\"924\":1,\"935\":1,\"936\":1,\"940\":1,\"943\":1,\"952\":1,\"955\":1,\"956\":1,\"959\":1,\"965\":1,\"972\":1,\"973\":1,\"975\":1,\"976\":1,\"981\":2,\"987\":5,\"999\":1,\"1000\":1,\"1001\":1,\"1003\":6,\"1004\":6,\"1005\":9,\"1007\":1,\"1008\":2,\"1011\":2,\"1012\":1,\"1013\":2,\"1014\":2,\"1015\":4,\"1025\":2,\"1028\":7,\"1031\":2,\"1034\":2,\"1037\":1,\"1042\":1,\"1043\":1,\"1046\":1,\"1047\":1,\"1048\":7,\"1049\":2,\"1050\":3,\"1052\":3,\"1056\":3,\"1057\":3,\"1058\":1,\"1059\":1,\"1060\":2,\"1061\":2,\"1063\":1,\"1064\":2,\"1065\":9,\"1066\":11,\"1067\":1,\"1068\":1,\"1069\":2,\"1071\":2,\"1072\":2,\"1073\":3,\"1074\":3,\"1075\":5,\"1076\":4,\"1080\":1,\"1081\":2,\"1082\":2,\"1083\":2,\"1084\":1,\"1085\":2,\"1086\":4,\"1093\":3,\"1095\":3,\"1096\":9,\"1097\":1,\"1098\":1,\"1101\":2,\"1102\":2,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1131\":1,\"1132\":4,\"1133\":5,\"1135\":1,\"1137\":1,\"1138\":8,\"1139\":6,\"1141\":1,\"1142\":4,\"1144\":3,\"1145\":1,\"1148\":3,\"1149\":5,\"1150\":5,\"1152\":1,\"1155\":3,\"1157\":1,\"1158\":2,\"1159\":1,\"1160\":5,\"1161\":5,\"1162\":2,\"1163\":1,\"1164\":5,\"1165\":4,\"1170\":1,\"1171\":1,\"1173\":1,\"1175\":1,\"1176\":1,\"1177\":4,\"1179\":2,\"1180\":1,\"1181\":3,\"1182\":1,\"1185\":1,\"1186\":1,\"1187\":7,\"1189\":1,\"1190\":6,\"1193\":1,\"1198\":3,\"1200\":1,\"1202\":7,\"1203\":6,\"1206\":1,\"1208\":1,\"1209\":4,\"1210\":2,\"1211\":6,\"1213\":1,\"1214\":5,\"1216\":1,\"1217\":1,\"1221\":5,\"1223\":1,\"1224\":1,\"1228\":4,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1239\":2,\"1240\":1,\"1241\":2,\"1243\":1,\"1244\":8,\"1245\":8,\"1246\":1,\"1250\":1,\"1251\":2,\"1252\":9,\"1253\":9,\"1254\":14,\"1255\":2,\"1257\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1269\":7,\"1270\":8,\"1272\":2,\"1273\":5,\"1275\":1,\"1277\":1,\"1279\":3,\"1281\":1,\"1283\":1,\"1285\":1,\"1286\":12,\"1287\":4,\"1301\":1,\"1302\":3,\"1303\":3,\"1304\":4,\"1336\":5,\"1337\":7,\"1349\":3,\"1350\":3,\"1351\":1,\"1352\":1,\"1357\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1371\":3,\"1373\":1,\"1374\":1,\"1376\":1,\"1377\":1,\"1388\":1,\"1390\":2,\"1394\":1,\"1396\":1,\"1398\":5,\"1399\":1,\"1400\":2,\"1401\":1,\"1403\":1,\"1405\":1,\"1407\":1,\"1409\":2,\"1411\":1,\"1413\":1,\"1415\":1,\"1430\":2,\"1432\":1,\"1434\":1,\"1436\":2,\"1437\":1,\"1438\":2,\"1440\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1451\":2,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1462\":1,\"1463\":2,\"1466\":2,\"1467\":1,\"1469\":1,\"1470\":2,\"1471\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1505\":3,\"1507\":1,\"1509\":1,\"1510\":1,\"1511\":2,\"1513\":1,\"1514\":3,\"1515\":3,\"1516\":1,\"1519\":1,\"1521\":1,\"1522\":11,\"1523\":12,\"1524\":5,\"1525\":1,\"1528\":2,\"1529\":3,\"1530\":1,\"1533\":1,\"1534\":1,\"1536\":1,\"1538\":1,\"1539\":1,\"1541\":1,\"1544\":1,\"1545\":6,\"1548\":1,\"1550\":1,\"1551\":19,\"1552\":5,\"1553\":17,\"1554\":3,\"1556\":1,\"1557\":2,\"1559\":2,\"1560\":2,\"1562\":1,\"1563\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":2,\"1569\":1,\"1570\":3,\"1571\":1,\"1573\":1,\"1574\":1,\"1581\":2,\"1582\":1,\"1584\":1,\"1585\":2,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":2,\"1603\":3,\"1604\":2,\"1606\":1,\"1608\":1,\"1610\":1,\"1611\":1,\"1612\":2,\"1614\":1,\"1615\":2,\"1618\":1,\"1619\":1,\"1621\":1,\"1622\":2,\"1623\":3,\"1625\":1,\"1626\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1637\":2,\"1638\":4,\"1639\":3,\"1640\":2,\"1641\":1,\"1642\":1,\"1643\":2,\"1644\":3,\"1645\":1,\"1646\":3,\"1647\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"1652\":3,\"1653\":1,\"1654\":3,\"1657\":1,\"1658\":1,\"1659\":4,\"1660\":6,\"1661\":6,\"1662\":7,\"1665\":1,\"1666\":1,\"1667\":3,\"1668\":1,\"1669\":1,\"1670\":5,\"1671\":8,\"1673\":1,\"1675\":1,\"1677\":1,\"1680\":1,\"1688\":1,\"1693\":1,\"1696\":1,\"1698\":1,\"1701\":1,\"1703\":1,\"1704\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1712\":2,\"1713\":1,\"1715\":3,\"1716\":1,\"1718\":2,\"1719\":4,\"1739\":7,\"1741\":2,\"1752\":1,\"1755\":1,\"1756\":1,\"1759\":1,\"1762\":1,\"1764\":1,\"1765\":4,\"1766\":2,\"1767\":2,\"1770\":1,\"1771\":1,\"1773\":3,\"1775\":1,\"1776\":1,\"1778\":20,\"1780\":1,\"1781\":1,\"1783\":1,\"1786\":2,\"1787\":3,\"1788\":7,\"1790\":1,\"1791\":2,\"1792\":1,\"1794\":1,\"1796\":1,\"1797\":6,\"1798\":6,\"1800\":5,\"1801\":6,\"1802\":1,\"1803\":4,\"1804\":12,\"1805\":21,\"1807\":1,\"1829\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1837\":3,\"1838\":1,\"1839\":1,\"1840\":1,\"1844\":4,\"1845\":1,\"1846\":3,\"1847\":4,\"1848\":1,\"1849\":6,\"1850\":16,\"1851\":8,\"1852\":18,\"1855\":2,\"1856\":7,\"1857\":4,\"1858\":8,\"1859\":2,\"1860\":2,\"1861\":1,\"1862\":2,\"1863\":2,\"1864\":3,\"1865\":3,\"1866\":2,\"1867\":2,\"1868\":1,\"1870\":2,\"1871\":4,\"1874\":4,\"1875\":1,\"1876\":1,\"1877\":18,\"1878\":10,\"1879\":1,\"1880\":2,\"1883\":3,\"1890\":5,\"1891\":1,\"1895\":3,\"1897\":3,\"1900\":3,\"1901\":1,\"1903\":1,\"1904\":1,\"1905\":1,\"1908\":1,\"1913\":1,\"1916\":1,\"1917\":1,\"1926\":1,\"1927\":6,\"1928\":1,\"1929\":3,\"1930\":3,\"1932\":6,\"1933\":1,\"1934\":1,\"1941\":4,\"1946\":1,\"1947\":4,\"1951\":1,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1,\"1957\":6,\"1958\":3,\"1959\":4,\"1960\":6,\"1962\":1,\"1964\":1,\"1971\":1,\"1972\":1,\"1973\":6,\"1976\":1,\"1977\":1,\"1978\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1989\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"1996\":1,\"1997\":1,\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":6,\"2002\":2,\"2004\":3,\"2011\":1,\"2012\":5,\"2019\":1,\"2021\":1,\"2022\":1,\"2025\":1,\"2029\":1,\"2030\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2040\":1,\"2041\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2046\":3,\"2048\":1,\"2050\":1,\"2051\":1,\"2053\":1,\"2054\":4,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2064\":1,\"2065\":1,\"2067\":1,\"2068\":2,\"2069\":1,\"2070\":1,\"2071\":1,\"2073\":1,\"2078\":3,\"2079\":1,\"2082\":1,\"2083\":2,\"2086\":1,\"2087\":1,\"2088\":2,\"2090\":8,\"2091\":2,\"2095\":5,\"2099\":9,\"2102\":8,\"2125\":1,\"2131\":2,\"2142\":1,\"2148\":1,\"2150\":1,\"2152\":2,\"2154\":1,\"2155\":3,\"2168\":1,\"2169\":1,\"2170\":6,\"2176\":1,\"2182\":1,\"2184\":1,\"2185\":2,\"2186\":1,\"2189\":1,\"2190\":1,\"2193\":1,\"2196\":1,\"2197\":2,\"2198\":1,\"2199\":2,\"2200\":1,\"2201\":1,\"2203\":3,\"2204\":1,\"2209\":1,\"2216\":1,\"2217\":1,\"2234\":1,\"2236\":1,\"2238\":1,\"2240\":1,\"2242\":1,\"2243\":2,\"2244\":3,\"2245\":2,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2255\":4,\"2256\":2,\"2258\":1,\"2260\":2,\"2263\":3,\"2264\":3,\"2267\":2,\"2275\":1,\"2276\":1,\"2278\":1,\"2279\":3,\"2280\":2,\"2282\":1,\"2284\":1,\"2286\":1,\"2288\":1,\"2289\":1,\"2290\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2312\":1,\"2343\":2,\"2344\":2,\"2347\":2,\"2350\":1,\"2354\":1,\"2359\":1,\"2364\":4,\"2368\":2,\"2371\":2,\"2372\":11,\"2373\":7,\"2375\":2,\"2378\":1,\"2380\":1,\"2384\":6,\"2385\":12,\"2386\":1,\"2387\":13,\"2388\":2,\"2389\":2,\"2394\":15,\"2395\":2,\"2398\":2,\"2399\":1,\"2400\":2,\"2401\":3,\"2403\":2,\"2405\":1,\"2407\":1,\"2408\":2,\"2411\":2,\"2412\":1,\"2413\":1,\"2414\":2,\"2415\":2,\"2418\":3,\"2419\":2,\"2420\":1,\"2421\":1,\"2422\":2,\"2424\":1,\"2429\":8,\"2430\":3,\"2431\":1,\"2433\":5,\"2437\":2,\"2441\":1,\"2449\":2,\"2450\":3,\"2452\":1,\"2456\":1,\"2460\":1,\"2461\":1,\"2462\":2,\"2465\":2,\"2467\":1,\"2468\":1,\"2471\":1,\"2472\":1,\"2473\":2,\"2476\":3,\"2478\":1,\"2481\":3,\"2482\":1,\"2486\":2,\"2487\":1,\"2490\":2,\"2491\":1,\"2492\":1,\"2494\":2,\"2497\":1,\"2499\":1,\"2500\":7,\"2501\":3,\"2503\":2,\"2507\":4,\"2508\":1,\"2510\":10,\"2512\":1,\"2513\":4,\"2514\":8,\"2515\":1,\"2520\":2,\"2521\":1,\"2522\":1,\"2523\":1,\"2524\":2,\"2525\":2,\"2529\":1,\"2530\":15,\"2531\":2,\"2534\":2,\"2535\":1,\"2536\":2,\"2537\":3,\"2539\":2,\"2541\":1,\"2542\":1,\"2543\":1,\"2544\":1,\"2545\":2,\"2547\":1,\"2550\":1,\"2552\":2,\"2554\":6,\"2555\":9,\"2558\":1,\"2559\":2,\"2563\":2,\"2564\":1,\"2565\":3,\"2566\":1,\"2568\":6,\"2572\":1,\"2573\":1,\"2574\":1,\"2580\":1,\"2581\":1,\"2583\":2,\"2584\":7,\"2585\":4,\"2592\":6,\"2593\":3,\"2596\":4,\"2600\":5,\"2605\":2,\"2607\":2,\"2609\":2,\"2612\":2,\"2615\":1,\"2617\":6,\"2618\":2,\"2622\":2,\"2624\":2,\"2626\":2,\"2628\":1,\"2630\":2,\"2633\":1,\"2635\":6,\"2637\":2,\"2638\":1,\"2640\":2,\"2641\":3,\"2642\":3,\"2643\":3,\"2644\":1,\"2645\":1,\"2648\":1,\"2654\":4,\"2657\":1,\"2658\":4,\"2659\":8}}],[\"fold\",{\"1\":{\"76\":5,\"429\":2,\"2007\":1,\"2012\":2}}],[\"foldedbatchsampler\",{\"0\":{\"2007\":1},\"1\":{\"2007\":1}}],[\"folded\",{\"0\":{\"2007\":1},\"1\":{\"73\":1,\"75\":1,\"76\":2,\"429\":2,\"2007\":1,\"2012\":4,\"2558\":1,\"2584\":1}}],[\"folder\",{\"1\":{\"3\":1,\"17\":1,\"2343\":1,\"2385\":1,\"2415\":1,\"2433\":1}}],[\"folders\",{\"1\":{\"1\":2,\"3\":3}}],[\"follow\",{\"1\":{\"3\":1,\"22\":1,\"84\":1,\"1778\":1,\"1801\":3,\"1805\":1,\"1846\":3,\"1847\":3,\"1850\":1,\"1852\":1,\"1877\":1,\"2363\":1,\"2384\":1,\"2387\":1,\"2389\":1,\"2408\":1,\"2409\":1,\"2413\":1,\"2422\":1,\"2446\":1,\"2449\":1,\"2459\":2,\"2465\":1,\"2466\":1,\"2481\":1,\"2503\":1,\"2506\":1,\"2512\":1,\"2525\":1,\"2543\":1,\"2545\":1,\"2575\":1,\"2585\":1,\"2600\":1,\"2653\":1,\"2657\":1}}],[\"followed\",{\"1\":{\"1\":1,\"1187\":2,\"1202\":2,\"1286\":1,\"1287\":1,\"1377\":1,\"1688\":1,\"1756\":1}}],[\"follows\",{\"1\":{\"1\":1,\"19\":1,\"34\":1,\"72\":1,\"76\":1,\"85\":1,\"90\":1,\"99\":1,\"149\":1,\"238\":1,\"1696\":1,\"1698\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"2090\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2354\":1,\"2385\":1,\"2386\":1,\"2388\":1,\"2421\":1,\"2524\":1,\"2544\":1,\"2567\":1,\"2600\":1}}],[\"followings\",{\"1\":{\"2394\":2,\"2530\":2}}],[\"following\",{\"1\":{\"1\":1,\"3\":1,\"4\":1,\"16\":3,\"17\":3,\"18\":1,\"19\":1,\"20\":1,\"27\":1,\"49\":2,\"52\":1,\"53\":1,\"54\":3,\"57\":1,\"59\":2,\"63\":1,\"65\":1,\"69\":1,\"80\":1,\"83\":1,\"92\":1,\"94\":1,\"97\":1,\"110\":1,\"112\":1,\"113\":2,\"114\":1,\"115\":2,\"116\":1,\"117\":1,\"118\":1,\"124\":1,\"132\":1,\"133\":1,\"142\":1,\"143\":1,\"144\":2,\"150\":1,\"235\":1,\"237\":2,\"238\":1,\"295\":1,\"638\":1,\"940\":1,\"1015\":1,\"1186\":1,\"1210\":1,\"1253\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1371\":1,\"1391\":1,\"1406\":1,\"1895\":1,\"1900\":1,\"2002\":1,\"2021\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2096\":1,\"2098\":1,\"2099\":3,\"2100\":1,\"2101\":1,\"2102\":3,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2363\":1,\"2368\":1,\"2371\":1,\"2372\":1,\"2375\":1,\"2385\":1,\"2393\":1,\"2394\":1,\"2398\":1,\"2405\":1,\"2410\":2,\"2431\":3,\"2441\":1,\"2486\":1,\"2490\":1,\"2492\":1,\"2494\":1,\"2500\":1,\"2506\":1,\"2512\":1,\"2529\":1,\"2530\":1,\"2534\":1,\"2541\":1,\"2558\":3,\"2564\":1,\"2568\":2,\"2569\":1,\"2572\":1,\"2584\":1,\"2587\":1,\"2598\":1,\"2600\":4,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2628\":1,\"2630\":1,\"2635\":1,\"2637\":2,\"2653\":1,\"2657\":1}}],[\"m05\",{\"1\":{\"2367\":2,\"2485\":2,\"2604\":2,\"2621\":2}}],[\"mravanelli\",{\"1\":{\"1917\":1}}],[\"mfa\",{\"1\":{\"2054\":2}}],[\"mfaconformerencoder\",{\"0\":{\"2054\":1},\"1\":{\"2054\":1}}],[\"mfd\",{\"1\":{\"1763\":1,\"1801\":1}}],[\"mfmcwf\",{\"1\":{\"1719\":6}}],[\"mfcc\",{\"1\":{\"283\":1,\"2049\":1,\"2441\":1}}],[\"mwf\",{\"0\":{\"1708\":1,\"1712\":1,\"1715\":1},\"1\":{\"1524\":1,\"1708\":2,\"1712\":2,\"1715\":3}}],[\"mmap\",{\"1\":{\"1398\":1}}],[\"mha\",{\"1\":{\"1209\":1}}],[\"mvn\",{\"0\":{\"932\":1,\"1906\":1,\"1920\":1,\"1949\":2},\"1\":{\"932\":1,\"1906\":2,\"1920\":2,\"1949\":4,\"2440\":5,\"2574\":1}}],[\"mvdr\",{\"0\":{\"885\":1,\"1706\":1,\"1707\":1,\"2489\":1,\"2609\":1,\"2626\":1},\"1\":{\"729\":1,\"885\":2,\"1158\":1,\"1524\":2,\"1611\":1,\"1706\":2,\"1707\":2,\"1712\":1,\"1739\":2,\"2490\":4,\"2492\":1,\"2609\":4,\"2626\":4,\"2628\":1}}],[\"m=3\",{\"1\":{\"880\":1}}],[\"mc\",{\"1\":{\"2367\":3,\"2470\":2,\"2472\":1,\"2474\":1,\"2476\":3,\"2478\":3,\"2485\":3,\"2490\":4,\"2491\":3,\"2604\":3,\"2609\":4,\"2610\":3,\"2621\":3,\"2626\":4,\"2627\":3,\"2647\":2,\"2648\":1,\"2649\":1}}],[\"mcwf\",{\"1\":{\"1719\":2}}],[\"mcep\",{\"1\":{\"562\":4}}],[\"mcd\",{\"0\":{\"562\":1},\"1\":{\"562\":3}}],[\"mttask\",{\"0\":{\"2108\":1},\"1\":{\"2108\":2}}],[\"mt=false\",{\"1\":{\"1028\":1}}],[\"mtinterface\",{\"0\":{\"781\":1},\"1\":{\"781\":2}}],[\"mt\",{\"0\":{\"255\":1,\"257\":1,\"391\":1,\"554\":1,\"599\":2,\"600\":2,\"601\":2,\"602\":2,\"603\":2,\"750\":1,\"781\":1,\"1970\":1,\"1971\":1,\"2108\":1,\"2671\":1,\"2697\":1},\"1\":{\"255\":2,\"257\":2,\"259\":2,\"274\":1,\"391\":7,\"554\":2,\"599\":2,\"600\":4,\"601\":4,\"602\":4,\"603\":4,\"646\":1,\"750\":3,\"781\":3,\"889\":1,\"1005\":4,\"1028\":3,\"1172\":1,\"1970\":3,\"1971\":2,\"2076\":3,\"2108\":2}}],[\"mtlalpha1\",{\"1\":{\"186\":2,\"187\":1,\"191\":1,\"192\":1,\"194\":1}}],[\"mtlalpha\",{\"1\":{\"150\":4,\"173\":1,\"251\":2,\"259\":2,\"2076\":2}}],[\"mtl\",{\"1\":{\"24\":1,\"811\":1}}],[\"mls\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"mlr\",{\"1\":{\"1524\":1}}],[\"mlabels\",{\"1\":{\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2}}],[\"mlmdecoder\",{\"0\":{\"1204\":1},\"1\":{\"1204\":2,\"1206\":1}}],[\"mlm\",{\"0\":{\"1204\":1},\"1\":{\"1204\":2}}],[\"ml\",{\"1\":{\"251\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2}}],[\"mlp\",{\"0\":{\"1820\":1},\"1\":{\"22\":4,\"770\":1,\"821\":1,\"825\":6,\"1151\":1,\"1820\":2,\"2002\":1,\"2095\":1,\"2263\":1}}],[\"mbg\",{\"1\":{\"1138\":3}}],[\"mb\",{\"1\":{\"216\":3,\"1142\":1,\"1334\":4}}],[\"mkdir\",{\"1\":{\"167\":2,\"178\":2,\"196\":2,\"200\":1,\"234\":2,\"2431\":1,\"2432\":1,\"2567\":1,\"2568\":1}}],[\"mkl\",{\"1\":{\"134\":4}}],[\"mdcdconfig\",{\"0\":{\"1784\":1},\"1\":{\"1784\":2}}],[\"mdc\",{\"0\":{\"1782\":1},\"1\":{\"1782\":2}}],[\"mdpi\",{\"1\":{\"1082\":1}}],[\"md\",{\"1\":{\"139\":6,\"140\":20,\"165\":7,\"168\":1,\"179\":1,\"195\":1,\"244\":21,\"1454\":1,\"2076\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2354\":1,\"2372\":1,\"2384\":1,\"2429\":1,\"2446\":1,\"2454\":1,\"2455\":2,\"2554\":1,\"2560\":1,\"2572\":1,\"2575\":1}}],[\"msd\",{\"1\":{\"1801\":1}}],[\"msfblock\",{\"0\":{\"1592\":1},\"1\":{\"1592\":1}}],[\"msg\",{\"1\":{\"594\":1}}],[\"mse\",{\"1\":{\"240\":2,\"1553\":1,\"1571\":1,\"1611\":1,\"1668\":1,\"1778\":2,\"1805\":2,\"1836\":2,\"1843\":2,\"1850\":2,\"1852\":2,\"1877\":2,\"2090\":1,\"2091\":1}}],[\"ms\",{\"1\":{\"110\":1,\"1071\":1,\"2267\":1}}],[\"mp\",{\"1\":{\"2040\":1}}],[\"mp=0\",{\"1\":{\"2040\":1}}],[\"mpd\",{\"1\":{\"1801\":1}}],[\"mpdr\",{\"1\":{\"1524\":1}}],[\"mp4\",{\"1\":{\"52\":2}}],[\"mp3\",{\"1\":{\"48\":1,\"49\":4,\"1927\":1}}],[\"mpirun\",{\"1\":{\"42\":1}}],[\"mpi\",{\"1\":{\"37\":1,\"42\":1,\"429\":1}}],[\"my\",{\"1\":{\"45\":1,\"99\":2,\"2440\":1,\"2564\":1}}],[\"m\",{\"1\":{\"34\":2,\"35\":1,\"36\":2,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"57\":1,\"58\":1,\"59\":3,\"60\":1,\"62\":4,\"63\":2,\"64\":2,\"65\":1,\"66\":6,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":2,\"72\":4,\"74\":1,\"75\":1,\"76\":2,\"77\":1,\"78\":1,\"79\":1,\"80\":1,\"81\":1,\"82\":1,\"201\":1,\"202\":1,\"871\":1,\"879\":1,\"880\":1,\"885\":1,\"893\":2,\"926\":2,\"950\":2,\"968\":2,\"1001\":1,\"1010\":2,\"1106\":1,\"1108\":2,\"1132\":1,\"1368\":3,\"1369\":2,\"1370\":2,\"1372\":3,\"1375\":6,\"1378\":2,\"1379\":3,\"1381\":2,\"1472\":3,\"1473\":2,\"1546\":3,\"1575\":3,\"1594\":1,\"1660\":2,\"1661\":2,\"1662\":1,\"1663\":2,\"1664\":3,\"1665\":5,\"1683\":2,\"1706\":1,\"1707\":1,\"1853\":2,\"1854\":4,\"2092\":1,\"2093\":1,\"2154\":2,\"2267\":3,\"2392\":2,\"2425\":2,\"2466\":1,\"2500\":4,\"2528\":2,\"2548\":2,\"2568\":2,\"2599\":2,\"2600\":5,\"2617\":4,\"2635\":4,\"2646\":1}}],[\"mexican\",{\"1\":{\"2585\":1}}],[\"mexico\",{\"1\":{\"2387\":1}}],[\"merging\",{\"1\":{\"2450\":1}}],[\"merges\",{\"1\":{\"1432\":1,\"1510\":1,\"1643\":1}}],[\"mergejson\",{\"0\":{\"566\":1},\"1\":{\"566\":2}}],[\"merge\",{\"0\":{\"564\":1,\"1728\":1},\"1\":{\"560\":1,\"564\":2,\"566\":1,\"691\":2,\"697\":4,\"797\":1,\"1140\":1,\"1141\":4,\"1169\":1,\"1170\":3,\"1432\":3,\"1510\":3,\"1539\":1,\"1643\":3,\"1728\":2,\"2440\":3,\"2450\":1,\"2517\":1,\"2564\":3}}],[\"merged\",{\"1\":{\"13\":1}}],[\"me\",{\"1\":{\"2398\":1,\"2534\":1}}],[\"mentionned\",{\"1\":{\"2585\":2}}],[\"mentioned\",{\"1\":{\"99\":1,\"2387\":2,\"2441\":1}}],[\"menu\",{\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"mediarecorder\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"mediadevices\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"media\",{\"1\":{\"2040\":1}}],[\"mediator\",{\"1\":{\"1951\":1,\"2168\":1}}],[\"median\",{\"1\":{\"1711\":1}}],[\"medium\",{\"1\":{\"98\":1}}],[\"measures\",{\"1\":{\"1297\":1,\"1357\":1}}],[\"measure=\",{\"1\":{\"1245\":1}}],[\"measure\",{\"1\":{\"1243\":1,\"1245\":3,\"1339\":1,\"1342\":1,\"1351\":1,\"1357\":1,\"2199\":2}}],[\"meanpooling\",{\"0\":{\"2052\":1},\"1\":{\"2052\":1}}],[\"meanpoolconv\",{\"0\":{\"1596\":1},\"1\":{\"1596\":1}}],[\"mean=true\",{\"1\":{\"1639\":1,\"1640\":1}}],[\"meanings\",{\"1\":{\"2430\":1,\"2555\":1}}],[\"meaning\",{\"1\":{\"115\":1,\"116\":1,\"1810\":1,\"2467\":1}}],[\"means=true\",{\"1\":{\"941\":1,\"960\":1}}],[\"means\",{\"1\":{\"59\":1,\"102\":1,\"112\":1,\"144\":1,\"251\":2,\"259\":2,\"496\":2,\"607\":1,\"705\":1,\"745\":1,\"746\":1,\"752\":1,\"760\":1,\"768\":1,\"809\":1,\"831\":1,\"932\":2,\"1005\":1,\"1028\":1,\"1093\":1,\"1101\":1,\"1178\":1,\"1245\":2,\"1528\":1,\"1862\":1,\"1880\":1,\"1906\":2,\"1920\":1,\"1932\":3,\"1949\":2,\"2373\":1,\"2430\":1,\"2431\":1,\"2555\":1}}],[\"mean\",{\"0\":{\"1819\":1,\"2052\":1},\"1\":{\"26\":4,\"115\":2,\"174\":2,\"238\":6,\"496\":1,\"506\":1,\"628\":3,\"702\":1,\"738\":2,\"760\":1,\"822\":1,\"932\":1,\"943\":1,\"950\":1,\"955\":1,\"965\":1,\"968\":1,\"972\":1,\"1019\":1,\"1037\":1,\"1039\":1,\"1061\":3,\"1096\":3,\"1211\":5,\"1224\":5,\"1336\":5,\"1348\":5,\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1604\":1,\"1612\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1623\":1,\"1637\":1,\"1639\":2,\"1640\":2,\"1710\":1,\"1711\":2,\"1804\":5,\"1805\":1,\"1819\":1,\"1853\":1,\"1854\":2,\"1863\":1,\"1864\":3,\"1865\":3,\"1874\":1,\"1877\":1,\"1878\":5,\"1906\":2,\"1949\":1,\"2000\":1,\"2052\":1,\"2088\":1,\"2267\":2,\"2268\":2,\"2295\":2,\"2296\":2,\"2375\":1,\"2401\":2,\"2440\":6,\"2537\":2,\"2558\":1}}],[\"mel2wav\",{\"1\":{\"1857\":1,\"1858\":1}}],[\"melody\",{\"1\":{\"1778\":6,\"1804\":4,\"1805\":6,\"2086\":10,\"2087\":10,\"2090\":9,\"2095\":9}}],[\"melscale\",{\"0\":{\"1785\":1,\"1916\":1},\"1\":{\"1785\":2,\"1916\":2}}],[\"mels=120\",{\"1\":{\"1526\":1}}],[\"melspectrogramloss\",{\"0\":{\"1859\":1},\"1\":{\"1859\":1}}],[\"melspectrogramtorch\",{\"0\":{\"1207\":1},\"1\":{\"1207\":1}}],[\"melspec\",{\"0\":{\"1207\":1},\"1\":{\"1207\":1,\"1526\":1}}],[\"mels\",{\"1\":{\"251\":2,\"259\":2,\"275\":2,\"509\":2,\"519\":2,\"752\":1,\"778\":2,\"947\":1,\"954\":1,\"959\":1,\"967\":1,\"971\":1,\"1133\":1,\"1158\":1,\"1190\":1,\"1204\":1,\"1207\":1,\"1214\":1,\"1244\":1,\"1273\":1,\"1778\":1,\"1785\":4,\"1805\":1,\"1810\":3,\"1850\":1,\"1852\":1,\"1859\":2,\"1877\":1,\"1912\":2,\"1989\":1,\"2248\":1,\"2254\":1,\"2259\":2,\"2260\":1,\"2317\":2,\"2330\":3}}],[\"mel\",{\"0\":{\"1912\":1,\"1989\":1,\"2248\":1},\"1\":{\"235\":3,\"238\":2,\"239\":1,\"241\":1,\"242\":1,\"275\":1,\"295\":2,\"701\":2,\"778\":4,\"802\":2,\"803\":1,\"804\":1,\"826\":1,\"1158\":1,\"1198\":1,\"1207\":2,\"1216\":2,\"1285\":1,\"1763\":1,\"1771\":1,\"1778\":10,\"1781\":2,\"1785\":3,\"1786\":3,\"1799\":1,\"1801\":1,\"1805\":6,\"1810\":3,\"1850\":6,\"1852\":9,\"1859\":7,\"1877\":6,\"1912\":4,\"1916\":4,\"1917\":1,\"1989\":2,\"2002\":1,\"2049\":1,\"2090\":4,\"2091\":2,\"2248\":2,\"2257\":1,\"2260\":1,\"2261\":1,\"2263\":2,\"2264\":1,\"2317\":1,\"2330\":3,\"2574\":1}}],[\"melganmultiscalediscriminator\",{\"0\":{\"1858\":1},\"1\":{\"1858\":2}}],[\"melgangenerator\",{\"0\":{\"1857\":1},\"1\":{\"1857\":2}}],[\"melgandiscriminator\",{\"0\":{\"1856\":1},\"1\":{\"1856\":2}}],[\"melgan\",{\"0\":{\"215\":1,\"216\":1,\"1856\":2,\"1857\":2,\"1858\":2,\"1860\":1,\"1867\":1,\"1870\":2,\"1871\":2,\"1872\":1,\"1873\":1,\"1883\":1},\"1\":{\"206\":1,\"215\":4,\"216\":4,\"1856\":3,\"1857\":4,\"1858\":4,\"1860\":2,\"1867\":2,\"1870\":3,\"1871\":3,\"1872\":1,\"1873\":1,\"1883\":1,\"2363\":9,\"2506\":2,\"2512\":4,\"2653\":9,\"2657\":4}}],[\"meth\",{\"1\":{\"652\":1}}],[\"methodologies\",{\"1\":{\"2468\":1}}],[\"method=\",{\"1\":{\"1184\":1,\"1474\":1,\"1774\":1,\"1811\":1}}],[\"method=2\",{\"1\":{\"986\":1}}],[\"methods\",{\"1\":{\"38\":1,\"59\":1,\"124\":1,\"745\":1,\"746\":1,\"1253\":2,\"1429\":1,\"1618\":1,\"1619\":1,\"2201\":1,\"2372\":1,\"2373\":1,\"2384\":1,\"2428\":1,\"2430\":1,\"2543\":2,\"2551\":1,\"2555\":1,\"2558\":1,\"2564\":1}}],[\"method\",{\"0\":{\"38\":1,\"598\":1},\"1\":{\"23\":1,\"38\":3,\"40\":1,\"41\":1,\"42\":1,\"60\":1,\"98\":1,\"119\":1,\"124\":2,\"429\":2,\"496\":2,\"509\":2,\"512\":2,\"522\":2,\"525\":2,\"598\":3,\"607\":2,\"608\":1,\"612\":1,\"627\":2,\"652\":1,\"700\":1,\"734\":1,\"745\":7,\"746\":6,\"760\":1,\"778\":1,\"797\":1,\"816\":2,\"817\":1,\"828\":1,\"831\":1,\"836\":3,\"917\":2,\"1015\":2,\"1037\":1,\"1048\":2,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":4,\"1153\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1171\":1,\"1172\":1,\"1177\":1,\"1182\":1,\"1198\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1252\":2,\"1253\":2,\"1254\":2,\"1257\":1,\"1265\":1,\"1267\":1,\"1286\":1,\"1336\":1,\"1337\":1,\"1343\":1,\"1344\":1,\"1346\":1,\"1349\":1,\"1350\":1,\"1429\":1,\"1476\":1,\"1552\":2,\"1598\":1,\"1639\":1,\"1658\":1,\"1698\":1,\"1704\":2,\"1707\":1,\"1712\":1,\"1713\":3,\"1715\":1,\"1804\":3,\"1805\":1,\"1811\":2,\"1883\":1,\"1892\":1,\"1893\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1923\":1,\"1964\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":1,\"2084\":1,\"2089\":1,\"2099\":2,\"2148\":2,\"2156\":2,\"2170\":1,\"2180\":2,\"2201\":1,\"2372\":1,\"2424\":1,\"2440\":3,\"2543\":2,\"2547\":1,\"2564\":3,\"2599\":1,\"2600\":2}}],[\"metrices\",{\"1\":{\"1639\":1}}],[\"metrics=none\",{\"1\":{\"2022\":1}}],[\"metrics\",{\"1\":{\"742\":1,\"1639\":1,\"2456\":1,\"2460\":1,\"2461\":1,\"2462\":1,\"2521\":1}}],[\"metric\",{\"1\":{\"499\":1,\"1639\":1,\"2418\":1}}],[\"metavar=none\",{\"1\":{\"2313\":1}}],[\"metadata\",{\"0\":{\"1155\":1},\"1\":{\"185\":1,\"1155\":2}}],[\"meta\",{\"1\":{\"169\":1,\"181\":1,\"1966\":1}}],[\"member\",{\"1\":{\"2168\":1,\"2170\":1}}],[\"memlstm\",{\"0\":{\"1598\":1},\"1\":{\"1598\":3,\"1652\":2,\"1654\":2}}],[\"mem\",{\"1\":{\"144\":2,\"265\":2,\"269\":2,\"997\":1,\"1598\":3,\"1652\":3,\"1654\":3,\"1670\":2}}],[\"mem=none\",{\"1\":{\"997\":1}}],[\"mem=false\",{\"1\":{\"987\":1}}],[\"mem=0\",{\"1\":{\"144\":1}}],[\"mem=\",{\"1\":{\"144\":1}}],[\"memory=none\",{\"1\":{\"1209\":2}}],[\"memory=true\",{\"1\":{\"174\":2}}],[\"memory\",{\"1\":{\"73\":1,\"80\":1,\"84\":1,\"102\":1,\"148\":1,\"608\":1,\"727\":1,\"731\":2,\"734\":1,\"785\":1,\"997\":1,\"1133\":10,\"1142\":6,\"1144\":1,\"1155\":3,\"1171\":1,\"1186\":2,\"1190\":1,\"1204\":1,\"1206\":1,\"1210\":4,\"1214\":4,\"1244\":1,\"1273\":8,\"1398\":1,\"1406\":1,\"1552\":1,\"1652\":1,\"1654\":1,\"1670\":10,\"1671\":11,\"1895\":1,\"1896\":1,\"1900\":1,\"2001\":3,\"2154\":1}}],[\"megadecoder\",{\"0\":{\"1066\":1},\"1\":{\"1066\":3}}],[\"megatron\",{\"1\":{\"668\":2}}],[\"mega\",{\"0\":{\"1065\":2,\"1066\":1,\"1069\":1,\"1070\":1,\"1078\":1,\"1079\":1},\"1\":{\"116\":21,\"1063\":1,\"1065\":8,\"1066\":4,\"1069\":1,\"1070\":1,\"1078\":1,\"1079\":1}}],[\"messages\",{\"1\":{\"2558\":1}}],[\"message\",{\"1\":{\"90\":1,\"108\":1,\"823\":3,\"1085\":3,\"2193\":1,\"2199\":1}}],[\"mechanism\",{\"0\":{\"54\":1},\"1\":{\"678\":1,\"679\":1,\"1771\":1,\"1788\":1}}],[\"meeting\",{\"1\":{\"130\":1}}],[\"meet\",{\"1\":{\"34\":1,\"136\":1}}],[\"mobile\",{\"1\":{\"1523\":1}}],[\"moving\",{\"1\":{\"1065\":1,\"1069\":1}}],[\"move\",{\"1\":{\"16\":2,\"201\":1,\"956\":1,\"973\":1,\"2372\":1,\"2584\":1}}],[\"motibated\",{\"1\":{\"833\":1}}],[\"moto\",{\"1\":{\"130\":1}}],[\"mol\",{\"1\":{\"295\":13}}],[\"mos\",{\"1\":{\"1526\":1,\"1850\":6,\"1877\":6}}],[\"mosestokenizer\",{\"1\":{\"2450\":1,\"2517\":1}}],[\"moses\",{\"1\":{\"200\":2}}],[\"mostly\",{\"1\":{\"836\":1,\"2387\":1}}],[\"most\",{\"1\":{\"17\":1,\"112\":1,\"186\":1,\"235\":1,\"1639\":1,\"1735\":1,\"2384\":1,\"2385\":1,\"2394\":1,\"2411\":1,\"2435\":1,\"2462\":1,\"2530\":1,\"2561\":1,\"2568\":1}}],[\"momentum=0\",{\"1\":{\"1476\":1}}],[\"momentum\",{\"1\":{\"115\":2,\"1465\":2,\"1972\":1}}],[\"monotonic\",{\"0\":{\"1882\":1},\"1\":{\"1804\":1,\"1877\":1,\"1878\":2,\"2252\":1}}],[\"mono\",{\"0\":{\"568\":1},\"1\":{\"568\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1}}],[\"monitor\",{\"1\":{\"188\":1,\"2373\":1,\"2375\":2,\"2385\":1,\"2401\":1,\"2430\":1,\"2537\":1,\"2555\":1,\"2558\":2,\"2559\":1,\"2571\":1}}],[\"monitored\",{\"1\":{\"17\":1,\"1773\":1,\"1778\":1,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"2082\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2170\":1,\"2240\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2278\":1,\"2279\":1}}],[\"month\",{\"1\":{\"130\":1}}],[\"monaural\",{\"1\":{\"48\":1,\"54\":2,\"1407\":1,\"1462\":1,\"1463\":1,\"1581\":1,\"1655\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":3}}],[\"more\",{\"1\":{\"19\":1,\"33\":1,\"44\":1,\"48\":1,\"56\":1,\"72\":1,\"79\":1,\"92\":1,\"95\":1,\"103\":1,\"106\":1,\"113\":1,\"115\":1,\"124\":1,\"141\":1,\"148\":1,\"150\":2,\"202\":1,\"272\":1,\"632\":1,\"692\":1,\"693\":1,\"1144\":1,\"1148\":1,\"1203\":2,\"1228\":2,\"1269\":2,\"1274\":1,\"1276\":1,\"1424\":1,\"1454\":1,\"1640\":1,\"1688\":1,\"1697\":1,\"1712\":2,\"1715\":2,\"1756\":1,\"1927\":3,\"2054\":1,\"2151\":1,\"2153\":1,\"2154\":1,\"2372\":2,\"2387\":1,\"2394\":1,\"2400\":1,\"2411\":1,\"2414\":1,\"2420\":1,\"2424\":1,\"2437\":1,\"2441\":2,\"2515\":1,\"2530\":1,\"2536\":1,\"2547\":1,\"2563\":1,\"2572\":1,\"2584\":1,\"2585\":1}}],[\"modality\",{\"1\":{\"1115\":4,\"1116\":2,\"1179\":1}}],[\"modulation\",{\"1\":{\"1705\":1}}],[\"modulated\",{\"1\":{\"1883\":1}}],[\"modulate\",{\"1\":{\"770\":1}}],[\"modulenotfounderror\",{\"0\":{\"127\":1}}],[\"module\",{\"0\":{\"127\":1,\"1097\":1,\"1460\":1,\"1924\":1,\"1933\":1,\"1934\":1,\"1944\":1},\"1\":{\"20\":1,\"21\":8,\"28\":2,\"114\":2,\"115\":27,\"116\":10,\"117\":2,\"121\":2,\"122\":1,\"136\":1,\"173\":2,\"217\":1,\"224\":1,\"231\":1,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"615\":1,\"626\":1,\"627\":1,\"637\":2,\"639\":3,\"642\":1,\"645\":1,\"646\":1,\"653\":1,\"655\":1,\"658\":1,\"665\":1,\"674\":3,\"676\":4,\"677\":3,\"678\":3,\"679\":3,\"681\":4,\"682\":4,\"684\":3,\"685\":3,\"686\":3,\"687\":3,\"688\":3,\"689\":3,\"690\":3,\"697\":1,\"698\":2,\"699\":2,\"700\":4,\"701\":4,\"702\":3,\"706\":2,\"708\":3,\"710\":2,\"711\":12,\"712\":4,\"713\":2,\"714\":2,\"715\":2,\"716\":2,\"718\":3,\"719\":2,\"720\":2,\"721\":2,\"722\":1,\"723\":2,\"725\":2,\"726\":3,\"729\":3,\"730\":3,\"732\":1,\"733\":1,\"735\":3,\"736\":3,\"737\":1,\"738\":3,\"740\":1,\"741\":1,\"742\":3,\"748\":1,\"749\":3,\"752\":3,\"753\":1,\"754\":8,\"755\":3,\"756\":3,\"757\":1,\"758\":4,\"759\":1,\"760\":4,\"761\":1,\"762\":4,\"763\":2,\"764\":4,\"766\":2,\"770\":1,\"772\":1,\"774\":4,\"775\":1,\"776\":1,\"778\":4,\"779\":1,\"781\":4,\"782\":3,\"785\":1,\"786\":3,\"793\":3,\"794\":2,\"800\":1,\"802\":4,\"803\":3,\"804\":2,\"805\":1,\"806\":2,\"807\":1,\"808\":1,\"810\":2,\"812\":4,\"813\":1,\"817\":1,\"818\":1,\"819\":3,\"820\":1,\"821\":5,\"822\":3,\"825\":3,\"826\":8,\"827\":4,\"828\":1,\"830\":3,\"831\":4,\"832\":1,\"835\":3,\"837\":3,\"838\":4,\"855\":1,\"856\":1,\"858\":1,\"859\":2,\"861\":2,\"867\":1,\"868\":1,\"870\":3,\"872\":3,\"873\":3,\"874\":3,\"878\":3,\"886\":1,\"891\":2,\"892\":2,\"893\":2,\"894\":1,\"897\":1,\"914\":2,\"926\":4,\"931\":1,\"944\":1,\"974\":1,\"975\":2,\"976\":2,\"1012\":2,\"1041\":1,\"1042\":2,\"1043\":2,\"1046\":4,\"1047\":2,\"1048\":4,\"1049\":13,\"1050\":20,\"1051\":5,\"1052\":4,\"1053\":2,\"1054\":7,\"1055\":3,\"1056\":22,\"1057\":7,\"1058\":4,\"1059\":2,\"1061\":3,\"1062\":3,\"1064\":2,\"1065\":15,\"1066\":8,\"1067\":3,\"1068\":9,\"1069\":5,\"1070\":9,\"1071\":8,\"1072\":2,\"1073\":1,\"1074\":6,\"1075\":3,\"1076\":1,\"1077\":4,\"1078\":3,\"1079\":4,\"1080\":2,\"1081\":3,\"1082\":3,\"1083\":1,\"1084\":3,\"1085\":1,\"1093\":4,\"1094\":1,\"1097\":6,\"1098\":4,\"1101\":1,\"1106\":2,\"1107\":2,\"1108\":2,\"1109\":3,\"1110\":1,\"1111\":3,\"1112\":1,\"1113\":2,\"1114\":2,\"1116\":3,\"1117\":3,\"1118\":1,\"1119\":3,\"1120\":1,\"1121\":3,\"1122\":1,\"1123\":3,\"1124\":1,\"1125\":3,\"1126\":1,\"1127\":3,\"1128\":1,\"1130\":2,\"1131\":1,\"1133\":3,\"1134\":3,\"1135\":1,\"1136\":3,\"1137\":1,\"1138\":4,\"1139\":4,\"1140\":3,\"1141\":6,\"1145\":4,\"1148\":11,\"1149\":5,\"1150\":3,\"1151\":3,\"1152\":1,\"1153\":3,\"1156\":2,\"1157\":1,\"1158\":2,\"1159\":1,\"1160\":2,\"1161\":2,\"1162\":2,\"1163\":2,\"1164\":2,\"1165\":2,\"1166\":2,\"1167\":2,\"1168\":2,\"1169\":4,\"1170\":11,\"1171\":3,\"1172\":2,\"1173\":2,\"1174\":3,\"1175\":1,\"1177\":2,\"1178\":3,\"1179\":3,\"1180\":3,\"1181\":3,\"1182\":4,\"1183\":2,\"1184\":2,\"1185\":1,\"1188\":2,\"1189\":1,\"1190\":2,\"1191\":1,\"1192\":1,\"1195\":1,\"1196\":2,\"1197\":2,\"1198\":3,\"1199\":1,\"1200\":3,\"1201\":1,\"1203\":11,\"1204\":2,\"1205\":1,\"1206\":3,\"1207\":2,\"1208\":1,\"1209\":2,\"1211\":3,\"1212\":3,\"1213\":1,\"1214\":2,\"1215\":2,\"1216\":1,\"1217\":4,\"1218\":2,\"1219\":1,\"1220\":2,\"1221\":1,\"1222\":2,\"1223\":1,\"1224\":3,\"1229\":3,\"1230\":1,\"1231\":3,\"1232\":1,\"1233\":3,\"1234\":1,\"1235\":3,\"1236\":1,\"1237\":3,\"1238\":1,\"1239\":2,\"1240\":1,\"1241\":2,\"1244\":3,\"1245\":1,\"1246\":1,\"1247\":2,\"1248\":2,\"1249\":3,\"1250\":1,\"1252\":2,\"1253\":3,\"1254\":4,\"1255\":1,\"1256\":3,\"1257\":2,\"1258\":1,\"1259\":3,\"1260\":1,\"1261\":4,\"1262\":1,\"1263\":3,\"1264\":1,\"1265\":3,\"1266\":1,\"1267\":3,\"1268\":1,\"1269\":3,\"1270\":3,\"1271\":2,\"1272\":4,\"1273\":2,\"1274\":5,\"1275\":1,\"1276\":5,\"1277\":1,\"1278\":2,\"1279\":2,\"1280\":3,\"1281\":1,\"1282\":2,\"1283\":1,\"1284\":2,\"1285\":1,\"1327\":1,\"1360\":3,\"1361\":1,\"1362\":3,\"1363\":1,\"1364\":3,\"1365\":1,\"1366\":3,\"1367\":1,\"1368\":3,\"1369\":3,\"1370\":3,\"1371\":3,\"1372\":3,\"1373\":3,\"1374\":2,\"1375\":4,\"1376\":2,\"1377\":1,\"1378\":3,\"1379\":2,\"1430\":2,\"1431\":3,\"1432\":1,\"1433\":3,\"1434\":1,\"1435\":3,\"1436\":1,\"1437\":3,\"1438\":1,\"1439\":3,\"1440\":1,\"1441\":3,\"1442\":1,\"1443\":3,\"1444\":1,\"1445\":3,\"1446\":1,\"1447\":3,\"1448\":1,\"1449\":3,\"1450\":1,\"1452\":8,\"1453\":1,\"1455\":3,\"1456\":3,\"1457\":1,\"1458\":3,\"1459\":1,\"1460\":5,\"1461\":1,\"1462\":1,\"1464\":3,\"1465\":2,\"1466\":2,\"1467\":1,\"1468\":3,\"1469\":1,\"1470\":2,\"1471\":2,\"1472\":3,\"1473\":3,\"1474\":3,\"1475\":1,\"1476\":4,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":3,\"1483\":1,\"1484\":3,\"1485\":3,\"1486\":1,\"1487\":3,\"1488\":1,\"1489\":3,\"1490\":1,\"1491\":3,\"1492\":1,\"1493\":3,\"1494\":1,\"1495\":3,\"1496\":1,\"1497\":3,\"1498\":1,\"1499\":3,\"1500\":1,\"1501\":3,\"1502\":1,\"1503\":3,\"1504\":1,\"1505\":3,\"1506\":3,\"1507\":1,\"1508\":3,\"1509\":1,\"1510\":2,\"1511\":2,\"1512\":3,\"1513\":1,\"1517\":3,\"1518\":3,\"1519\":1,\"1520\":3,\"1521\":1,\"1522\":1,\"1524\":3,\"1525\":3,\"1530\":2,\"1531\":7,\"1532\":3,\"1533\":1,\"1535\":3,\"1536\":1,\"1537\":3,\"1538\":1,\"1540\":3,\"1541\":1,\"1542\":3,\"1543\":3,\"1544\":1,\"1545\":1,\"1546\":3,\"1547\":3,\"1548\":1,\"1549\":3,\"1550\":1,\"1551\":2,\"1552\":2,\"1553\":3,\"1554\":2,\"1555\":3,\"1556\":1,\"1559\":2,\"1560\":3,\"1561\":3,\"1562\":1,\"1563\":2,\"1564\":3,\"1565\":1,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1570\":2,\"1571\":2,\"1572\":1,\"1573\":3,\"1574\":1,\"1575\":3,\"1576\":1,\"1577\":1,\"1578\":3,\"1579\":3,\"1580\":3,\"1581\":4,\"1582\":1,\"1583\":3,\"1584\":1,\"1586\":3,\"1587\":1,\"1588\":3,\"1589\":1,\"1590\":3,\"1591\":1,\"1592\":3,\"1593\":1,\"1594\":3,\"1595\":3,\"1596\":3,\"1597\":1,\"1598\":4,\"1599\":1,\"1601\":3,\"1602\":3,\"1604\":2,\"1605\":3,\"1606\":1,\"1607\":3,\"1608\":1,\"1609\":3,\"1610\":1,\"1611\":2,\"1613\":3,\"1614\":1,\"1616\":2,\"1617\":2,\"1620\":4,\"1621\":1,\"1624\":3,\"1625\":1,\"1627\":3,\"1628\":1,\"1629\":3,\"1630\":1,\"1631\":3,\"1632\":1,\"1633\":3,\"1634\":1,\"1635\":3,\"1636\":1,\"1639\":2,\"1640\":2,\"1641\":2,\"1642\":1,\"1643\":2,\"1644\":2,\"1645\":3,\"1646\":2,\"1647\":1,\"1648\":3,\"1649\":1,\"1650\":4,\"1651\":1,\"1652\":3,\"1653\":1,\"1654\":2,\"1655\":3,\"1656\":3,\"1657\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1663\":3,\"1664\":2,\"1665\":1,\"1666\":2,\"1667\":2,\"1668\":2,\"1670\":3,\"1672\":3,\"1673\":1,\"1674\":3,\"1675\":1,\"1676\":3,\"1677\":1,\"1716\":1,\"1718\":2,\"1719\":2,\"1760\":2,\"1761\":4,\"1762\":1,\"1763\":3,\"1764\":1,\"1765\":6,\"1766\":1,\"1767\":4,\"1768\":4,\"1769\":3,\"1770\":1,\"1771\":3,\"1772\":3,\"1773\":1,\"1774\":2,\"1775\":1,\"1776\":2,\"1777\":2,\"1778\":2,\"1779\":3,\"1780\":1,\"1781\":3,\"1782\":3,\"1783\":1,\"1785\":3,\"1786\":4,\"1787\":3,\"1788\":3,\"1789\":3,\"1790\":1,\"1791\":2,\"1792\":1,\"1793\":3,\"1794\":1,\"1795\":3,\"1796\":1,\"1797\":3,\"1798\":5,\"1799\":3,\"1800\":6,\"1801\":2,\"1802\":1,\"1803\":5,\"1804\":3,\"1805\":3,\"1806\":3,\"1807\":1,\"1808\":3,\"1828\":2,\"1829\":1,\"1830\":2,\"1831\":1,\"1832\":2,\"1833\":3,\"1834\":3,\"1835\":3,\"1836\":3,\"1837\":1,\"1838\":3,\"1839\":3,\"1840\":4,\"1841\":2,\"1842\":3,\"1843\":3,\"1844\":6,\"1845\":4,\"1846\":6,\"1847\":7,\"1848\":6,\"1849\":8,\"1850\":3,\"1851\":8,\"1852\":2,\"1853\":3,\"1854\":3,\"1855\":4,\"1856\":5,\"1857\":8,\"1858\":9,\"1859\":1,\"1860\":4,\"1861\":5,\"1862\":5,\"1863\":4,\"1864\":4,\"1865\":2,\"1866\":4,\"1867\":5,\"1868\":4,\"1869\":3,\"1870\":5,\"1871\":6,\"1872\":3,\"1873\":3,\"1874\":5,\"1875\":3,\"1876\":3,\"1877\":3,\"1878\":4,\"1879\":2,\"1880\":4,\"1890\":4,\"1891\":1,\"1892\":2,\"1893\":2,\"1902\":3,\"1903\":1,\"1906\":3,\"1907\":3,\"1908\":1,\"1910\":4,\"1911\":1,\"1912\":4,\"1913\":1,\"1914\":4,\"1915\":4,\"1917\":2,\"1918\":4,\"1919\":4,\"1920\":3,\"1924\":1,\"1930\":2,\"1931\":1,\"1932\":3,\"1933\":5,\"1934\":5,\"1937\":1,\"1944\":9,\"1951\":3,\"1952\":1,\"1953\":2,\"1954\":1,\"1955\":2,\"1956\":1,\"1957\":2,\"1958\":2,\"1959\":1,\"1960\":2,\"1970\":2,\"1975\":2,\"1976\":3,\"1977\":1,\"1978\":3,\"1979\":1,\"1980\":3,\"1981\":2,\"1982\":1,\"1983\":4,\"1984\":2,\"1985\":1,\"1986\":3,\"1987\":2,\"1988\":1,\"1989\":2,\"1990\":1,\"1991\":2,\"1992\":1,\"1993\":2,\"1994\":3,\"1995\":1,\"1996\":3,\"1997\":2,\"1998\":1,\"1999\":2,\"2000\":2,\"2001\":2,\"2002\":2,\"2003\":4,\"2004\":2,\"2024\":3,\"2025\":1,\"2026\":4,\"2027\":3,\"2028\":1,\"2029\":3,\"2030\":2,\"2031\":1,\"2032\":3,\"2033\":1,\"2034\":3,\"2035\":1,\"2036\":3,\"2037\":1,\"2038\":3,\"2039\":1,\"2040\":2,\"2041\":1,\"2042\":3,\"2043\":1,\"2044\":2,\"2045\":1,\"2046\":2,\"2047\":3,\"2048\":1,\"2049\":2,\"2050\":2,\"2051\":1,\"2052\":2,\"2053\":1,\"2054\":10,\"2055\":2,\"2056\":1,\"2057\":2,\"2058\":1,\"2059\":3,\"2060\":1,\"2061\":3,\"2062\":1,\"2063\":3,\"2064\":2,\"2065\":1,\"2066\":2,\"2067\":1,\"2068\":2,\"2069\":1,\"2070\":2,\"2071\":1,\"2072\":2,\"2073\":1,\"2074\":3,\"2075\":3,\"2076\":3,\"2077\":3,\"2078\":6,\"2081\":4,\"2082\":1,\"2083\":4,\"2084\":3,\"2086\":2,\"2087\":2,\"2088\":3,\"2089\":3,\"2090\":5,\"2091\":3,\"2095\":3,\"2099\":1,\"2121\":1,\"2122\":1,\"2125\":1,\"2131\":2,\"2148\":5,\"2149\":3,\"2150\":1,\"2151\":1,\"2155\":2,\"2156\":3,\"2157\":1,\"2159\":1,\"2168\":4,\"2169\":1,\"2170\":4,\"2185\":2,\"2198\":2,\"2201\":4,\"2203\":2,\"2233\":3,\"2234\":1,\"2235\":3,\"2237\":3,\"2238\":1,\"2239\":4,\"2240\":1,\"2241\":2,\"2242\":1,\"2243\":6,\"2244\":6,\"2245\":3,\"2246\":2,\"2247\":1,\"2248\":2,\"2249\":1,\"2250\":2,\"2251\":1,\"2252\":3,\"2253\":2,\"2254\":2,\"2255\":6,\"2256\":3,\"2257\":4,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":3,\"2262\":4,\"2263\":3,\"2264\":7,\"2265\":4,\"2266\":2,\"2267\":1,\"2275\":3,\"2276\":1,\"2277\":3,\"2278\":1,\"2279\":6,\"2280\":3,\"2281\":2,\"2282\":1,\"2283\":3,\"2284\":1,\"2285\":3,\"2286\":1,\"2287\":3,\"2288\":3,\"2289\":1,\"2290\":2,\"2291\":1,\"2292\":2,\"2293\":1,\"2294\":2,\"2295\":2,\"2296\":2,\"2297\":3,\"2298\":1,\"2299\":3,\"2300\":1,\"2301\":2,\"2302\":2,\"2303\":2,\"2304\":2,\"2305\":2,\"2317\":2,\"2368\":1,\"2486\":1,\"2490\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"modules\",{\"0\":{\"637\":1,\"639\":1,\"646\":1,\"1051\":1,\"1054\":1,\"1055\":1,\"1062\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1081\":1,\"1086\":1,\"1100\":1,\"1789\":1,\"1806\":1,\"1826\":1},\"1\":{\"15\":1,\"28\":3,\"29\":2,\"30\":2,\"56\":1,\"85\":2,\"112\":3,\"114\":1,\"116\":1,\"124\":1,\"307\":3,\"333\":3,\"384\":3,\"408\":3,\"416\":3,\"422\":3,\"478\":3,\"635\":2,\"637\":4,\"639\":4,\"642\":5,\"646\":2,\"658\":4,\"691\":1,\"693\":1,\"697\":1,\"760\":1,\"778\":1,\"797\":1,\"826\":1,\"831\":1,\"857\":1,\"868\":1,\"1049\":5,\"1050\":7,\"1051\":1,\"1054\":3,\"1055\":1,\"1056\":8,\"1062\":1,\"1065\":4,\"1066\":1,\"1068\":4,\"1069\":1,\"1070\":5,\"1074\":2,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1081\":1,\"1086\":1,\"1100\":1,\"1109\":1,\"1375\":1,\"1437\":1,\"1443\":1,\"1452\":1,\"1476\":1,\"1506\":1,\"1543\":1,\"1564\":1,\"1570\":1,\"1598\":1,\"1629\":1,\"1645\":1,\"1655\":1,\"1656\":1,\"1667\":1,\"1671\":1,\"1709\":1,\"1789\":2,\"1806\":2,\"1826\":2,\"1857\":1,\"1858\":1,\"1870\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1924\":2,\"1932\":3,\"1976\":1,\"1978\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2084\":1,\"2089\":1,\"2131\":1,\"2264\":2,\"2288\":1,\"2400\":1,\"2536\":1}}],[\"mode=false\",{\"1\":{\"677\":1,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"711\":1,\"758\":1,\"892\":1,\"1149\":1,\"1150\":1}}],[\"mode=\",{\"1\":{\"648\":1,\"835\":1,\"953\":1,\"962\":1,\"967\":1,\"968\":1,\"970\":1,\"973\":1,\"987\":1,\"989\":2,\"1245\":1,\"1430\":1,\"1670\":1,\"1704\":1,\"1713\":1,\"1961\":2,\"2022\":2,\"2099\":1,\"2498\":3,\"2616\":3,\"2634\":3}}],[\"mode\",{\"0\":{\"34\":1,\"36\":1,\"435\":1,\"463\":1,\"2227\":1},\"1\":{\"34\":3,\"36\":1,\"62\":2,\"74\":3,\"75\":2,\"76\":3,\"77\":2,\"78\":1,\"82\":1,\"98\":1,\"111\":1,\"135\":1,\"150\":5,\"249\":1,\"286\":1,\"363\":1,\"608\":1,\"645\":1,\"648\":1,\"677\":2,\"678\":2,\"679\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"758\":2,\"825\":1,\"835\":1,\"836\":1,\"889\":3,\"892\":2,\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"987\":5,\"989\":1,\"1025\":1,\"1149\":1,\"1150\":1,\"1187\":1,\"1202\":1,\"1203\":2,\"1220\":1,\"1243\":1,\"1245\":4,\"1257\":2,\"1261\":1,\"1269\":3,\"1286\":1,\"1287\":1,\"1289\":1,\"1352\":2,\"1430\":2,\"1516\":2,\"1523\":2,\"1531\":2,\"1598\":1,\"1652\":1,\"1654\":1,\"1670\":2,\"1671\":4,\"1704\":1,\"1713\":1,\"1757\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1834\":3,\"1869\":3,\"1871\":3,\"1872\":3,\"1873\":3,\"1876\":3,\"1919\":3,\"1948\":3,\"1964\":1,\"2011\":1,\"2012\":4,\"2097\":9,\"2099\":10,\"2102\":1,\"2193\":5,\"2197\":1,\"2216\":1,\"2217\":1,\"2227\":1,\"2568\":1,\"2569\":1}}],[\"modelexport\",{\"1\":{\"2599\":2,\"2600\":4}}],[\"modeldownloader\",{\"1\":{\"2358\":2,\"2371\":2,\"2514\":2,\"2520\":2,\"2579\":2,\"2591\":1,\"2612\":2,\"2617\":2,\"2630\":2,\"2635\":2,\"2659\":2}}],[\"model2\",{\"1\":{\"2201\":1}}],[\"model1\",{\"1\":{\"2201\":1}}],[\"modeling=true\",{\"1\":{\"1430\":1}}],[\"modeling\",{\"0\":{\"2374\":1,\"2434\":1,\"2557\":1},\"1\":{\"681\":1,\"682\":1,\"1430\":3,\"1462\":1,\"1463\":2,\"1581\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":2,\"1671\":3,\"2090\":1,\"2142\":1,\"2257\":1,\"2261\":1,\"2262\":1,\"2374\":2,\"2414\":1,\"2434\":2,\"2467\":1,\"2473\":1,\"2557\":2,\"2569\":1}}],[\"models=\",{\"1\":{\"812\":1}}],[\"models\",{\"0\":{\"197\":1,\"198\":1,\"204\":1,\"221\":1,\"228\":1,\"1454\":1,\"1962\":2,\"2483\":1,\"2574\":1,\"2584\":1,\"2620\":1},\"1\":{\"20\":2,\"21\":1,\"30\":2,\"97\":2,\"98\":1,\"99\":1,\"102\":1,\"112\":5,\"120\":1,\"155\":3,\"156\":1,\"158\":1,\"159\":1,\"161\":2,\"162\":1,\"164\":2,\"195\":1,\"197\":2,\"198\":3,\"201\":1,\"209\":1,\"213\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"240\":1,\"286\":4,\"295\":8,\"296\":3,\"499\":1,\"650\":1,\"697\":1,\"726\":1,\"750\":1,\"791\":1,\"796\":1,\"815\":2,\"824\":1,\"872\":1,\"874\":1,\"1059\":1,\"1173\":1,\"1180\":1,\"1241\":1,\"1252\":1,\"1253\":1,\"1269\":2,\"1327\":1,\"1454\":6,\"1660\":1,\"1661\":1,\"1662\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1829\":1,\"1844\":1,\"1951\":1,\"1962\":3,\"2168\":1,\"2170\":1,\"2186\":1,\"2202\":2,\"2204\":1,\"2267\":1,\"2355\":2,\"2357\":1,\"2358\":1,\"2363\":2,\"2368\":1,\"2371\":2,\"2391\":1,\"2401\":1,\"2429\":1,\"2431\":8,\"2432\":3,\"2440\":3,\"2451\":1,\"2452\":2,\"2467\":1,\"2473\":1,\"2481\":2,\"2486\":1,\"2490\":1,\"2494\":3,\"2500\":1,\"2506\":2,\"2510\":1,\"2520\":2,\"2527\":1,\"2537\":1,\"2543\":1,\"2552\":1,\"2558\":3,\"2574\":3,\"2579\":1,\"2583\":2,\"2584\":5,\"2585\":5,\"2599\":2,\"2600\":1,\"2605\":1,\"2609\":1,\"2612\":2,\"2618\":2,\"2622\":1,\"2626\":1,\"2630\":3,\"2635\":1,\"2641\":2,\"2642\":1,\"2653\":2}}],[\"model\",{\"0\":{\"66\":1,\"97\":1,\"99\":1,\"194\":1,\"195\":1,\"209\":1,\"213\":1,\"285\":1,\"303\":1,\"310\":1,\"318\":1,\"324\":1,\"330\":1,\"336\":1,\"342\":1,\"346\":1,\"354\":1,\"360\":1,\"372\":1,\"383\":1,\"387\":1,\"394\":1,\"402\":1,\"411\":1,\"419\":1,\"425\":1,\"432\":1,\"440\":1,\"446\":1,\"452\":1,\"458\":1,\"467\":1,\"473\":1,\"481\":1,\"488\":1,\"641\":1,\"643\":1,\"645\":1,\"910\":1,\"1057\":1,\"1113\":1,\"1171\":1,\"1172\":1,\"1205\":1,\"1206\":1,\"1218\":1,\"1219\":1,\"1252\":1,\"1320\":1,\"1321\":1,\"1371\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1745\":1,\"1773\":1,\"1837\":1,\"1892\":1,\"1893\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2046\":1,\"2076\":1,\"2082\":1,\"2152\":1,\"2154\":1,\"2157\":2,\"2159\":2,\"2165\":1,\"2168\":1,\"2170\":1,\"2240\":1,\"2278\":1,\"2294\":1,\"2357\":1,\"2358\":1,\"2363\":1,\"2364\":1,\"2371\":1,\"2399\":1,\"2402\":1,\"2454\":1,\"2455\":1,\"2472\":1,\"2494\":1,\"2499\":1,\"2505\":1,\"2506\":1,\"2507\":1,\"2509\":1,\"2511\":1,\"2512\":1,\"2513\":1,\"2519\":1,\"2520\":1,\"2535\":1,\"2538\":1,\"2573\":1,\"2577\":1,\"2578\":1,\"2579\":1,\"2585\":1,\"2587\":1,\"2588\":1,\"2599\":2,\"2600\":1,\"2612\":1,\"2617\":1,\"2630\":1,\"2635\":1,\"2641\":1,\"2642\":1,\"2643\":1,\"2645\":1,\"2648\":1,\"2649\":1,\"2652\":1,\"2653\":1,\"2654\":1,\"2656\":1,\"2657\":1,\"2658\":1},\"1\":{\"4\":1,\"20\":2,\"21\":2,\"24\":3,\"28\":8,\"29\":1,\"30\":4,\"46\":1,\"48\":1,\"56\":2,\"57\":4,\"60\":1,\"65\":1,\"66\":7,\"69\":1,\"80\":1,\"91\":1,\"96\":1,\"97\":4,\"98\":2,\"99\":2,\"102\":2,\"106\":1,\"110\":4,\"113\":7,\"118\":2,\"121\":2,\"150\":8,\"157\":1,\"161\":1,\"164\":1,\"168\":2,\"169\":1,\"173\":5,\"174\":10,\"175\":4,\"179\":2,\"181\":1,\"192\":1,\"194\":11,\"197\":2,\"198\":2,\"201\":1,\"202\":1,\"209\":1,\"210\":3,\"211\":3,\"212\":3,\"213\":1,\"214\":1,\"215\":1,\"216\":2,\"217\":14,\"218\":1,\"221\":1,\"222\":2,\"223\":2,\"224\":14,\"225\":1,\"229\":2,\"230\":2,\"231\":14,\"232\":1,\"240\":3,\"241\":5,\"242\":2,\"243\":1,\"245\":5,\"247\":4,\"249\":11,\"251\":3,\"253\":3,\"255\":3,\"257\":5,\"259\":3,\"261\":3,\"263\":5,\"265\":3,\"267\":5,\"269\":3,\"271\":5,\"272\":5,\"282\":1,\"285\":3,\"286\":12,\"295\":8,\"296\":8,\"301\":2,\"307\":6,\"315\":4,\"321\":4,\"327\":2,\"333\":6,\"339\":2,\"343\":4,\"350\":4,\"357\":4,\"363\":4,\"368\":4,\"375\":2,\"380\":2,\"384\":2,\"391\":4,\"398\":22,\"399\":2,\"408\":6,\"416\":6,\"422\":6,\"429\":6,\"437\":4,\"443\":4,\"449\":2,\"455\":2,\"464\":4,\"470\":4,\"476\":2,\"478\":6,\"485\":4,\"491\":2,\"541\":2,\"607\":4,\"610\":3,\"626\":3,\"627\":4,\"632\":4,\"634\":5,\"635\":8,\"637\":4,\"639\":8,\"641\":9,\"642\":4,\"643\":7,\"645\":9,\"646\":5,\"651\":1,\"652\":1,\"653\":5,\"655\":5,\"658\":5,\"659\":2,\"660\":2,\"661\":4,\"662\":2,\"676\":3,\"709\":4,\"723\":1,\"725\":1,\"735\":2,\"740\":4,\"741\":4,\"742\":3,\"754\":3,\"767\":1,\"770\":2,\"771\":1,\"772\":2,\"775\":4,\"776\":4,\"781\":3,\"785\":2,\"792\":1,\"806\":1,\"809\":1,\"810\":2,\"812\":3,\"813\":2,\"817\":2,\"818\":2,\"820\":5,\"821\":2,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"853\":1,\"858\":1,\"868\":7,\"888\":2,\"891\":2,\"894\":5,\"910\":2,\"914\":1,\"975\":3,\"976\":4,\"977\":1,\"978\":1,\"1001\":2,\"1042\":3,\"1043\":4,\"1044\":1,\"1045\":1,\"1057\":1,\"1113\":3,\"1116\":2,\"1141\":1,\"1160\":5,\"1161\":5,\"1162\":2,\"1163\":1,\"1164\":5,\"1165\":4,\"1170\":1,\"1171\":3,\"1172\":2,\"1177\":2,\"1178\":2,\"1179\":2,\"1180\":3,\"1190\":3,\"1191\":1,\"1192\":1,\"1198\":1,\"1205\":3,\"1206\":3,\"1209\":3,\"1210\":1,\"1214\":2,\"1215\":2,\"1218\":1,\"1219\":1,\"1233\":1,\"1241\":2,\"1243\":1,\"1245\":3,\"1248\":2,\"1251\":1,\"1252\":6,\"1253\":15,\"1254\":5,\"1269\":3,\"1279\":5,\"1284\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1308\":1,\"1309\":1,\"1310\":1,\"1320\":2,\"1321\":2,\"1327\":1,\"1337\":2,\"1371\":2,\"1429\":1,\"1454\":8,\"1462\":1,\"1463\":3,\"1505\":2,\"1515\":1,\"1516\":2,\"1526\":2,\"1529\":2,\"1534\":2,\"1539\":2,\"1543\":2,\"1551\":9,\"1552\":4,\"1553\":10,\"1554\":2,\"1558\":3,\"1605\":1,\"1611\":2,\"1626\":2,\"1638\":2,\"1645\":3,\"1654\":2,\"1655\":1,\"1656\":1,\"1658\":2,\"1659\":1,\"1660\":4,\"1661\":4,\"1662\":5,\"1669\":2,\"1670\":1,\"1671\":2,\"1719\":5,\"1745\":3,\"1760\":1,\"1773\":2,\"1778\":7,\"1791\":2,\"1804\":5,\"1805\":2,\"1828\":1,\"1837\":2,\"1850\":1,\"1852\":7,\"1877\":1,\"1890\":1,\"1892\":2,\"1893\":2,\"1930\":4,\"1931\":1,\"1932\":4,\"1937\":1,\"1951\":5,\"1953\":2,\"1955\":2,\"1958\":2,\"1962\":7,\"1963\":2,\"1964\":1,\"1969\":3,\"1970\":3,\"1973\":2,\"1974\":1,\"1975\":3,\"1984\":3,\"1993\":1,\"2002\":1,\"2012\":1,\"2019\":4,\"2021\":1,\"2027\":3,\"2028\":1,\"2046\":4,\"2049\":2,\"2055\":2,\"2064\":2,\"2076\":3,\"2082\":3,\"2086\":2,\"2087\":2,\"2090\":1,\"2095\":1,\"2096\":7,\"2097\":3,\"2098\":7,\"2099\":14,\"2100\":7,\"2101\":7,\"2102\":8,\"2103\":8,\"2104\":8,\"2105\":7,\"2107\":7,\"2108\":7,\"2109\":8,\"2110\":7,\"2111\":7,\"2112\":7,\"2113\":8,\"2114\":7,\"2115\":8,\"2116\":8,\"2117\":7,\"2118\":8,\"2123\":1,\"2124\":1,\"2125\":1,\"2128\":1,\"2129\":1,\"2132\":1,\"2149\":6,\"2151\":3,\"2152\":1,\"2154\":1,\"2156\":2,\"2157\":5,\"2158\":14,\"2159\":3,\"2165\":2,\"2168\":3,\"2170\":2,\"2185\":4,\"2186\":2,\"2198\":2,\"2201\":8,\"2202\":4,\"2203\":4,\"2204\":2,\"2209\":1,\"2240\":3,\"2243\":1,\"2244\":1,\"2254\":1,\"2255\":2,\"2259\":1,\"2263\":1,\"2264\":1,\"2278\":3,\"2279\":1,\"2294\":3,\"2349\":5,\"2350\":1,\"2355\":4,\"2357\":7,\"2358\":1,\"2363\":6,\"2364\":1,\"2368\":6,\"2369\":1,\"2371\":6,\"2372\":1,\"2375\":3,\"2377\":2,\"2378\":1,\"2385\":1,\"2394\":1,\"2399\":2,\"2411\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2420\":1,\"2429\":1,\"2431\":5,\"2437\":1,\"2440\":8,\"2441\":7,\"2444\":2,\"2445\":2,\"2446\":3,\"2450\":1,\"2452\":2,\"2454\":1,\"2455\":1,\"2457\":1,\"2460\":2,\"2461\":3,\"2466\":1,\"2467\":1,\"2468\":3,\"2472\":5,\"2473\":2,\"2474\":5,\"2476\":4,\"2478\":4,\"2481\":2,\"2482\":1,\"2485\":1,\"2486\":6,\"2487\":1,\"2490\":8,\"2491\":1,\"2494\":5,\"2497\":2,\"2500\":7,\"2501\":1,\"2504\":1,\"2506\":7,\"2507\":3,\"2508\":2,\"2510\":19,\"2512\":5,\"2513\":3,\"2514\":6,\"2515\":1,\"2517\":1,\"2518\":3,\"2519\":1,\"2520\":3,\"2521\":1,\"2530\":1,\"2535\":2,\"2542\":1,\"2552\":1,\"2558\":3,\"2559\":3,\"2563\":1,\"2564\":6,\"2572\":4,\"2573\":3,\"2575\":1,\"2576\":1,\"2578\":8,\"2579\":1,\"2583\":2,\"2584\":36,\"2585\":13,\"2587\":1,\"2588\":2,\"2591\":1,\"2597\":1,\"2598\":5,\"2599\":3,\"2600\":22,\"2602\":1,\"2605\":6,\"2606\":1,\"2607\":1,\"2609\":7,\"2610\":1,\"2612\":6,\"2614\":1,\"2615\":1,\"2617\":4,\"2618\":3,\"2619\":1,\"2622\":6,\"2623\":1,\"2624\":1,\"2626\":7,\"2627\":1,\"2630\":5,\"2632\":1,\"2633\":1,\"2635\":4,\"2642\":1,\"2643\":4,\"2644\":2,\"2645\":1,\"2646\":2,\"2648\":5,\"2649\":5,\"2651\":1,\"2653\":6,\"2654\":1,\"2657\":2,\"2658\":1,\"2659\":6}}],[\"mods\",{\"1\":{\"28\":4,\"29\":3,\"251\":6,\"255\":4,\"259\":4,\"265\":6,\"269\":6,\"637\":1}}],[\"modifying\",{\"1\":{\"1746\":1,\"1947\":1}}],[\"modify\",{\"1\":{\"25\":1,\"60\":1,\"85\":1,\"144\":1,\"235\":1,\"240\":1,\"295\":2,\"2572\":2}}],[\"modification\",{\"1\":{\"2387\":1}}],[\"modifications\",{\"0\":{\"2439\":1},\"1\":{\"5\":1,\"100\":1,\"700\":1,\"1138\":1,\"1139\":1,\"2394\":2,\"2530\":2,\"2573\":1,\"2635\":1}}],[\"modifies\",{\"1\":{\"1662\":1}}],[\"modified\",{\"1\":{\"23\":3,\"25\":3,\"60\":3,\"113\":1,\"115\":1,\"117\":2,\"119\":2,\"700\":5,\"837\":1,\"935\":1,\"1037\":1,\"1048\":3,\"1138\":5,\"1139\":5,\"1458\":1,\"2259\":1,\"2421\":1,\"2584\":1}}],[\"mod\",{\"1\":{\"21\":3,\"115\":14,\"726\":2,\"858\":2,\"859\":2,\"1049\":2,\"1050\":2,\"1056\":4,\"1093\":4}}],[\"mi\",{\"1\":{\"2457\":1}}],[\"miedo\",{\"1\":{\"2457\":1}}],[\"million\",{\"1\":{\"2154\":1}}],[\"millions\",{\"1\":{\"2154\":1}}],[\"millisecond\",{\"1\":{\"1558\":2}}],[\"milliseconds\",{\"1\":{\"108\":1,\"109\":1,\"122\":1}}],[\"mics\",{\"1\":{\"1560\":2,\"1670\":1,\"1719\":1}}],[\"mic\",{\"1\":{\"1460\":1,\"1535\":1,\"1559\":2,\"1560\":2,\"1655\":2,\"1718\":2,\"1719\":2}}],[\"microphones\",{\"1\":{\"1655\":2,\"1660\":2,\"1661\":2,\"1662\":2,\"1719\":5}}],[\"microphone\",{\"1\":{\"286\":1,\"296\":1,\"940\":1,\"1515\":1,\"1523\":2,\"1671\":1,\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"microsoft\",{\"1\":{\"49\":1}}],[\"midi\",{\"1\":{\"1393\":1,\"1415\":1,\"1773\":15,\"1778\":1,\"1798\":3,\"1804\":2,\"2082\":15,\"2084\":4,\"2086\":6,\"2087\":6,\"2089\":6,\"2090\":3,\"2094\":1,\"2095\":1,\"2196\":1,\"2266\":2,\"2267\":5}}],[\"mid\",{\"1\":{\"1388\":1,\"1389\":5}}],[\"midreader\",{\"0\":{\"1388\":1},\"1\":{\"1388\":2}}],[\"middle\",{\"0\":{\"149\":1},\"1\":{\"68\":1,\"69\":1,\"1895\":1,\"1900\":1}}],[\"mimicing\",{\"1\":{\"1154\":1}}],[\"mitigate\",{\"1\":{\"148\":1}}],[\"misc\",{\"1\":{\"2429\":7}}],[\"mismatches\",{\"1\":{\"2584\":1}}],[\"mismatch\",{\"1\":{\"2152\":1,\"2157\":1,\"2501\":1,\"2522\":1,\"2573\":1,\"2581\":1,\"2584\":1,\"2585\":5,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1}}],[\"miss\",{\"1\":{\"2311\":1}}],[\"missed\",{\"1\":{\"127\":1}}],[\"missing\",{\"0\":{\"1016\":2},\"1\":{\"112\":1,\"1016\":4,\"1017\":1,\"2401\":1,\"2403\":1,\"2537\":1,\"2539\":1}}],[\"mish\",{\"0\":{\"1067\":1,\"2252\":1},\"1\":{\"115\":2,\"1067\":4,\"1096\":2,\"2252\":3}}],[\"mins\",{\"1\":{\"2387\":1}}],[\"min=0\",{\"1\":{\"1245\":1,\"1618\":1}}],[\"minlen\",{\"1\":{\"384\":2,\"691\":1,\"692\":1,\"697\":1}}],[\"minlenratio=0\",{\"1\":{\"2079\":1,\"2358\":1,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":2,\"2579\":1,\"2592\":1,\"2654\":1,\"2658\":1}}],[\"minlenratio<0\",{\"1\":{\"697\":1}}],[\"minlenratio\",{\"1\":{\"150\":4,\"217\":1,\"224\":1,\"231\":1,\"249\":2,\"251\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"267\":2,\"307\":2,\"315\":2,\"327\":2,\"391\":2,\"399\":4,\"408\":2,\"422\":2,\"443\":4,\"449\":2,\"464\":2,\"470\":2,\"485\":2,\"692\":2,\"693\":2,\"697\":2,\"698\":1,\"699\":1,\"797\":1,\"821\":1,\"826\":1,\"857\":2,\"1985\":1,\"2002\":2,\"2079\":2,\"2095\":2,\"2263\":2,\"2264\":2}}],[\"minute\",{\"1\":{\"2355\":1,\"2362\":1}}],[\"minutes\",{\"1\":{\"87\":1,\"167\":1,\"178\":1,\"197\":1,\"200\":1,\"234\":1,\"240\":1,\"2372\":1,\"2375\":1,\"2394\":1,\"2397\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2409\":2,\"2429\":1,\"2466\":1,\"2530\":1,\"2533\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2552\":1,\"2558\":1,\"2559\":1,\"2571\":1,\"2584\":3,\"2585\":1}}],[\"minus\",{\"1\":{\"1057\":1,\"1227\":1,\"1228\":3,\"1344\":1,\"1345\":3,\"1346\":1,\"1347\":3}}],[\"minues\",{\"1\":{\"196\":1,\"234\":1}}],[\"minor\",{\"1\":{\"119\":1}}],[\"min\",{\"1\":{\"62\":2,\"115\":1,\"150\":1,\"245\":2,\"249\":2,\"272\":2,\"297\":1,\"301\":2,\"585\":2,\"692\":1,\"693\":1,\"697\":2,\"794\":1,\"797\":1,\"857\":1,\"1003\":2,\"1004\":2,\"1005\":2,\"1006\":1,\"1028\":2,\"1096\":2,\"1115\":4,\"1207\":1,\"1243\":1,\"1245\":2,\"1269\":4,\"1354\":2,\"1618\":4,\"1619\":4,\"1785\":3,\"1799\":1,\"1810\":2,\"1886\":3,\"1887\":3,\"1888\":3,\"1925\":2,\"1962\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2012\":2,\"2018\":3,\"2021\":2,\"2022\":4,\"2023\":2,\"2193\":1,\"2274\":2,\"2294\":1,\"2317\":1,\"2330\":1,\"2375\":1,\"2432\":1,\"2440\":1}}],[\"mind\",{\"1\":{\"57\":1,\"85\":1,\"92\":1,\"112\":1}}],[\"mininum\",{\"1\":{\"1005\":1}}],[\"minimization\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"minimal\",{\"1\":{\"207\":1,\"2125\":1}}],[\"minimum\",{\"1\":{\"115\":1,\"121\":1,\"150\":2,\"275\":1,\"297\":1,\"794\":2,\"821\":1,\"826\":1,\"885\":1,\"1003\":1,\"1004\":1,\"1028\":1,\"1093\":1,\"1096\":1,\"1136\":1,\"1269\":3,\"1530\":1,\"1563\":1,\"1600\":1,\"1603\":1,\"1622\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1785\":1,\"1810\":1,\"1859\":1,\"2002\":1,\"2079\":3,\"2095\":1,\"2263\":1,\"2264\":1,\"2274\":1,\"2317\":1,\"2330\":1,\"2394\":1,\"2530\":1}}],[\"miniconda\",{\"1\":{\"135\":2}}],[\"mini\",{\"0\":{\"72\":1,\"73\":1,\"95\":1},\"1\":{\"56\":2,\"57\":3,\"73\":1,\"74\":1,\"75\":1,\"76\":2,\"77\":3,\"78\":2,\"79\":3,\"80\":3,\"750\":1,\"958\":1,\"987\":2,\"1638\":1,\"1897\":1,\"2012\":2,\"2099\":4,\"2102\":4,\"2375\":1,\"2411\":3,\"2558\":2,\"2584\":4,\"2585\":2}}],[\"minibatch=none\",{\"1\":{\"1647\":2}}],[\"minibatches\",{\"0\":{\"172\":1},\"1\":{\"170\":1,\"172\":1,\"251\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"1028\":1,\"1551\":1,\"1553\":1}}],[\"minibatch\",{\"0\":{\"26\":1},\"1\":{\"26\":5,\"174\":2,\"617\":1,\"710\":2,\"999\":1,\"1028\":5,\"1142\":3,\"1154\":3,\"1186\":2,\"1210\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1324\":1}}],[\"might\",{\"1\":{\"15\":1,\"25\":1,\"38\":1,\"60\":1,\"74\":2,\"83\":1,\"85\":1,\"135\":1,\"226\":1,\"595\":1,\"1013\":1,\"1464\":1,\"2212\":1,\"2368\":1,\"2486\":1,\"2490\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"mixwav\",{\"1\":{\"2367\":5,\"2369\":2,\"2372\":3,\"2470\":2,\"2472\":1,\"2474\":1,\"2476\":3,\"2478\":3,\"2485\":5,\"2487\":2,\"2491\":2,\"2497\":3,\"2498\":8,\"2500\":1,\"2604\":5,\"2606\":2,\"2610\":2,\"2614\":3,\"2615\":3,\"2616\":9,\"2617\":1,\"2621\":5,\"2623\":2,\"2627\":2,\"2632\":3,\"2633\":3,\"2634\":9,\"2635\":1,\"2647\":2,\"2648\":1,\"2649\":1}}],[\"mixitsolver\",{\"0\":{\"1600\":1},\"1\":{\"1600\":1}}],[\"mixit\",{\"0\":{\"1600\":1},\"1\":{\"1600\":2}}],[\"mixing\",{\"1\":{\"21\":1,\"568\":1,\"1062\":1,\"1081\":1,\"2181\":1}}],[\"mixture\",{\"0\":{\"2495\":1,\"2613\":1,\"2631\":1},\"1\":{\"1379\":2,\"1555\":1,\"1600\":1,\"1655\":1,\"1660\":3,\"1661\":3,\"1662\":3,\"1664\":2,\"1665\":2,\"1719\":7,\"2181\":1,\"2372\":1,\"2481\":1,\"2497\":1,\"2498\":2,\"2614\":1,\"2615\":1,\"2616\":2,\"2618\":1,\"2632\":1,\"2633\":1,\"2634\":2}}],[\"mixed\",{\"0\":{\"81\":1},\"1\":{\"21\":1,\"1181\":1,\"1511\":2,\"1611\":1,\"1617\":1,\"1644\":2,\"2197\":1,\"2440\":1,\"2497\":1,\"2498\":1,\"2500\":1,\"2558\":1}}],[\"mix\",{\"0\":{\"568\":1,\"629\":1,\"749\":1,\"794\":1},\"1\":{\"11\":1,\"568\":2,\"629\":1,\"749\":1,\"794\":1,\"854\":1,\"889\":1,\"1551\":8,\"1553\":8,\"1554\":8,\"1570\":1,\"1646\":1,\"1661\":3,\"1662\":3,\"1678\":1,\"2181\":1,\"2184\":1,\"2197\":2,\"2200\":1,\"2498\":2,\"2500\":2,\"2616\":4,\"2617\":1,\"2634\":4,\"2635\":1}}],[\"mae\",{\"1\":{\"2090\":1,\"2091\":1}}],[\"maes\",{\"1\":{\"23\":2,\"113\":1,\"119\":2,\"249\":1,\"700\":6,\"751\":1,\"1048\":4,\"1057\":1,\"1059\":1,\"1060\":1,\"1138\":6,\"1139\":6,\"1176\":1}}],[\"magnification\",{\"1\":{\"2018\":1}}],[\"magnitude\",{\"1\":{\"632\":1,\"1604\":1,\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1,\"2151\":1}}],[\"mag\",{\"1\":{\"1604\":1}}],[\"magically\",{\"1\":{\"834\":1}}],[\"maruyama\",{\"1\":{\"1638\":1,\"1646\":1}}],[\"martix\",{\"1\":{\"1013\":1,\"1015\":1}}],[\"margin=false\",{\"1\":{\"2030\":1,\"2040\":1}}],[\"margin=0\",{\"1\":{\"705\":1,\"2030\":1,\"2040\":1,\"2041\":1}}],[\"marginal\",{\"1\":{\"1618\":3,\"1619\":3,\"1638\":2}}],[\"margin\",{\"1\":{\"249\":6,\"705\":2,\"2030\":4,\"2040\":6,\"2543\":1}}],[\"marked\",{\"1\":{\"2400\":1,\"2536\":1}}],[\"marker\",{\"1\":{\"108\":4,\"109\":2,\"110\":2,\"501\":2,\"2312\":1}}],[\"markers\",{\"1\":{\"107\":1}}],[\"markov\",{\"1\":{\"2385\":1}}],[\"marking\",{\"1\":{\"108\":2}}],[\"matically\",{\"1\":{\"2384\":1}}],[\"material\",{\"1\":{\"2363\":1,\"2421\":1,\"2506\":1,\"2653\":1}}],[\"materials\",{\"1\":{\"152\":1,\"2384\":1}}],[\"matmul\",{\"0\":{\"1727\":1},\"1\":{\"1727\":2}}],[\"mat^h\",{\"1\":{\"1705\":1}}],[\"matters\",{\"1\":{\"1551\":1,\"1553\":1}}],[\"matthew\",{\"1\":{\"130\":1}}],[\"mat=none\",{\"1\":{\"1524\":1}}],[\"matlab\",{\"1\":{\"1025\":1}}],[\"matrices\",{\"1\":{\"1010\":1,\"1015\":1,\"1357\":1,\"1932\":1}}],[\"matrix\",{\"0\":{\"887\":1,\"1010\":1,\"1703\":1,\"1711\":1,\"1714\":1,\"1810\":1},\"1\":{\"629\":4,\"743\":2,\"771\":1,\"799\":1,\"809\":1,\"875\":2,\"887\":2,\"1010\":1,\"1013\":2,\"1014\":1,\"1015\":2,\"1142\":1,\"1148\":1,\"1155\":2,\"1186\":3,\"1203\":1,\"1210\":3,\"1228\":3,\"1247\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1339\":1,\"1342\":1,\"1345\":3,\"1347\":3,\"1524\":5,\"1680\":1,\"1695\":3,\"1696\":2,\"1697\":2,\"1698\":3,\"1701\":1,\"1702\":2,\"1703\":6,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":3,\"1708\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1719\":1,\"1736\":3,\"1737\":2,\"1746\":2,\"1785\":1,\"1810\":5,\"1829\":1,\"1841\":1,\"1851\":1,\"1889\":2,\"2054\":1,\"2275\":1}}],[\"mat|hdf5|sound\",{\"1\":{\"276\":1,\"279\":1,\"281\":1,\"283\":1,\"284\":1}}],[\"matshow\",{\"1\":{\"171\":1,\"175\":1,\"185\":1,\"193\":1,\"202\":1}}],[\"mat\",{\"1\":{\"171\":1,\"174\":1,\"175\":1,\"185\":1,\"193\":1,\"238\":2,\"247\":1,\"496\":3,\"506\":2,\"509\":1,\"512\":1,\"519\":1,\"522\":2,\"525\":1,\"533\":1,\"541\":1,\"941\":1,\"987\":1,\"1013\":2,\"1014\":1,\"1015\":2,\"1524\":1,\"1705\":4,\"1746\":2,\"2431\":1}}],[\"matplotlib\",{\"1\":{\"171\":1,\"174\":1,\"175\":1,\"185\":1,\"193\":1,\"202\":1,\"238\":1,\"429\":2,\"629\":1,\"648\":3,\"2186\":1,\"2193\":2,\"2202\":2,\"2204\":1,\"2359\":1,\"2360\":1,\"2384\":1,\"2386\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2498\":1,\"2521\":1,\"2522\":1,\"2523\":1,\"2580\":1,\"2581\":1,\"2582\":1,\"2616\":1,\"2634\":1}}],[\"matcher\",{\"1\":{\"2500\":2,\"2617\":2,\"2635\":2}}],[\"matched\",{\"1\":{\"637\":1,\"2514\":1,\"2659\":1}}],[\"matchs\",{\"1\":{\"1924\":1}}],[\"matching\",{\"1\":{\"642\":1,\"1839\":3}}],[\"match\",{\"1\":{\"28\":1,\"29\":1,\"658\":1,\"754\":1,\"820\":1,\"821\":1,\"826\":1,\"1679\":1,\"1778\":9,\"1781\":1,\"1805\":6,\"1850\":6,\"1852\":9,\"1877\":6,\"2642\":1}}],[\"masao\",{\"1\":{\"2597\":2}}],[\"massively\",{\"1\":{\"2584\":1}}],[\"maskalongaxisvariablemaxwidth\",{\"0\":{\"1915\":1},\"1\":{\"1915\":1}}],[\"maskalongaxis\",{\"0\":{\"1914\":1},\"1\":{\"1914\":1}}],[\"masknet\",{\"1\":{\"1658\":1,\"1664\":1,\"1665\":1}}],[\"maskdecoder\",{\"0\":{\"1594\":1},\"1\":{\"1594\":3}}],[\"masked\",{\"1\":{\"903\":2,\"1019\":1,\"1037\":1,\"1039\":1,\"1115\":2,\"1206\":1,\"1269\":4,\"1375\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":2,\"1523\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1669\":1,\"1671\":1,\"1890\":3,\"1892\":1,\"2441\":1}}],[\"maskestimator\",{\"0\":{\"782\":1,\"1595\":1},\"1\":{\"782\":1,\"1595\":1}}],[\"mask=2\",{\"1\":{\"965\":1,\"968\":2,\"972\":1}}],[\"mask=none\",{\"1\":{\"784\":1,\"1116\":2,\"1209\":2,\"1581\":1}}],[\"mask=false\",{\"1\":{\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"1116\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1571\":1}}],[\"maskparallelscorerinterface\",{\"0\":{\"783\":1},\"1\":{\"783\":1,\"1133\":1}}],[\"masks=2\",{\"1\":{\"1037\":2}}],[\"masks=1\",{\"1\":{\"1019\":1,\"1039\":1}}],[\"masks=none\",{\"1\":{\"737\":2,\"1829\":1,\"1842\":2}}],[\"masks\",{\"1\":{\"737\":4,\"749\":4,\"762\":1,\"763\":2,\"782\":2,\"797\":1,\"899\":1,\"900\":1,\"901\":1,\"902\":1,\"929\":1,\"943\":1,\"955\":1,\"965\":1,\"972\":1,\"1019\":2,\"1037\":4,\"1039\":2,\"1065\":1,\"1269\":2,\"1375\":1,\"1377\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":7,\"1523\":1,\"1524\":7,\"1525\":2,\"1528\":1,\"1534\":1,\"1539\":1,\"1551\":1,\"1553\":1,\"1558\":1,\"1595\":2,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1669\":1,\"1671\":1,\"1739\":5,\"1829\":1,\"1842\":2,\"2260\":2,\"2265\":3}}],[\"masking=false\",{\"1\":{\"755\":1,\"822\":1,\"2088\":1}}],[\"masking=true\",{\"1\":{\"702\":1,\"755\":1,\"821\":1,\"822\":1,\"826\":1,\"2088\":1,\"2095\":1,\"2263\":1,\"2264\":1}}],[\"masking\",{\"0\":{\"1354\":1},\"1\":{\"702\":1,\"754\":4,\"755\":4,\"821\":1,\"822\":4,\"826\":4,\"950\":1,\"968\":1,\"1019\":1,\"1039\":1,\"1115\":2,\"1269\":4,\"1354\":2,\"1516\":2,\"1523\":4,\"1658\":3,\"1778\":2,\"1850\":2,\"1851\":6,\"1852\":2,\"1879\":6,\"2000\":2,\"2003\":2,\"2086\":5,\"2087\":5,\"2088\":4,\"2090\":6,\"2091\":6,\"2095\":5,\"2243\":6,\"2244\":6,\"2245\":6,\"2255\":6,\"2256\":6,\"2263\":5,\"2264\":6,\"2279\":6,\"2280\":6,\"2641\":1}}],[\"mask\",{\"0\":{\"782\":1,\"898\":2,\"899\":1,\"901\":1,\"903\":1,\"905\":2,\"920\":2,\"921\":2,\"924\":2,\"965\":1,\"972\":1,\"1019\":1,\"1039\":1,\"1101\":1,\"1102\":1,\"1366\":1,\"1375\":1,\"1595\":1,\"1826\":1,\"1914\":1,\"1915\":1,\"1940\":2},\"1\":{\"251\":2,\"307\":2,\"408\":2,\"636\":1,\"702\":1,\"711\":9,\"712\":12,\"714\":4,\"715\":4,\"716\":4,\"717\":1,\"718\":4,\"719\":4,\"720\":4,\"721\":4,\"722\":4,\"725\":5,\"726\":6,\"729\":1,\"730\":1,\"731\":9,\"732\":2,\"740\":5,\"741\":5,\"747\":1,\"748\":1,\"749\":4,\"756\":1,\"771\":3,\"775\":5,\"776\":5,\"777\":1,\"782\":1,\"784\":2,\"785\":7,\"797\":6,\"809\":3,\"821\":1,\"827\":5,\"887\":2,\"898\":4,\"899\":3,\"900\":5,\"901\":3,\"902\":5,\"903\":2,\"904\":1,\"905\":4,\"920\":3,\"921\":5,\"924\":3,\"943\":3,\"950\":9,\"955\":3,\"965\":5,\"968\":9,\"972\":5,\"1019\":3,\"1037\":2,\"1039\":3,\"1049\":11,\"1050\":11,\"1051\":3,\"1052\":18,\"1053\":5,\"1054\":3,\"1055\":3,\"1056\":11,\"1065\":11,\"1068\":9,\"1069\":3,\"1076\":12,\"1101\":6,\"1102\":3,\"1115\":28,\"1116\":6,\"1133\":12,\"1141\":4,\"1149\":3,\"1150\":3,\"1151\":1,\"1158\":1,\"1166\":1,\"1170\":4,\"1178\":1,\"1179\":1,\"1180\":10,\"1181\":2,\"1182\":2,\"1200\":1,\"1205\":3,\"1206\":2,\"1209\":5,\"1214\":3,\"1219\":4,\"1239\":1,\"1242\":1,\"1244\":1,\"1252\":1,\"1254\":1,\"1257\":7,\"1269\":30,\"1272\":1,\"1273\":9,\"1366\":2,\"1371\":1,\"1375\":9,\"1377\":1,\"1454\":3,\"1463\":3,\"1505\":4,\"1515\":4,\"1516\":10,\"1517\":1,\"1523\":3,\"1524\":5,\"1525\":4,\"1528\":4,\"1529\":1,\"1534\":4,\"1539\":4,\"1551\":4,\"1553\":7,\"1558\":3,\"1566\":3,\"1567\":3,\"1568\":3,\"1569\":3,\"1570\":3,\"1571\":3,\"1594\":1,\"1595\":2,\"1611\":9,\"1626\":4,\"1645\":3,\"1654\":4,\"1658\":6,\"1659\":3,\"1664\":6,\"1665\":6,\"1669\":4,\"1671\":3,\"1711\":2,\"1739\":3,\"1771\":1,\"1772\":3,\"1776\":3,\"1777\":3,\"1787\":3,\"1788\":1,\"1789\":1,\"1791\":1,\"1798\":1,\"1804\":2,\"1806\":1,\"1808\":6,\"1826\":2,\"1829\":1,\"1833\":3,\"1835\":3,\"1838\":1,\"1842\":2,\"1853\":3,\"1855\":3,\"1863\":1,\"1864\":1,\"1865\":1,\"1868\":3,\"1874\":1,\"1878\":2,\"1880\":3,\"1914\":3,\"1915\":4,\"1940\":6,\"1986\":1,\"1993\":3,\"2001\":3,\"2024\":1,\"2028\":1,\"2029\":1,\"2086\":1,\"2087\":1,\"2095\":1,\"2260\":1,\"2263\":1,\"2283\":1,\"2290\":1,\"2292\":2,\"2295\":2,\"2296\":2,\"2305\":3}}],[\"maskctcmodel\",{\"0\":{\"1206\":1},\"1\":{\"1205\":1,\"1206\":2}}],[\"maskctcinference\",{\"0\":{\"1205\":1},\"1\":{\"1205\":2}}],[\"maskctc\",{\"0\":{\"321\":1,\"905\":1,\"920\":1,\"1205\":1,\"1206\":1},\"1\":{\"111\":1,\"249\":4,\"321\":6,\"905\":1,\"920\":1,\"1205\":2,\"1206\":2}}],[\"masuyama\",{\"1\":{\"130\":1}}],[\"master\",{\"0\":{\"2214\":1,\"2215\":1},\"1\":{\"5\":1,\"13\":1,\"36\":4,\"38\":4,\"39\":6,\"195\":1,\"199\":1,\"202\":1,\"295\":1,\"377\":4,\"429\":4,\"596\":2,\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"1101\":1,\"1102\":1,\"1180\":1,\"1454\":2,\"1695\":1,\"1717\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"2131\":1,\"2154\":1,\"2180\":4,\"2214\":1,\"2215\":1,\"2361\":1,\"2366\":1,\"2372\":1,\"2377\":2,\"2379\":1,\"2382\":1,\"2384\":2,\"2387\":1,\"2393\":1,\"2394\":1,\"2429\":2,\"2430\":1,\"2446\":1,\"2499\":1,\"2529\":1,\"2530\":1,\"2552\":1,\"2554\":1,\"2555\":1,\"2564\":1,\"2574\":1,\"2575\":1,\"2576\":1,\"2587\":1,\"2601\":1,\"2618\":1,\"2635\":1,\"2646\":1,\"2650\":1}}],[\"maps\",{\"1\":{\"619\":1,\"621\":1,\"727\":2,\"728\":2,\"1561\":1,\"1767\":1,\"1768\":1,\"1786\":1}}],[\"map\",{\"1\":{\"52\":1,\"1011\":1,\"1136\":2,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1241\":1,\"1245\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1427\":4,\"2032\":1,\"2157\":1,\"2410\":1,\"2467\":1}}],[\"mapping\",{\"1\":{\"44\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1327\":1,\"1355\":1,\"1384\":1,\"1386\":1,\"1388\":1,\"1390\":1,\"1394\":1,\"1398\":1,\"1399\":1,\"1401\":1,\"1405\":1,\"1409\":1,\"1413\":1,\"1523\":3,\"1658\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2176\":1,\"2385\":1}}],[\"maxpool=true\",{\"1\":{\"1468\":1,\"1627\":1}}],[\"maxpool2d\",{\"1\":{\"1097\":1}}],[\"max=0\",{\"1\":{\"1245\":1,\"1618\":1}}],[\"max=none\",{\"1\":{\"1241\":2}}],[\"maxu\",{\"1\":{\"1142\":2,\"1154\":2,\"1186\":2,\"1210\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1324\":1,\"1334\":2}}],[\"maxt\",{\"1\":{\"1142\":2,\"1186\":2,\"1210\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1324\":1,\"1334\":2}}],[\"maxtasksperchild\",{\"1\":{\"997\":1}}],[\"maxtasksperchild=20\",{\"1\":{\"997\":1}}],[\"maximizing\",{\"1\":{\"1529\":1,\"1568\":1}}],[\"maximizes\",{\"1\":{\"1003\":1,\"1004\":1}}],[\"maximum\",{\"0\":{\"1335\":1},\"1\":{\"23\":6,\"26\":3,\"49\":2,\"76\":1,\"113\":1,\"115\":2,\"116\":2,\"119\":3,\"150\":3,\"274\":2,\"275\":1,\"689\":1,\"691\":2,\"692\":1,\"693\":1,\"697\":3,\"700\":4,\"770\":1,\"772\":1,\"797\":3,\"800\":1,\"810\":1,\"813\":1,\"818\":1,\"821\":1,\"826\":1,\"857\":1,\"863\":1,\"865\":1,\"950\":3,\"956\":1,\"968\":3,\"973\":1,\"997\":1,\"1003\":1,\"1004\":3,\"1005\":2,\"1007\":1,\"1019\":1,\"1028\":8,\"1037\":2,\"1039\":1,\"1048\":3,\"1057\":1,\"1059\":1,\"1065\":2,\"1066\":2,\"1069\":1,\"1077\":1,\"1078\":1,\"1079\":2,\"1093\":1,\"1096\":1,\"1138\":4,\"1139\":4,\"1142\":2,\"1186\":2,\"1210\":2,\"1226\":1,\"1241\":1,\"1245\":1,\"1248\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":2,\"1335\":1,\"1462\":2,\"1463\":1,\"1778\":1,\"1785\":1,\"1804\":1,\"1805\":1,\"1808\":1,\"1810\":1,\"1848\":1,\"1849\":1,\"1856\":1,\"1858\":1,\"1859\":1,\"1877\":1,\"1878\":1,\"1915\":1,\"2002\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2079\":1,\"2095\":1,\"2260\":1,\"2263\":1,\"2264\":1,\"2274\":1,\"2317\":1,\"2330\":1}}],[\"maxchars\",{\"1\":{\"274\":1}}],[\"maxframes\",{\"1\":{\"274\":1}}],[\"maxlen=none\",{\"1\":{\"901\":1}}],[\"maxlenratio=4\",{\"1\":{\"2520\":1}}],[\"maxlenratio=10\",{\"1\":{\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"maxlenratio=30\",{\"1\":{\"2079\":1}}],[\"maxlenratio=0\",{\"1\":{\"692\":1,\"693\":1,\"697\":1,\"797\":1,\"857\":1,\"2358\":1,\"2520\":1,\"2579\":1,\"2592\":1}}],[\"maxlenratio<0\",{\"1\":{\"697\":1,\"797\":1}}],[\"maxlenratio\",{\"1\":{\"150\":6,\"217\":1,\"224\":1,\"231\":1,\"249\":2,\"251\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"267\":2,\"307\":2,\"315\":2,\"327\":2,\"391\":2,\"399\":4,\"408\":2,\"422\":2,\"443\":4,\"449\":2,\"464\":2,\"470\":2,\"485\":2,\"691\":2,\"692\":4,\"693\":2,\"697\":4,\"698\":1,\"699\":1,\"797\":2,\"821\":1,\"826\":1,\"857\":2,\"1985\":1,\"2002\":2,\"2095\":2,\"2263\":2,\"2264\":2}}],[\"maxlen\",{\"1\":{\"26\":2,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"384\":2,\"691\":2,\"692\":2,\"697\":2,\"731\":5,\"797\":2,\"1133\":13,\"1190\":3,\"1204\":4,\"1214\":8,\"1244\":4,\"1273\":11,\"2001\":4}}],[\"max\",{\"0\":{\"1346\":1},\"1\":{\"23\":3,\"26\":2,\"69\":1,\"76\":1,\"106\":1,\"115\":2,\"116\":2,\"119\":2,\"144\":2,\"245\":2,\"249\":4,\"272\":2,\"301\":2,\"307\":4,\"377\":2,\"408\":4,\"429\":6,\"443\":2,\"499\":2,\"612\":1,\"677\":2,\"678\":1,\"679\":1,\"681\":2,\"684\":3,\"685\":3,\"686\":2,\"687\":2,\"688\":3,\"689\":3,\"692\":1,\"693\":1,\"697\":2,\"700\":4,\"758\":4,\"762\":2,\"763\":2,\"770\":2,\"772\":2,\"797\":3,\"810\":2,\"813\":2,\"818\":2,\"835\":1,\"857\":1,\"863\":3,\"865\":3,\"950\":1,\"956\":1,\"968\":4,\"973\":2,\"1004\":7,\"1005\":4,\"1028\":4,\"1048\":4,\"1065\":2,\"1066\":2,\"1077\":2,\"1078\":2,\"1079\":6,\"1093\":2,\"1096\":2,\"1102\":1,\"1115\":2,\"1133\":2,\"1138\":4,\"1139\":4,\"1148\":1,\"1149\":1,\"1150\":1,\"1154\":1,\"1169\":3,\"1179\":1,\"1207\":1,\"1214\":1,\"1219\":2,\"1241\":1,\"1243\":1,\"1245\":2,\"1273\":1,\"1346\":2,\"1354\":2,\"1367\":1,\"1371\":1,\"1375\":4,\"1463\":1,\"1528\":3,\"1559\":1,\"1560\":1,\"1618\":3,\"1619\":3,\"1718\":1,\"1778\":5,\"1785\":2,\"1801\":3,\"1804\":2,\"1805\":5,\"1810\":2,\"1826\":1,\"1845\":1,\"1846\":2,\"1847\":3,\"1848\":2,\"1849\":3,\"1850\":3,\"1852\":3,\"1856\":2,\"1858\":2,\"1870\":1,\"1877\":5,\"1878\":2,\"1896\":1,\"1915\":2,\"1925\":2,\"1954\":2,\"1956\":2,\"1962\":1,\"1986\":1,\"2001\":1,\"2018\":4,\"2024\":1,\"2028\":1,\"2054\":1,\"2099\":1,\"2102\":1,\"2106\":4,\"2182\":2,\"2186\":1,\"2202\":2,\"2204\":3,\"2222\":1,\"2260\":2,\"2266\":1,\"2267\":1,\"2270\":2,\"2271\":2,\"2272\":2,\"2274\":2,\"2290\":1,\"2294\":2,\"2317\":1,\"2330\":1,\"2440\":2,\"2558\":2,\"2584\":2}}],[\"macosx\",{\"1\":{\"134\":1}}],[\"macos12\",{\"1\":{\"133\":1}}],[\"macaron\",{\"1\":{\"21\":2,\"711\":2,\"1050\":3,\"1056\":3,\"1148\":3,\"1149\":1,\"1169\":1,\"1170\":2,\"1203\":3,\"1505\":3,\"1771\":3,\"1778\":1,\"1787\":3,\"1788\":3,\"1798\":3,\"1804\":3,\"1805\":1,\"1850\":1,\"1851\":3,\"1852\":1,\"1874\":3,\"1877\":1,\"1878\":3,\"2003\":1,\"2026\":1,\"2054\":3,\"2090\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2279\":3}}],[\"machines\",{\"1\":{\"141\":2,\"142\":1,\"2372\":3,\"2429\":3,\"2554\":3}}],[\"machine\",{\"0\":{\"126\":1},\"1\":{\"5\":1,\"45\":2,\"85\":1,\"126\":1,\"141\":1,\"142\":1,\"255\":1,\"554\":1}}],[\"made\",{\"1\":{\"21\":1,\"84\":1,\"113\":2,\"612\":1,\"2421\":1}}],[\"major\",{\"1\":{\"16\":1,\"83\":1,\"1409\":1,\"2394\":1,\"2400\":1,\"2530\":1,\"2536\":1,\"2564\":1}}],[\"majority\",{\"1\":{\"14\":1,\"1245\":1}}],[\"maybe\",{\"1\":{\"44\":1,\"2148\":1}}],[\"may\",{\"1\":{\"14\":1,\"48\":1,\"49\":1,\"74\":1,\"96\":1,\"106\":1,\"112\":4,\"126\":1,\"149\":1,\"698\":1,\"699\":1,\"944\":1,\"1160\":2,\"1161\":2,\"1162\":2,\"1163\":2,\"1164\":2,\"1171\":1,\"1177\":2,\"1206\":1,\"1252\":2,\"1253\":2,\"1254\":2,\"1279\":2,\"1429\":2,\"1432\":1,\"1436\":1,\"1484\":2,\"1510\":1,\"1511\":1,\"1552\":1,\"1601\":2,\"1622\":1,\"1639\":1,\"1643\":1,\"1644\":1,\"1735\":1,\"1953\":1,\"1955\":1,\"2358\":1,\"2372\":1,\"2385\":1,\"2389\":1,\"2393\":1,\"2394\":1,\"2397\":1,\"2408\":1,\"2409\":1,\"2422\":1,\"2427\":1,\"2429\":1,\"2441\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2499\":1,\"2503\":1,\"2520\":1,\"2523\":1,\"2525\":1,\"2529\":1,\"2530\":1,\"2533\":1,\"2545\":1,\"2550\":1,\"2552\":1,\"2579\":1,\"2584\":3,\"2585\":3,\"2617\":1,\"2635\":1,\"2638\":1,\"2642\":1}}],[\"maketrans\",{\"1\":{\"2358\":1,\"2520\":1,\"2579\":1}}],[\"make\",{\"0\":{\"283\":1,\"284\":1,\"560\":1,\"620\":1,\"898\":1,\"899\":1,\"901\":1,\"1028\":1,\"1101\":1,\"1102\":1,\"1725\":1,\"1726\":1,\"2379\":1},\"1\":{\"60\":1,\"73\":1,\"82\":1,\"100\":1,\"106\":1,\"113\":1,\"126\":1,\"134\":3,\"135\":4,\"167\":2,\"172\":3,\"178\":2,\"196\":4,\"200\":2,\"234\":3,\"235\":2,\"243\":1,\"283\":3,\"284\":3,\"294\":1,\"295\":1,\"560\":2,\"585\":1,\"608\":1,\"620\":3,\"731\":1,\"738\":1,\"898\":1,\"899\":2,\"900\":5,\"901\":2,\"902\":5,\"1000\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":3,\"1048\":1,\"1101\":1,\"1102\":1,\"1245\":1,\"1452\":1,\"1662\":1,\"1725\":1,\"1726\":1,\"1906\":1,\"1928\":1,\"2372\":1,\"2373\":1,\"2375\":1,\"2387\":1,\"2389\":1,\"2394\":1,\"2397\":1,\"2408\":1,\"2409\":1,\"2422\":1,\"2429\":1,\"2433\":1,\"2441\":1,\"2446\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2514\":1,\"2525\":1,\"2530\":1,\"2533\":1,\"2545\":1,\"2552\":1,\"2553\":1,\"2555\":1,\"2568\":1,\"2575\":1,\"2584\":3,\"2591\":1,\"2635\":1,\"2638\":1,\"2642\":1,\"2645\":1,\"2659\":1}}],[\"makesymlinktobestmodel\",{\"0\":{\"610\":1},\"1\":{\"610\":2}}],[\"makes\",{\"1\":{\"26\":2,\"239\":1,\"610\":1,\"2168\":1,\"2170\":1}}],[\"makefile\",{\"1\":{\"12\":1,\"135\":1}}],[\"making\",{\"1\":{\"5\":1,\"1660\":1,\"1661\":1,\"1662\":2,\"2584\":1}}],[\"mannually\",{\"1\":{\"2384\":1}}],[\"manner\",{\"1\":{\"897\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"2079\":1,\"2080\":1,\"2387\":1,\"2479\":1}}],[\"mandrain\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"mandarin\",{\"0\":{\"226\":1,\"227\":1,\"2589\":1},\"1\":{\"226\":1,\"2363\":4,\"2506\":2,\"2593\":1,\"2653\":4}}],[\"mandatory\",{\"1\":{\"21\":1,\"28\":1,\"59\":1,\"74\":1,\"84\":2,\"114\":1,\"115\":2,\"124\":1}}],[\"manifold\",{\"1\":{\"1529\":1,\"1568\":1}}],[\"manage\",{\"1\":{\"1432\":1,\"1436\":1,\"1510\":1,\"1511\":1,\"1643\":1,\"1644\":1,\"2176\":1}}],[\"manual\",{\"1\":{\"127\":1}}],[\"manually\",{\"1\":{\"12\":1,\"17\":1,\"19\":1,\"33\":1,\"95\":1,\"135\":1,\"148\":1,\"295\":1,\"613\":1,\"2372\":1,\"2585\":1}}],[\"many\",{\"1\":{\"11\":1,\"56\":2,\"60\":1,\"113\":1,\"997\":2,\"1187\":2,\"1202\":2,\"1286\":2,\"1287\":2,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2429\":1,\"2431\":1,\"2467\":1,\"2468\":2,\"2552\":1}}],[\"mainly\",{\"1\":{\"2046\":1,\"2372\":1,\"2385\":1,\"2429\":1,\"2450\":1,\"2482\":1,\"2554\":1}}],[\"mainprocesserror\",{\"0\":{\"593\":1},\"1\":{\"593\":1}}],[\"main\",{\"0\":{\"84\":1,\"1093\":1,\"1961\":1,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":1,\"1968\":1,\"2696\":1},\"1\":{\"1\":1,\"16\":3,\"17\":8,\"22\":3,\"27\":1,\"56\":2,\"115\":4,\"116\":1,\"117\":1,\"121\":2,\"124\":3,\"235\":2,\"593\":2,\"607\":1,\"626\":2,\"627\":1,\"639\":2,\"646\":1,\"658\":1,\"706\":2,\"727\":5,\"728\":5,\"754\":4,\"815\":2,\"820\":4,\"821\":4,\"825\":2,\"826\":4,\"834\":1,\"1057\":1,\"1058\":3,\"1069\":1,\"1087\":3,\"1088\":3,\"1089\":3,\"1091\":3,\"1093\":3,\"1130\":1,\"1484\":3,\"1551\":1,\"1553\":1,\"1601\":3,\"1961\":2,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":2,\"1968\":2,\"1972\":1,\"2090\":1,\"2099\":2,\"2201\":1,\"2294\":1,\"2349\":1,\"2354\":1,\"2372\":1,\"2375\":1,\"2380\":1,\"2384\":1,\"2388\":2,\"2406\":1,\"2407\":1,\"2421\":2,\"2429\":1,\"2441\":1,\"2442\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":2,\"2543\":1,\"2544\":2,\"2554\":1,\"2558\":1,\"2568\":1,\"2585\":2}}],[\"mutablemapping\",{\"1\":{\"2316\":1}}],[\"mutlitokenizercommonpreprocessor\",{\"0\":{\"2191\":1},\"1\":{\"2191\":1}}],[\"mutually\",{\"1\":{\"1905\":1}}],[\"mun\",{\"1\":{\"2064\":1}}],[\"mu=256\",{\"1\":{\"869\":1,\"877\":1}}],[\"mul\",{\"1\":{\"1659\":1,\"1665\":1}}],[\"muladdadaptlayer\",{\"0\":{\"1601\":1},\"1\":{\"1601\":2}}],[\"mulcatblock\",{\"0\":{\"1602\":1},\"1\":{\"1602\":4}}],[\"mulcat\",{\"1\":{\"1531\":2,\"1602\":1,\"1645\":1}}],[\"mult=\",{\"1\":{\"1605\":1}}],[\"mult\",{\"0\":{\"1290\":1,\"1291\":1,\"1292\":1},\"1\":{\"1115\":2,\"1179\":1,\"1180\":1,\"1269\":2,\"2018\":2}}],[\"multitask\",{\"0\":{\"1955\":1},\"1\":{\"1955\":1}}],[\"multifrequencydiscriminator\",{\"0\":{\"1786\":1},\"1\":{\"1786\":1}}],[\"multiresl1specloss\",{\"0\":{\"1604\":1},\"1\":{\"1604\":1,\"1719\":1}}],[\"multiref\",{\"0\":{\"515\":1},\"1\":{\"515\":2}}],[\"multiscale\",{\"0\":{\"1821\":1},\"1\":{\"1782\":1,\"1821\":1}}],[\"multisoundscpreader\",{\"0\":{\"1390\":1},\"1\":{\"1390\":1}}],[\"multisequential\",{\"0\":{\"787\":1},\"1\":{\"787\":2,\"914\":1}}],[\"multimask\",{\"0\":{\"1375\":1},\"1\":{\"1375\":2,\"1377\":1}}],[\"multiblankrnntnumba\",{\"0\":{\"1286\":1},\"1\":{\"1286\":2}}],[\"multiblankrnntlossnumba\",{\"0\":{\"1211\":1},\"1\":{\"1211\":2}}],[\"multiblank\",{\"0\":{\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":1,\"1337\":1},\"1\":{\"1211\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":1,\"1337\":1}}],[\"multiblankgpurnnt\",{\"0\":{\"1210\":1},\"1\":{\"1210\":1}}],[\"multiblocks\",{\"0\":{\"1068\":1},\"1\":{\"1068\":6,\"1087\":1}}],[\"multichannel\",{\"1\":{\"729\":1,\"885\":1,\"1198\":1,\"1524\":1,\"1706\":1,\"1707\":1,\"1712\":2,\"1715\":2}}],[\"multilevellm\",{\"0\":{\"611\":1},\"1\":{\"611\":1}}],[\"multilingual\",{\"1\":{\"255\":2,\"259\":2,\"2357\":3,\"2359\":1,\"2380\":1,\"2388\":1,\"2421\":1,\"2456\":1,\"2460\":1,\"2521\":1,\"2524\":1,\"2544\":1,\"2578\":3,\"2580\":1,\"2584\":1}}],[\"multilayerpitsolver\",{\"0\":{\"1603\":1},\"1\":{\"1603\":1}}],[\"multilayeredconv1d\",{\"0\":{\"786\":1},\"1\":{\"711\":2,\"713\":1,\"786\":2}}],[\"multilayer\",{\"0\":{\"1603\":1},\"1\":{\"102\":2,\"1239\":1,\"1603\":1,\"1791\":1}}],[\"multiprocessiterator\",{\"1\":{\"997\":2}}],[\"multiprocessing\",{\"0\":{\"36\":1,\"40\":1,\"42\":1},\"1\":{\"32\":4,\"34\":3,\"35\":1,\"36\":3,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"377\":2,\"429\":2,\"2121\":1,\"2122\":1,\"2180\":2,\"2216\":1,\"2217\":1}}],[\"multiprocessparallelupdater\",{\"1\":{\"727\":1}}],[\"multiprocess\",{\"1\":{\"598\":1}}],[\"multiplication\",{\"1\":{\"1242\":1,\"1452\":1}}],[\"multiplier\",{\"1\":{\"1130\":1}}],[\"multipliers\",{\"1\":{\"1130\":1}}],[\"multiplied\",{\"1\":{\"72\":1,\"834\":1,\"1198\":1}}],[\"multiply\",{\"1\":{\"343\":2,\"1807\":1}}],[\"multipleiterfactory\",{\"0\":{\"1898\":1},\"1\":{\"1898\":1}}],[\"multiple\",{\"0\":{\"148\":1,\"1898\":1},\"1\":{\"3\":2,\"28\":1,\"45\":2,\"47\":1,\"93\":1,\"141\":2,\"148\":7,\"149\":1,\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"280\":1,\"295\":1,\"493\":1,\"515\":1,\"704\":1,\"705\":1,\"940\":1,\"997\":1,\"998\":1,\"1137\":1,\"1138\":1,\"1375\":2,\"1390\":1,\"1406\":1,\"1407\":1,\"1537\":1,\"1603\":1,\"1645\":1,\"1661\":1,\"1670\":2,\"1671\":1,\"1693\":1,\"1755\":1,\"1898\":1,\"2099\":1,\"2201\":1,\"2309\":1,\"2389\":1,\"2392\":1,\"2408\":1,\"2422\":1,\"2425\":1,\"2429\":1,\"2440\":1,\"2441\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2506\":1,\"2525\":1,\"2528\":1,\"2545\":1,\"2548\":1,\"2554\":1,\"2564\":1,\"2639\":1}}],[\"multiheaddamped\",{\"1\":{\"1069\":1}}],[\"multiheaddampedema\",{\"0\":{\"1069\":1},\"1\":{\"1069\":10}}],[\"multiheadattention\",{\"0\":{\"784\":1,\"1993\":1},\"1\":{\"784\":2,\"1993\":1}}],[\"multiheadedattention\",{\"0\":{\"785\":1,\"1209\":1,\"2253\":1},\"1\":{\"711\":1,\"771\":1,\"785\":2,\"809\":1,\"827\":1,\"1076\":1,\"1209\":2,\"2253\":2}}],[\"multihead\",{\"0\":{\"1993\":1},\"1\":{\"31\":1,\"1851\":1,\"1993\":2,\"2002\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1}}],[\"multi\",{\"0\":{\"22\":1,\"54\":1,\"71\":1,\"94\":1,\"118\":1,\"216\":1,\"405\":1,\"713\":1,\"786\":1,\"909\":1,\"1068\":1,\"1069\":1,\"1142\":1,\"1143\":1,\"1154\":1,\"1155\":1,\"1186\":1,\"1194\":1,\"1202\":1,\"1210\":1,\"1211\":2,\"1224\":2,\"1225\":1,\"1226\":1,\"1227\":1,\"1286\":2,\"1287\":2,\"1288\":1,\"1293\":2,\"1294\":2,\"1295\":2,\"1296\":2,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1306\":1,\"1307\":1,\"1315\":1,\"1316\":1,\"1318\":1,\"1324\":1,\"1325\":1,\"1331\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":2,\"1337\":1,\"1338\":1,\"1344\":1,\"1346\":1,\"1348\":2,\"1349\":1,\"1350\":1,\"1353\":1,\"1359\":1,\"1375\":1,\"1390\":1,\"1424\":1,\"2179\":1,\"2225\":1,\"2488\":1,\"2491\":1,\"2511\":1,\"2608\":1,\"2610\":1,\"2625\":1,\"2627\":1,\"2656\":1},\"1\":{\"19\":1,\"21\":2,\"22\":1,\"31\":1,\"32\":8,\"33\":1,\"34\":1,\"54\":4,\"72\":1,\"82\":1,\"84\":1,\"95\":1,\"115\":3,\"118\":1,\"148\":1,\"216\":1,\"243\":1,\"307\":2,\"429\":2,\"568\":1,\"625\":1,\"627\":1,\"629\":1,\"633\":1,\"638\":1,\"686\":5,\"687\":4,\"688\":5,\"689\":6,\"713\":1,\"728\":1,\"749\":1,\"754\":1,\"760\":1,\"763\":2,\"771\":1,\"778\":1,\"784\":3,\"785\":1,\"786\":3,\"787\":2,\"799\":1,\"809\":1,\"812\":1,\"826\":1,\"831\":1,\"892\":1,\"909\":3,\"942\":1,\"999\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1,\"1068\":1,\"1069\":2,\"1133\":1,\"1138\":9,\"1142\":1,\"1143\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1154\":1,\"1155\":1,\"1171\":2,\"1181\":1,\"1186\":1,\"1194\":1,\"1202\":1,\"1203\":1,\"1209\":1,\"1210\":4,\"1211\":4,\"1224\":2,\"1225\":1,\"1226\":1,\"1227\":1,\"1272\":1,\"1286\":5,\"1287\":2,\"1288\":1,\"1293\":2,\"1294\":2,\"1295\":2,\"1296\":2,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":3,\"1303\":3,\"1304\":3,\"1306\":1,\"1307\":1,\"1315\":1,\"1316\":1,\"1318\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1331\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":5,\"1337\":4,\"1338\":1,\"1344\":1,\"1346\":1,\"1348\":2,\"1349\":1,\"1350\":1,\"1353\":1,\"1359\":1,\"1375\":2,\"1390\":1,\"1405\":1,\"1406\":3,\"1407\":6,\"1408\":1,\"1424\":1,\"1425\":1,\"1466\":1,\"1476\":1,\"1505\":1,\"1528\":1,\"1551\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1603\":2,\"1604\":2,\"1611\":1,\"1622\":1,\"1655\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":1,\"1708\":1,\"1712\":1,\"1715\":2,\"1719\":6,\"1739\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1778\":3,\"1786\":3,\"1788\":1,\"1801\":5,\"1805\":4,\"1845\":1,\"1846\":3,\"1847\":7,\"1850\":2,\"1852\":3,\"1858\":2,\"1877\":2,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2001\":1,\"2004\":1,\"2029\":1,\"2040\":2,\"2054\":2,\"2064\":1,\"2084\":1,\"2089\":1,\"2106\":2,\"2125\":1,\"2174\":1,\"2179\":1,\"2182\":1,\"2222\":1,\"2225\":2,\"2229\":2,\"2231\":1,\"2253\":2,\"2262\":1,\"2264\":1,\"2267\":1,\"2275\":1,\"2363\":4,\"2385\":1,\"2415\":1,\"2452\":1,\"2481\":1,\"2490\":1,\"2506\":1,\"2512\":11,\"2513\":4,\"2514\":3,\"2515\":1,\"2618\":1,\"2653\":4,\"2657\":8,\"2659\":1}}],[\"mulenc\",{\"0\":{\"638\":1},\"1\":{\"638\":2,\"889\":1}}],[\"mu\",{\"0\":{\"869\":1,\"877\":1},\"1\":{\"238\":2,\"869\":4,\"877\":4,\"1524\":1,\"1712\":1,\"1715\":1,\"1927\":1}}],[\"musical\",{\"1\":{\"2090\":2}}],[\"musicxml\",{\"1\":{\"1416\":4}}],[\"music\",{\"1\":{\"130\":1,\"2086\":1,\"2087\":1}}],[\"muskits\",{\"1\":{\"130\":1}}],[\"must\",{\"1\":{\"1\":1,\"45\":1,\"56\":1,\"60\":2,\"76\":1,\"134\":1,\"135\":1,\"144\":1,\"204\":2,\"237\":1,\"296\":3,\"745\":3,\"746\":3,\"747\":1,\"875\":1,\"1186\":1,\"1187\":2,\"1202\":2,\"1244\":1,\"1245\":1,\"1248\":2,\"1252\":1,\"1253\":1,\"1286\":1,\"1287\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1392\":1,\"1427\":1,\"1522\":1,\"1537\":1,\"1618\":1,\"1619\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":1,\"1671\":1,\"1679\":1,\"1719\":1,\"1735\":2,\"1972\":1,\"2153\":1,\"2168\":1,\"2170\":5,\"2185\":1,\"2203\":1}}],[\"much\",{\"1\":{\"84\":2,\"204\":1,\"1138\":1,\"1245\":2,\"2389\":1,\"2408\":1,\"2420\":1,\"2422\":1,\"2441\":1,\"2449\":1,\"2450\":1,\"2465\":1,\"2481\":1,\"2482\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"gj1iw\",{\"1\":{\"2490\":1,\"2609\":1,\"2626\":1}}],[\"gs\",{\"1\":{\"2086\":1,\"2087\":1}}],[\"gsm\",{\"1\":{\"1927\":2}}],[\"gst+xvector\",{\"1\":{\"2512\":5,\"2657\":5}}],[\"gst\",{\"0\":{\"2253\":1,\"2257\":1,\"2261\":1,\"2262\":1},\"1\":{\"1850\":9,\"1851\":26,\"1852\":9,\"2095\":26,\"2243\":26,\"2244\":26,\"2253\":1,\"2255\":26,\"2257\":1,\"2261\":11,\"2262\":11,\"2263\":26,\"2264\":26,\"2512\":2,\"2514\":1,\"2657\":2,\"2659\":1}}],[\"gss\",{\"1\":{\"1243\":2}}],[\"g=none\",{\"1\":{\"1771\":1,\"1772\":1,\"1788\":1,\"1806\":1,\"1808\":2}}],[\"gfp\",{\"1\":{\"1517\":1}}],[\"glob\",{\"1\":{\"2514\":5,\"2568\":3,\"2659\":5}}],[\"globallayernorm\",{\"0\":{\"1372\":1,\"1575\":1},\"1\":{\"1372\":1,\"1575\":2}}],[\"globalmvn\",{\"0\":{\"760\":1,\"1906\":1},\"1\":{\"760\":1,\"1906\":2}}],[\"global\",{\"0\":{\"1225\":1,\"1315\":1,\"1353\":1,\"1359\":1,\"1906\":1},\"1\":{\"506\":1,\"758\":3,\"760\":1,\"1225\":1,\"1241\":1,\"1315\":1,\"1343\":1,\"1353\":1,\"1359\":1,\"1372\":1,\"1517\":1,\"1575\":1,\"1670\":1,\"1671\":1,\"1765\":5,\"1771\":5,\"1772\":5,\"1778\":1,\"1788\":4,\"1800\":3,\"1803\":5,\"1804\":3,\"1805\":1,\"1808\":3,\"1833\":1,\"1835\":2,\"1844\":7,\"1850\":1,\"1851\":4,\"1852\":1,\"1863\":5,\"1864\":5,\"1865\":5,\"1868\":4,\"1877\":1,\"1878\":3,\"1880\":6,\"1906\":3,\"2078\":1,\"2079\":1,\"2083\":1,\"2095\":2,\"2243\":1,\"2244\":1,\"2255\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2440\":2,\"2592\":1}}],[\"glstm\",{\"0\":{\"1572\":1},\"1\":{\"1522\":9,\"1523\":8,\"1572\":2}}],[\"gln\",{\"1\":{\"1370\":1,\"1372\":2,\"1377\":2,\"1378\":1,\"1379\":2,\"1537\":1,\"1539\":1,\"1546\":1,\"1575\":2,\"1581\":1,\"1598\":1,\"1648\":1,\"1652\":2,\"1658\":2,\"1659\":2,\"1663\":1,\"1664\":2,\"1665\":2}}],[\"gluconv2d\",{\"0\":{\"1576\":1},\"1\":{\"1545\":1,\"1576\":2}}],[\"gluconvtranspose2d\",{\"0\":{\"1577\":1},\"1\":{\"1545\":2,\"1577\":1}}],[\"gluconv\",{\"1\":{\"1545\":3}}],[\"glu\",{\"1\":{\"1115\":2,\"1241\":1,\"1576\":1,\"1577\":1}}],[\"gdca\",{\"1\":{\"2095\":1}}],[\"gdcattloc\",{\"0\":{\"758\":1},\"1\":{\"758\":1}}],[\"gdown==4\",{\"1\":{\"2504\":1,\"2651\":1}}],[\"gdown\",{\"1\":{\"207\":1,\"2367\":1,\"2368\":1,\"2372\":1,\"2396\":2,\"2450\":1,\"2454\":1,\"2460\":1,\"2470\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2497\":1,\"2500\":1,\"2506\":1,\"2510\":2,\"2512\":1,\"2517\":1,\"2519\":2,\"2532\":1,\"2567\":1,\"2604\":1,\"2605\":1,\"2609\":1,\"2614\":1,\"2621\":1,\"2622\":1,\"2626\":1,\"2632\":1,\"2647\":2}}],[\"gpt2\",{\"1\":{\"668\":1}}],[\"gpurnnt\",{\"0\":{\"1186\":1},\"1\":{\"1186\":1,\"1210\":1}}],[\"gpu=false\",{\"1\":{\"1526\":1}}],[\"gpu=2\",{\"1\":{\"144\":1}}],[\"gpu=$0\",{\"1\":{\"144\":3}}],[\"gpu=\",{\"1\":{\"144\":3}}],[\"gpu=0\",{\"1\":{\"144\":2}}],[\"gpus\",{\"0\":{\"71\":1,\"72\":1,\"94\":1,\"95\":1},\"1\":{\"3\":1,\"19\":2,\"33\":2,\"34\":1,\"36\":1,\"41\":1,\"72\":3,\"80\":1,\"82\":1,\"95\":2,\"148\":5,\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"286\":1,\"296\":1,\"626\":1,\"627\":1,\"734\":1}}],[\"gpu\",{\"0\":{\"3\":1,\"19\":1,\"148\":1,\"1186\":1,\"1210\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1,\"1337\":1,\"1350\":1,\"2426\":1,\"2549\":1},\"1\":{\"1\":4,\"2\":1,\"3\":3,\"4\":1,\"5\":1,\"7\":2,\"8\":2,\"19\":6,\"31\":1,\"32\":9,\"33\":1,\"34\":1,\"40\":1,\"72\":3,\"73\":1,\"80\":1,\"84\":1,\"95\":1,\"98\":1,\"144\":2,\"148\":5,\"249\":1,\"253\":1,\"257\":1,\"261\":1,\"286\":1,\"296\":1,\"363\":2,\"627\":1,\"725\":1,\"728\":2,\"734\":1,\"745\":2,\"746\":2,\"806\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1,\"1046\":1,\"1066\":1,\"1073\":1,\"1075\":1,\"1083\":1,\"1171\":1,\"1186\":1,\"1206\":1,\"1210\":1,\"1270\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1323\":1,\"1324\":1,\"1334\":1,\"1337\":2,\"1350\":2,\"1427\":2,\"1552\":1,\"2389\":3,\"2403\":1,\"2408\":3,\"2422\":3,\"2426\":1,\"2431\":1,\"2432\":3,\"2440\":1,\"2449\":3,\"2465\":3,\"2481\":3,\"2492\":1,\"2503\":3,\"2525\":3,\"2539\":1,\"2545\":3,\"2549\":1,\"2558\":1,\"2559\":4,\"2564\":1,\"2569\":1,\"2571\":1,\"2584\":1,\"2628\":1}}],[\"g2pk\",{\"0\":{\"2122\":1},\"1\":{\"461\":3,\"2122\":5}}],[\"g2p\",{\"0\":{\"2121\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2144\":1,\"2145\":1,\"2146\":1},\"1\":{\"207\":1,\"217\":5,\"224\":1,\"461\":7,\"2121\":8,\"2122\":3,\"2125\":2,\"2130\":1,\"2131\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2143\":2,\"2144\":1,\"2145\":1,\"2146\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2507\":2,\"2510\":4,\"2513\":2}}],[\"ghi\",{\"1\":{\"2337\":2}}],[\"gh\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1}}],[\"gz\",{\"1\":{\"167\":3,\"178\":3,\"196\":2,\"200\":1,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":2,\"223\":2,\"229\":2,\"230\":2,\"234\":2,\"2567\":1}}],[\"ganttstask\",{\"0\":{\"2104\":1},\"1\":{\"2104\":2}}],[\"gantraineroptions\",{\"0\":{\"2186\":1},\"1\":{\"2185\":2,\"2186\":1}}],[\"gantrainer\",{\"0\":{\"2185\":1},\"1\":{\"2103\":1,\"2104\":1,\"2185\":2,\"2186\":1}}],[\"gansvstask\",{\"0\":{\"2103\":1},\"1\":{\"2103\":2}}],[\"gan\",{\"0\":{\"1760\":2,\"1761\":1,\"1763\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1774\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1781\":1,\"1782\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1806\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1822\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1827\":1,\"1828\":2,\"1829\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1882\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"2103\":1,\"2104\":1,\"2170\":1,\"2185\":1,\"2186\":1,\"2690\":1,\"2691\":1},\"1\":{\"1760\":5,\"1761\":1,\"1763\":1,\"1765\":2,\"1766\":1,\"1767\":2,\"1768\":2,\"1769\":1,\"1771\":1,\"1772\":1,\"1773\":2,\"1774\":1,\"1776\":1,\"1777\":1,\"1778\":2,\"1779\":1,\"1781\":1,\"1782\":2,\"1784\":2,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":2,\"1791\":1,\"1793\":2,\"1795\":2,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":2,\"1801\":1,\"1803\":2,\"1804\":1,\"1805\":2,\"1806\":2,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":2,\"1812\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":2,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":2,\"1821\":1,\"1822\":1,\"1823\":2,\"1824\":2,\"1825\":1,\"1826\":2,\"1827\":2,\"1828\":5,\"1829\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":2,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":2,\"1838\":1,\"1839\":1,\"1840\":2,\"1841\":2,\"1842\":1,\"1843\":1,\"1844\":2,\"1845\":2,\"1846\":3,\"1847\":4,\"1848\":1,\"1849\":2,\"1850\":3,\"1851\":1,\"1852\":2,\"1853\":2,\"1854\":1,\"1855\":2,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":2,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":2,\"1876\":1,\"1877\":2,\"1878\":1,\"1879\":2,\"1880\":2,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"2103\":3,\"2104\":3,\"2170\":2,\"2185\":5,\"2186\":1,\"2203\":3}}],[\"ganesan\",{\"1\":{\"130\":1}}],[\"gain\",{\"1\":{\"1688\":1,\"1693\":2,\"1755\":2,\"1756\":1,\"1905\":1,\"1921\":5,\"1936\":3,\"2181\":1}}],[\"gain=1\",{\"1\":{\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1}}],[\"gating\",{\"1\":{\"1054\":1,\"1151\":1,\"1153\":1}}],[\"gate=none\",{\"1\":{\"1241\":1}}],[\"gates\",{\"1\":{\"1233\":1}}],[\"gated\",{\"1\":{\"1065\":1,\"1243\":1,\"1576\":1,\"1577\":1,\"1862\":1,\"1871\":3,\"1873\":3,\"1880\":1}}],[\"gate\",{\"1\":{\"919\":1,\"1140\":1,\"1151\":1,\"1153\":3,\"1169\":1,\"1243\":1,\"1862\":2,\"1880\":2,\"2440\":1,\"2564\":1}}],[\"gatherable\",{\"0\":{\"2153\":1},\"1\":{\"2153\":2}}],[\"gathering\",{\"1\":{\"1964\":1}}],[\"gather\",{\"1\":{\"628\":1}}],[\"gaussianupsampling\",{\"0\":{\"1842\":1,\"1986\":1},\"1\":{\"1842\":1,\"1986\":1}}],[\"gaussianfourierprojection\",{\"0\":{\"1573\":1},\"1\":{\"1573\":1}}],[\"gaussian\",{\"0\":{\"2273\":1},\"1\":{\"738\":1,\"1573\":1,\"1639\":1,\"1797\":1,\"1842\":1,\"1986\":2,\"2273\":3}}],[\"garcia\",{\"1\":{\"130\":1}}],[\"gao\",{\"1\":{\"130\":1,\"1572\":1}}],[\"gao2022euro\",{\"1\":{\"130\":1}}],[\"gamma=0\",{\"1\":{\"1130\":1}}],[\"gamma=1\",{\"1\":{\"770\":1}}],[\"gamma\",{\"1\":{\"23\":1,\"119\":1,\"249\":2,\"700\":2,\"770\":1,\"917\":2,\"1048\":2,\"1138\":2,\"1139\":2,\"1231\":1,\"2018\":2,\"2023\":2}}],[\"gcc\",{\"1\":{\"126\":1,\"132\":1,\"134\":1,\"2441\":1,\"2442\":1}}],[\"gcp\",{\"0\":{\"126\":1}}],[\"gnome\",{\"1\":{\"88\":1}}],[\"gumbel\",{\"1\":{\"2294\":2}}],[\"guard\",{\"1\":{\"1758\":1,\"1759\":1}}],[\"guaranteed\",{\"1\":{\"595\":1,\"745\":1,\"746\":1,\"2019\":1,\"2212\":1}}],[\"guarantees\",{\"1\":{\"79\":1,\"1895\":1,\"1900\":1}}],[\"gu\",{\"1\":{\"1462\":1,\"1463\":1}}],[\"guo\",{\"1\":{\"130\":1}}],[\"guidedmultiheadattentionloss\",{\"0\":{\"763\":1},\"1\":{\"763\":1}}],[\"guided\",{\"0\":{\"1999\":1},\"1\":{\"762\":5,\"763\":3,\"821\":6,\"826\":12,\"1999\":2,\"2095\":9,\"2263\":9,\"2264\":17}}],[\"guidedattentionloss\",{\"0\":{\"762\":1},\"1\":{\"762\":1,\"763\":1}}],[\"guide\",{\"0\":{\"2670\":1},\"1\":{\"45\":1,\"1409\":1}}],[\"ge\",{\"1\":{\"2568\":1}}],[\"gev\",{\"0\":{\"1704\":1,\"1717\":1},\"1\":{\"1704\":2,\"1717\":2}}],[\"geometry\",{\"1\":{\"1559\":1,\"1560\":1,\"1655\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1718\":1,\"1719\":1}}],[\"gelu\",{\"1\":{\"1115\":2,\"1177\":1,\"1241\":1}}],[\"german\",{\"1\":{\"461\":1}}],[\"getnframes\",{\"1\":{\"2592\":1}}],[\"getnchannels\",{\"1\":{\"2592\":1}}],[\"getframerate\",{\"1\":{\"2592\":1}}],[\"getsampwidth\",{\"1\":{\"2592\":1}}],[\"getpreferredencoding\",{\"1\":{\"2482\":1}}],[\"getusermedia\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"getting\",{\"1\":{\"623\":1,\"1662\":1,\"2468\":1}}],[\"getcwd\",{\"1\":{\"194\":1}}],[\"get\",{\"0\":{\"544\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"890\":1,\"1020\":1,\"1021\":1,\"1022\":1,\"1023\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1815\":1,\"1884\":2,\"1885\":2,\"1937\":1,\"1966\":1,\"2154\":1,\"2155\":2,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1,\"2232\":1,\"2312\":1,\"2322\":2,\"2324\":1,\"2352\":1,\"2353\":1},\"1\":{\"54\":1,\"132\":3,\"134\":1,\"148\":1,\"167\":1,\"173\":2,\"175\":2,\"178\":1,\"194\":2,\"196\":1,\"200\":1,\"203\":1,\"217\":2,\"224\":2,\"231\":6,\"234\":1,\"544\":3,\"629\":2,\"640\":1,\"641\":3,\"642\":1,\"643\":1,\"650\":1,\"672\":2,\"676\":5,\"678\":1,\"691\":1,\"696\":1,\"697\":1,\"706\":2,\"709\":5,\"725\":1,\"742\":5,\"781\":1,\"794\":1,\"797\":1,\"799\":1,\"806\":1,\"815\":1,\"817\":1,\"824\":1,\"825\":3,\"829\":2,\"883\":1,\"884\":1,\"885\":1,\"886\":2,\"887\":1,\"888\":2,\"889\":1,\"890\":2,\"918\":1,\"981\":2,\"1020\":1,\"1021\":1,\"1022\":1,\"1023\":1,\"1024\":1,\"1046\":1,\"1069\":2,\"1071\":4,\"1073\":1,\"1079\":1,\"1083\":1,\"1096\":1,\"1097\":1,\"1098\":2,\"1099\":2,\"1137\":1,\"1191\":1,\"1192\":1,\"1195\":1,\"1198\":1,\"1201\":1,\"1221\":1,\"1270\":1,\"1319\":2,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":2,\"1324\":1,\"1355\":1,\"1389\":1,\"1392\":1,\"1395\":1,\"1397\":1,\"1402\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1414\":1,\"1416\":1,\"1526\":1,\"1604\":1,\"1647\":2,\"1696\":1,\"1697\":2,\"1698\":1,\"1699\":2,\"1700\":2,\"1701\":2,\"1702\":2,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":2,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":1,\"1715\":1,\"1716\":2,\"1752\":1,\"1815\":1,\"1884\":3,\"1885\":3,\"1897\":1,\"1917\":1,\"1937\":1,\"1961\":1,\"1966\":1,\"1982\":1,\"1986\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2023\":1,\"2026\":1,\"2028\":1,\"2084\":1,\"2089\":2,\"2099\":2,\"2123\":1,\"2128\":1,\"2135\":1,\"2154\":7,\"2155\":2,\"2176\":2,\"2193\":6,\"2199\":2,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":2,\"2217\":2,\"2218\":2,\"2219\":1,\"2232\":2,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2294\":1,\"2312\":2,\"2322\":3,\"2323\":1,\"2324\":2,\"2352\":2,\"2353\":2,\"2372\":1,\"2405\":1,\"2411\":1,\"2419\":1,\"2420\":1,\"2429\":1,\"2441\":2,\"2500\":1,\"2514\":1,\"2541\":1,\"2542\":1,\"2543\":2,\"2564\":1,\"2573\":1,\"2617\":1,\"2635\":1,\"2659\":1}}],[\"generic\",{\"1\":{\"1252\":1,\"1254\":1,\"2350\":1}}],[\"generalization\",{\"1\":{\"2543\":1}}],[\"generalize\",{\"1\":{\"2473\":1}}],[\"generalized\",{\"0\":{\"1695\":1},\"1\":{\"1695\":4,\"1704\":2}}],[\"generally\",{\"1\":{\"1142\":1,\"1155\":1,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1177\":2,\"1186\":1,\"1209\":1,\"1210\":1,\"1245\":2,\"1247\":1,\"1248\":1,\"1252\":2,\"1253\":4,\"1254\":2,\"1279\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1304\":1,\"2385\":1,\"2564\":1}}],[\"general\",{\"0\":{\"113\":1,\"848\":1,\"849\":1},\"1\":{\"57\":1,\"140\":1,\"150\":1,\"162\":1,\"163\":2,\"244\":1,\"848\":2,\"849\":2,\"853\":1,\"1778\":1,\"1852\":1,\"2372\":1,\"2384\":1,\"2387\":1,\"2429\":1,\"2452\":1,\"2554\":1}}],[\"generator=true\",{\"1\":{\"2170\":1}}],[\"generatoraversarialloss\",{\"1\":{\"1843\":1}}],[\"generatoradversarialloss\",{\"0\":{\"1843\":1},\"1\":{\"1843\":1}}],[\"generator\",{\"0\":{\"1776\":1,\"1777\":1,\"1797\":1,\"1804\":1,\"1851\":1,\"1878\":1,\"2285\":2,\"2292\":2,\"2299\":2},\"1\":{\"238\":2,\"678\":1,\"1013\":1,\"1760\":2,\"1765\":1,\"1773\":4,\"1776\":2,\"1777\":3,\"1778\":11,\"1797\":2,\"1800\":1,\"1804\":10,\"1805\":20,\"1828\":2,\"1836\":1,\"1837\":4,\"1839\":1,\"1843\":3,\"1844\":1,\"1850\":31,\"1851\":29,\"1852\":11,\"1857\":1,\"1862\":1,\"1871\":1,\"1877\":18,\"1878\":3,\"2170\":14,\"2186\":2,\"2204\":2,\"2285\":2,\"2292\":3,\"2294\":2,\"2299\":2,\"2302\":1,\"2305\":1,\"2432\":1}}],[\"generated\",{\"1\":{\"84\":1,\"235\":1,\"241\":1,\"242\":2,\"820\":1,\"835\":2,\"1269\":1,\"1407\":2,\"1778\":3,\"1804\":2,\"1805\":2,\"1850\":1,\"1851\":1,\"1852\":2,\"1859\":1,\"1877\":2,\"1878\":2,\"2099\":1,\"2102\":1,\"2254\":1,\"2259\":2,\"2301\":1,\"2302\":1,\"2303\":1,\"2365\":1,\"2401\":2,\"2508\":1,\"2510\":1,\"2515\":1,\"2537\":2,\"2556\":1,\"2655\":1,\"2660\":1}}],[\"generates\",{\"1\":{\"25\":1,\"695\":1,\"696\":1,\"706\":2,\"734\":2,\"773\":2,\"796\":1,\"815\":1,\"817\":1,\"828\":2,\"1133\":1,\"1190\":2,\"1214\":1,\"1221\":1,\"1244\":2,\"1273\":1,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"2001\":1,\"2078\":1}}],[\"generate\",{\"0\":{\"12\":1,\"282\":1,\"541\":1},\"1\":{\"10\":1,\"12\":1,\"25\":1,\"54\":1,\"135\":1,\"202\":1,\"235\":2,\"241\":1,\"242\":2,\"243\":1,\"282\":2,\"295\":1,\"541\":3,\"754\":1,\"778\":1,\"820\":1,\"821\":1,\"826\":1,\"835\":2,\"914\":1,\"1198\":1,\"1375\":1,\"1384\":1,\"1386\":1,\"1429\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1664\":1,\"1665\":1,\"1776\":1,\"1808\":1,\"1899\":1,\"1912\":1,\"1961\":2,\"1962\":1,\"2002\":1,\"2005\":1,\"2079\":1,\"2090\":1,\"2095\":1,\"2125\":1,\"2243\":1,\"2244\":1,\"2254\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2373\":1,\"2401\":1,\"2433\":1,\"2492\":1,\"2537\":1,\"2555\":1,\"2628\":1,\"2638\":1}}],[\"generation\",{\"0\":{\"9\":1,\"209\":1,\"404\":1},\"1\":{\"46\":1,\"98\":1,\"295\":2,\"384\":1,\"835\":2,\"2079\":1}}],[\"genration\",{\"1\":{\"835\":1}}],[\"gen\",{\"0\":{\"1384\":1,\"1386\":1},\"1\":{\"202\":1,\"1198\":1,\"1384\":1,\"1386\":1,\"2002\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1}}],[\"gengeration\",{\"1\":{\"49\":1}}],[\"grup\",{\"1\":{\"251\":2}}],[\"gru\",{\"0\":{\"1816\":1},\"1\":{\"251\":2,\"701\":3,\"821\":2,\"1532\":1,\"1534\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1581\":1,\"1650\":1,\"1671\":1,\"1816\":2,\"1850\":2,\"1851\":6,\"1852\":2,\"2095\":6,\"2243\":6,\"2244\":6,\"2255\":6,\"2257\":7,\"2261\":6,\"2263\":6,\"2264\":6,\"2558\":1}}],[\"grrifin\",{\"1\":{\"242\":2}}],[\"griffin\",{\"0\":{\"2317\":1,\"2325\":2,\"2330\":1},\"1\":{\"235\":1,\"242\":1,\"275\":2,\"519\":1,\"2317\":3,\"2325\":5,\"2330\":2,\"2506\":1,\"2510\":2}}],[\"gridnet\",{\"1\":{\"1660\":2,\"1661\":2,\"1662\":2,\"2497\":1}}],[\"gridnetv3block\",{\"0\":{\"1580\":1},\"1\":{\"1580\":1}}],[\"gridnetv2block\",{\"0\":{\"1579\":1},\"1\":{\"1579\":2,\"1580\":1}}],[\"gridnetblock\",{\"0\":{\"1578\":1},\"1\":{\"1578\":2}}],[\"grid\",{\"0\":{\"1018\":1,\"1021\":1,\"1022\":1},\"1\":{\"142\":2,\"174\":1,\"1018\":2,\"1021\":1,\"1022\":1,\"1025\":3,\"1228\":1,\"1345\":1,\"1347\":1}}],[\"grammatek\",{\"1\":{\"2125\":1}}],[\"grab\",{\"1\":{\"1327\":1}}],[\"graphs\",{\"1\":{\"608\":1}}],[\"graph\",{\"1\":{\"429\":2,\"485\":2,\"759\":4,\"1136\":1,\"2186\":1,\"2202\":2,\"2204\":1,\"2237\":1}}],[\"gratis\",{\"1\":{\"245\":2,\"301\":2}}],[\"graves\",{\"1\":{\"119\":1}}],[\"gradescope\",{\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":2,\"2487\":1,\"2491\":1,\"2497\":1,\"2500\":1,\"2501\":1,\"2503\":1,\"2525\":1,\"2545\":1,\"2560\":1}}],[\"gradmultiply\",{\"0\":{\"1187\":1},\"1\":{\"1187\":1}}],[\"grad=1\",{\"1\":{\"627\":1,\"727\":1,\"728\":1,\"742\":2,\"976\":1,\"1043\":1}}],[\"gradclip\",{\"1\":{\"253\":2}}],[\"grad3\",{\"1\":{\"110\":3}}],[\"gradscaler\",{\"1\":{\"2185\":1,\"2201\":2,\"2203\":1}}],[\"grads\",{\"1\":{\"80\":1,\"746\":1,\"1142\":2,\"1186\":3,\"1187\":1,\"1202\":1,\"1210\":3,\"1286\":1,\"1287\":1,\"1301\":3,\"1304\":3,\"1337\":2,\"1349\":2,\"1350\":2}}],[\"grad\",{\"0\":{\"1301\":1,\"1304\":1},\"1\":{\"80\":6,\"174\":4,\"218\":1,\"225\":1,\"232\":1,\"251\":6,\"253\":2,\"255\":4,\"259\":6,\"265\":4,\"269\":4,\"604\":2,\"627\":3,\"727\":1,\"728\":1,\"745\":2,\"746\":1,\"759\":2,\"792\":1,\"976\":2,\"1043\":2,\"1086\":6,\"1115\":2,\"1142\":6,\"1145\":2,\"1179\":1,\"1180\":1,\"1186\":2,\"1187\":3,\"1202\":3,\"1210\":2,\"1269\":2,\"1286\":3,\"1287\":3,\"1301\":1,\"1304\":1,\"1376\":1,\"2186\":4,\"2202\":8,\"2204\":4,\"2302\":1,\"2365\":1,\"2440\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2558\":1,\"2584\":2,\"2655\":1,\"2660\":1}}],[\"gradients\",{\"1\":{\"80\":2,\"632\":1,\"745\":11,\"746\":11,\"1142\":1,\"1145\":1,\"1186\":2,\"1210\":2,\"1269\":1,\"1301\":3,\"1304\":3,\"1688\":1,\"1756\":1,\"2151\":1}}],[\"gradient\",{\"0\":{\"80\":1,\"632\":1,\"2151\":2,\"2302\":1},\"1\":{\"80\":2,\"627\":1,\"632\":2,\"727\":1,\"728\":1,\"745\":2,\"746\":2,\"759\":2,\"792\":1,\"976\":1,\"1043\":1,\"1086\":5,\"1142\":1,\"1186\":1,\"1187\":5,\"1202\":5,\"1210\":1,\"1211\":1,\"1224\":1,\"1286\":5,\"1287\":5,\"1301\":1,\"1304\":1,\"1337\":2,\"1349\":2,\"1350\":2,\"1850\":2,\"1851\":6,\"1852\":2,\"2151\":2,\"2244\":6,\"2255\":6,\"2279\":6,\"2302\":2,\"2440\":1,\"2558\":1}}],[\"ground\",{\"1\":{\"710\":2,\"1106\":1,\"1107\":1,\"1108\":1,\"1142\":1,\"1155\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1767\":1,\"1804\":2,\"1808\":3,\"1851\":3,\"1877\":1,\"1878\":1,\"2499\":1,\"2617\":1,\"2635\":1}}],[\"groundtruth\",{\"1\":{\"98\":1,\"175\":1,\"194\":1,\"600\":1,\"633\":1,\"702\":1,\"710\":1,\"738\":1,\"821\":1,\"1836\":1,\"1839\":1,\"1859\":3,\"2090\":1,\"2243\":2,\"2244\":4,\"2255\":4,\"2259\":1,\"2279\":4}}],[\"group=1\",{\"1\":{\"2042\":1}}],[\"groupnorm\",{\"1\":{\"1531\":1,\"1645\":1}}],[\"grouped\",{\"1\":{\"1522\":4,\"1523\":4,\"1572\":6,\"1688\":1,\"1756\":1}}],[\"group\",{\"1\":{\"115\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"853\":1,\"1136\":1,\"1145\":1,\"1269\":2,\"1572\":1,\"1670\":7,\"1671\":6,\"2122\":1}}],[\"groups=2\",{\"1\":{\"1522\":1,\"1572\":1}}],[\"groups=1\",{\"1\":{\"1478\":1,\"1480\":1,\"2074\":1,\"2075\":1}}],[\"groups=0\",{\"1\":{\"1198\":1}}],[\"groups=none\",{\"1\":{\"1198\":1}}],[\"groups\",{\"0\":{\"1973\":1,\"1974\":1},\"1\":{\"21\":2,\"115\":1,\"712\":2,\"792\":2,\"952\":1,\"1052\":2,\"1115\":2,\"1198\":4,\"1269\":3,\"1522\":2,\"1523\":3,\"1572\":2,\"1670\":2,\"1671\":2,\"1778\":1,\"1801\":1,\"1805\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1846\":1,\"1847\":1,\"1849\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1973\":1,\"1974\":1}}],[\"great\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"greater\",{\"1\":{\"26\":1}}],[\"green\",{\"1\":{\"2500\":2,\"2617\":2,\"2635\":2}}],[\"green=lambda\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"greek\",{\"1\":{\"461\":1}}],[\"greedy\",{\"1\":{\"23\":1,\"443\":2,\"700\":2,\"742\":1,\"1138\":5,\"1139\":2}}],[\"gres\",{\"1\":{\"40\":1}}],[\"gist\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"gin\",{\"1\":{\"1806\":1,\"1808\":3}}],[\"giving\",{\"1\":{\"25\":1,\"79\":1}}],[\"give\",{\"1\":{\"25\":2,\"64\":1,\"76\":1,\"79\":1,\"80\":1,\"92\":2,\"96\":1,\"135\":2,\"1013\":1,\"1962\":1}}],[\"given\",{\"1\":{\"17\":1,\"30\":1,\"57\":1,\"59\":2,\"62\":1,\"96\":1,\"241\":1,\"564\":1,\"596\":2,\"602\":1,\"603\":1,\"605\":2,\"622\":1,\"649\":1,\"652\":1,\"657\":1,\"700\":1,\"710\":2,\"725\":1,\"745\":4,\"746\":4,\"754\":1,\"806\":1,\"820\":1,\"821\":1,\"824\":1,\"826\":1,\"855\":1,\"856\":1,\"918\":1,\"923\":1,\"937\":1,\"938\":1,\"981\":1,\"1001\":1,\"1034\":1,\"1046\":2,\"1048\":1,\"1060\":1,\"1066\":2,\"1073\":1,\"1075\":2,\"1083\":1,\"1098\":1,\"1102\":1,\"1138\":1,\"1139\":1,\"1143\":1,\"1187\":1,\"1198\":1,\"1202\":1,\"1270\":1,\"1286\":1,\"1287\":1,\"1375\":1,\"1406\":4,\"1407\":1,\"1603\":1,\"1618\":1,\"1619\":1,\"1693\":2,\"1752\":1,\"1755\":2,\"1785\":1,\"1897\":1,\"1905\":2,\"1924\":1,\"1933\":1,\"1934\":1,\"1937\":1,\"2002\":1,\"2079\":1,\"2090\":1,\"2095\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2156\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2347\":1,\"2400\":1,\"2471\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2536\":1}}],[\"gives\",{\"1\":{\"17\":1,\"59\":1,\"1928\":1}}],[\"git+https\",{\"1\":{\"2466\":1,\"2482\":1,\"2646\":1}}],[\"git\",{\"1\":{\"5\":2,\"99\":1,\"134\":2,\"135\":2,\"167\":3,\"178\":3,\"196\":3,\"200\":3,\"207\":4,\"234\":3,\"2359\":2,\"2372\":1,\"2383\":2,\"2384\":2,\"2393\":2,\"2409\":1,\"2427\":2,\"2429\":3,\"2441\":1,\"2442\":1,\"2450\":6,\"2456\":2,\"2466\":2,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2504\":2,\"2517\":6,\"2521\":2,\"2529\":3,\"2550\":3,\"2580\":2,\"2584\":1,\"2585\":2,\"2646\":2,\"2648\":1,\"2649\":1}}],[\"github\",{\"1\":{\"5\":1,\"13\":1,\"47\":1,\"97\":1,\"134\":2,\"135\":1,\"166\":3,\"167\":3,\"177\":3,\"178\":3,\"195\":3,\"196\":3,\"199\":3,\"200\":1,\"202\":1,\"206\":2,\"207\":1,\"233\":3,\"234\":3,\"295\":2,\"628\":1,\"668\":1,\"734\":1,\"740\":1,\"741\":1,\"771\":1,\"772\":1,\"775\":1,\"776\":1,\"809\":1,\"810\":1,\"817\":1,\"1037\":1,\"1047\":1,\"1076\":1,\"1101\":1,\"1102\":1,\"1115\":1,\"1148\":1,\"1180\":1,\"1203\":2,\"1214\":1,\"1215\":1,\"1284\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1454\":2,\"1605\":2,\"1695\":1,\"1717\":1,\"1735\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"1860\":1,\"1917\":1,\"1958\":1,\"1986\":1,\"2054\":1,\"2125\":1,\"2131\":2,\"2154\":1,\"2259\":1,\"2294\":1,\"2309\":1,\"2324\":1,\"2354\":3,\"2355\":2,\"2359\":1,\"2360\":1,\"2361\":2,\"2366\":1,\"2372\":5,\"2377\":3,\"2378\":1,\"2379\":1,\"2382\":2,\"2383\":1,\"2384\":4,\"2387\":1,\"2390\":3,\"2393\":2,\"2394\":1,\"2409\":1,\"2414\":1,\"2424\":3,\"2427\":1,\"2429\":6,\"2430\":1,\"2436\":1,\"2437\":1,\"2446\":2,\"2450\":3,\"2456\":1,\"2458\":1,\"2466\":2,\"2482\":1,\"2499\":1,\"2504\":1,\"2517\":3,\"2521\":1,\"2523\":1,\"2526\":3,\"2529\":2,\"2530\":1,\"2547\":3,\"2550\":1,\"2552\":1,\"2554\":2,\"2555\":1,\"2558\":1,\"2562\":1,\"2563\":1,\"2564\":1,\"2573\":1,\"2574\":1,\"2575\":1,\"2576\":1,\"2580\":1,\"2582\":1,\"2584\":1,\"2587\":1,\"2597\":2,\"2601\":1,\"2618\":1,\"2635\":1,\"2646\":3,\"2650\":2}}],[\"gobal\",{\"1\":{\"2440\":1}}],[\"gov\",{\"1\":{\"1515\":1}}],[\"goes\",{\"1\":{\"632\":1,\"2151\":1}}],[\"got\",{\"1\":{\"174\":1}}],[\"googledrive\",{\"1\":{\"2368\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"google\",{\"0\":{\"277\":1},\"1\":{\"135\":1,\"198\":1,\"199\":1,\"200\":2,\"202\":1,\"210\":2,\"211\":2,\"212\":2,\"214\":2,\"215\":2,\"216\":2,\"222\":4,\"223\":4,\"229\":4,\"230\":4,\"277\":3,\"295\":1,\"952\":1,\"2360\":1,\"2363\":4,\"2373\":1,\"2396\":1,\"2433\":1,\"2458\":1,\"2501\":1,\"2506\":4,\"2522\":1,\"2523\":1,\"2532\":1,\"2555\":1,\"2567\":1,\"2581\":1,\"2582\":1,\"2584\":1,\"2593\":3,\"2600\":2,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1,\"2653\":4}}],[\"good\",{\"1\":{\"62\":1,\"198\":1,\"1639\":1,\"2050\":1,\"2558\":1}}],[\"going\",{\"1\":{\"54\":2,\"2400\":1,\"2450\":1,\"2536\":1}}],[\"go\",{\"1\":{\"5\":1,\"70\":1,\"79\":1,\"186\":1,\"197\":1,\"198\":1,\"235\":1,\"2046\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2393\":1,\"2427\":1,\"2429\":1,\"2529\":1,\"2550\":1,\"2554\":1}}],[\"gtn\",{\"0\":{\"759\":1},\"1\":{\"759\":5}}],[\"gtnctclossfunction\",{\"0\":{\"759\":1},\"1\":{\"759\":1}}],[\"gtnctc\",{\"1\":{\"251\":1,\"259\":1,\"1145\":1}}],[\"gtwavdir\",{\"1\":{\"562\":2}}],[\"gt\",{\"1\":{\"3\":4,\"10\":1,\"12\":1,\"36\":2,\"38\":6,\"39\":3,\"44\":3,\"51\":4,\"52\":6,\"57\":3,\"66\":4,\"80\":1,\"85\":2,\"88\":2,\"96\":3,\"97\":1,\"98\":3,\"99\":5,\"127\":2,\"128\":1,\"134\":7,\"135\":13,\"136\":6,\"137\":1,\"175\":5,\"194\":5,\"203\":4,\"217\":3,\"224\":3,\"231\":2,\"237\":11,\"238\":3,\"239\":3,\"274\":2,\"275\":17,\"276\":11,\"277\":5,\"278\":4,\"279\":11,\"280\":3,\"281\":8,\"282\":13,\"283\":10,\"284\":10,\"285\":12,\"286\":10,\"290\":2,\"292\":1,\"294\":5,\"295\":1,\"296\":8,\"297\":10,\"298\":6,\"1804\":4,\"1808\":2,\"2373\":6,\"2375\":1,\"2430\":3,\"2431\":1,\"2555\":6,\"2559\":1,\"2567\":1,\"2584\":5,\"2637\":2,\"2638\":3,\"2643\":6}}],[\"g\",{\"1\":{\"1\":2,\"3\":1,\"5\":1,\"18\":1,\"19\":2,\"21\":5,\"25\":7,\"27\":1,\"28\":2,\"38\":1,\"45\":3,\"49\":1,\"57\":2,\"58\":1,\"59\":1,\"60\":2,\"63\":1,\"64\":3,\"75\":2,\"80\":1,\"85\":2,\"90\":1,\"91\":1,\"92\":2,\"98\":1,\"99\":1,\"102\":1,\"108\":3,\"112\":1,\"127\":1,\"135\":2,\"136\":5,\"144\":7,\"150\":1,\"168\":1,\"169\":4,\"179\":1,\"181\":4,\"235\":2,\"240\":1,\"274\":1,\"275\":1,\"276\":1,\"277\":1,\"279\":1,\"280\":2,\"281\":1,\"283\":1,\"284\":1,\"286\":1,\"294\":3,\"296\":1,\"297\":1,\"298\":1,\"528\":1,\"564\":1,\"665\":1,\"689\":1,\"691\":1,\"693\":1,\"697\":1,\"794\":2,\"797\":1,\"835\":1,\"838\":1,\"857\":1,\"872\":1,\"873\":1,\"874\":1,\"909\":1,\"1012\":1,\"1015\":3,\"1132\":1,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1177\":1,\"1187\":1,\"1202\":1,\"1211\":1,\"1244\":1,\"1245\":2,\"1252\":2,\"1253\":3,\"1254\":2,\"1279\":2,\"1286\":2,\"1287\":1,\"1327\":2,\"1336\":1,\"1337\":1,\"1356\":1,\"1375\":2,\"1454\":2,\"1463\":1,\"1505\":1,\"1515\":2,\"1516\":1,\"1523\":1,\"1528\":2,\"1529\":1,\"1530\":1,\"1534\":1,\"1539\":1,\"1543\":1,\"1551\":1,\"1553\":1,\"1558\":1,\"1618\":1,\"1619\":1,\"1626\":1,\"1638\":2,\"1645\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":1,\"1671\":1,\"1719\":2,\"1739\":2,\"1761\":2,\"1763\":2,\"1765\":2,\"1768\":2,\"1771\":1,\"1772\":1,\"1773\":1,\"1778\":1,\"1785\":1,\"1788\":1,\"1800\":1,\"1803\":2,\"1805\":3,\"1808\":2,\"1833\":2,\"1835\":2,\"1837\":1,\"1844\":4,\"1850\":1,\"1852\":1,\"1863\":2,\"1864\":2,\"1865\":2,\"1868\":2,\"1877\":1,\"1880\":2,\"1930\":1,\"1932\":1,\"1946\":1,\"1947\":1,\"1962\":1,\"2011\":1,\"2012\":1,\"2046\":2,\"2050\":1,\"2090\":1,\"2099\":1,\"2102\":1,\"2155\":5,\"2170\":1,\"2193\":2,\"2275\":1,\"2373\":1,\"2378\":1,\"2385\":2,\"2386\":2,\"2394\":1,\"2414\":1,\"2437\":1,\"2481\":1,\"2501\":1,\"2514\":1,\"2530\":1,\"2555\":1,\"2563\":1,\"2574\":1,\"2584\":1,\"2585\":2,\"2618\":1,\"2639\":1,\"2641\":1,\"2643\":1,\"2659\":1}}],[\"vpsde\",{\"1\":{\"2255\":1,\"2260\":2}}],[\"vuv\",{\"1\":{\"1778\":1,\"2090\":3,\"2091\":3}}],[\"vuepress\",{\"1\":{\"528\":2,\"564\":1,\"2155\":1}}],[\"vbar^h\",{\"1\":{\"1698\":1}}],[\"vbar\",{\"1\":{\"1698\":2}}],[\"vboxnet\",{\"1\":{\"45\":2}}],[\"vram\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"vrv9glv7trc0w\",{\"1\":{\"200\":1}}],[\"v=none\",{\"1\":{\"1341\":1}}],[\"vs\",{\"1\":{\"1198\":1,\"1244\":2,\"1252\":2,\"2091\":2}}],[\"vjp\",{\"1\":{\"1187\":2,\"1202\":2,\"1286\":1,\"1287\":1}}],[\"v+1\",{\"1\":{\"1142\":4,\"1144\":1,\"1154\":1,\"1155\":1,\"1186\":4,\"1210\":1,\"1228\":3,\"1298\":2,\"1299\":2,\"1301\":3,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":2,\"1345\":3,\"1347\":3,\"1349\":2,\"1350\":2}}],[\"v6\",{\"1\":{\"628\":1}}],[\"vctk\",{\"1\":{\"2512\":17,\"2657\":17}}],[\"vc\",{\"0\":{\"267\":1,\"269\":1,\"821\":1,\"1041\":2,\"1042\":2,\"1043\":2,\"1044\":2,\"1045\":2,\"2681\":1},\"1\":{\"267\":3,\"269\":3,\"821\":4,\"1041\":2,\"1042\":2,\"1043\":4,\"1044\":5,\"1045\":5}}],[\"v4\",{\"1\":{\"212\":1,\"528\":1}}],[\"v3qrmlixf\",{\"1\":{\"2506\":1}}],[\"v3\",{\"1\":{\"210\":1,\"211\":1,\"212\":1,\"215\":1,\"230\":1,\"295\":9,\"528\":1,\"2454\":1,\"2455\":2}}],[\"v13\",{\"1\":{\"1400\":1}}],[\"v1\",{\"1\":{\"150\":1,\"197\":1,\"198\":1,\"201\":1,\"216\":1,\"222\":1,\"223\":1,\"229\":2,\"230\":1,\"249\":1,\"257\":1,\"261\":1,\"282\":1,\"286\":13,\"295\":30,\"296\":5,\"2363\":9,\"2506\":1,\"2510\":1,\"2512\":6,\"2653\":9,\"2657\":6}}],[\"vowels\",{\"1\":{\"2126\":1,\"2142\":3}}],[\"vowels=false\",{\"1\":{\"2122\":1}}],[\"volume\",{\"1\":{\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2200\":1}}],[\"volumeperturbation\",{\"0\":{\"961\":1},\"1\":{\"961\":1}}],[\"vol\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"vocie\",{\"1\":{\"2458\":1,\"2523\":1}}],[\"vocaburary\",{\"1\":{\"2349\":2}}],[\"vocabulary=true\",{\"1\":{\"2348\":1}}],[\"vocabulary\",{\"0\":{\"375\":1,\"463\":1,\"491\":1},\"1\":{\"375\":3,\"461\":4,\"491\":3,\"582\":1,\"606\":1,\"614\":1,\"691\":1,\"693\":1,\"697\":1,\"734\":1,\"743\":1,\"750\":1,\"767\":1,\"773\":1,\"797\":1,\"807\":1,\"814\":1,\"817\":1,\"828\":1,\"857\":1,\"1057\":2,\"1066\":1,\"1073\":1,\"1075\":1,\"1142\":2,\"1154\":1,\"1155\":1,\"1186\":2,\"1210\":2,\"1228\":2,\"1298\":3,\"1299\":3,\"1301\":3,\"1302\":3,\"1303\":3,\"1304\":3,\"1334\":3,\"1337\":2,\"1345\":2,\"1347\":2,\"1349\":1,\"1350\":1,\"1787\":1,\"1798\":1,\"1804\":1,\"1874\":1,\"1878\":1,\"2123\":1,\"2128\":1,\"2135\":1,\"2585\":2}}],[\"vocabs\",{\"1\":{\"1787\":2,\"1798\":2,\"1804\":2,\"1874\":2,\"1878\":2}}],[\"vocabsize\",{\"1\":{\"582\":2}}],[\"vocabrary\",{\"1\":{\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1877\":1}}],[\"vocab=true\",{\"1\":{\"609\":1,\"611\":1}}],[\"vocab\",{\"1\":{\"606\":1,\"614\":2,\"676\":2,\"691\":5,\"692\":1,\"693\":2,\"695\":1,\"696\":1,\"697\":3,\"698\":1,\"699\":1,\"733\":1,\"734\":3,\"767\":1,\"768\":2,\"773\":4,\"781\":1,\"797\":3,\"807\":2,\"815\":1,\"817\":3,\"828\":4,\"857\":2,\"1057\":2,\"1060\":1,\"1066\":2,\"1073\":2,\"1075\":2,\"1083\":2,\"1133\":3,\"1142\":1,\"1167\":1,\"1168\":1,\"1171\":1,\"1172\":2,\"1186\":1,\"1190\":3,\"1196\":1,\"1197\":1,\"1204\":1,\"1206\":1,\"1210\":1,\"1214\":2,\"1220\":1,\"1221\":1,\"1244\":4,\"1270\":2,\"1271\":1,\"1273\":2,\"1298\":1,\"1299\":1,\"1301\":1,\"1304\":1,\"1787\":1,\"1892\":1,\"1893\":1,\"1953\":1,\"1955\":1,\"1957\":3,\"1958\":2,\"1959\":1,\"1960\":3,\"1970\":2,\"1975\":1,\"1984\":3,\"1996\":1,\"2001\":2,\"2004\":2,\"2027\":1,\"2076\":2,\"2294\":1,\"2349\":2}}],[\"vocoders\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"vocoder\",{\"0\":{\"213\":1,\"406\":1,\"460\":1,\"469\":1,\"475\":1,\"1766\":1,\"1769\":1,\"1776\":1,\"1777\":1,\"1779\":1,\"1785\":1,\"1786\":1,\"1799\":1,\"1801\":1,\"1803\":1,\"1810\":1,\"2254\":1},\"1\":{\"214\":1,\"215\":1,\"216\":1,\"217\":6,\"218\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":6,\"225\":1,\"229\":1,\"230\":1,\"231\":6,\"232\":1,\"282\":1,\"295\":4,\"399\":6,\"455\":4,\"464\":6,\"470\":6,\"541\":1,\"1766\":1,\"1769\":1,\"1776\":1,\"1777\":1,\"1778\":10,\"1779\":1,\"1785\":1,\"1786\":1,\"1799\":1,\"1801\":1,\"1803\":1,\"1804\":3,\"1805\":5,\"1810\":1,\"1850\":2,\"1852\":10,\"1877\":2,\"1947\":1,\"1980\":2,\"1985\":2,\"1988\":1,\"1990\":1,\"1992\":1,\"2077\":2,\"2087\":1,\"2090\":1,\"2095\":1,\"2109\":3,\"2113\":3,\"2115\":3,\"2116\":3,\"2235\":2,\"2236\":1,\"2243\":1,\"2244\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2254\":3,\"2255\":1,\"2263\":1,\"2264\":1,\"2277\":2,\"2279\":1,\"2363\":5,\"2364\":2,\"2506\":3,\"2507\":2,\"2510\":7,\"2512\":1,\"2513\":2,\"2519\":2,\"2520\":2,\"2653\":5,\"2654\":2,\"2657\":1,\"2658\":2}}],[\"voiced\",{\"1\":{\"1797\":4}}],[\"voice\",{\"1\":{\"130\":1,\"269\":1,\"758\":1,\"1645\":1,\"1773\":1,\"2003\":2,\"2078\":1,\"2079\":1,\"2081\":1,\"2082\":1,\"2083\":2,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":3,\"2103\":1,\"2196\":1,\"2210\":1,\"2360\":2,\"2458\":1,\"2467\":1,\"2512\":1,\"2522\":1,\"2523\":1,\"2581\":1,\"2582\":2,\"2657\":1}}],[\"vorbis\",{\"1\":{\"49\":1,\"1927\":1}}],[\"v\",{\"1\":{\"116\":2,\"135\":2,\"144\":2,\"207\":2,\"231\":1,\"686\":4,\"687\":4,\"688\":4,\"689\":4,\"1001\":3,\"1065\":4,\"1066\":2,\"1076\":2,\"1210\":1,\"1302\":1,\"1303\":1,\"1304\":2,\"1334\":3,\"1337\":2,\"1339\":5,\"1341\":2,\"1528\":1,\"1797\":1,\"2086\":1,\"2087\":1,\"2091\":2,\"2193\":2,\"2230\":1,\"2253\":1,\"2514\":2,\"2659\":2}}],[\"vez\",{\"1\":{\"2457\":1}}],[\"vesion\",{\"0\":{\"2450\":1}}],[\"vec\",{\"1\":{\"1136\":1,\"1695\":4}}],[\"vectorizing\",{\"1\":{\"1661\":1}}],[\"vectorized\",{\"1\":{\"694\":1,\"705\":1}}],[\"vector\",{\"0\":{\"854\":1,\"885\":1,\"1678\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1715\":1},\"1\":{\"235\":1,\"605\":2,\"710\":1,\"711\":8,\"712\":1,\"730\":1,\"731\":1,\"754\":1,\"782\":1,\"785\":1,\"793\":2,\"809\":1,\"821\":1,\"826\":1,\"854\":2,\"875\":1,\"885\":5,\"919\":1,\"1052\":1,\"1076\":1,\"1081\":2,\"1086\":4,\"1186\":4,\"1209\":1,\"1210\":4,\"1244\":1,\"1298\":2,\"1299\":2,\"1301\":3,\"1302\":3,\"1303\":3,\"1304\":4,\"1337\":3,\"1349\":3,\"1350\":3,\"1515\":1,\"1525\":1,\"1528\":1,\"1529\":1,\"1566\":1,\"1595\":1,\"1678\":2,\"1680\":2,\"1696\":4,\"1697\":4,\"1698\":4,\"1701\":1,\"1702\":2,\"1703\":3,\"1704\":5,\"1705\":5,\"1706\":5,\"1707\":5,\"1708\":5,\"1712\":5,\"1713\":3,\"1715\":5,\"1717\":3,\"2070\":2,\"2244\":1,\"2255\":1,\"2279\":1,\"2514\":3,\"2659\":3}}],[\"vectors\",{\"1\":{\"104\":1,\"680\":3,\"683\":3,\"703\":1,\"735\":1,\"754\":2,\"821\":2,\"826\":2,\"875\":1,\"1011\":2,\"1149\":1,\"1150\":1,\"1529\":1,\"1568\":1,\"1717\":1,\"1932\":3,\"2068\":1,\"2070\":1,\"2514\":1,\"2659\":1}}],[\"venv\",{\"1\":{\"128\":1,\"135\":2,\"167\":2,\"178\":2,\"196\":2,\"200\":2,\"234\":2,\"528\":2,\"564\":1,\"2155\":1}}],[\"verify\",{\"0\":{\"934\":1,\"935\":1},\"1\":{\"658\":1,\"934\":2,\"935\":2}}],[\"verification\",{\"0\":{\"658\":1},\"1\":{\"70\":1,\"658\":1,\"2040\":1,\"2044\":1,\"2046\":1,\"2049\":1,\"2054\":1,\"2064\":1,\"2410\":1}}],[\"very\",{\"1\":{\"83\":1,\"785\":1,\"952\":1,\"2385\":1,\"2387\":1,\"2564\":1,\"2574\":1,\"2584\":1,\"2585\":1,\"2600\":1}}],[\"verbose=false\",{\"1\":{\"1241\":1,\"1245\":1,\"1248\":1,\"2022\":1}}],[\"verbose\",{\"1\":{\"17\":3,\"62\":1,\"74\":1,\"240\":2,\"245\":2,\"247\":2,\"249\":2,\"251\":2,\"253\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2,\"276\":1,\"281\":1,\"298\":1,\"493\":2,\"496\":2,\"506\":2,\"509\":2,\"512\":2,\"522\":2,\"525\":2,\"528\":2,\"533\":2,\"560\":2,\"564\":2,\"566\":2,\"585\":2}}],[\"version=none\",{\"1\":{\"2315\":1}}],[\"version=10\",{\"1\":{\"2372\":2,\"2584\":1}}],[\"version=11\",{\"1\":{\"135\":1,\"2394\":1,\"2409\":1,\"2429\":2,\"2530\":1,\"2552\":2}}],[\"version=1\",{\"1\":{\"135\":2,\"2394\":1,\"2409\":1,\"2429\":1,\"2530\":1,\"2552\":1}}],[\"version|default=none\",{\"1\":{\"135\":1}}],[\"versions\",{\"1\":{\"112\":1,\"2393\":2,\"2427\":2,\"2429\":1,\"2529\":2,\"2550\":2,\"2552\":1}}],[\"version\",{\"0\":{\"2160\":2},\"1\":{\"1\":2,\"5\":3,\"82\":1,\"112\":4,\"113\":1,\"119\":2,\"120\":1,\"135\":5,\"156\":1,\"161\":2,\"286\":2,\"296\":2,\"528\":1,\"628\":1,\"684\":1,\"685\":1,\"771\":1,\"772\":1,\"1048\":1,\"1245\":1,\"1247\":1,\"1400\":1,\"2019\":1,\"2160\":2,\"2429\":1,\"2441\":3,\"2442\":3,\"2450\":2,\"2482\":2,\"2552\":1,\"2567\":1}}],[\"vmnet\",{\"1\":{\"45\":2}}],[\"vggrnnencoder\",{\"0\":{\"1282\":1},\"1\":{\"1282\":2}}],[\"vgg2l\",{\"0\":{\"833\":1,\"890\":1},\"1\":{\"833\":1,\"890\":2}}],[\"vgggru\",{\"1\":{\"251\":2}}],[\"vgggrup\",{\"1\":{\"251\":2}}],[\"vggbgru\",{\"1\":{\"251\":2}}],[\"vggbgrup\",{\"1\":{\"251\":2}}],[\"vggblstm\",{\"1\":{\"251\":2}}],[\"vggblstmp\",{\"1\":{\"251\":2}}],[\"vgglstm\",{\"1\":{\"251\":2}}],[\"vgglstmp\",{\"1\":{\"251\":2}}],[\"vgg\",{\"0\":{\"1282\":1},\"1\":{\"31\":1,\"115\":2,\"833\":1,\"890\":1,\"1053\":3,\"1097\":3,\"1282\":1}}],[\"viterbi\",{\"0\":{\"1889\":1},\"1\":{\"1889\":1}}],[\"vitsgenerator\",{\"0\":{\"1878\":1},\"1\":{\"1878\":1}}],[\"vits\",{\"0\":{\"1771\":1,\"1772\":1,\"1781\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1798\":1,\"1804\":1,\"1805\":3,\"1826\":1,\"1833\":1,\"1835\":1,\"1838\":1,\"1840\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1868\":1,\"1874\":1,\"1875\":1,\"1877\":3,\"1878\":1,\"1882\":1,\"1886\":1,\"1887\":1,\"1888\":1},\"1\":{\"461\":1,\"1771\":1,\"1772\":1,\"1781\":1,\"1787\":1,\"1788\":1,\"1789\":2,\"1798\":1,\"1804\":2,\"1805\":10,\"1826\":2,\"1833\":2,\"1835\":1,\"1838\":1,\"1840\":2,\"1853\":2,\"1854\":1,\"1855\":2,\"1863\":2,\"1864\":1,\"1865\":1,\"1868\":1,\"1874\":2,\"1875\":2,\"1877\":11,\"1878\":4,\"1886\":1,\"1887\":1,\"1888\":1,\"2363\":13,\"2364\":2,\"2506\":3,\"2507\":4,\"2510\":5,\"2512\":4,\"2513\":4,\"2653\":13,\"2654\":2,\"2657\":4,\"2658\":2}}],[\"vincent\",{\"1\":{\"1132\":1}}],[\"visulizing\",{\"1\":{\"754\":1,\"820\":1,\"821\":1,\"826\":1}}],[\"visualize\",{\"1\":{\"629\":1,\"1198\":1,\"2416\":1}}],[\"visualization\",{\"1\":{\"629\":1}}],[\"vis\",{\"1\":{\"629\":2,\"799\":1}}],[\"vision\",{\"1\":{\"2030\":1}}],[\"visingergenerator\",{\"0\":{\"1804\":1},\"1\":{\"1804\":1}}],[\"visinger\",{\"1\":{\"1771\":2,\"1787\":1,\"1798\":1,\"1804\":2,\"1805\":1}}],[\"visinger2vocodergenerator\",{\"0\":{\"1803\":1},\"1\":{\"1803\":1}}],[\"visinger2discriminator\",{\"0\":{\"1801\":1},\"1\":{\"1801\":1}}],[\"visinger2\",{\"0\":{\"1766\":2,\"1769\":2,\"1776\":2,\"1777\":2,\"1779\":2,\"1785\":2,\"1786\":2,\"1799\":2,\"1801\":2,\"1803\":2,\"1809\":1,\"1810\":2,\"1812\":1,\"1813\":1,\"1814\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1822\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1827\":1},\"1\":{\"1766\":2,\"1769\":2,\"1776\":2,\"1777\":2,\"1779\":2,\"1785\":2,\"1786\":2,\"1791\":1,\"1799\":2,\"1801\":3,\"1803\":2,\"1804\":1,\"1809\":1,\"1810\":2,\"1812\":1,\"1813\":1,\"1814\":1,\"1816\":2,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":2,\"1821\":1,\"1822\":1,\"1823\":2,\"1824\":2,\"1825\":1,\"1827\":2}}],[\"visit\",{\"1\":{\"202\":1}}],[\"visible\",{\"1\":{\"19\":2,\"71\":2}}],[\"vietnamese\",{\"1\":{\"461\":1}}],[\"view\",{\"1\":{\"217\":1,\"218\":1,\"224\":1,\"225\":1,\"231\":1,\"232\":1,\"2365\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2593\":2,\"2655\":1,\"2660\":1}}],[\"viewer\",{\"1\":{\"88\":1}}],[\"viewpoint\",{\"1\":{\"74\":2}}],[\"via\",{\"1\":{\"109\":1,\"235\":1,\"238\":2,\"240\":1,\"241\":1,\"608\":1,\"1143\":1,\"1227\":1,\"1466\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1947\":1,\"2121\":1,\"2122\":1}}],[\"video\",{\"0\":{\"52\":1},\"1\":{\"1116\":4,\"1179\":1}}],[\"virbr\",{\"1\":{\"45\":2}}],[\"virtual\",{\"1\":{\"45\":3}}],[\"vivos\",{\"1\":{\"28\":2}}],[\"v0\",{\"1\":{\"26\":2}}],[\"vacaciones\",{\"1\":{\"2457\":1}}],[\"van\",{\"1\":{\"1705\":1}}],[\"vandermonde\",{\"1\":{\"1069\":2}}],[\"vadscpwriter\",{\"0\":{\"1411\":1},\"1\":{\"1411\":2,\"1412\":1}}],[\"vadscpreader\",{\"0\":{\"1409\":1},\"1\":{\"1409\":2,\"1410\":1}}],[\"vad\",{\"0\":{\"1409\":1,\"1411\":1},\"1\":{\"1409\":4,\"1411\":3,\"1412\":1}}],[\"vaswani\",{\"1\":{\"834\":2,\"1547\":1}}],[\"vaswanirule\",{\"0\":{\"834\":1},\"1\":{\"834\":1}}],[\"var=\",{\"1\":{\"1716\":1}}],[\"var=none\",{\"1\":{\"784\":1}}],[\"varoable\",{\"1\":{\"1301\":1,\"1304\":1}}],[\"vars=false\",{\"1\":{\"941\":1,\"960\":1}}],[\"vars\",{\"1\":{\"251\":2,\"259\":2,\"496\":2,\"752\":1,\"760\":1,\"831\":1,\"932\":2,\"959\":1,\"1906\":2,\"1920\":1,\"1949\":2,\"1986\":2}}],[\"var\",{\"1\":{\"238\":2,\"784\":3,\"1294\":1,\"1295\":1,\"1296\":1,\"1850\":2,\"1906\":1,\"2176\":7,\"2360\":1,\"2440\":3,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"variety\",{\"1\":{\"2452\":1,\"2585\":2}}],[\"varied\",{\"1\":{\"1897\":1}}],[\"variacne\",{\"1\":{\"2265\":1}}],[\"variational\",{\"1\":{\"1798\":1,\"1805\":1,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1}}],[\"variably\",{\"1\":{\"1003\":1}}],[\"variable|np\",{\"1\":{\"875\":2}}],[\"variable|n\",{\"1\":{\"703\":1}}],[\"variables\",{\"1\":{\"3\":1,\"15\":1,\"37\":1,\"85\":1,\"142\":2,\"728\":1,\"745\":9,\"746\":7,\"1001\":1,\"1186\":1,\"1210\":1,\"2343\":1,\"2372\":1,\"2385\":1,\"2429\":1,\"2554\":1}}],[\"variable\",{\"0\":{\"2231\":1},\"1\":{\"1\":1,\"59\":2,\"60\":1,\"73\":2,\"74\":1,\"98\":1,\"135\":1,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"676\":4,\"703\":2,\"717\":1,\"731\":1,\"732\":4,\"742\":4,\"743\":1,\"745\":3,\"746\":3,\"747\":5,\"768\":2,\"777\":1,\"781\":3,\"784\":6,\"793\":2,\"875\":2,\"1004\":1,\"1142\":6,\"1186\":1,\"1210\":1,\"1298\":4,\"1299\":4,\"1301\":2,\"1302\":4,\"1303\":4,\"1304\":2,\"1390\":1,\"1551\":1,\"1553\":1,\"1915\":1,\"2099\":2,\"2176\":1,\"2231\":1,\"2568\":1,\"2569\":1}}],[\"variancepredictor\",{\"0\":{\"2265\":1},\"1\":{\"2265\":1}}],[\"varianceloss\",{\"0\":{\"1879\":1},\"1\":{\"1879\":2}}],[\"variancenorm2d\",{\"0\":{\"1676\":1},\"1\":{\"1676\":1}}],[\"variance=false\",{\"1\":{\"1604\":1}}],[\"variance\",{\"0\":{\"1757\":1,\"2265\":1},\"1\":{\"238\":4,\"496\":1,\"506\":1,\"760\":1,\"770\":1,\"885\":1,\"932\":1,\"1551\":6,\"1553\":8,\"1604\":2,\"1618\":1,\"1619\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1719\":2,\"1757\":1,\"1850\":1,\"1879\":1,\"1906\":1,\"1949\":1,\"2265\":2,\"2375\":1,\"2558\":1}}],[\"variant\",{\"1\":{\"115\":2,\"713\":1,\"1084\":1,\"1096\":1}}],[\"variants\",{\"1\":{\"31\":1}}],[\"various\",{\"1\":{\"22\":1,\"23\":1,\"48\":1,\"114\":1,\"119\":1,\"155\":1,\"156\":1,\"158\":1,\"1551\":1,\"1553\":1,\"2131\":2,\"2354\":1,\"2355\":1,\"2372\":4,\"2377\":1,\"2384\":1,\"2385\":2,\"2388\":1,\"2421\":1,\"2424\":1,\"2429\":2,\"2436\":1,\"2481\":1,\"2524\":1,\"2544\":1,\"2547\":1,\"2554\":2,\"2558\":1,\"2562\":1,\"2618\":1}}],[\"val\",{\"1\":{\"115\":2,\"607\":2,\"1065\":2,\"1069\":2,\"1070\":2,\"1078\":2,\"1079\":2,\"1096\":4,\"1526\":1,\"1695\":2,\"2017\":1,\"2186\":1,\"2193\":3,\"2202\":2,\"2204\":1}}],[\"validated\",{\"1\":{\"1008\":1}}],[\"validates\",{\"1\":{\"745\":1}}],[\"validate\",{\"0\":{\"1103\":1,\"1104\":1,\"1105\":1},\"1\":{\"124\":1,\"1103\":2,\"1104\":2,\"1105\":2,\"1149\":1,\"1150\":1,\"2185\":2,\"2198\":1,\"2201\":1,\"2203\":2}}],[\"validation\",{\"0\":{\"1103\":1,\"1104\":1,\"1105\":1},\"1\":{\"17\":5,\"24\":1,\"56\":1,\"69\":2,\"72\":2,\"112\":1,\"113\":3,\"124\":1,\"150\":1,\"174\":1,\"238\":3,\"239\":1,\"607\":2,\"754\":2,\"820\":2,\"821\":2,\"826\":2,\"975\":2,\"1042\":2,\"1057\":4,\"1103\":1,\"1104\":1,\"1105\":1,\"1243\":1,\"2198\":1,\"2294\":1,\"2373\":4,\"2385\":2,\"2430\":2,\"2555\":2}}],[\"valid2\",{\"1\":{\"75\":1,\"76\":2,\"77\":1,\"78\":1}}],[\"valid\",{\"0\":{\"933\":1},\"1\":{\"57\":2,\"72\":1,\"74\":4,\"75\":5,\"76\":12,\"77\":7,\"78\":7,\"79\":4,\"110\":4,\"150\":1,\"174\":5,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"429\":7,\"933\":5,\"934\":1,\"1008\":1,\"1103\":1,\"1269\":1,\"1429\":1,\"1927\":2,\"1962\":1,\"1964\":1,\"2126\":1,\"2201\":1,\"2350\":3,\"2351\":3,\"2357\":11,\"2371\":4,\"2373\":5,\"2375\":8,\"2377\":2,\"2397\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2430\":3,\"2431\":2,\"2433\":3,\"2436\":2,\"2440\":8,\"2441\":6,\"2444\":2,\"2445\":2,\"2446\":2,\"2454\":1,\"2455\":1,\"2460\":2,\"2461\":1,\"2472\":2,\"2474\":1,\"2476\":2,\"2478\":1,\"2490\":2,\"2492\":1,\"2494\":2,\"2500\":3,\"2533\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2555\":5,\"2558\":5,\"2559\":4,\"2562\":2,\"2564\":7,\"2569\":1,\"2572\":4,\"2578\":11,\"2584\":6,\"2585\":2,\"2589\":1,\"2590\":1,\"2599\":1,\"2600\":4,\"2609\":2,\"2612\":4,\"2617\":1,\"2626\":2,\"2628\":1,\"2630\":5,\"2635\":1,\"2648\":2,\"2649\":1}}],[\"valued=false\",{\"1\":{\"1482\":1,\"1547\":1,\"1561\":1}}],[\"valued\",{\"1\":{\"1452\":2,\"1482\":2,\"1701\":1,\"1737\":1,\"1758\":1,\"1759\":1}}],[\"valueerror\",{\"1\":{\"1011\":1,\"1025\":1}}],[\"value=tensor\",{\"1\":{\"2521\":1,\"2522\":1,\"2523\":1}}],[\"value=\",{\"1\":{\"60\":1}}],[\"value=0\",{\"1\":{\"60\":1,\"774\":1,\"1781\":1}}],[\"values\",{\"1\":{\"19\":1,\"22\":1,\"23\":1,\"56\":1,\"57\":1,\"82\":1,\"102\":1,\"115\":1,\"118\":1,\"119\":1,\"150\":1,\"202\":1,\"493\":1,\"606\":1,\"607\":2,\"623\":1,\"628\":1,\"691\":5,\"692\":2,\"697\":6,\"734\":1,\"754\":2,\"767\":1,\"797\":2,\"817\":1,\"820\":2,\"821\":2,\"825\":1,\"826\":2,\"828\":1,\"834\":1,\"909\":1,\"989\":1,\"1001\":3,\"1011\":2,\"1025\":2,\"1026\":1,\"1035\":1,\"1067\":1,\"1096\":1,\"1149\":1,\"1150\":1,\"1198\":1,\"1245\":1,\"1255\":2,\"1269\":1,\"1766\":1,\"1904\":1,\"1905\":1,\"1916\":1,\"1917\":2,\"1927\":2,\"1928\":1,\"1959\":1,\"2168\":1,\"2193\":3,\"2206\":1,\"2322\":1,\"2600\":1}}],[\"value\",{\"0\":{\"64\":1,\"2230\":1},\"1\":{\"18\":1,\"23\":3,\"24\":1,\"45\":1,\"56\":3,\"58\":1,\"59\":1,\"60\":5,\"64\":2,\"90\":1,\"102\":1,\"115\":23,\"116\":1,\"119\":3,\"150\":3,\"171\":1,\"182\":1,\"185\":1,\"493\":1,\"610\":1,\"623\":2,\"627\":1,\"676\":1,\"697\":2,\"700\":1,\"702\":2,\"710\":2,\"727\":1,\"734\":1,\"737\":1,\"738\":3,\"740\":3,\"741\":3,\"743\":2,\"745\":1,\"746\":1,\"754\":2,\"755\":2,\"759\":2,\"760\":1,\"762\":1,\"763\":1,\"767\":1,\"771\":3,\"774\":3,\"775\":3,\"776\":3,\"778\":1,\"781\":1,\"785\":12,\"797\":1,\"809\":3,\"817\":1,\"820\":1,\"821\":1,\"822\":3,\"825\":3,\"826\":2,\"828\":1,\"834\":5,\"875\":2,\"903\":1,\"906\":3,\"917\":2,\"925\":1,\"976\":1,\"1002\":1,\"1011\":2,\"1016\":1,\"1027\":1,\"1028\":1,\"1043\":1,\"1047\":1,\"1048\":2,\"1057\":2,\"1061\":2,\"1065\":2,\"1066\":1,\"1067\":1,\"1069\":1,\"1070\":1,\"1072\":2,\"1074\":1,\"1076\":11,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":4,\"1082\":4,\"1086\":6,\"1093\":3,\"1096\":9,\"1098\":2,\"1101\":1,\"1133\":2,\"1138\":1,\"1139\":1,\"1142\":2,\"1186\":2,\"1187\":1,\"1194\":1,\"1202\":1,\"1209\":11,\"1210\":2,\"1211\":3,\"1214\":1,\"1224\":2,\"1225\":2,\"1226\":1,\"1273\":1,\"1286\":2,\"1287\":1,\"1301\":2,\"1304\":2,\"1326\":1,\"1336\":1,\"1337\":2,\"1349\":2,\"1350\":2,\"1392\":1,\"1398\":1,\"1639\":3,\"1640\":1,\"1660\":1,\"1661\":1,\"1712\":1,\"1715\":1,\"1741\":3,\"1778\":9,\"1781\":2,\"1805\":9,\"1833\":1,\"1836\":2,\"1839\":1,\"1841\":1,\"1843\":1,\"1851\":2,\"1859\":2,\"1868\":1,\"1877\":2,\"1879\":3,\"1905\":2,\"1932\":1,\"1957\":1,\"1960\":1,\"1965\":1,\"1972\":2,\"1993\":3,\"2000\":3,\"2001\":1,\"2019\":1,\"2030\":2,\"2049\":1,\"2055\":1,\"2064\":1,\"2083\":1,\"2086\":11,\"2087\":11,\"2088\":2,\"2090\":13,\"2091\":4,\"2095\":11,\"2153\":2,\"2170\":1,\"2175\":2,\"2177\":2,\"2188\":2,\"2193\":2,\"2205\":2,\"2208\":2,\"2230\":2,\"2239\":1,\"2243\":4,\"2244\":5,\"2245\":4,\"2255\":4,\"2256\":4,\"2259\":3,\"2260\":1,\"2263\":2,\"2264\":5,\"2279\":4,\"2280\":4,\"2319\":1,\"2326\":1,\"2327\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2336\":1,\"2338\":1,\"2339\":1,\"2440\":1,\"2568\":1,\"2600\":1}}],[\"v70\",{\"1\":{\"1524\":1}}],[\"v7\",{\"1\":{\"1\":1,\"84\":2}}],[\"v2\",{\"0\":{\"650\":1,\"1313\":1,\"1697\":1,\"2452\":1},\"1\":{\"1\":1,\"150\":1,\"214\":1,\"249\":1,\"257\":1,\"261\":1,\"286\":3,\"295\":7,\"296\":1,\"650\":2,\"1313\":1,\"1517\":1,\"1697\":2,\"1731\":1,\"2363\":4,\"2447\":1,\"2451\":1,\"2452\":3,\"2460\":3,\"2461\":2,\"2512\":2,\"2516\":1,\"2653\":4,\"2657\":2}}],[\"ndev\",{\"1\":{\"2568\":3}}],[\"ndim\",{\"1\":{\"2049\":2,\"2055\":2,\"2064\":2,\"2070\":2}}],[\"ndarray\",{\"1\":{\"617\":1,\"618\":1,\"619\":1,\"629\":1,\"648\":1,\"676\":6,\"731\":3,\"742\":2,\"752\":1,\"756\":1,\"781\":3,\"794\":1,\"799\":1,\"808\":1,\"812\":1,\"821\":1,\"835\":1,\"856\":1,\"869\":2,\"875\":2,\"877\":2,\"889\":2,\"898\":3,\"923\":1,\"927\":2,\"943\":1,\"950\":1,\"955\":1,\"956\":2,\"965\":1,\"968\":1,\"972\":1,\"973\":2,\"1013\":1,\"1392\":2,\"1800\":1,\"1883\":1,\"1923\":2,\"2123\":1,\"2128\":1,\"2135\":1,\"2175\":2,\"2182\":2,\"2189\":2,\"2199\":3,\"2205\":6,\"2208\":2,\"2210\":1,\"2230\":2,\"2325\":1,\"2330\":1}}],[\"nhid\",{\"1\":{\"1958\":1}}],[\"nhyp\",{\"1\":{\"1552\":1}}],[\"nbi8qai42v4gkyxx3\",{\"1\":{\"2510\":1}}],[\"nb\",{\"1\":{\"1927\":1,\"2032\":1}}],[\"nbests\",{\"1\":{\"2359\":2,\"2360\":2,\"2472\":2,\"2474\":2,\"2476\":2,\"2478\":2,\"2580\":2,\"2581\":2,\"2582\":2,\"2592\":7,\"2596\":7,\"2648\":2,\"2649\":2}}],[\"nbest=1\",{\"1\":{\"2358\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2520\":1,\"2579\":1,\"2592\":1,\"2648\":1,\"2649\":1}}],[\"nbest\",{\"0\":{\"1428\":1,\"1962\":2},\"1\":{\"249\":2,\"251\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"307\":2,\"315\":6,\"327\":2,\"333\":2,\"384\":2,\"391\":2,\"399\":4,\"408\":2,\"416\":2,\"422\":2,\"443\":4,\"449\":2,\"478\":2,\"485\":6,\"600\":2,\"633\":2,\"700\":8,\"1048\":7,\"1138\":8,\"1139\":8,\"1428\":2,\"1962\":4,\"2186\":2,\"2193\":1,\"2202\":4,\"2204\":2,\"2440\":2,\"2558\":2,\"2584\":1,\"2600\":6}}],[\"nyquist\",{\"0\":{\"1822\":1},\"1\":{\"1822\":1}}],[\"nref\",{\"1\":{\"1552\":1}}],[\"n$\",{\"1\":{\"1517\":3}}],[\"nf=128\",{\"1\":{\"1605\":1}}],[\"nframes=wavfile\",{\"1\":{\"2592\":1}}],[\"nframe\",{\"1\":{\"1560\":1}}],[\"nfreqs\",{\"1\":{\"1517\":1}}],[\"nfs\",{\"1\":{\"38\":1}}],[\"n=$\",{\"1\":{\"2568\":1}}],[\"n=none\",{\"1\":{\"1647\":2}}],[\"n=30\",{\"1\":{\"1646\":1}}],[\"n=1000\",{\"1\":{\"1618\":1,\"1619\":1}}],[\"n=64\",{\"1\":{\"1245\":1}}],[\"n=0\",{\"1\":{\"692\":1}}],[\"nmic2\",{\"1\":{\"1560\":1}}],[\"nmic1\",{\"1\":{\"1560\":1}}],[\"nmask\",{\"1\":{\"1525\":1}}],[\"nmask=1\",{\"1\":{\"782\":1,\"1595\":1}}],[\"nmt\",{\"1\":{\"255\":1}}],[\"nclases\",{\"1\":{\"2030\":1}}],[\"nclasses\",{\"1\":{\"2030\":1,\"2040\":1}}],[\"nclass\",{\"1\":{\"1567\":2}}],[\"ncbi\",{\"1\":{\"1515\":1}}],[\"ncsnv1\",{\"1\":{\"1731\":1,\"1732\":1}}],[\"ncsnv2\",{\"1\":{\"1451\":1,\"1732\":1}}],[\"ncsn++\",{\"1\":{\"1605\":1}}],[\"ncsnpp\",{\"0\":{\"1456\":1,\"1458\":1,\"1468\":1,\"1474\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1508\":1,\"1512\":1,\"1542\":1,\"1549\":1,\"1573\":1,\"1583\":1,\"1592\":1,\"1596\":1,\"1605\":2,\"1607\":1,\"1613\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1686\":1,\"1688\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1699\":1,\"1709\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":1},\"1\":{\"1456\":1,\"1458\":1,\"1468\":1,\"1474\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":2,\"1503\":1,\"1508\":1,\"1512\":1,\"1542\":1,\"1549\":1,\"1573\":1,\"1583\":1,\"1592\":1,\"1596\":1,\"1605\":4,\"1607\":2,\"1613\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1686\":1,\"1688\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1699\":1,\"1709\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":1}}],[\"ncsn\",{\"0\":{\"1731\":1,\"1732\":1},\"1\":{\"1451\":1,\"1731\":1,\"1732\":1}}],[\"ncols\",{\"1\":{\"579\":2}}],[\"nchar\",{\"1\":{\"579\":2}}],[\"nccl\",{\"0\":{\"44\":1},\"1\":{\"1\":1,\"44\":7,\"45\":5,\"82\":1,\"148\":1,\"2180\":2}}],[\"nlu\",{\"1\":{\"2467\":2,\"2468\":1}}],[\"nlayers\",{\"1\":{\"1958\":1}}],[\"nlm\",{\"1\":{\"1515\":1}}],[\"nll\",{\"1\":{\"315\":2,\"485\":2,\"1171\":7,\"1206\":7,\"1552\":7,\"1804\":1,\"1833\":1,\"1838\":1,\"1840\":1,\"1855\":1,\"1865\":1,\"1868\":1,\"1878\":1,\"1953\":4,\"1954\":3,\"1955\":4,\"1956\":4}}],[\"nlp\",{\"1\":{\"307\":2,\"2178\":1,\"2179\":1,\"2380\":1,\"2388\":1,\"2421\":1,\"2468\":1,\"2524\":1,\"2544\":1}}],[\"nltk\",{\"1\":{\"217\":2}}],[\"ng\",{\"1\":{\"461\":14}}],[\"ngrambase\",{\"0\":{\"790\":1},\"1\":{\"790\":1}}],[\"ngrampartscorer\",{\"0\":{\"789\":1},\"1\":{\"789\":1}}],[\"ngramfullscorer\",{\"0\":{\"788\":1},\"1\":{\"788\":1}}],[\"ngram\",{\"0\":{\"788\":1,\"789\":1,\"790\":1},\"1\":{\"249\":5,\"307\":4,\"384\":4,\"391\":4,\"408\":4,\"422\":4,\"443\":8,\"478\":4,\"788\":1,\"789\":1,\"790\":1}}],[\"ngpu=none\",{\"1\":{\"626\":1}}],[\"ngpu=0\",{\"1\":{\"19\":1}}],[\"ngpu=1\",{\"1\":{\"18\":1,\"90\":1}}],[\"ngpu1\",{\"1\":{\"2\":1}}],[\"ngpu\",{\"1\":{\"1\":1,\"3\":2,\"4\":1,\"18\":3,\"19\":7,\"32\":5,\"34\":2,\"35\":1,\"36\":2,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"71\":2,\"72\":2,\"90\":3,\"94\":2,\"168\":1,\"180\":1,\"186\":1,\"187\":1,\"245\":2,\"247\":2,\"249\":2,\"251\":2,\"253\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2,\"286\":3,\"296\":3,\"301\":2,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"377\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"626\":1,\"627\":2,\"1964\":1,\"2099\":1,\"2180\":2,\"2186\":1,\"2201\":1,\"2202\":2,\"2204\":1,\"2216\":1,\"2217\":1,\"2375\":1,\"2440\":1,\"2461\":1,\"2558\":1,\"2564\":1,\"2569\":1}}],[\"nutts\",{\"1\":{\"633\":1}}],[\"nulldecoder\",{\"0\":{\"1616\":1},\"1\":{\"1616\":1}}],[\"nullencoder\",{\"0\":{\"1617\":1},\"1\":{\"1454\":1,\"1617\":1}}],[\"null\",{\"0\":{\"1616\":1,\"1617\":1},\"1\":{\"200\":1,\"1093\":1,\"1101\":1,\"1616\":2,\"1617\":2,\"2320\":1,\"2328\":1,\"2340\":1,\"2439\":1,\"2440\":3}}],[\"num=0\",{\"1\":{\"1797\":1}}],[\"num2tuple\",{\"0\":{\"1734\":1},\"1\":{\"1734\":2}}],[\"numgroups\",{\"1\":{\"1688\":1,\"1756\":1}}],[\"numbre\",{\"1\":{\"1670\":1,\"1671\":1}}],[\"numba\",{\"1\":{\"1186\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1286\":1,\"2393\":1,\"2427\":1,\"2529\":1,\"2550\":1}}],[\"numbern\",{\"1\":{\"2274\":1}}],[\"numbers\",{\"1\":{\"58\":1,\"80\":1,\"705\":1,\"1132\":1,\"1390\":1,\"1622\":1,\"2592\":1,\"2596\":1}}],[\"number\",{\"0\":{\"69\":1,\"72\":1,\"93\":1,\"95\":1},\"1\":{\"21\":6,\"23\":4,\"26\":3,\"33\":2,\"36\":1,\"37\":1,\"40\":1,\"47\":2,\"48\":1,\"68\":1,\"69\":2,\"71\":1,\"72\":3,\"75\":2,\"76\":4,\"77\":1,\"78\":2,\"80\":3,\"93\":2,\"95\":2,\"99\":2,\"109\":1,\"113\":2,\"115\":4,\"116\":3,\"119\":3,\"121\":2,\"122\":1,\"141\":1,\"143\":2,\"148\":3,\"150\":1,\"238\":2,\"239\":1,\"240\":1,\"274\":2,\"275\":4,\"276\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":2,\"283\":1,\"284\":1,\"286\":1,\"294\":1,\"296\":1,\"297\":1,\"593\":1,\"614\":2,\"618\":2,\"619\":3,\"621\":1,\"626\":1,\"627\":1,\"632\":2,\"635\":1,\"668\":2,\"670\":1,\"672\":1,\"676\":2,\"683\":1,\"691\":2,\"693\":2,\"694\":6,\"697\":2,\"700\":4,\"701\":6,\"711\":3,\"712\":1,\"713\":2,\"723\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"731\":3,\"732\":3,\"734\":1,\"737\":2,\"740\":2,\"741\":2,\"743\":1,\"745\":1,\"746\":1,\"747\":4,\"748\":3,\"749\":3,\"754\":9,\"760\":1,\"765\":4,\"767\":2,\"768\":1,\"771\":2,\"773\":1,\"775\":2,\"776\":2,\"778\":3,\"781\":2,\"784\":4,\"785\":2,\"786\":2,\"794\":1,\"797\":2,\"798\":5,\"801\":1,\"802\":3,\"804\":2,\"805\":3,\"806\":2,\"807\":2,\"808\":3,\"809\":2,\"817\":1,\"821\":24,\"826\":15,\"828\":1,\"833\":1,\"835\":7,\"857\":2,\"858\":1,\"866\":1,\"870\":1,\"878\":1,\"905\":1,\"914\":1,\"918\":1,\"933\":1,\"943\":1,\"950\":2,\"955\":1,\"965\":1,\"968\":2,\"972\":1,\"997\":2,\"1003\":2,\"1004\":2,\"1007\":1,\"1011\":1,\"1019\":1,\"1028\":6,\"1037\":2,\"1039\":1,\"1048\":4,\"1049\":2,\"1050\":2,\"1051\":1,\"1052\":3,\"1054\":1,\"1055\":1,\"1056\":2,\"1057\":2,\"1058\":3,\"1059\":1,\"1062\":2,\"1065\":2,\"1066\":3,\"1068\":2,\"1069\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":5,\"1077\":2,\"1078\":1,\"1079\":2,\"1081\":2,\"1093\":2,\"1101\":2,\"1133\":3,\"1138\":4,\"1139\":4,\"1142\":1,\"1145\":1,\"1148\":3,\"1149\":3,\"1150\":3,\"1155\":1,\"1181\":2,\"1186\":1,\"1187\":1,\"1198\":7,\"1200\":1,\"1202\":1,\"1203\":3,\"1209\":2,\"1210\":2,\"1222\":4,\"1228\":2,\"1241\":1,\"1244\":1,\"1245\":3,\"1248\":1,\"1252\":1,\"1255\":1,\"1269\":5,\"1270\":2,\"1272\":3,\"1282\":4,\"1302\":1,\"1303\":1,\"1304\":1,\"1337\":1,\"1345\":2,\"1347\":2,\"1349\":1,\"1350\":1,\"1351\":1,\"1375\":5,\"1377\":3,\"1379\":5,\"1398\":1,\"1400\":1,\"1417\":1,\"1422\":1,\"1430\":1,\"1454\":1,\"1462\":2,\"1463\":3,\"1470\":1,\"1505\":4,\"1515\":2,\"1516\":2,\"1522\":9,\"1523\":9,\"1528\":2,\"1529\":2,\"1531\":2,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":2,\"1539\":3,\"1543\":5,\"1545\":4,\"1551\":2,\"1553\":2,\"1558\":2,\"1559\":1,\"1560\":1,\"1572\":2,\"1576\":2,\"1577\":2,\"1581\":1,\"1618\":1,\"1619\":1,\"1626\":2,\"1638\":1,\"1645\":3,\"1646\":2,\"1652\":1,\"1654\":2,\"1655\":8,\"1656\":2,\"1658\":4,\"1659\":4,\"1660\":5,\"1661\":5,\"1662\":5,\"1664\":8,\"1665\":7,\"1669\":4,\"1670\":4,\"1671\":8,\"1698\":1,\"1704\":1,\"1707\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1718\":1,\"1719\":10,\"1739\":1,\"1758\":1,\"1759\":1,\"1765\":4,\"1766\":5,\"1771\":4,\"1772\":3,\"1773\":3,\"1776\":2,\"1777\":1,\"1785\":2,\"1786\":1,\"1787\":6,\"1788\":4,\"1797\":1,\"1798\":3,\"1800\":4,\"1801\":1,\"1803\":5,\"1804\":15,\"1808\":4,\"1810\":4,\"1833\":4,\"1834\":1,\"1835\":2,\"1836\":1,\"1838\":1,\"1839\":2,\"1843\":1,\"1844\":4,\"1846\":1,\"1847\":1,\"1848\":4,\"1849\":4,\"1851\":21,\"1856\":4,\"1857\":4,\"1858\":5,\"1859\":1,\"1860\":2,\"1861\":5,\"1862\":8,\"1863\":6,\"1864\":6,\"1865\":5,\"1866\":1,\"1867\":1,\"1868\":4,\"1870\":1,\"1871\":4,\"1872\":2,\"1873\":2,\"1874\":3,\"1878\":14,\"1880\":9,\"1883\":1,\"1895\":3,\"1897\":2,\"1900\":3,\"1904\":1,\"1905\":1,\"1912\":3,\"1916\":1,\"1917\":3,\"1932\":1,\"1941\":1,\"1962\":1,\"1971\":1,\"2001\":5,\"2002\":12,\"2004\":5,\"2022\":1,\"2029\":3,\"2030\":1,\"2040\":2,\"2054\":3,\"2078\":7,\"2082\":3,\"2086\":12,\"2087\":14,\"2090\":10,\"2095\":23,\"2099\":2,\"2102\":2,\"2151\":2,\"2154\":4,\"2197\":2,\"2199\":1,\"2217\":1,\"2243\":16,\"2244\":20,\"2255\":20,\"2257\":4,\"2258\":2,\"2259\":1,\"2260\":6,\"2261\":6,\"2262\":2,\"2263\":23,\"2264\":22,\"2265\":2,\"2279\":14,\"2294\":1,\"2317\":3,\"2325\":2,\"2330\":2,\"2372\":1,\"2373\":1,\"2375\":1,\"2384\":1,\"2403\":1,\"2419\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2440\":1,\"2441\":1,\"2473\":1,\"2539\":1,\"2554\":1,\"2555\":1,\"2558\":1,\"2559\":1,\"2564\":1,\"2638\":1}}],[\"numeracal\",{\"1\":{\"1332\":1}}],[\"numerical\",{\"1\":{\"1047\":1,\"1072\":1,\"1080\":1}}],[\"numelementsbatchsampler\",{\"0\":{\"2009\":1},\"1\":{\"2009\":1}}],[\"numel\",{\"1\":{\"73\":1,\"75\":1,\"76\":1,\"78\":3,\"429\":2,\"1427\":3,\"2012\":4,\"2440\":1}}],[\"numspk=false\",{\"1\":{\"1622\":1}}],[\"numspk\",{\"1\":{\"363\":2,\"1551\":1,\"1553\":2,\"1554\":1,\"1622\":1,\"2184\":1,\"2200\":1}}],[\"numpy=false\",{\"1\":{\"1738\":1}}],[\"numpy\",{\"0\":{\"856\":1},\"1\":{\"174\":3,\"218\":1,\"225\":1,\"232\":1,\"536\":1,\"605\":2,\"629\":1,\"648\":1,\"676\":1,\"747\":1,\"750\":2,\"799\":1,\"821\":1,\"856\":3,\"870\":1,\"878\":1,\"923\":1,\"927\":2,\"943\":1,\"950\":1,\"955\":1,\"956\":2,\"965\":1,\"968\":1,\"972\":1,\"973\":2,\"1388\":1,\"1394\":1,\"1396\":1,\"1397\":2,\"1401\":1,\"1408\":4,\"1409\":1,\"1413\":1,\"2099\":1,\"2175\":1,\"2205\":3,\"2237\":1,\"2272\":1,\"2365\":1,\"2386\":2,\"2474\":1,\"2498\":3,\"2500\":1,\"2508\":1,\"2510\":1,\"2514\":2,\"2515\":1,\"2521\":1,\"2522\":1,\"2523\":1,\"2591\":1,\"2616\":3,\"2617\":1,\"2634\":3,\"2635\":1,\"2649\":1,\"2655\":1,\"2659\":2,\"2660\":1}}],[\"num\",{\"0\":{\"1023\":1,\"1417\":1,\"2009\":1,\"2217\":1},\"1\":{\"52\":2,\"69\":2,\"94\":1,\"115\":2,\"116\":5,\"121\":2,\"134\":2,\"144\":3,\"245\":2,\"247\":2,\"249\":3,\"251\":7,\"255\":2,\"259\":4,\"265\":4,\"269\":4,\"276\":1,\"281\":1,\"298\":1,\"307\":2,\"315\":4,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":4,\"350\":2,\"357\":2,\"368\":2,\"377\":4,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":10,\"437\":2,\"441\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":4,\"496\":2,\"499\":2,\"501\":2,\"509\":2,\"512\":2,\"522\":2,\"525\":2,\"530\":2,\"546\":2,\"551\":2,\"557\":2,\"560\":2,\"638\":1,\"698\":1,\"699\":1,\"711\":2,\"747\":1,\"749\":4,\"794\":3,\"826\":2,\"933\":2,\"934\":2,\"983\":1,\"986\":1,\"991\":1,\"994\":1,\"1003\":2,\"1004\":2,\"1006\":1,\"1015\":3,\"1019\":2,\"1023\":2,\"1024\":2,\"1028\":2,\"1036\":1,\"1037\":4,\"1039\":2,\"1062\":4,\"1065\":2,\"1066\":4,\"1069\":10,\"1073\":2,\"1074\":2,\"1075\":2,\"1076\":2,\"1081\":4,\"1093\":2,\"1101\":2,\"1130\":1,\"1133\":1,\"1140\":1,\"1142\":2,\"1144\":1,\"1148\":2,\"1149\":2,\"1150\":2,\"1167\":1,\"1168\":1,\"1169\":1,\"1181\":2,\"1186\":2,\"1196\":1,\"1197\":1,\"1203\":2,\"1204\":1,\"1210\":5,\"1218\":5,\"1220\":4,\"1222\":2,\"1227\":2,\"1228\":2,\"1231\":1,\"1257\":2,\"1269\":6,\"1270\":2,\"1271\":1,\"1272\":2,\"1273\":1,\"1282\":2,\"1289\":2,\"1302\":3,\"1303\":3,\"1304\":4,\"1337\":4,\"1349\":2,\"1350\":2,\"1363\":1,\"1366\":1,\"1367\":1,\"1371\":1,\"1374\":2,\"1375\":7,\"1376\":3,\"1377\":1,\"1417\":1,\"1418\":1,\"1423\":1,\"1428\":1,\"1446\":1,\"1454\":3,\"1460\":2,\"1462\":4,\"1463\":7,\"1465\":2,\"1476\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":2,\"1495\":2,\"1497\":2,\"1499\":2,\"1501\":1,\"1503\":2,\"1505\":3,\"1515\":3,\"1516\":6,\"1523\":3,\"1524\":2,\"1528\":3,\"1529\":3,\"1531\":4,\"1532\":2,\"1534\":3,\"1535\":3,\"1537\":2,\"1539\":3,\"1543\":2,\"1551\":1,\"1552\":2,\"1553\":6,\"1554\":1,\"1558\":3,\"1559\":3,\"1560\":3,\"1564\":1,\"1583\":1,\"1605\":1,\"1607\":1,\"1611\":2,\"1613\":1,\"1622\":1,\"1626\":3,\"1645\":5,\"1652\":2,\"1654\":3,\"1658\":3,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":3,\"1670\":4,\"1671\":7,\"1676\":1,\"1705\":2,\"1718\":3,\"1719\":1,\"1734\":1,\"1797\":2,\"1895\":2,\"1896\":3,\"1897\":2,\"1900\":2,\"1914\":1,\"1915\":1,\"1940\":1,\"1994\":1,\"2001\":2,\"2004\":2,\"2009\":1,\"2013\":1,\"2026\":1,\"2029\":2,\"2054\":2,\"2063\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":5,\"2100\":1,\"2101\":1,\"2102\":4,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":4,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2123\":1,\"2128\":1,\"2135\":1,\"2178\":1,\"2179\":1,\"2181\":1,\"2184\":3,\"2191\":1,\"2195\":1,\"2197\":4,\"2200\":3,\"2204\":2,\"2217\":1,\"2264\":4,\"2367\":2,\"2386\":2,\"2440\":4,\"2485\":2,\"2492\":1,\"2558\":4,\"2564\":1,\"2584\":2,\"2600\":3,\"2604\":2,\"2621\":2,\"2628\":1,\"2642\":1}}],[\"nnet\",{\"1\":{\"1136\":1,\"1137\":1}}],[\"nn\",{\"0\":{\"186\":1},\"1\":{\"174\":2,\"203\":2,\"626\":1,\"627\":1,\"632\":1,\"639\":2,\"645\":1,\"646\":1,\"653\":1,\"655\":1,\"676\":2,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"706\":1,\"708\":1,\"710\":1,\"711\":4,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"729\":1,\"730\":1,\"749\":1,\"752\":1,\"756\":1,\"758\":1,\"760\":1,\"778\":1,\"781\":2,\"782\":1,\"787\":1,\"793\":1,\"812\":2,\"819\":1,\"830\":1,\"831\":1,\"835\":1,\"838\":3,\"856\":1,\"872\":1,\"873\":1,\"874\":1,\"878\":1,\"893\":1,\"926\":1,\"975\":1,\"976\":1,\"997\":1,\"998\":1,\"1042\":1,\"1043\":1,\"1046\":1,\"1049\":4,\"1050\":6,\"1054\":2,\"1056\":7,\"1061\":1,\"1065\":3,\"1067\":1,\"1068\":3,\"1070\":4,\"1074\":2,\"1082\":1,\"1084\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1133\":3,\"1134\":1,\"1136\":1,\"1140\":1,\"1141\":1,\"1145\":1,\"1148\":2,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1174\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1188\":1,\"1190\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1200\":1,\"1203\":2,\"1204\":1,\"1206\":1,\"1207\":1,\"1211\":1,\"1212\":1,\"1214\":2,\"1215\":1,\"1217\":1,\"1218\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1242\":1,\"1244\":3,\"1247\":1,\"1249\":1,\"1252\":3,\"1253\":1,\"1254\":3,\"1257\":2,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":2,\"1274\":1,\"1276\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1378\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":2,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1505\":1,\"1506\":2,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1524\":1,\"1525\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1542\":1,\"1543\":2,\"1546\":1,\"1547\":1,\"1549\":1,\"1552\":1,\"1553\":4,\"1554\":1,\"1555\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":2,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":1,\"1575\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1601\":1,\"1602\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1613\":1,\"1616\":1,\"1617\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":2,\"1631\":1,\"1633\":1,\"1635\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":2,\"1656\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1688\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1756\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1785\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1799\":1,\"1806\":1,\"1828\":1,\"1840\":1,\"1842\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1857\":1,\"1875\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1902\":1,\"1906\":1,\"1907\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1917\":3,\"1918\":1,\"1919\":1,\"1920\":1,\"1930\":1,\"1932\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1970\":1,\"1973\":2,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2003\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":2,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":2,\"2084\":1,\"2089\":1,\"2148\":1,\"2149\":3,\"2153\":1,\"2155\":6,\"2168\":2,\"2170\":2,\"2233\":1,\"2235\":1,\"2237\":1,\"2239\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2266\":1,\"2275\":1,\"2277\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2401\":1,\"2537\":1}}],[\"nvcc\",{\"1\":{\"135\":2}}],[\"nvidia\",{\"1\":{\"44\":1,\"45\":1,\"148\":1,\"668\":1,\"1144\":1,\"1228\":1,\"1345\":1,\"1347\":1,\"2401\":1,\"2426\":1,\"2537\":1,\"2549\":1}}],[\"navigator\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"naviecomplexlstm\",{\"0\":{\"1609\":1},\"1\":{\"1609\":1}}],[\"nargs=none\",{\"1\":{\"2313\":1}}],[\"na\",{\"1\":{\"1975\":1,\"2194\":1}}],[\"nakatani\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"nachmani\",{\"1\":{\"1645\":1}}],[\"naivernnloss\",{\"0\":{\"2088\":1},\"1\":{\"2088\":1}}],[\"naivernndp\",{\"0\":{\"2087\":1},\"1\":{\"2087\":3}}],[\"naivernn\",{\"0\":{\"2086\":1},\"1\":{\"2086\":4}}],[\"naive\",{\"0\":{\"1729\":1,\"1730\":1,\"2086\":2,\"2087\":2,\"2088\":2},\"1\":{\"1530\":1,\"1563\":1,\"1729\":1,\"1730\":1,\"2086\":5,\"2087\":3,\"2088\":2,\"2394\":1,\"2530\":1}}],[\"naming\",{\"1\":{\"1407\":2}}],[\"name==\",{\"1\":{\"2155\":1}}],[\"name=none\",{\"1\":{\"1008\":1,\"1466\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1571\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1668\":1}}],[\"name=\",{\"1\":{\"987\":2,\"1025\":1,\"1323\":1,\"2099\":1}}],[\"name|default=root\",{\"1\":{\"135\":1}}],[\"name|default=venv\",{\"1\":{\"135\":1}}],[\"nameshere\",{\"1\":{\"2431\":1}}],[\"namesapce\",{\"1\":{\"1016\":1}}],[\"namespace=none\",{\"1\":{\"2309\":1}}],[\"namespaceornone\",{\"1\":{\"1016\":1}}],[\"namespace\",{\"1\":{\"56\":1,\"60\":1,\"194\":1,\"217\":2,\"224\":2,\"231\":2,\"602\":1,\"603\":1,\"606\":1,\"622\":1,\"636\":1,\"645\":1,\"646\":1,\"649\":1,\"650\":1,\"657\":1,\"659\":3,\"660\":3,\"661\":3,\"662\":3,\"668\":1,\"669\":1,\"672\":1,\"676\":4,\"734\":1,\"742\":2,\"747\":1,\"754\":2,\"781\":4,\"812\":4,\"817\":1,\"821\":2,\"826\":2,\"828\":1,\"855\":1,\"867\":1,\"870\":1,\"889\":1,\"894\":2,\"935\":2,\"937\":1,\"938\":1,\"1016\":1,\"1017\":3,\"1032\":1,\"1033\":1,\"2096\":3,\"2097\":3,\"2098\":3,\"2099\":16,\"2100\":3,\"2101\":3,\"2102\":4,\"2103\":4,\"2104\":4,\"2105\":3,\"2107\":3,\"2108\":3,\"2109\":3,\"2110\":3,\"2111\":2,\"2112\":3,\"2113\":3,\"2114\":3,\"2115\":3,\"2116\":3,\"2117\":3,\"2118\":4,\"2168\":1,\"2170\":1,\"2185\":1,\"2201\":1,\"2203\":1,\"2314\":4,\"2318\":1,\"2320\":4,\"2328\":4,\"2335\":1,\"2340\":4}}],[\"names\",{\"1\":{\"30\":1,\"45\":1,\"56\":2,\"59\":9,\"441\":3,\"640\":1,\"691\":1,\"697\":2,\"754\":1,\"820\":1,\"821\":1,\"826\":2,\"1327\":1,\"1454\":1,\"1963\":1,\"2096\":8,\"2097\":2,\"2098\":8,\"2099\":8,\"2100\":8,\"2101\":8,\"2102\":8,\"2103\":8,\"2104\":8,\"2105\":8,\"2107\":8,\"2108\":8,\"2109\":8,\"2110\":8,\"2111\":8,\"2112\":8,\"2113\":8,\"2114\":8,\"2115\":8,\"2116\":8,\"2117\":8,\"2118\":8,\"2167\":1,\"2178\":1,\"2179\":1,\"2183\":1,\"2190\":1,\"2264\":1,\"2346\":1,\"2584\":1}}],[\"named\",{\"0\":{\"127\":1,\"246\":1,\"248\":1,\"250\":1,\"252\":1,\"254\":1,\"256\":1,\"258\":1,\"260\":1,\"262\":1,\"264\":1,\"266\":1,\"268\":1,\"270\":1,\"300\":1,\"302\":1,\"308\":1,\"316\":1,\"322\":1,\"328\":1,\"334\":1,\"340\":1,\"344\":1,\"351\":1,\"358\":1,\"364\":1,\"369\":1,\"376\":1,\"379\":1,\"381\":1,\"385\":1,\"392\":1,\"400\":1,\"409\":1,\"417\":1,\"423\":1,\"430\":1,\"438\":1,\"442\":1,\"444\":1,\"450\":1,\"456\":1,\"462\":1,\"465\":1,\"471\":1,\"477\":1,\"479\":1,\"486\":1,\"492\":1,\"495\":1,\"498\":1,\"500\":1,\"502\":1,\"505\":1,\"508\":1,\"511\":1,\"514\":1,\"521\":1,\"524\":1,\"527\":1,\"529\":1,\"532\":1,\"535\":1,\"540\":1,\"543\":1,\"548\":1,\"553\":1,\"556\":1,\"559\":1,\"561\":1,\"563\":1,\"565\":1,\"567\":1,\"571\":1,\"573\":1,\"575\":1,\"578\":1,\"581\":1,\"584\":1,\"587\":1,\"592\":1},\"1\":{\"25\":1,\"47\":1,\"64\":1,\"124\":1,\"398\":11,\"1640\":1,\"2046\":1,\"2368\":1,\"2440\":1,\"2486\":1,\"2490\":1,\"2564\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"name\",{\"1\":{\"25\":1,\"28\":1,\"38\":1,\"45\":1,\"47\":1,\"56\":1,\"57\":10,\"59\":6,\"60\":1,\"62\":1,\"74\":4,\"75\":4,\"76\":8,\"77\":4,\"78\":4,\"79\":2,\"88\":1,\"96\":1,\"97\":2,\"99\":2,\"102\":1,\"104\":1,\"108\":3,\"109\":1,\"110\":1,\"135\":1,\"143\":1,\"144\":1,\"203\":2,\"286\":5,\"295\":2,\"296\":5,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"375\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":4,\"437\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"491\":2,\"501\":1,\"621\":1,\"626\":1,\"652\":1,\"665\":4,\"672\":1,\"673\":1,\"674\":2,\"727\":2,\"728\":2,\"821\":1,\"834\":1,\"872\":2,\"873\":2,\"874\":2,\"989\":1,\"996\":1,\"1008\":3,\"1011\":2,\"1012\":2,\"1025\":2,\"1110\":1,\"1190\":3,\"1191\":1,\"1192\":1,\"1212\":1,\"1217\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1319\":1,\"1322\":1,\"1327\":1,\"1407\":4,\"1438\":1,\"1454\":3,\"1551\":2,\"1553\":2,\"1570\":2,\"1600\":1,\"1604\":1,\"1646\":2,\"1647\":2,\"1667\":2,\"1700\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1834\":1,\"1844\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1856\":2,\"1857\":2,\"1858\":3,\"1866\":1,\"1867\":2,\"1871\":1,\"1876\":1,\"1877\":1,\"1905\":1,\"1930\":1,\"1944\":1,\"1957\":1,\"1961\":2,\"1962\":1,\"1977\":1,\"1979\":1,\"2002\":1,\"2028\":1,\"2095\":1,\"2099\":1,\"2106\":2,\"2123\":1,\"2148\":2,\"2152\":1,\"2155\":5,\"2167\":2,\"2176\":2,\"2178\":2,\"2179\":2,\"2181\":3,\"2182\":1,\"2183\":2,\"2184\":4,\"2189\":1,\"2190\":2,\"2191\":2,\"2194\":4,\"2195\":2,\"2196\":4,\"2199\":2,\"2200\":4,\"2263\":1,\"2289\":1,\"2346\":2,\"2352\":1,\"2353\":1,\"2385\":1,\"2431\":1,\"2432\":2,\"2441\":1,\"2501\":4,\"2522\":3,\"2559\":1,\"2568\":1,\"2581\":3,\"2599\":2,\"2600\":9,\"2607\":4,\"2615\":2,\"2624\":4,\"2633\":2,\"2637\":2,\"2638\":1,\"2644\":1}}],[\"nat\",{\"1\":{\"1983\":1,\"1994\":1}}],[\"native\",{\"0\":{\"1754\":1},\"1\":{\"1216\":1,\"1754\":1}}],[\"natural\",{\"1\":{\"235\":1,\"802\":1,\"803\":1,\"804\":1,\"2263\":1,\"2467\":1,\"2468\":1,\"2543\":1}}],[\"nan\",{\"1\":{\"130\":1,\"737\":1,\"738\":1,\"1145\":2,\"1392\":1}}],[\"nanxin\",{\"1\":{\"130\":1}}],[\"naoyuki\",{\"1\":{\"130\":1,\"2357\":1,\"2578\":1,\"2599\":1,\"2600\":2}}],[\"nil\",{\"1\":{\"2320\":1,\"2328\":1,\"2340\":1}}],[\"nin\",{\"0\":{\"1607\":1},\"1\":{\"1607\":2}}],[\"ninputs=1\",{\"1\":{\"1484\":1,\"1601\":1,\"1725\":1}}],[\"nih\",{\"1\":{\"1515\":1}}],[\"ni\",{\"1\":{\"130\":1,\"1604\":1,\"1655\":1,\"1719\":1}}],[\"nishitoba\",{\"1\":{\"130\":1}}],[\"nist\",{\"0\":{\"53\":1},\"1\":{\"989\":1,\"2372\":1}}],[\"nice\",{\"1\":{\"2645\":1}}],[\"nic\",{\"1\":{\"44\":2}}],[\"nj=4\",{\"1\":{\"143\":1}}],[\"nj=1\",{\"1\":{\"98\":1}}],[\"nj=10\",{\"1\":{\"47\":1}}],[\"nj\",{\"1\":{\"47\":2,\"93\":2,\"98\":2,\"143\":3,\"238\":1,\"241\":2,\"242\":1,\"275\":2,\"276\":2,\"279\":2,\"280\":2,\"281\":2,\"282\":2,\"283\":2,\"284\":2,\"294\":2,\"297\":2,\"2373\":2,\"2375\":3,\"2377\":4,\"2403\":1,\"2430\":2,\"2431\":1,\"2432\":3,\"2433\":1,\"2436\":4,\"2440\":2,\"2492\":1,\"2539\":1,\"2555\":2,\"2558\":1,\"2559\":2,\"2562\":4,\"2564\":2,\"2569\":2,\"2628\":1}}],[\"npz\",{\"1\":{\"2520\":2}}],[\"npsd\",{\"1\":{\"1715\":1}}],[\"npsd^\",{\"1\":{\"885\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":1,\"1712\":2}}],[\"nplr\",{\"0\":{\"1339\":1},\"1\":{\"1245\":4,\"1248\":1,\"1339\":3}}],[\"nprocs\",{\"1\":{\"596\":1}}],[\"npyscpwriter\",{\"0\":{\"1396\":1},\"1\":{\"1396\":2,\"1397\":1}}],[\"npyscpreader\",{\"0\":{\"1394\":1},\"1\":{\"1394\":2,\"1395\":1}}],[\"npy\",{\"0\":{\"1394\":1,\"1396\":1},\"1\":{\"58\":2,\"60\":1,\"74\":2,\"75\":4,\"76\":8,\"77\":4,\"78\":4,\"79\":2,\"496\":1,\"506\":1,\"760\":1,\"1394\":2,\"1395\":5,\"1396\":2,\"1397\":4,\"1800\":1,\"1906\":1}}],[\"np\",{\"0\":{\"2272\":1},\"1\":{\"42\":1,\"617\":1,\"618\":1,\"619\":2,\"742\":2,\"744\":2,\"778\":1,\"794\":1,\"808\":1,\"856\":1,\"876\":2,\"889\":2,\"898\":1,\"928\":1,\"959\":2,\"989\":1,\"1013\":1,\"1392\":3,\"1418\":2,\"1423\":1,\"1797\":1,\"1923\":2,\"2210\":2,\"2272\":1,\"2386\":1,\"2474\":2,\"2500\":2,\"2514\":4,\"2591\":1,\"2592\":1,\"2596\":1,\"2617\":2,\"2635\":2,\"2649\":2,\"2659\":4}}],[\"n3\",{\"1\":{\"41\":1}}],[\"n5\",{\"1\":{\"41\":1}}],[\"n20\",{\"1\":{\"191\":1}}],[\"n2\",{\"1\":{\"40\":1,\"1203\":1}}],[\"nsample\",{\"1\":{\"1918\":1}}],[\"nsamples\",{\"1\":{\"1373\":1,\"1910\":1,\"1918\":1,\"2084\":4,\"2089\":3}}],[\"nspk=2\",{\"1\":{\"1560\":1}}],[\"nspkrs\",{\"1\":{\"633\":1}}],[\"ns\",{\"1\":{\"282\":1,\"295\":2}}],[\"nstep\",{\"1\":{\"23\":3,\"113\":2,\"119\":1,\"249\":2,\"700\":2,\"1048\":2,\"1057\":2,\"1059\":2,\"1138\":2,\"1139\":2}}],[\"nsc\",{\"1\":{\"23\":2,\"249\":1,\"700\":5,\"751\":1,\"1048\":1,\"1060\":1,\"1138\":5,\"1139\":5,\"1176\":1}}],[\"n\",{\"0\":{\"139\":1,\"140\":1},\"1\":{\"11\":1,\"23\":2,\"32\":12,\"115\":1,\"116\":1,\"148\":1,\"174\":5,\"202\":1,\"217\":1,\"224\":1,\"231\":1,\"237\":1,\"238\":4,\"239\":1,\"247\":2,\"249\":2,\"251\":4,\"255\":2,\"259\":4,\"272\":6,\"275\":6,\"282\":4,\"321\":2,\"449\":2,\"509\":6,\"512\":4,\"519\":6,\"541\":4,\"562\":2,\"600\":2,\"606\":1,\"614\":6,\"633\":2,\"667\":1,\"668\":1,\"669\":1,\"671\":1,\"672\":3,\"676\":2,\"691\":4,\"692\":2,\"693\":1,\"695\":7,\"696\":5,\"697\":2,\"699\":1,\"700\":6,\"703\":1,\"710\":1,\"725\":9,\"731\":2,\"732\":2,\"733\":1,\"734\":9,\"736\":2,\"737\":2,\"740\":2,\"741\":2,\"747\":2,\"748\":2,\"752\":2,\"767\":3,\"768\":2,\"771\":4,\"773\":8,\"775\":2,\"776\":2,\"778\":4,\"781\":2,\"784\":3,\"785\":9,\"794\":1,\"797\":3,\"800\":2,\"801\":2,\"802\":6,\"803\":2,\"804\":2,\"806\":21,\"807\":7,\"809\":4,\"812\":2,\"815\":1,\"817\":5,\"825\":4,\"828\":10,\"835\":15,\"857\":1,\"863\":2,\"865\":2,\"881\":1,\"885\":2,\"914\":3,\"943\":1,\"945\":1,\"947\":3,\"950\":2,\"951\":2,\"953\":2,\"954\":2,\"955\":1,\"959\":1,\"965\":2,\"966\":1,\"967\":3,\"968\":4,\"969\":2,\"970\":2,\"971\":2,\"972\":2,\"997\":4,\"1001\":2,\"1010\":2,\"1025\":3,\"1048\":4,\"1062\":2,\"1063\":2,\"1066\":5,\"1068\":2,\"1073\":20,\"1074\":2,\"1075\":11,\"1081\":1,\"1083\":1,\"1133\":11,\"1138\":6,\"1139\":7,\"1153\":3,\"1158\":2,\"1178\":2,\"1182\":3,\"1190\":6,\"1192\":1,\"1195\":1,\"1205\":1,\"1207\":2,\"1209\":9,\"1214\":5,\"1216\":1,\"1221\":1,\"1241\":1,\"1243\":3,\"1244\":11,\"1245\":5,\"1246\":2,\"1247\":1,\"1248\":8,\"1252\":5,\"1253\":2,\"1270\":21,\"1273\":5,\"1297\":1,\"1314\":1,\"1339\":1,\"1341\":3,\"1342\":1,\"1351\":2,\"1352\":3,\"1357\":1,\"1368\":3,\"1372\":3,\"1375\":2,\"1377\":1,\"1379\":3,\"1427\":3,\"1430\":2,\"1451\":1,\"1464\":1,\"1468\":1,\"1470\":2,\"1471\":2,\"1472\":3,\"1485\":1,\"1489\":2,\"1505\":2,\"1514\":1,\"1515\":1,\"1524\":2,\"1526\":1,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":1,\"1534\":2,\"1539\":2,\"1552\":1,\"1558\":1,\"1563\":1,\"1575\":3,\"1578\":2,\"1579\":2,\"1580\":1,\"1585\":1,\"1594\":1,\"1600\":2,\"1603\":1,\"1618\":1,\"1619\":1,\"1622\":1,\"1624\":2,\"1626\":2,\"1638\":2,\"1639\":2,\"1640\":2,\"1643\":1,\"1644\":1,\"1645\":2,\"1646\":1,\"1654\":2,\"1655\":2,\"1658\":2,\"1659\":4,\"1660\":13,\"1661\":13,\"1662\":11,\"1664\":5,\"1665\":5,\"1669\":2,\"1688\":4,\"1693\":5,\"1696\":1,\"1697\":1,\"1698\":1,\"1704\":1,\"1705\":3,\"1706\":3,\"1707\":3,\"1708\":3,\"1712\":1,\"1715\":1,\"1719\":10,\"1739\":2,\"1752\":2,\"1755\":5,\"1756\":4,\"1769\":1,\"1776\":3,\"1777\":3,\"1778\":2,\"1785\":7,\"1803\":3,\"1804\":4,\"1805\":2,\"1806\":1,\"1807\":1,\"1808\":2,\"1810\":7,\"1812\":1,\"1816\":1,\"1820\":1,\"1850\":2,\"1852\":2,\"1859\":5,\"1877\":2,\"1905\":2,\"1912\":4,\"1918\":1,\"1929\":3,\"1941\":6,\"1947\":3,\"1957\":4,\"1958\":5,\"1959\":1,\"1960\":4,\"1962\":1,\"1987\":1,\"1989\":2,\"1991\":1,\"1993\":2,\"2001\":5,\"2002\":2,\"2084\":1,\"2089\":1,\"2090\":1,\"2095\":1,\"2099\":1,\"2143\":2,\"2236\":1,\"2241\":1,\"2243\":1,\"2244\":1,\"2246\":1,\"2248\":2,\"2250\":1,\"2253\":2,\"2255\":1,\"2263\":1,\"2265\":4,\"2268\":2,\"2269\":1,\"2270\":2,\"2271\":2,\"2272\":2,\"2317\":6,\"2325\":8,\"2330\":6,\"2357\":3,\"2373\":3,\"2375\":1,\"2385\":4,\"2387\":3,\"2430\":3,\"2431\":1,\"2498\":1,\"2500\":6,\"2554\":1,\"2555\":3,\"2559\":1,\"2568\":2,\"2578\":3,\"2592\":1,\"2600\":2,\"2616\":1,\"2617\":6,\"2634\":1,\"2635\":6}}],[\"ne\",{\"1\":{\"2568\":1}}],[\"necessitated\",{\"1\":{\"2452\":1}}],[\"necessarily\",{\"1\":{\"1011\":2}}],[\"necessary\",{\"0\":{\"48\":1},\"1\":{\"60\":1,\"706\":1,\"815\":1,\"863\":1,\"865\":1,\"1739\":2,\"1917\":1,\"2201\":1,\"2384\":1,\"2385\":1,\"2387\":1,\"2429\":2,\"2552\":2}}],[\"neil\",{\"1\":{\"2388\":1,\"2524\":1,\"2618\":1}}],[\"neighbor\",{\"1\":{\"1755\":1,\"1756\":1}}],[\"neighborhood\",{\"1\":{\"21\":2,\"115\":1}}],[\"near\",{\"1\":{\"1860\":1}}],[\"nearest\",{\"1\":{\"1011\":2,\"1755\":1,\"1756\":1,\"1834\":1,\"1869\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1876\":1}}],[\"never\",{\"1\":{\"1243\":1}}],[\"nevertheless\",{\"1\":{\"1015\":1}}],[\"nesterov\",{\"1\":{\"1972\":1}}],[\"nesteddictaction\",{\"0\":{\"2313\":1},\"1\":{\"2313\":1}}],[\"nested\",{\"0\":{\"2313\":1},\"1\":{\"64\":1,\"1340\":1,\"2313\":1}}],[\"nessasary\",{\"1\":{\"1180\":1}}],[\"neurips\",{\"1\":{\"1857\":1,\"1858\":1}}],[\"neurons\",{\"1\":{\"770\":1}}],[\"neuralbeamformer\",{\"0\":{\"1611\":1},\"1\":{\"1611\":1}}],[\"neural\",{\"0\":{\"173\":1,\"174\":1,\"1611\":1,\"2489\":1,\"2609\":1,\"2626\":1},\"1\":{\"5\":1,\"168\":1,\"170\":2,\"172\":1,\"180\":1,\"217\":1,\"224\":1,\"231\":1,\"235\":1,\"255\":1,\"807\":1,\"815\":1,\"826\":1,\"1198\":1,\"1429\":1,\"1484\":1,\"1566\":1,\"1601\":1,\"1611\":1,\"1639\":1,\"1719\":1,\"2142\":1,\"2156\":1,\"2264\":1,\"2400\":1,\"2536\":1}}],[\"nelson\",{\"1\":{\"130\":2}}],[\"negate\",{\"0\":{\"1338\":1},\"1\":{\"1338\":1}}],[\"negative\",{\"1\":{\"115\":1,\"150\":1,\"727\":1,\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"1093\":1,\"1096\":1,\"1101\":1,\"1171\":2,\"1186\":1,\"1206\":2,\"1210\":1,\"1552\":2,\"1639\":1,\"1640\":1,\"1765\":1,\"1778\":3,\"1800\":1,\"1801\":2,\"1803\":1,\"1804\":1,\"1805\":2,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":1,\"1849\":1,\"1850\":3,\"1851\":1,\"1852\":3,\"1856\":1,\"1857\":1,\"1858\":1,\"1861\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1870\":1,\"1871\":1,\"1877\":2,\"1878\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1}}],[\"neg\",{\"1\":{\"115\":1,\"1096\":2}}],[\"next\",{\"0\":{\"243\":1},\"1\":{\"65\":1,\"171\":1,\"202\":1,\"695\":3,\"696\":2,\"697\":7,\"706\":7,\"711\":11,\"734\":4,\"773\":3,\"795\":1,\"796\":5,\"806\":2,\"815\":2,\"817\":2,\"828\":4,\"838\":2,\"981\":2,\"1073\":2,\"1133\":2,\"1190\":4,\"1214\":2,\"1221\":2,\"1244\":4,\"1255\":1,\"1270\":2,\"1273\":2,\"1451\":2,\"1514\":2,\"1557\":2,\"1585\":2,\"1612\":2,\"1615\":2,\"1623\":2,\"1637\":2,\"1957\":4,\"1958\":2,\"1959\":2,\"1960\":4,\"2001\":2,\"2099\":1,\"2102\":1,\"2199\":2,\"2209\":1,\"2422\":1,\"2432\":1,\"2499\":1,\"2500\":1,\"2545\":1,\"2585\":1,\"2617\":2,\"2635\":2}}],[\"netlib\",{\"1\":{\"1695\":1}}],[\"net\",{\"1\":{\"80\":1,\"130\":1,\"668\":1,\"858\":2,\"862\":2,\"886\":2,\"910\":2,\"934\":2,\"1652\":1,\"1752\":1,\"1841\":1,\"1862\":2,\"2363\":1,\"2506\":1,\"2653\":1}}],[\"nets\",{\"0\":{\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"695\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"705\":1,\"706\":1,\"707\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":2,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"782\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"789\":1,\"790\":1,\"791\":1,\"792\":1,\"793\":1,\"794\":1,\"795\":1,\"797\":1,\"798\":1,\"799\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":1,\"813\":1,\"814\":1,\"816\":1,\"817\":1,\"818\":1,\"819\":1,\"820\":1,\"821\":1,\"822\":1,\"823\":1,\"824\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"857\":1,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":1,\"867\":1,\"868\":1,\"869\":1,\"870\":1,\"871\":1,\"872\":1,\"873\":1,\"874\":1,\"875\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":2,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":2,\"890\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":2,\"901\":2,\"903\":2,\"905\":1,\"906\":2,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":2,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":2,\"926\":2,\"927\":2,\"929\":2,\"930\":2,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":1,\"2676\":1},\"1\":{\"20\":1,\"24\":2,\"173\":1,\"194\":1,\"629\":1,\"646\":1,\"676\":2,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"695\":1,\"696\":1,\"697\":2,\"698\":2,\"699\":2,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"705\":1,\"706\":3,\"707\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":2,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"742\":2,\"743\":1,\"745\":1,\"746\":1,\"747\":2,\"748\":1,\"749\":2,\"750\":2,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":2,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"781\":2,\"782\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"789\":1,\"790\":1,\"791\":1,\"792\":1,\"793\":2,\"794\":1,\"795\":1,\"796\":1,\"797\":1,\"798\":1,\"799\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":2,\"813\":1,\"814\":1,\"815\":8,\"816\":1,\"817\":1,\"818\":1,\"819\":1,\"820\":2,\"821\":1,\"822\":1,\"823\":1,\"824\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"829\":2,\"830\":1,\"831\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"857\":2,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":1,\"867\":2,\"868\":1,\"869\":1,\"870\":2,\"871\":1,\"872\":1,\"873\":2,\"874\":2,\"875\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":2,\"881\":1,\"882\":1,\"883\":2,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":2,\"890\":2,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":2,\"901\":2,\"903\":2,\"905\":1,\"906\":2,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":2,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":2,\"926\":2,\"927\":2,\"929\":2,\"930\":2,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":1,\"1017\":1,\"1133\":1,\"1149\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1204\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1971\":1,\"2001\":1,\"2004\":1,\"2029\":1}}],[\"network=none\",{\"1\":{\"692\":1}}],[\"networks\",{\"0\":{\"173\":1,\"174\":1},\"1\":{\"170\":2,\"235\":1,\"762\":1,\"764\":1,\"807\":1,\"815\":1,\"1352\":1,\"1572\":1,\"1639\":1,\"2400\":1,\"2536\":1}}],[\"network\",{\"0\":{\"117\":1,\"240\":1,\"241\":1,\"766\":1,\"1064\":1,\"1321\":1},\"1\":{\"5\":1,\"22\":2,\"45\":3,\"114\":1,\"117\":4,\"164\":1,\"168\":1,\"172\":1,\"180\":1,\"235\":3,\"240\":3,\"678\":1,\"700\":3,\"701\":2,\"725\":1,\"727\":1,\"728\":1,\"764\":3,\"766\":5,\"786\":1,\"802\":2,\"803\":2,\"821\":2,\"825\":1,\"826\":1,\"858\":4,\"862\":1,\"886\":1,\"910\":3,\"911\":1,\"934\":1,\"1048\":3,\"1050\":1,\"1053\":1,\"1056\":1,\"1057\":3,\"1059\":3,\"1064\":3,\"1138\":3,\"1139\":3,\"1171\":1,\"1173\":1,\"1198\":1,\"1206\":1,\"1211\":2,\"1224\":2,\"1287\":2,\"1321\":1,\"1336\":2,\"1348\":2,\"1484\":1,\"1515\":2,\"1522\":1,\"1523\":1,\"1537\":1,\"1539\":1,\"1558\":1,\"1566\":1,\"1581\":1,\"1601\":1,\"1662\":1,\"1670\":1,\"1671\":1,\"1834\":1,\"1862\":3,\"1876\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2027\":1,\"2076\":1,\"2078\":2,\"2081\":2,\"2083\":2,\"2090\":1,\"2095\":1,\"2156\":1,\"2263\":1,\"2264\":1,\"2473\":1}}],[\"newer\",{\"1\":{\"2441\":1}}],[\"newly\",{\"1\":{\"2430\":1,\"2555\":1,\"2567\":1}}],[\"newdatadir\",{\"1\":{\"288\":1}}],[\"newtask\",{\"1\":{\"56\":2,\"59\":1,\"60\":1}}],[\"newmodel\",{\"1\":{\"56\":2}}],[\"new\",{\"0\":{\"124\":1,\"152\":1,\"1733\":1,\"1933\":1,\"1934\":1,\"2379\":1,\"2388\":1,\"2524\":1,\"2566\":1,\"2636\":1,\"2641\":1,\"2643\":1,\"2644\":1,\"2645\":1},\"1\":{\"11\":1,\"14\":1,\"25\":2,\"48\":2,\"59\":3,\"60\":1,\"85\":1,\"112\":1,\"124\":4,\"135\":1,\"140\":1,\"144\":1,\"152\":1,\"161\":1,\"162\":3,\"182\":1,\"237\":1,\"244\":1,\"253\":1,\"265\":1,\"269\":1,\"635\":1,\"637\":1,\"640\":1,\"642\":1,\"643\":3,\"661\":1,\"672\":1,\"676\":1,\"691\":10,\"692\":1,\"694\":1,\"695\":1,\"696\":1,\"697\":14,\"698\":1,\"699\":1,\"706\":5,\"710\":2,\"712\":4,\"725\":3,\"734\":2,\"749\":1,\"765\":1,\"767\":1,\"773\":2,\"781\":1,\"796\":1,\"797\":5,\"798\":1,\"806\":3,\"809\":1,\"810\":1,\"815\":4,\"816\":1,\"817\":1,\"824\":2,\"828\":2,\"836\":1,\"866\":1,\"913\":2,\"922\":1,\"1046\":3,\"1052\":4,\"1066\":5,\"1069\":2,\"1071\":1,\"1073\":2,\"1075\":3,\"1083\":2,\"1116\":1,\"1133\":1,\"1190\":2,\"1214\":1,\"1221\":1,\"1244\":2,\"1270\":3,\"1273\":1,\"1553\":1,\"1643\":1,\"1644\":1,\"1733\":1,\"1923\":1,\"1933\":2,\"1934\":2,\"1944\":2,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"2001\":1,\"2168\":1,\"2360\":6,\"2391\":4,\"2392\":1,\"2394\":4,\"2400\":1,\"2423\":1,\"2425\":1,\"2433\":1,\"2438\":1,\"2458\":6,\"2472\":1,\"2476\":2,\"2492\":1,\"2521\":1,\"2522\":1,\"2523\":7,\"2527\":4,\"2528\":1,\"2529\":1,\"2530\":4,\"2536\":1,\"2546\":1,\"2548\":1,\"2558\":2,\"2564\":2,\"2566\":2,\"2573\":1,\"2582\":6,\"2584\":7,\"2585\":1,\"2618\":3,\"2628\":1,\"2635\":2,\"2637\":1,\"2638\":1,\"2642\":2,\"2643\":1,\"2644\":3,\"2645\":1,\"2648\":1}}],[\"needs\",{\"1\":{\"109\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1187\":4,\"1189\":1,\"1202\":4,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1245\":1,\"1246\":1,\"1250\":1,\"1254\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1286\":4,\"1287\":4,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1917\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2499\":1,\"2584\":1,\"2585\":1}}],[\"needed\",{\"1\":{\"19\":1,\"27\":1,\"101\":1,\"115\":1,\"1243\":1,\"1245\":1,\"1406\":1,\"2372\":1,\"2394\":1,\"2450\":1,\"2530\":1}}],[\"need\",{\"1\":{\"2\":1,\"3\":2,\"25\":1,\"33\":1,\"44\":1,\"45\":1,\"47\":1,\"57\":2,\"60\":1,\"74\":1,\"76\":1,\"77\":1,\"78\":1,\"84\":1,\"85\":4,\"92\":2,\"94\":1,\"95\":1,\"97\":1,\"99\":1,\"100\":1,\"102\":1,\"113\":1,\"115\":1,\"124\":3,\"126\":1,\"127\":1,\"134\":2,\"135\":1,\"136\":1,\"144\":1,\"148\":1,\"182\":1,\"295\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"745\":1,\"746\":1,\"797\":1,\"1243\":1,\"1245\":1,\"1429\":1,\"2019\":2,\"2099\":1,\"2355\":1,\"2362\":1,\"2363\":1,\"2373\":1,\"2375\":1,\"2384\":5,\"2387\":3,\"2394\":4,\"2395\":1,\"2398\":1,\"2401\":1,\"2403\":1,\"2412\":1,\"2430\":2,\"2431\":1,\"2433\":1,\"2468\":1,\"2481\":1,\"2504\":1,\"2506\":1,\"2514\":1,\"2530\":4,\"2531\":1,\"2534\":1,\"2537\":1,\"2539\":1,\"2555\":2,\"2558\":1,\"2559\":1,\"2565\":1,\"2566\":1,\"2568\":2,\"2576\":1,\"2598\":2,\"2635\":1,\"2651\":1,\"2653\":1,\"2659\":1}}],[\"nohier\",{\"1\":{\"2460\":3,\"2461\":2}}],[\"nout\",{\"1\":{\"2030\":2,\"2034\":1,\"2040\":1}}],[\"nom\",{\"1\":{\"1870\":1}}],[\"nomask\",{\"0\":{\"1368\":1,\"1369\":1,\"1370\":1,\"1372\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":1},\"1\":{\"1115\":2,\"1269\":2,\"1368\":1,\"1369\":2,\"1370\":1,\"1372\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":2,\"1890\":2,\"1892\":1}}],[\"nolinear\",{\"1\":{\"1380\":1,\"1682\":1}}],[\"noqa\",{\"1\":{\"1217\":1,\"1973\":1}}],[\"noaliassafedumper\",{\"0\":{\"2315\":1},\"1\":{\"2315\":1}}],[\"noatt\",{\"0\":{\"791\":1},\"1\":{\"791\":1}}],[\"noamp\",{\"1\":{\"2454\":1,\"2455\":2}}],[\"noamlr\",{\"0\":{\"2019\":1},\"1\":{\"2019\":2,\"2021\":2,\"2584\":1}}],[\"noamopt\",{\"0\":{\"792\":1},\"1\":{\"792\":2,\"888\":1}}],[\"noamscheduler\",{\"0\":{\"670\":1},\"1\":{\"670\":2}}],[\"noam\",{\"0\":{\"2019\":1},\"1\":{\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"670\":2,\"2019\":3}}],[\"noscheduler\",{\"0\":{\"669\":1},\"1\":{\"669\":2}}],[\"noises\",{\"1\":{\"1714\":1,\"2197\":1}}],[\"noise^\",{\"1\":{\"1713\":1}}],[\"noise1\",{\"1\":{\"1611\":1}}],[\"noiseinjection\",{\"0\":{\"948\":1},\"1\":{\"948\":2}}],[\"noise=false\",{\"1\":{\"627\":1}}],[\"noise\",{\"0\":{\"632\":1,\"1777\":1,\"2151\":2,\"2274\":1},\"1\":{\"251\":2,\"259\":2,\"455\":4,\"464\":4,\"470\":4,\"632\":4,\"885\":1,\"948\":1,\"1179\":4,\"1451\":1,\"1466\":1,\"1505\":3,\"1514\":1,\"1516\":3,\"1523\":3,\"1524\":3,\"1528\":3,\"1534\":3,\"1539\":3,\"1553\":1,\"1557\":1,\"1558\":3,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":3,\"1571\":1,\"1573\":1,\"1585\":1,\"1604\":1,\"1611\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1626\":3,\"1637\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1654\":1,\"1658\":3,\"1666\":1,\"1667\":2,\"1668\":1,\"1669\":3,\"1680\":3,\"1698\":3,\"1704\":4,\"1705\":1,\"1706\":2,\"1707\":5,\"1712\":6,\"1713\":4,\"1715\":7,\"1739\":3,\"1777\":3,\"1778\":6,\"1797\":4,\"1804\":6,\"1805\":6,\"1845\":1,\"1846\":1,\"1847\":1,\"1849\":1,\"1856\":1,\"1858\":1,\"1861\":1,\"1862\":2,\"1868\":3,\"1871\":10,\"1877\":6,\"1878\":6,\"1880\":1,\"1929\":2,\"2151\":4,\"2178\":4,\"2179\":4,\"2184\":7,\"2186\":1,\"2191\":4,\"2194\":4,\"2195\":4,\"2197\":15,\"2200\":7,\"2202\":2,\"2204\":1,\"2260\":4,\"2273\":3,\"2274\":3,\"2364\":2,\"2507\":2,\"2510\":4,\"2513\":2,\"2638\":3,\"2640\":2,\"2654\":2,\"2658\":2}}],[\"noisy\",{\"0\":{\"2369\":1,\"2487\":1,\"2491\":1,\"2606\":1,\"2610\":1,\"2623\":1,\"2627\":1},\"1\":{\"247\":1,\"1516\":2,\"1646\":3,\"1712\":1,\"1715\":1,\"2040\":1,\"2260\":2,\"2367\":1,\"2369\":1,\"2481\":1,\"2485\":1,\"2487\":2,\"2491\":2,\"2604\":1,\"2606\":1,\"2610\":1,\"2621\":1,\"2623\":1,\"2627\":1,\"2638\":1}}],[\"nonpadding\",{\"1\":{\"1182\":1}}],[\"nondifferentiable\",{\"1\":{\"950\":1,\"968\":1}}],[\"nonlinear=\",{\"1\":{\"1375\":1,\"1595\":1,\"1664\":2,\"1665\":2}}],[\"nonlinearity=\",{\"1\":{\"1605\":1}}],[\"nonlinearity\",{\"1\":{\"1031\":1}}],[\"nonlinear\",{\"0\":{\"1380\":1,\"1682\":1},\"1\":{\"803\":1,\"1375\":1,\"1380\":1,\"1505\":3,\"1515\":3,\"1524\":1,\"1525\":1,\"1528\":3,\"1529\":3,\"1534\":3,\"1539\":3,\"1626\":3,\"1654\":3,\"1658\":5,\"1659\":6,\"1664\":2,\"1665\":2,\"1669\":3,\"1671\":1,\"1682\":2,\"1765\":4,\"1778\":6,\"1800\":4,\"1801\":4,\"1803\":4,\"1805\":4,\"1834\":4,\"1844\":4,\"1845\":2,\"1846\":2,\"1847\":4,\"1848\":4,\"1849\":4,\"1850\":6,\"1851\":4,\"1852\":6,\"1856\":4,\"1857\":6,\"1858\":4,\"1861\":6,\"1866\":4,\"1867\":4,\"1870\":2,\"1876\":4,\"1877\":4}}],[\"nonsplit\",{\"1\":{\"461\":2,\"2120\":1,\"2137\":1,\"2178\":1}}],[\"noneordict\",{\"1\":{\"1659\":1}}],[\"noneorint\",{\"1\":{\"1430\":1,\"1470\":1,\"1471\":1,\"1670\":2}}],[\"nonepredictor\",{\"0\":{\"1615\":1},\"1\":{\"1615\":1}}],[\"nonenorm2d\",{\"0\":{\"1613\":1},\"1\":{\"1613\":1}}],[\"nonecorrector\",{\"0\":{\"1612\":1},\"1\":{\"1612\":1}}],[\"none=\",{\"1\":{\"1063\":2,\"1115\":1,\"2180\":6}}],[\"none\",{\"0\":{\"2306\":2,\"2319\":1,\"2326\":1,\"2327\":1,\"2339\":1},\"1\":{\"116\":2,\"295\":28,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"422\":1,\"429\":2,\"443\":2,\"449\":1,\"461\":2,\"478\":1,\"638\":1,\"669\":1,\"691\":13,\"692\":2,\"693\":7,\"697\":15,\"700\":2,\"725\":7,\"744\":1,\"745\":2,\"746\":2,\"751\":10,\"752\":4,\"754\":2,\"759\":4,\"770\":1,\"773\":1,\"778\":4,\"797\":3,\"798\":2,\"806\":7,\"815\":2,\"824\":7,\"825\":1,\"827\":2,\"834\":2,\"857\":3,\"865\":1,\"915\":1,\"929\":2,\"1013\":4,\"1015\":2,\"1046\":4,\"1048\":2,\"1049\":2,\"1050\":2,\"1051\":4,\"1052\":4,\"1053\":4,\"1054\":4,\"1055\":4,\"1056\":2,\"1057\":3,\"1060\":11,\"1062\":2,\"1063\":10,\"1065\":13,\"1066\":2,\"1068\":4,\"1069\":10,\"1071\":2,\"1073\":14,\"1074\":2,\"1075\":4,\"1076\":4,\"1081\":2,\"1083\":15,\"1093\":4,\"1098\":4,\"1101\":2,\"1113\":8,\"1114\":1,\"1115\":3,\"1119\":2,\"1127\":2,\"1133\":6,\"1138\":4,\"1139\":4,\"1140\":3,\"1141\":2,\"1145\":2,\"1148\":5,\"1149\":7,\"1150\":7,\"1158\":8,\"1169\":7,\"1170\":2,\"1171\":9,\"1172\":5,\"1176\":10,\"1178\":2,\"1179\":2,\"1180\":2,\"1181\":2,\"1186\":1,\"1187\":2,\"1190\":2,\"1193\":7,\"1195\":4,\"1200\":3,\"1202\":2,\"1203\":4,\"1206\":7,\"1207\":2,\"1210\":1,\"1211\":2,\"1214\":4,\"1215\":6,\"1216\":4,\"1218\":2,\"1219\":12,\"1220\":1,\"1222\":3,\"1224\":2,\"1239\":3,\"1248\":1,\"1255\":2,\"1256\":2,\"1257\":4,\"1269\":10,\"1270\":7,\"1272\":5,\"1273\":14,\"1282\":2,\"1284\":2,\"1285\":4,\"1286\":2,\"1287\":2,\"1336\":2,\"1348\":2,\"1371\":17,\"1386\":2,\"1407\":2,\"1426\":2,\"1430\":1,\"1431\":2,\"1432\":2,\"1435\":2,\"1441\":2,\"1445\":2,\"1454\":6,\"1462\":2,\"1463\":4,\"1464\":2,\"1505\":2,\"1510\":4,\"1511\":2,\"1515\":2,\"1516\":2,\"1517\":1,\"1523\":2,\"1524\":5,\"1528\":2,\"1529\":2,\"1534\":2,\"1539\":2,\"1551\":3,\"1552\":5,\"1553\":22,\"1554\":5,\"1558\":2,\"1603\":1,\"1611\":2,\"1616\":2,\"1617\":2,\"1626\":2,\"1643\":6,\"1644\":6,\"1645\":2,\"1646\":2,\"1652\":2,\"1654\":4,\"1658\":2,\"1659\":2,\"1660\":4,\"1661\":4,\"1662\":4,\"1664\":1,\"1665\":1,\"1669\":2,\"1670\":2,\"1671\":4,\"1698\":2,\"1705\":2,\"1707\":2,\"1719\":4,\"1741\":2,\"1765\":2,\"1773\":126,\"1777\":1,\"1778\":44,\"1785\":6,\"1791\":3,\"1798\":2,\"1800\":2,\"1803\":2,\"1804\":55,\"1805\":52,\"1808\":1,\"1810\":3,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":2,\"1834\":2,\"1835\":2,\"1837\":42,\"1844\":4,\"1850\":16,\"1851\":26,\"1852\":4,\"1859\":8,\"1862\":4,\"1863\":2,\"1864\":2,\"1865\":2,\"1868\":4,\"1870\":3,\"1871\":2,\"1876\":2,\"1877\":24,\"1878\":26,\"1880\":6,\"1892\":4,\"1893\":4,\"1894\":2,\"1895\":6,\"1896\":8,\"1897\":2,\"1898\":2,\"1900\":4,\"1902\":2,\"1906\":4,\"1909\":2,\"1910\":2,\"1912\":9,\"1914\":2,\"1915\":2,\"1917\":2,\"1918\":7,\"1919\":2,\"1920\":2,\"1926\":6,\"1929\":4,\"1932\":4,\"1941\":4,\"1947\":4,\"1949\":2,\"1954\":2,\"1956\":2,\"1957\":1,\"1958\":2,\"1960\":3,\"1962\":2,\"1964\":3,\"1970\":3,\"1975\":6,\"1984\":34,\"1985\":12,\"1987\":5,\"1989\":8,\"1991\":5,\"2001\":14,\"2002\":22,\"2003\":6,\"2004\":12,\"2005\":1,\"2006\":2,\"2007\":2,\"2010\":1,\"2011\":2,\"2012\":2,\"2014\":2,\"2015\":2,\"2016\":2,\"2017\":2,\"2027\":24,\"2029\":3,\"2044\":2,\"2046\":13,\"2052\":2,\"2054\":3,\"2068\":2,\"2076\":23,\"2082\":129,\"2084\":14,\"2086\":50,\"2087\":50,\"2089\":24,\"2090\":50,\"2095\":54,\"2099\":14,\"2102\":2,\"2106\":2,\"2109\":6,\"2113\":6,\"2115\":6,\"2116\":6,\"2120\":4,\"2128\":3,\"2129\":2,\"2130\":3,\"2131\":5,\"2133\":2,\"2136\":4,\"2137\":16,\"2176\":5,\"2178\":30,\"2179\":30,\"2180\":12,\"2181\":8,\"2182\":2,\"2184\":12,\"2185\":2,\"2186\":2,\"2189\":4,\"2191\":23,\"2193\":23,\"2194\":20,\"2195\":24,\"2196\":16,\"2197\":6,\"2199\":11,\"2200\":16,\"2201\":7,\"2202\":2,\"2203\":2,\"2204\":2,\"2213\":2,\"2214\":2,\"2216\":2,\"2217\":2,\"2218\":2,\"2219\":2,\"2230\":2,\"2236\":3,\"2237\":8,\"2240\":56,\"2241\":12,\"2243\":22,\"2244\":26,\"2246\":5,\"2248\":8,\"2250\":5,\"2254\":2,\"2255\":26,\"2259\":2,\"2260\":9,\"2263\":22,\"2264\":20,\"2265\":2,\"2266\":8,\"2278\":54,\"2279\":24,\"2290\":3,\"2292\":3,\"2294\":21,\"2295\":2,\"2304\":1,\"2305\":1,\"2306\":4,\"2316\":2,\"2317\":12,\"2319\":3,\"2320\":2,\"2325\":4,\"2326\":1,\"2327\":3,\"2328\":2,\"2330\":4,\"2339\":3,\"2340\":2,\"2345\":2,\"2363\":6,\"2364\":3,\"2369\":1,\"2372\":1,\"2386\":1,\"2487\":1,\"2491\":1,\"2497\":1,\"2500\":3,\"2501\":1,\"2506\":2,\"2507\":2,\"2510\":9,\"2512\":2,\"2513\":2,\"2514\":3,\"2558\":1,\"2592\":2,\"2596\":2,\"2606\":1,\"2607\":1,\"2610\":1,\"2614\":1,\"2615\":1,\"2617\":3,\"2623\":1,\"2624\":1,\"2627\":1,\"2632\":1,\"2633\":1,\"2635\":3,\"2653\":6,\"2654\":3,\"2657\":2,\"2658\":3,\"2659\":3}}],[\"nonetype\",{\"1\":{\"106\":1,\"2106\":2,\"2180\":6,\"2202\":2}}],[\"non\",{\"0\":{\"52\":1,\"899\":1,\"2210\":1},\"1\":{\"45\":1,\"82\":2,\"107\":1,\"111\":1,\"136\":1,\"461\":4,\"579\":2,\"635\":1,\"637\":1,\"791\":1,\"795\":1,\"899\":2,\"900\":5,\"1011\":1,\"1187\":1,\"1202\":1,\"1205\":1,\"1286\":1,\"1287\":1,\"1356\":1,\"1375\":2,\"1379\":1,\"1398\":1,\"1430\":1,\"1470\":1,\"1658\":1,\"1664\":3,\"1665\":3,\"1670\":2,\"1671\":1,\"1983\":1,\"1986\":1,\"1994\":1,\"2120\":2,\"2130\":2,\"2136\":2,\"2137\":2,\"2166\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2210\":2,\"2252\":1,\"2584\":1,\"2600\":1}}],[\"nodeid\",{\"1\":{\"2216\":1,\"2217\":1}}],[\"node54\",{\"1\":{\"1695\":1}}],[\"node\",{\"0\":{\"34\":1,\"40\":1,\"42\":1,\"2216\":1},\"1\":{\"32\":3,\"36\":1,\"84\":1,\"94\":1,\"148\":1,\"597\":1,\"745\":2,\"746\":2,\"2216\":3,\"2217\":1}}],[\"nodes\",{\"0\":{\"2217\":1},\"1\":{\"32\":2,\"94\":1,\"148\":1,\"377\":2,\"2217\":2}}],[\"nodev\",{\"1\":{\"28\":1,\"171\":1,\"175\":1,\"191\":1,\"192\":1,\"194\":2,\"215\":1,\"216\":1,\"238\":4,\"239\":4,\"240\":9,\"241\":2,\"242\":2,\"2373\":8,\"2375\":6,\"2377\":2,\"2385\":6,\"2430\":5,\"2431\":1,\"2432\":6,\"2433\":4,\"2436\":2,\"2440\":1,\"2555\":8,\"2558\":4,\"2559\":2,\"2562\":2,\"2564\":1,\"2568\":1,\"2569\":3,\"2584\":4,\"2585\":2}}],[\"norms\",{\"1\":{\"1244\":2,\"1252\":2}}],[\"norms=true\",{\"1\":{\"1244\":1,\"1252\":1}}],[\"norm=\",{\"1\":{\"1581\":1}}],[\"norm=none\",{\"1\":{\"1244\":1,\"1252\":1,\"1254\":1}}],[\"norm=false\",{\"1\":{\"1199\":1,\"1767\":1,\"1768\":1,\"1782\":1,\"1793\":1,\"1795\":1}}],[\"norm=true\",{\"1\":{\"802\":1,\"1162\":1,\"1163\":1,\"1279\":1,\"1800\":1,\"2078\":1,\"2083\":1}}],[\"norm=1\",{\"1\":{\"778\":1}}],[\"norm\",{\"0\":{\"769\":1,\"1381\":1,\"1683\":1,\"1685\":1},\"1\":{\"23\":1,\"115\":31,\"116\":1,\"119\":1,\"174\":3,\"217\":1,\"224\":1,\"231\":1,\"249\":2,\"251\":4,\"259\":4,\"496\":4,\"700\":2,\"711\":1,\"712\":2,\"749\":1,\"752\":2,\"760\":2,\"769\":1,\"778\":1,\"802\":1,\"821\":1,\"826\":1,\"831\":2,\"923\":2,\"932\":4,\"941\":2,\"959\":1,\"960\":2,\"1048\":2,\"1049\":4,\"1050\":4,\"1051\":2,\"1052\":2,\"1054\":4,\"1056\":4,\"1066\":1,\"1068\":4,\"1075\":1,\"1093\":10,\"1115\":2,\"1133\":1,\"1138\":4,\"1139\":4,\"1148\":1,\"1149\":1,\"1150\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1200\":1,\"1203\":1,\"1237\":1,\"1244\":3,\"1252\":3,\"1254\":1,\"1269\":7,\"1272\":1,\"1370\":1,\"1377\":2,\"1378\":1,\"1379\":2,\"1381\":3,\"1430\":2,\"1505\":1,\"1506\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1537\":2,\"1539\":2,\"1546\":1,\"1581\":1,\"1598\":2,\"1648\":2,\"1652\":2,\"1658\":2,\"1659\":2,\"1663\":1,\"1664\":2,\"1665\":2,\"1669\":1,\"1670\":2,\"1671\":2,\"1683\":3,\"1685\":1,\"1765\":5,\"1778\":7,\"1800\":4,\"1801\":5,\"1803\":5,\"1804\":7,\"1805\":8,\"1807\":1,\"1810\":2,\"1835\":1,\"1844\":5,\"1845\":2,\"1846\":5,\"1847\":7,\"1848\":8,\"1849\":10,\"1850\":7,\"1851\":5,\"1852\":7,\"1857\":5,\"1858\":5,\"1861\":4,\"1862\":5,\"1863\":3,\"1864\":2,\"1865\":2,\"1870\":2,\"1871\":5,\"1877\":8,\"1878\":7,\"1880\":5,\"1906\":4,\"1920\":2,\"1949\":4,\"2001\":1,\"2002\":2,\"2004\":1,\"2027\":1,\"2029\":1,\"2040\":1,\"2054\":1,\"2078\":1,\"2083\":1,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2279\":2,\"2290\":2,\"2292\":2}}],[\"normalzed\",{\"1\":{\"238\":1}}],[\"normalizing\",{\"1\":{\"1719\":1}}],[\"normalize=false\",{\"1\":{\"1314\":1}}],[\"normalizer\",{\"1\":{\"1265\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"2358\":1,\"2359\":2,\"2360\":1,\"2472\":2,\"2474\":1,\"2476\":3,\"2520\":1,\"2521\":5,\"2579\":1,\"2580\":2,\"2581\":1,\"2582\":1,\"2648\":2,\"2649\":1}}],[\"normalized=false\",{\"1\":{\"1799\":1}}],[\"normalizedpositionfeedforward\",{\"1\":{\"1070\":1}}],[\"normalizedpositionwisefeedforward\",{\"0\":{\"1070\":1},\"1\":{\"1066\":1,\"1070\":4}}],[\"normalized\",{\"1\":{\"691\":1,\"693\":1,\"697\":1,\"797\":1,\"1047\":2,\"1072\":2,\"1080\":2,\"1158\":1,\"1171\":1,\"1172\":1,\"1206\":1,\"1643\":1,\"1644\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1680\":1,\"1693\":1,\"1696\":1,\"1697\":1,\"1702\":1,\"1708\":1,\"1719\":2,\"1755\":1,\"1859\":3,\"1892\":1,\"1918\":1,\"1955\":1,\"1970\":1,\"1975\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"2027\":1,\"2076\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2267\":2,\"2268\":2,\"2440\":1,\"2558\":1,\"2584\":1}}],[\"normalize\",{\"0\":{\"1902\":1},\"1\":{\"102\":1,\"297\":1,\"307\":2,\"327\":2,\"343\":4,\"350\":4,\"368\":4,\"391\":2,\"399\":2,\"408\":2,\"422\":2,\"443\":2,\"449\":2,\"509\":1,\"512\":1,\"525\":1,\"585\":1,\"691\":2,\"693\":2,\"697\":2,\"700\":1,\"711\":2,\"749\":2,\"754\":2,\"768\":3,\"797\":1,\"826\":2,\"1048\":1,\"1057\":2,\"1113\":1,\"1115\":2,\"1133\":2,\"1138\":2,\"1139\":2,\"1148\":2,\"1149\":2,\"1150\":2,\"1167\":1,\"1168\":1,\"1171\":1,\"1178\":2,\"1180\":2,\"1181\":1,\"1196\":1,\"1197\":1,\"1200\":2,\"1203\":2,\"1204\":1,\"1206\":1,\"1207\":1,\"1271\":1,\"1272\":2,\"1273\":1,\"1371\":1,\"1505\":2,\"1531\":2,\"1551\":7,\"1553\":8,\"1604\":3,\"1645\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":2,\"1719\":1,\"1771\":3,\"1773\":3,\"1778\":2,\"1787\":2,\"1788\":2,\"1798\":2,\"1800\":2,\"1804\":2,\"1805\":1,\"1837\":3,\"1850\":2,\"1851\":4,\"1852\":2,\"1874\":2,\"1877\":1,\"1878\":2,\"1892\":1,\"1893\":1,\"1902\":2,\"1975\":1,\"1984\":2,\"1996\":1,\"2001\":2,\"2004\":2,\"2026\":1,\"2027\":1,\"2029\":2,\"2046\":1,\"2054\":2,\"2076\":1,\"2082\":3,\"2090\":4,\"2127\":1,\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2200\":1,\"2240\":3,\"2243\":4,\"2244\":4,\"2255\":3,\"2264\":4,\"2278\":2,\"2279\":4,\"2368\":2,\"2371\":2,\"2375\":1,\"2385\":1,\"2440\":6,\"2486\":2,\"2490\":2,\"2494\":2,\"2558\":2,\"2605\":2,\"2609\":2,\"2612\":2,\"2622\":2,\"2626\":2,\"2630\":2}}],[\"normalization=<class\",{\"1\":{\"1501\":1,\"1629\":1}}],[\"normalization=true\",{\"1\":{\"887\":1,\"1711\":1}}],[\"normalization\",{\"0\":{\"1047\":1,\"1072\":1,\"1080\":1,\"1098\":2,\"1212\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1503\":1,\"1583\":1,\"1613\":1,\"1676\":1,\"1680\":1,\"1709\":2,\"2440\":1},\"1\":{\"21\":2,\"23\":1,\"56\":1,\"115\":15,\"116\":4,\"119\":1,\"235\":1,\"238\":1,\"242\":1,\"496\":1,\"506\":1,\"712\":1,\"730\":1,\"754\":2,\"760\":1,\"778\":1,\"802\":1,\"821\":1,\"826\":3,\"887\":1,\"932\":1,\"1047\":2,\"1049\":3,\"1050\":3,\"1051\":1,\"1052\":1,\"1054\":3,\"1056\":3,\"1057\":1,\"1065\":4,\"1066\":6,\"1068\":3,\"1070\":4,\"1071\":3,\"1072\":2,\"1074\":7,\"1075\":6,\"1080\":2,\"1093\":4,\"1098\":8,\"1127\":1,\"1198\":2,\"1210\":1,\"1211\":1,\"1212\":1,\"1244\":1,\"1252\":1,\"1254\":1,\"1286\":1,\"1336\":1,\"1368\":1,\"1372\":1,\"1381\":1,\"1430\":2,\"1470\":1,\"1471\":1,\"1472\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1525\":1,\"1537\":1,\"1539\":1,\"1575\":1,\"1581\":1,\"1583\":1,\"1613\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":2,\"1671\":2,\"1676\":1,\"1680\":2,\"1683\":1,\"1709\":3,\"1711\":1,\"1713\":2,\"1719\":1,\"1765\":2,\"1787\":1,\"1788\":1,\"1800\":4,\"1801\":2,\"1803\":2,\"1804\":3,\"1810\":1,\"1844\":2,\"1848\":2,\"1849\":4,\"1851\":1,\"1857\":2,\"1858\":2,\"1861\":2,\"1862\":2,\"1864\":1,\"1865\":1,\"1870\":2,\"1871\":2,\"1878\":3,\"1880\":2,\"1906\":3,\"1949\":1,\"1973\":3,\"2002\":1,\"2078\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2440\":3,\"2441\":1,\"2592\":1,\"2596\":1}}],[\"normally\",{\"1\":{\"45\":1,\"128\":1,\"1171\":1,\"1206\":1,\"1552\":1,\"1954\":1,\"1956\":1}}],[\"normal\",{\"0\":{\"128\":1,\"897\":1},\"1\":{\"1\":1,\"3\":1,\"632\":1,\"897\":1,\"1248\":1,\"1269\":2,\"1339\":1,\"1342\":1,\"1406\":1,\"1484\":2,\"1601\":2,\"1785\":1,\"2151\":1}}],[\"nowadays\",{\"1\":{\"2385\":1}}],[\"now\",{\"1\":{\"14\":1,\"35\":1,\"107\":1,\"202\":1,\"217\":1,\"224\":1,\"231\":1,\"235\":1,\"239\":1,\"1140\":2,\"1148\":2,\"1149\":3,\"1150\":3,\"1169\":2,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1200\":1,\"1203\":2,\"1214\":1,\"1269\":1,\"1272\":1,\"1419\":1,\"1463\":1,\"1551\":1,\"1553\":1,\"1719\":1,\"2029\":1,\"2354\":1,\"2392\":3,\"2425\":3,\"2433\":1,\"2512\":1,\"2528\":3,\"2548\":3,\"2553\":1,\"2569\":1,\"2571\":1,\"2600\":2,\"2657\":1}}],[\"no\",{\"0\":{\"127\":1,\"2315\":1,\"2342\":2},\"1\":{\"6\":1,\"8\":1,\"31\":2,\"44\":2,\"49\":1,\"57\":1,\"73\":3,\"106\":1,\"173\":1,\"207\":1,\"210\":2,\"211\":2,\"212\":2,\"218\":1,\"222\":2,\"223\":2,\"225\":1,\"229\":2,\"230\":2,\"232\":1,\"273\":1,\"274\":1,\"289\":1,\"291\":1,\"293\":1,\"429\":2,\"461\":3,\"593\":2,\"608\":1,\"632\":1,\"705\":1,\"711\":1,\"749\":1,\"1064\":1,\"1115\":8,\"1133\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1203\":1,\"1211\":1,\"1224\":1,\"1233\":1,\"1255\":1,\"1269\":7,\"1272\":1,\"1336\":1,\"1348\":1,\"1505\":1,\"1669\":1,\"1671\":2,\"1696\":1,\"1698\":1,\"1862\":1,\"1880\":1,\"1917\":1,\"1956\":1,\"1973\":2,\"2001\":1,\"2004\":1,\"2011\":1,\"2022\":1,\"2029\":1,\"2121\":1,\"2122\":1,\"2126\":1,\"2151\":1,\"2186\":1,\"2202\":2,\"2204\":1,\"2315\":1,\"2342\":3,\"2365\":1,\"2368\":1,\"2372\":1,\"2384\":1,\"2396\":1,\"2398\":1,\"2440\":1,\"2450\":1,\"2486\":1,\"2490\":1,\"2507\":2,\"2508\":1,\"2510\":5,\"2513\":2,\"2515\":1,\"2517\":1,\"2534\":1,\"2568\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2655\":1,\"2660\":1}}],[\"notify\",{\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"notime\",{\"1\":{\"2194\":1}}],[\"notation\",{\"1\":{\"729\":1,\"730\":1,\"1524\":1,\"1525\":1}}],[\"notable\",{\"1\":{\"62\":1}}],[\"notset\",{\"1\":{\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1}}],[\"nothing\",{\"1\":{\"74\":1,\"610\":1,\"615\":1,\"669\":1,\"1612\":1,\"1615\":1,\"2050\":1,\"2430\":2,\"2440\":1,\"2555\":2}}],[\"not\",{\"1\":{\"5\":1,\"11\":1,\"17\":1,\"19\":1,\"24\":2,\"29\":1,\"33\":1,\"35\":1,\"41\":1,\"44\":1,\"48\":1,\"59\":1,\"60\":4,\"72\":1,\"74\":2,\"79\":1,\"80\":1,\"82\":1,\"84\":2,\"85\":1,\"95\":1,\"112\":2,\"118\":1,\"124\":1,\"127\":1,\"132\":1,\"136\":1,\"144\":2,\"148\":1,\"150\":2,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"222\":1,\"223\":1,\"224\":1,\"229\":1,\"230\":1,\"231\":2,\"235\":1,\"237\":1,\"238\":1,\"295\":1,\"595\":1,\"607\":1,\"608\":1,\"613\":1,\"677\":2,\"678\":1,\"679\":1,\"684\":2,\"685\":1,\"686\":2,\"687\":2,\"688\":1,\"689\":1,\"740\":4,\"741\":4,\"745\":5,\"746\":5,\"754\":1,\"758\":1,\"770\":1,\"775\":4,\"776\":4,\"785\":1,\"944\":1,\"956\":1,\"973\":1,\"987\":2,\"997\":1,\"999\":1,\"1011\":2,\"1015\":2,\"1052\":2,\"1132\":1,\"1140\":2,\"1142\":4,\"1148\":2,\"1149\":3,\"1150\":3,\"1169\":2,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1187\":4,\"1200\":1,\"1202\":4,\"1203\":2,\"1210\":1,\"1222\":1,\"1241\":1,\"1243\":1,\"1245\":1,\"1255\":4,\"1269\":2,\"1272\":1,\"1282\":1,\"1286\":2,\"1287\":2,\"1337\":1,\"1356\":1,\"1371\":2,\"1377\":1,\"1452\":1,\"1462\":2,\"1463\":2,\"1464\":1,\"1505\":1,\"1510\":1,\"1511\":1,\"1516\":1,\"1524\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1551\":2,\"1552\":1,\"1553\":2,\"1554\":1,\"1558\":2,\"1603\":1,\"1604\":1,\"1611\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1626\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1660\":3,\"1661\":3,\"1662\":3,\"1665\":1,\"1669\":2,\"1671\":1,\"1688\":1,\"1713\":1,\"1719\":2,\"1741\":1,\"1756\":1,\"1758\":1,\"1759\":1,\"1771\":2,\"1778\":3,\"1781\":3,\"1787\":3,\"1801\":1,\"1805\":3,\"1833\":1,\"1838\":1,\"1840\":1,\"1850\":3,\"1852\":3,\"1855\":1,\"1865\":1,\"1868\":1,\"1877\":3,\"1890\":1,\"1917\":1,\"1925\":1,\"1928\":1,\"1932\":1,\"1935\":1,\"1940\":1,\"1943\":1,\"1980\":2,\"1985\":1,\"2011\":2,\"2012\":1,\"2029\":1,\"2077\":2,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2099\":1,\"2125\":1,\"2177\":1,\"2188\":1,\"2208\":1,\"2212\":1,\"2235\":2,\"2237\":1,\"2243\":2,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2277\":2,\"2279\":1,\"2309\":3,\"2312\":1,\"2355\":1,\"2362\":1,\"2363\":1,\"2373\":2,\"2385\":2,\"2387\":2,\"2389\":1,\"2394\":2,\"2401\":1,\"2408\":1,\"2429\":1,\"2430\":1,\"2440\":3,\"2441\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2501\":1,\"2503\":1,\"2504\":1,\"2506\":1,\"2525\":1,\"2530\":2,\"2537\":1,\"2543\":1,\"2552\":1,\"2555\":2,\"2558\":1,\"2565\":1,\"2573\":3,\"2576\":1,\"2584\":4,\"2592\":2,\"2596\":2,\"2651\":1,\"2653\":1,\"2661\":1}}],[\"noted\",{\"1\":{\"1432\":1,\"1436\":1,\"1510\":1,\"1511\":1,\"1643\":1,\"1644\":1,\"2394\":2,\"2396\":1,\"2410\":1,\"2450\":1,\"2530\":2,\"2532\":1}}],[\"notebooks\",{\"1\":{\"138\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"notebook\",{\"0\":{\"138\":1,\"151\":1},\"1\":{\"195\":1,\"199\":1,\"202\":1,\"206\":1,\"233\":2,\"295\":2,\"2354\":2,\"2361\":1,\"2389\":1,\"2401\":1,\"2408\":1,\"2421\":1,\"2422\":1,\"2440\":1,\"2449\":1,\"2465\":1,\"2480\":1,\"2481\":1,\"2502\":1,\"2503\":1,\"2516\":1,\"2525\":1,\"2537\":1,\"2545\":1,\"2558\":1,\"2560\":1,\"2571\":1,\"2573\":1,\"2576\":1,\"2584\":1,\"2586\":1,\"2597\":1,\"2601\":1,\"2618\":2,\"2646\":1,\"2650\":1}}],[\"notes❗\",{\"0\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"notes\",{\"0\":{\"24\":1,\"30\":1,\"109\":1},\"1\":{\"82\":1,\"650\":1,\"668\":1,\"710\":1,\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"1001\":1,\"1025\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"1917\":1,\"2573\":1}}],[\"note\",{\"0\":{\"597\":1,\"608\":1,\"613\":1,\"734\":1,\"737\":1,\"739\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"804\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1144\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1228\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1345\":1,\"1347\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1392\":1,\"1393\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1927\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2079\":1,\"2080\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2237\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1},\"1\":{\"3\":1,\"11\":1,\"17\":2,\"19\":1,\"22\":1,\"24\":1,\"33\":1,\"35\":1,\"36\":1,\"45\":2,\"46\":1,\"47\":1,\"49\":3,\"52\":1,\"62\":1,\"69\":1,\"73\":1,\"76\":1,\"85\":1,\"91\":1,\"94\":1,\"102\":2,\"113\":2,\"118\":1,\"119\":2,\"122\":1,\"135\":1,\"137\":1,\"144\":1,\"148\":2,\"150\":1,\"173\":1,\"226\":1,\"275\":1,\"279\":1,\"282\":1,\"283\":1,\"284\":1,\"295\":1,\"607\":1,\"745\":1,\"746\":1,\"812\":1,\"1011\":1,\"1028\":1,\"1132\":2,\"1171\":1,\"1172\":1,\"1198\":1,\"1214\":1,\"1248\":1,\"1337\":1,\"1377\":1,\"1389\":1,\"1393\":2,\"1406\":1,\"1414\":1,\"1419\":1,\"1427\":1,\"1505\":1,\"1516\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1551\":6,\"1552\":2,\"1553\":6,\"1558\":1,\"1603\":1,\"1611\":1,\"1618\":1,\"1619\":1,\"1622\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1669\":1,\"1713\":1,\"1739\":1,\"1797\":1,\"1808\":6,\"1892\":1,\"1893\":1,\"1905\":2,\"1928\":1,\"1941\":1,\"1946\":1,\"1947\":1,\"1956\":1,\"1970\":1,\"1972\":1,\"1975\":1,\"1984\":1,\"2019\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2027\":1,\"2076\":1,\"2090\":2,\"2099\":2,\"2102\":2,\"2209\":1,\"2355\":1,\"2362\":1,\"2373\":2,\"2375\":1,\"2385\":1,\"2387\":2,\"2393\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2430\":3,\"2431\":1,\"2440\":3,\"2504\":1,\"2529\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2542\":1,\"2543\":2,\"2554\":1,\"2555\":3,\"2556\":1,\"2559\":1,\"2560\":1,\"2564\":2,\"2565\":1,\"2570\":1,\"2572\":1,\"2576\":1,\"2583\":1,\"2585\":1,\"2600\":1,\"2651\":1}}],[\"oewzvwnel4u\",{\"1\":{\"2593\":1}}],[\"ok\",{\"1\":{\"2584\":1}}],[\"okey\",{\"1\":{\"909\":1,\"1003\":1,\"1004\":1,\"1005\":1}}],[\"okey=\",{\"1\":{\"799\":1,\"909\":1,\"1003\":3,\"1004\":3,\"1005\":3}}],[\"o73\",{\"1\":{\"2568\":1}}],[\"o73a\",{\"1\":{\"2568\":2}}],[\"ovr\",{\"1\":{\"1526\":1}}],[\"overtones\",{\"1\":{\"1797\":1}}],[\"overfit\",{\"1\":{\"1180\":1}}],[\"overlapping\",{\"1\":{\"1670\":1,\"1671\":1,\"1735\":1}}],[\"overlap=false\",{\"1\":{\"1652\":1}}],[\"overlap\",{\"0\":{\"1735\":1},\"1\":{\"1115\":4,\"1269\":9,\"1652\":2,\"1654\":3,\"1735\":3,\"1821\":1}}],[\"overridden\",{\"1\":{\"752\":1,\"756\":1,\"760\":1,\"778\":1,\"831\":1,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1134\":1,\"1136\":1,\"1151\":1,\"1156\":1,\"1158\":1,\"1174\":1,\"1184\":1,\"1187\":2,\"1188\":1,\"1202\":2,\"1207\":1,\"1212\":1,\"1215\":1,\"1220\":1,\"1222\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1245\":1,\"1249\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":1,\"1518\":1,\"1520\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1543\":1,\"1547\":1,\"1549\":1,\"1555\":1,\"1561\":1,\"1564\":1,\"1573\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1596\":1,\"1598\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1613\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1641\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1656\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1761\":1,\"1763\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1801\":1,\"1806\":1,\"1890\":1,\"1902\":1,\"1907\":1,\"1912\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1976\":1,\"1978\":1,\"1981\":1,\"1984\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1994\":1,\"1997\":1,\"2024\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2050\":1,\"2052\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2149\":1,\"2168\":1,\"2233\":1,\"2237\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2275\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1}}],[\"override\",{\"1\":{\"607\":1,\"1327\":1,\"2201\":1}}],[\"overal\",{\"1\":{\"118\":1}}],[\"overall\",{\"1\":{\"22\":1,\"2394\":1,\"2530\":1}}],[\"overwrite\",{\"1\":{\"25\":1,\"96\":1,\"943\":1,\"950\":1,\"955\":1,\"956\":1,\"965\":1,\"968\":1,\"972\":1,\"973\":1,\"2437\":1,\"2558\":1,\"2563\":1}}],[\"overwritten\",{\"1\":{\"25\":1,\"1228\":1,\"1345\":1,\"1347\":1,\"1845\":1,\"1847\":1}}],[\"overview\",{\"0\":{\"2452\":1,\"2468\":1},\"1\":{\"24\":1,\"233\":1,\"2394\":1,\"2530\":1}}],[\"over\",{\"1\":{\"5\":1,\"45\":1,\"107\":1,\"607\":1,\"613\":1,\"975\":1,\"1042\":1,\"1211\":1,\"1224\":1,\"1255\":1,\"1269\":2,\"1274\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1336\":1,\"1348\":1,\"1543\":2,\"1564\":1,\"1679\":1,\"2086\":1,\"2087\":1,\"2199\":1,\"2400\":1,\"2536\":1,\"2584\":4}}],[\"ohio\",{\"1\":{\"1523\":1}}],[\"oom\",{\"1\":{\"1171\":2,\"1206\":2,\"1427\":1,\"1552\":2,\"1953\":2,\"1955\":2}}],[\"oov=\",{\"1\":{\"2348\":1}}],[\"oovs\",{\"1\":{\"618\":1,\"619\":1}}],[\"oov\",{\"1\":{\"276\":2,\"298\":2,\"609\":1,\"611\":1,\"618\":1}}],[\"omegaconf\",{\"0\":{\"1340\":1},\"1\":{\"1340\":1}}],[\"omp\",{\"1\":{\"1142\":1,\"1186\":1,\"1210\":1}}],[\"omitted\",{\"1\":{\"22\":1,\"150\":1,\"276\":1,\"564\":1}}],[\"oaxis\",{\"1\":{\"909\":1,\"1005\":1,\"1028\":1}}],[\"oaxis=0\",{\"1\":{\"799\":1,\"909\":1,\"1005\":2,\"1028\":2}}],[\"odd\",{\"1\":{\"1917\":1}}],[\"ode\",{\"1\":{\"1618\":1,\"1619\":1,\"1638\":3,\"1646\":2,\"1647\":1}}],[\"od\",{\"1\":{\"870\":1}}],[\"odim\",{\"0\":{\"890\":1},\"1\":{\"21\":6,\"26\":3,\"173\":2,\"194\":2,\"217\":2,\"224\":2,\"231\":2,\"646\":2,\"676\":2,\"682\":3,\"701\":4,\"703\":2,\"712\":2,\"714\":3,\"715\":3,\"716\":3,\"717\":1,\"718\":3,\"719\":3,\"720\":3,\"721\":3,\"722\":3,\"725\":2,\"731\":2,\"735\":1,\"742\":4,\"754\":5,\"755\":3,\"759\":1,\"777\":1,\"781\":2,\"802\":3,\"804\":2,\"806\":2,\"820\":1,\"821\":5,\"822\":3,\"826\":5,\"867\":2,\"870\":2,\"890\":2,\"896\":2,\"1145\":4,\"1778\":4,\"1804\":2,\"1805\":4,\"1829\":3,\"1850\":3,\"1851\":2,\"1852\":3,\"1877\":3,\"1917\":1,\"2000\":3,\"2001\":2,\"2002\":5,\"2003\":1,\"2004\":1,\"2078\":5,\"2079\":1,\"2086\":5,\"2087\":5,\"2088\":3,\"2090\":4,\"2091\":3,\"2095\":4,\"2243\":4,\"2244\":4,\"2245\":3,\"2255\":4,\"2256\":3,\"2261\":1,\"2263\":4,\"2264\":4,\"2279\":3,\"2280\":2}}],[\"octaves\",{\"1\":{\"1941\":2}}],[\"octave\",{\"1\":{\"1941\":5,\"2266\":1,\"2267\":2}}],[\"occurs\",{\"1\":{\"1144\":1,\"1228\":1,\"1345\":1,\"1347\":1}}],[\"occurrences\",{\"1\":{\"618\":1}}],[\"oc\",{\"0\":{\"1108\":1},\"1\":{\"1108\":1,\"2394\":1,\"2530\":1,\"2543\":2}}],[\"ochiai17a\",{\"1\":{\"1524\":2}}],[\"ochiai\",{\"1\":{\"130\":1,\"729\":1,\"1524\":1}}],[\"o3\",{\"1\":{\"251\":1,\"253\":1,\"255\":1,\"259\":1}}],[\"o2\",{\"1\":{\"251\":1,\"253\":1,\"255\":1,\"259\":1}}],[\"o1\",{\"1\":{\"251\":1,\"253\":1,\"255\":1,\"259\":1}}],[\"o0\",{\"1\":{\"251\":1,\"253\":1,\"255\":1,\"259\":1}}],[\"othger\",{\"1\":{\"115\":1}}],[\"others=\",{\"1\":{\"1530\":1,\"1563\":1,\"1603\":1,\"1622\":1}}],[\"others\",{\"0\":{\"159\":1,\"2729\":1},\"1\":{\"130\":2,\"1218\":1,\"1375\":1,\"1443\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":2,\"1529\":1,\"1530\":2,\"1534\":1,\"1539\":1,\"1553\":1,\"1554\":1,\"1558\":1,\"1563\":1,\"1600\":2,\"1603\":1,\"1622\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":2,\"1669\":1,\"1671\":1,\"2394\":1,\"2530\":1,\"2564\":1}}],[\"other\",{\"0\":{\"133\":1,\"159\":1,\"1243\":1,\"2492\":1,\"2628\":1},\"1\":{\"35\":1,\"49\":1,\"59\":1,\"62\":1,\"74\":2,\"85\":1,\"92\":1,\"102\":1,\"109\":1,\"111\":1,\"112\":2,\"118\":1,\"119\":1,\"124\":1,\"135\":1,\"240\":1,\"295\":1,\"595\":1,\"629\":1,\"791\":1,\"799\":1,\"1115\":6,\"1132\":1,\"1133\":1,\"1138\":1,\"1143\":1,\"1180\":2,\"1187\":1,\"1202\":1,\"1204\":1,\"1214\":1,\"1243\":1,\"1244\":1,\"1269\":5,\"1273\":1,\"1286\":1,\"1375\":1,\"1429\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1529\":1,\"1530\":1,\"1534\":1,\"1539\":1,\"1552\":1,\"1558\":1,\"1611\":2,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":1,\"1671\":1,\"1719\":2,\"1846\":1,\"1847\":1,\"1927\":1,\"2121\":1,\"2122\":1,\"2148\":1,\"2168\":1,\"2212\":1,\"2309\":1,\"2354\":2,\"2372\":4,\"2384\":2,\"2388\":1,\"2393\":1,\"2394\":1,\"2395\":1,\"2412\":1,\"2415\":1,\"2421\":1,\"2427\":1,\"2428\":1,\"2429\":3,\"2431\":1,\"2436\":1,\"2450\":1,\"2452\":1,\"2468\":1,\"2481\":1,\"2501\":1,\"2524\":1,\"2529\":1,\"2530\":1,\"2531\":1,\"2544\":1,\"2550\":1,\"2551\":1,\"2552\":3,\"2558\":1,\"2562\":1,\"2564\":1,\"2566\":1,\"2568\":1,\"2573\":1,\"2584\":2,\"2618\":1,\"2638\":1}}],[\"otherwise\",{\"1\":{\"21\":1,\"122\":1,\"778\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1,\"1149\":1,\"1150\":1,\"1478\":1,\"2559\":1,\"2638\":1,\"2642\":1}}],[\"otpions\",{\"1\":{\"91\":1}}],[\"obj\",{\"0\":{\"1317\":1},\"1\":{\"1317\":2,\"1404\":2,\"1416\":2,\"2161\":1,\"2163\":1,\"2176\":2,\"2324\":1}}],[\"objectives\",{\"0\":{\"2381\":1,\"2391\":1,\"2407\":1,\"2423\":1,\"2448\":1,\"2464\":1,\"2527\":1,\"2546\":1}}],[\"object>\",{\"1\":{\"2096\":9,\"2097\":4,\"2098\":6,\"2100\":7,\"2101\":24,\"2102\":6,\"2103\":10,\"2104\":7,\"2105\":6,\"2107\":2,\"2108\":7,\"2109\":14,\"2110\":9,\"2111\":10,\"2112\":13,\"2113\":9,\"2114\":8,\"2115\":6,\"2116\":7,\"2117\":4,\"2118\":5}}],[\"objects\",{\"1\":{\"676\":1,\"745\":2,\"746\":2,\"1951\":1,\"2168\":1,\"2176\":1,\"2324\":1}}],[\"object\",{\"0\":{\"652\":1},\"1\":{\"56\":1,\"57\":2,\"64\":1,\"84\":1,\"106\":1,\"599\":2,\"623\":1,\"624\":2,\"625\":1,\"626\":2,\"628\":1,\"629\":2,\"648\":1,\"652\":6,\"661\":1,\"667\":1,\"671\":1,\"672\":1,\"673\":1,\"676\":1,\"698\":1,\"699\":1,\"700\":1,\"704\":1,\"705\":1,\"707\":1,\"711\":1,\"712\":2,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"742\":1,\"749\":1,\"750\":2,\"771\":1,\"781\":1,\"785\":1,\"792\":2,\"794\":1,\"809\":1,\"810\":1,\"814\":1,\"816\":2,\"818\":1,\"819\":1,\"820\":1,\"824\":1,\"827\":1,\"836\":2,\"936\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"945\":1,\"947\":1,\"948\":1,\"949\":1,\"951\":1,\"952\":1,\"953\":1,\"954\":1,\"957\":1,\"958\":1,\"960\":1,\"961\":1,\"962\":1,\"974\":1,\"980\":1,\"981\":2,\"982\":1,\"985\":1,\"987\":1,\"989\":1,\"990\":1,\"993\":1,\"999\":2,\"1000\":1,\"1041\":1,\"1047\":1,\"1048\":2,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":2,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1083\":1,\"1115\":1,\"1138\":1,\"1139\":1,\"1142\":1,\"1154\":1,\"1155\":1,\"1173\":1,\"1186\":1,\"1193\":1,\"1209\":1,\"1355\":1,\"1356\":1,\"1382\":1,\"1393\":1,\"1396\":1,\"1403\":1,\"1406\":1,\"1407\":1,\"1411\":1,\"1415\":1,\"1420\":1,\"1424\":1,\"1429\":1,\"1526\":1,\"1527\":1,\"1784\":1,\"1904\":1,\"1905\":1,\"1916\":1,\"1961\":1,\"2096\":1,\"2097\":6,\"2098\":1,\"2099\":4,\"2100\":1,\"2101\":1,\"2102\":3,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2121\":1,\"2122\":1,\"2123\":1,\"2125\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2131\":1,\"2133\":1,\"2135\":1,\"2153\":2,\"2155\":3,\"2166\":1,\"2176\":2,\"2177\":1,\"2180\":1,\"2187\":1,\"2192\":1,\"2193\":1,\"2199\":1,\"2201\":1,\"2202\":1,\"2312\":3,\"2313\":1,\"2317\":1,\"2350\":1}}],[\"obtain\",{\"1\":{\"692\":2,\"693\":2,\"697\":2,\"704\":1,\"797\":2,\"857\":2,\"896\":1,\"1709\":1,\"1904\":1,\"1916\":1,\"1917\":1}}],[\"obtained\",{\"1\":{\"15\":1,\"48\":1,\"85\":2,\"201\":1,\"1011\":1,\"1715\":1,\"1851\":1,\"1917\":1}}],[\"observe\",{\"1\":{\"2193\":2}}],[\"observed\",{\"1\":{\"1619\":1,\"1698\":2}}],[\"observer\",{\"1\":{\"608\":1}}],[\"observation\",{\"1\":{\"1524\":2,\"1698\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1}}],[\"observations\",{\"1\":{\"616\":1}}],[\"ogg\",{\"1\":{\"49\":1,\"1927\":1}}],[\"olens\",{\"1\":{\"702\":2,\"735\":2,\"754\":4,\"755\":2,\"762\":2,\"763\":2,\"821\":3,\"822\":2,\"826\":4,\"1133\":1,\"1190\":1,\"1204\":1,\"1214\":1,\"1244\":1,\"1273\":1,\"1373\":1,\"1841\":2,\"1999\":1,\"2000\":2,\"2002\":1,\"2088\":2,\"2091\":2,\"2245\":2,\"2256\":2,\"2280\":2}}],[\"olen\",{\"1\":{\"26\":3,\"1145\":1}}],[\"olddatadir\",{\"1\":{\"288\":1}}],[\"old\",{\"1\":{\"26\":1,\"771\":1,\"772\":1,\"913\":2,\"1944\":1}}],[\"older\",{\"1\":{\"5\":1,\"14\":1}}],[\"o\",{\"1\":{\"25\":2,\"98\":2,\"301\":1,\"503\":1,\"566\":1,\"705\":2,\"1099\":1,\"1141\":2,\"1170\":2,\"2143\":1,\"2367\":1,\"2368\":1,\"2372\":1,\"2431\":1,\"2432\":1,\"2454\":1,\"2457\":1,\"2460\":1,\"2470\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2497\":1,\"2500\":1,\"2506\":1,\"2510\":2,\"2512\":1,\"2519\":2,\"2568\":4,\"2569\":2,\"2604\":1,\"2605\":1,\"2609\":1,\"2614\":1,\"2621\":1,\"2622\":1,\"2626\":1,\"2632\":1,\"2647\":2}}],[\"ornstein\",{\"1\":{\"1618\":1,\"1619\":1}}],[\"ornone\",{\"1\":{\"1524\":2}}],[\"oracle\",{\"1\":{\"1375\":1,\"1524\":4}}],[\"ori\",{\"1\":{\"1174\":1}}],[\"originated\",{\"1\":{\"2154\":1,\"2309\":1}}],[\"originally\",{\"1\":{\"49\":1,\"1015\":1,\"2099\":1,\"2102\":1}}],[\"original\",{\"1\":{\"46\":1,\"48\":1,\"120\":1,\"239\":3,\"242\":1,\"243\":1,\"826\":1,\"935\":1,\"940\":1,\"1115\":1,\"1138\":1,\"1186\":2,\"1210\":2,\"1339\":1,\"1451\":1,\"1551\":1,\"1553\":1,\"1712\":1,\"1715\":1,\"2019\":1,\"2387\":1,\"2411\":1}}],[\"origin\",{\"1\":{\"1214\":1,\"1558\":1}}],[\"orig\",{\"0\":{\"1720\":1},\"1\":{\"546\":2,\"590\":2,\"1720\":2,\"2472\":2,\"2476\":2,\"2478\":2,\"2648\":2}}],[\"orginal\",{\"1\":{\"2385\":1}}],[\"org>\",{\"1\":{\"198\":1}}],[\"organizd\",{\"1\":{\"2452\":1}}],[\"organizes\",{\"1\":{\"2564\":1}}],[\"organized\",{\"1\":{\"171\":1}}],[\"organize\",{\"1\":{\"168\":1,\"169\":1,\"179\":1,\"181\":1}}],[\"organization=\",{\"1\":{\"130\":3}}],[\"org\",{\"1\":{\"38\":1,\"82\":1,\"84\":1,\"130\":3,\"668\":1,\"678\":1,\"681\":1,\"682\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"692\":1,\"693\":1,\"700\":6,\"706\":2,\"729\":1,\"758\":1,\"770\":2,\"771\":1,\"772\":1,\"809\":1,\"810\":1,\"813\":1,\"885\":1,\"950\":1,\"968\":1,\"1031\":1,\"1037\":1,\"1048\":5,\"1049\":1,\"1056\":1,\"1061\":1,\"1066\":1,\"1067\":1,\"1072\":1,\"1075\":1,\"1080\":1,\"1084\":3,\"1138\":7,\"1139\":6,\"1150\":1,\"1198\":1,\"1210\":1,\"1211\":1,\"1269\":1,\"1286\":2,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":2,\"1337\":2,\"1371\":3,\"1462\":2,\"1463\":2,\"1466\":1,\"1528\":1,\"1529\":2,\"1566\":1,\"1568\":2,\"1645\":1,\"1695\":1,\"1696\":1,\"1698\":1,\"1706\":1,\"1707\":1,\"1712\":1,\"1715\":2,\"1767\":1,\"1782\":1,\"1793\":1,\"1829\":1,\"1842\":1,\"1850\":1,\"1917\":1,\"1921\":1,\"1922\":1,\"1932\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1973\":1,\"1976\":1,\"1986\":1,\"2003\":1,\"2019\":1,\"2032\":1,\"2040\":1,\"2078\":2,\"2081\":1,\"2083\":1,\"2095\":1,\"2260\":1,\"2357\":1,\"2371\":1,\"2372\":1,\"2373\":2,\"2382\":1,\"2384\":1,\"2401\":3,\"2429\":1,\"2430\":1,\"2432\":5,\"2433\":1,\"2467\":2,\"2482\":1,\"2512\":2,\"2537\":3,\"2554\":1,\"2555\":2,\"2586\":1,\"2612\":1,\"2617\":1,\"2630\":1,\"2635\":1,\"2646\":1,\"2657\":2}}],[\"order=true\",{\"1\":{\"2521\":1}}],[\"order=2\",{\"1\":{\"939\":1,\"963\":1,\"1036\":1}}],[\"ordereddict\",{\"1\":{\"1375\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":2,\"1529\":1,\"1534\":1,\"1539\":1,\"1553\":1,\"1554\":1,\"1558\":1,\"1611\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1669\":1,\"1671\":1}}],[\"ordering\",{\"1\":{\"1243\":1,\"1251\":1,\"1603\":1,\"1622\":1}}],[\"order\",{\"0\":{\"1563\":1},\"1\":{\"22\":1,\"48\":1,\"74\":1,\"75\":1,\"82\":1,\"107\":1,\"114\":1,\"118\":1,\"121\":1,\"124\":1,\"612\":1,\"745\":1,\"746\":1,\"959\":1,\"987\":1,\"997\":1,\"1001\":3,\"1026\":1,\"1031\":4,\"1035\":1,\"1069\":4,\"1269\":1,\"1551\":2,\"1552\":1,\"1553\":2,\"1563\":2,\"1600\":1,\"1603\":1,\"1622\":1,\"1688\":1,\"1695\":1,\"1756\":1,\"2585\":2}}],[\"or\",{\"0\":{\"127\":1,\"149\":1,\"1508\":1,\"1688\":1,\"1693\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1755\":1,\"1756\":1,\"2319\":1,\"2326\":1,\"2327\":1,\"2338\":1,\"2339\":1},\"1\":{\"16\":2,\"19\":1,\"21\":10,\"22\":1,\"26\":2,\"28\":1,\"30\":1,\"34\":1,\"45\":2,\"48\":4,\"57\":2,\"60\":1,\"73\":1,\"75\":1,\"82\":2,\"84\":1,\"96\":1,\"99\":1,\"102\":1,\"104\":1,\"112\":3,\"113\":4,\"114\":1,\"115\":11,\"116\":2,\"124\":2,\"134\":1,\"142\":1,\"143\":1,\"221\":1,\"228\":1,\"235\":2,\"237\":1,\"239\":1,\"241\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"265\":1,\"269\":1,\"276\":1,\"286\":2,\"296\":1,\"375\":2,\"493\":1,\"496\":1,\"506\":2,\"607\":1,\"623\":1,\"626\":1,\"634\":1,\"648\":1,\"653\":1,\"665\":3,\"674\":1,\"676\":1,\"700\":1,\"711\":3,\"712\":2,\"728\":2,\"729\":1,\"730\":2,\"740\":2,\"741\":2,\"745\":4,\"746\":4,\"747\":1,\"749\":1,\"760\":1,\"771\":1,\"774\":2,\"775\":2,\"776\":2,\"781\":1,\"785\":2,\"794\":3,\"809\":1,\"812\":1,\"825\":1,\"826\":1,\"835\":1,\"858\":1,\"860\":2,\"862\":2,\"863\":1,\"865\":1,\"872\":1,\"873\":1,\"874\":1,\"886\":1,\"910\":1,\"921\":2,\"927\":2,\"934\":1,\"944\":1,\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"987\":1,\"997\":2,\"998\":1,\"1011\":3,\"1013\":1,\"1015\":1,\"1025\":2,\"1048\":1,\"1052\":2,\"1057\":1,\"1063\":1,\"1064\":3,\"1066\":2,\"1073\":10,\"1093\":1,\"1097\":1,\"1101\":1,\"1130\":1,\"1133\":1,\"1138\":1,\"1139\":1,\"1141\":2,\"1142\":2,\"1145\":1,\"1148\":2,\"1149\":1,\"1150\":1,\"1154\":1,\"1170\":2,\"1171\":1,\"1186\":2,\"1187\":3,\"1190\":2,\"1191\":1,\"1192\":1,\"1198\":1,\"1202\":3,\"1203\":3,\"1206\":1,\"1209\":2,\"1210\":2,\"1216\":2,\"1222\":1,\"1228\":1,\"1243\":1,\"1245\":1,\"1248\":5,\"1255\":1,\"1269\":2,\"1272\":1,\"1274\":1,\"1276\":1,\"1282\":1,\"1286\":1,\"1287\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1304\":1,\"1327\":2,\"1345\":1,\"1347\":1,\"1352\":2,\"1355\":1,\"1379\":1,\"1424\":1,\"1427\":1,\"1428\":1,\"1429\":3,\"1452\":1,\"1462\":1,\"1463\":1,\"1484\":4,\"1505\":1,\"1508\":1,\"1517\":2,\"1522\":1,\"1523\":2,\"1524\":3,\"1525\":5,\"1531\":1,\"1537\":1,\"1539\":1,\"1543\":1,\"1551\":7,\"1552\":1,\"1553\":6,\"1554\":5,\"1558\":2,\"1566\":2,\"1567\":2,\"1569\":2,\"1571\":2,\"1572\":1,\"1598\":2,\"1601\":4,\"1604\":1,\"1618\":1,\"1619\":1,\"1640\":1,\"1646\":2,\"1652\":2,\"1654\":2,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":1,\"1660\":4,\"1661\":4,\"1662\":4,\"1664\":1,\"1665\":1,\"1666\":2,\"1668\":2,\"1669\":2,\"1671\":2,\"1688\":4,\"1693\":4,\"1695\":2,\"1698\":1,\"1704\":1,\"1705\":1,\"1707\":1,\"1708\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1716\":2,\"1719\":5,\"1729\":1,\"1730\":1,\"1735\":1,\"1739\":2,\"1741\":1,\"1755\":4,\"1756\":4,\"1760\":1,\"1771\":4,\"1773\":1,\"1778\":9,\"1781\":3,\"1787\":3,\"1797\":1,\"1800\":1,\"1801\":1,\"1804\":2,\"1805\":9,\"1811\":1,\"1828\":1,\"1836\":3,\"1837\":1,\"1839\":2,\"1843\":2,\"1850\":4,\"1851\":6,\"1852\":2,\"1871\":1,\"1877\":4,\"1878\":5,\"1905\":1,\"1918\":3,\"1936\":1,\"1953\":1,\"1955\":1,\"1971\":2,\"1980\":2,\"1985\":1,\"1993\":1,\"2001\":1,\"2004\":1,\"2010\":1,\"2011\":1,\"2012\":3,\"2028\":1,\"2029\":1,\"2032\":1,\"2046\":2,\"2049\":1,\"2054\":2,\"2077\":2,\"2083\":3,\"2086\":9,\"2087\":10,\"2090\":13,\"2091\":1,\"2095\":12,\"2099\":2,\"2123\":1,\"2153\":3,\"2156\":1,\"2170\":2,\"2235\":2,\"2236\":1,\"2239\":1,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":1,\"2267\":1,\"2277\":2,\"2279\":2,\"2319\":3,\"2320\":1,\"2326\":1,\"2327\":3,\"2328\":1,\"2338\":2,\"2339\":3,\"2340\":1,\"2357\":1,\"2359\":1,\"2364\":3,\"2372\":1,\"2378\":1,\"2387\":1,\"2429\":1,\"2441\":1,\"2456\":1,\"2460\":1,\"2467\":2,\"2492\":1,\"2494\":1,\"2501\":1,\"2507\":2,\"2510\":7,\"2513\":2,\"2514\":1,\"2521\":1,\"2554\":1,\"2564\":1,\"2580\":1,\"2583\":1,\"2584\":4,\"2585\":1,\"2586\":1,\"2588\":1,\"2628\":1,\"2630\":1,\"2635\":2,\"2640\":2,\"2645\":1,\"2654\":3,\"2658\":3,\"2659\":1}}],[\"ouptut\",{\"1\":{\"2055\":1,\"2064\":1,\"2070\":1}}],[\"ouvpsde\",{\"0\":{\"1619\":1},\"1\":{\"1619\":3}}],[\"ouvesde\",{\"0\":{\"1618\":1},\"1\":{\"1618\":2}}],[\"ou\",{\"1\":{\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1}}],[\"outchannels\",{\"1\":{\"1688\":1,\"1756\":1}}],[\"outlined\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1}}],[\"out=1\",{\"1\":{\"1198\":1}}],[\"out=0\",{\"1\":{\"1028\":1}}],[\"out=inf\",{\"1\":{\"1028\":1}}],[\"outout\",{\"1\":{\"784\":1}}],[\"outs\",{\"1\":{\"702\":2,\"755\":6,\"822\":4,\"1879\":6,\"2000\":4,\"2002\":2,\"2088\":4,\"2091\":10,\"2245\":10,\"2256\":10,\"2280\":10}}],[\"outside\",{\"1\":{\"1\":1,\"700\":1,\"952\":1,\"1011\":1,\"1138\":1,\"1139\":1,\"1693\":1,\"1755\":1}}],[\"outyaml\",{\"1\":{\"503\":1}}],[\"outplanes\",{\"1\":{\"1312\":1,\"1313\":1}}],[\"outpath\",{\"1\":{\"398\":22,\"1967\":1,\"1968\":1}}],[\"outputdim\",{\"1\":{\"1211\":1,\"1224\":1,\"1287\":1,\"1336\":1,\"1348\":1}}],[\"outputdir\",{\"1\":{\"528\":1}}],[\"outputdiror\",{\"1\":{\"528\":1}}],[\"output=none\",{\"1\":{\"1162\":1,\"1177\":1}}],[\"output=\",{\"1\":{\"987\":1}}],[\"output=true\",{\"1\":{\"987\":1}}],[\"output=false\",{\"1\":{\"826\":1}}],[\"outputfile\",{\"1\":{\"276\":1}}],[\"outputs\",{\"1\":{\"98\":1,\"115\":2,\"202\":1,\"241\":2,\"242\":2,\"272\":7,\"682\":1,\"701\":3,\"702\":1,\"737\":2,\"738\":2,\"739\":1,\"742\":2,\"745\":2,\"746\":1,\"747\":1,\"754\":2,\"755\":3,\"764\":1,\"802\":1,\"804\":1,\"821\":3,\"822\":2,\"825\":3,\"826\":2,\"875\":1,\"925\":2,\"1057\":2,\"1058\":3,\"1145\":1,\"1187\":2,\"1202\":2,\"1284\":1,\"1286\":2,\"1287\":2,\"1377\":1,\"1561\":1,\"1773\":1,\"1778\":4,\"1805\":4,\"1836\":10,\"1839\":7,\"1843\":5,\"1845\":1,\"1846\":1,\"1847\":1,\"1850\":4,\"1851\":1,\"1852\":4,\"1858\":1,\"1870\":1,\"1877\":4,\"1879\":3,\"1880\":2,\"1890\":1,\"1963\":1,\"1980\":1,\"1983\":2,\"1986\":4,\"2000\":2,\"2001\":4,\"2002\":10,\"2004\":4,\"2077\":1,\"2078\":3,\"2079\":2,\"2082\":2,\"2086\":2,\"2087\":2,\"2088\":2,\"2090\":2,\"2091\":5,\"2095\":5,\"2235\":1,\"2240\":2,\"2243\":2,\"2244\":2,\"2245\":5,\"2255\":2,\"2256\":5,\"2259\":2,\"2263\":5,\"2264\":2,\"2277\":1,\"2278\":2,\"2279\":2,\"2280\":5}}],[\"output\",{\"0\":{\"306\":1,\"353\":1,\"371\":1,\"933\":1,\"1687\":1,\"1689\":1},\"1\":{\"21\":8,\"22\":2,\"23\":1,\"24\":1,\"25\":1,\"26\":3,\"47\":4,\"49\":3,\"51\":1,\"52\":2,\"54\":5,\"98\":1,\"102\":2,\"115\":10,\"116\":1,\"135\":2,\"150\":2,\"171\":1,\"173\":1,\"174\":1,\"175\":2,\"185\":1,\"193\":1,\"194\":1,\"239\":1,\"245\":2,\"265\":1,\"269\":1,\"272\":2,\"281\":1,\"299\":2,\"301\":1,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":4,\"350\":4,\"357\":2,\"363\":2,\"368\":4,\"375\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":2,\"441\":2,\"443\":2,\"449\":2,\"455\":2,\"461\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"491\":2,\"493\":1,\"564\":4,\"566\":4,\"582\":2,\"619\":1,\"629\":1,\"646\":1,\"676\":1,\"682\":1,\"683\":1,\"691\":1,\"692\":3,\"693\":3,\"697\":5,\"698\":2,\"699\":2,\"700\":7,\"701\":1,\"703\":1,\"711\":3,\"712\":7,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":7,\"726\":7,\"731\":1,\"732\":1,\"735\":1,\"740\":1,\"741\":1,\"743\":1,\"745\":6,\"746\":6,\"747\":1,\"748\":1,\"749\":1,\"754\":3,\"759\":3,\"762\":1,\"763\":1,\"766\":12,\"771\":2,\"775\":1,\"776\":1,\"777\":1,\"781\":1,\"785\":1,\"786\":1,\"787\":1,\"797\":4,\"799\":1,\"801\":1,\"802\":1,\"804\":1,\"806\":5,\"809\":2,\"821\":3,\"824\":2,\"825\":14,\"826\":6,\"827\":4,\"857\":2,\"858\":1,\"867\":1,\"870\":1,\"890\":3,\"909\":4,\"910\":1,\"929\":1,\"933\":1,\"934\":1,\"987\":2,\"1003\":4,\"1004\":5,\"1005\":6,\"1011\":2,\"1028\":9,\"1046\":3,\"1047\":1,\"1048\":4,\"1049\":3,\"1050\":3,\"1051\":2,\"1052\":10,\"1053\":5,\"1054\":1,\"1055\":1,\"1056\":3,\"1060\":1,\"1062\":2,\"1064\":8,\"1065\":2,\"1066\":4,\"1068\":5,\"1069\":2,\"1070\":2,\"1072\":1,\"1073\":4,\"1074\":2,\"1075\":5,\"1076\":3,\"1080\":1,\"1081\":3,\"1083\":4,\"1086\":3,\"1087\":3,\"1094\":1,\"1097\":3,\"1099\":1,\"1103\":2,\"1104\":5,\"1105\":2,\"1106\":2,\"1108\":2,\"1114\":1,\"1115\":2,\"1116\":2,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1132\":3,\"1133\":14,\"1136\":1,\"1137\":1,\"1138\":8,\"1139\":7,\"1140\":5,\"1141\":1,\"1145\":2,\"1148\":7,\"1149\":4,\"1150\":4,\"1159\":1,\"1160\":3,\"1161\":3,\"1162\":1,\"1163\":2,\"1164\":3,\"1165\":2,\"1167\":2,\"1168\":2,\"1169\":5,\"1170\":1,\"1172\":1,\"1177\":1,\"1178\":3,\"1179\":1,\"1180\":4,\"1181\":3,\"1185\":1,\"1187\":2,\"1190\":3,\"1191\":2,\"1192\":2,\"1195\":3,\"1196\":2,\"1197\":2,\"1198\":6,\"1199\":1,\"1200\":3,\"1201\":3,\"1202\":3,\"1203\":7,\"1204\":3,\"1208\":2,\"1209\":1,\"1211\":4,\"1214\":4,\"1216\":1,\"1220\":1,\"1222\":3,\"1223\":1,\"1224\":4,\"1227\":2,\"1228\":2,\"1233\":1,\"1240\":1,\"1243\":2,\"1244\":5,\"1247\":1,\"1248\":1,\"1252\":5,\"1253\":7,\"1254\":5,\"1255\":5,\"1269\":2,\"1270\":5,\"1271\":2,\"1272\":4,\"1273\":5,\"1276\":1,\"1279\":4,\"1280\":1,\"1282\":3,\"1283\":1,\"1285\":1,\"1286\":3,\"1287\":5,\"1336\":4,\"1345\":2,\"1347\":2,\"1348\":4,\"1369\":1,\"1371\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1377\":1,\"1383\":2,\"1406\":1,\"1407\":5,\"1427\":1,\"1430\":2,\"1432\":2,\"1436\":1,\"1454\":3,\"1460\":1,\"1462\":1,\"1464\":1,\"1470\":2,\"1471\":2,\"1473\":1,\"1480\":1,\"1482\":1,\"1491\":1,\"1501\":1,\"1505\":2,\"1510\":2,\"1511\":1,\"1512\":1,\"1516\":2,\"1517\":1,\"1518\":2,\"1522\":4,\"1523\":3,\"1528\":1,\"1531\":7,\"1532\":3,\"1534\":1,\"1535\":3,\"1537\":4,\"1539\":2,\"1543\":1,\"1545\":5,\"1551\":1,\"1553\":5,\"1558\":1,\"1561\":1,\"1576\":2,\"1577\":5,\"1580\":1,\"1581\":1,\"1594\":2,\"1596\":1,\"1602\":2,\"1603\":1,\"1605\":1,\"1611\":1,\"1617\":1,\"1622\":1,\"1626\":1,\"1627\":1,\"1629\":1,\"1639\":1,\"1640\":1,\"1643\":3,\"1644\":1,\"1645\":2,\"1652\":3,\"1655\":1,\"1656\":1,\"1658\":1,\"1660\":2,\"1661\":2,\"1662\":5,\"1664\":1,\"1665\":1,\"1669\":2,\"1670\":7,\"1671\":1,\"1674\":1,\"1687\":1,\"1689\":1,\"1719\":5,\"1735\":4,\"1765\":2,\"1766\":2,\"1767\":1,\"1768\":1,\"1771\":3,\"1775\":1,\"1777\":1,\"1778\":1,\"1781\":5,\"1788\":3,\"1792\":1,\"1797\":2,\"1800\":3,\"1803\":2,\"1805\":1,\"1811\":2,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1835\":1,\"1838\":1,\"1839\":1,\"1844\":3,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":3,\"1850\":1,\"1851\":3,\"1852\":1,\"1855\":1,\"1856\":2,\"1857\":3,\"1858\":2,\"1860\":2,\"1861\":3,\"1862\":3,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1871\":3,\"1872\":1,\"1873\":1,\"1877\":1,\"1880\":2,\"1910\":1,\"1917\":2,\"1918\":1,\"1962\":2,\"1964\":1,\"1970\":1,\"1971\":4,\"1980\":1,\"1982\":1,\"1986\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1993\":1,\"2001\":9,\"2002\":5,\"2004\":7,\"2024\":1,\"2025\":1,\"2026\":3,\"2028\":3,\"2029\":4,\"2032\":1,\"2037\":1,\"2039\":1,\"2040\":1,\"2044\":1,\"2045\":1,\"2049\":6,\"2051\":1,\"2053\":1,\"2054\":5,\"2055\":2,\"2056\":1,\"2057\":1,\"2058\":1,\"2063\":1,\"2064\":2,\"2065\":1,\"2066\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":2,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2077\":1,\"2078\":4,\"2079\":2,\"2084\":3,\"2086\":2,\"2087\":2,\"2089\":2,\"2090\":4,\"2095\":5,\"2096\":3,\"2098\":3,\"2099\":3,\"2100\":3,\"2101\":3,\"2102\":3,\"2103\":3,\"2104\":3,\"2105\":3,\"2107\":3,\"2108\":3,\"2109\":3,\"2110\":3,\"2111\":3,\"2112\":3,\"2113\":3,\"2114\":3,\"2115\":3,\"2116\":3,\"2117\":3,\"2118\":3,\"2183\":2,\"2186\":1,\"2190\":2,\"2193\":1,\"2198\":1,\"2201\":1,\"2202\":2,\"2204\":1,\"2234\":1,\"2235\":1,\"2238\":1,\"2242\":1,\"2243\":4,\"2244\":4,\"2247\":1,\"2249\":1,\"2251\":1,\"2252\":1,\"2255\":4,\"2258\":1,\"2259\":3,\"2260\":3,\"2263\":5,\"2264\":5,\"2267\":1,\"2277\":1,\"2279\":7,\"2286\":1,\"2292\":1,\"2293\":1,\"2304\":1,\"2305\":1,\"2343\":1,\"2344\":3,\"2347\":3,\"2348\":1,\"2349\":3,\"2350\":1,\"2360\":2,\"2368\":1,\"2371\":1,\"2375\":1,\"2392\":1,\"2403\":1,\"2425\":1,\"2430\":1,\"2432\":1,\"2440\":2,\"2458\":2,\"2468\":1,\"2486\":1,\"2487\":1,\"2490\":1,\"2491\":1,\"2494\":1,\"2497\":1,\"2521\":10,\"2522\":6,\"2523\":8,\"2528\":1,\"2539\":1,\"2548\":1,\"2554\":1,\"2555\":1,\"2558\":2,\"2564\":1,\"2572\":1,\"2582\":2,\"2584\":1,\"2592\":4,\"2596\":3,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1}}],[\"outdir=none\",{\"1\":{\"619\":1}}],[\"outdir=whisper\",{\"1\":{\"98\":1}}],[\"outdir\",{\"1\":{\"98\":1,\"173\":1,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"280\":2,\"503\":2,\"519\":1,\"528\":4,\"541\":1,\"619\":1,\"629\":2,\"799\":1,\"909\":2,\"995\":1,\"1396\":1,\"1403\":1,\"1407\":2,\"1415\":1}}],[\"outdated\",{\"1\":{\"14\":1}}],[\"out\",{\"1\":{\"14\":1,\"26\":3,\"52\":2,\"115\":1,\"173\":1,\"175\":1,\"194\":1,\"251\":3,\"255\":3,\"259\":3,\"263\":2,\"265\":3,\"267\":2,\"269\":3,\"276\":1,\"496\":1,\"499\":2,\"506\":1,\"522\":1,\"533\":1,\"536\":1,\"560\":2,\"564\":2,\"568\":1,\"572\":2,\"682\":2,\"700\":12,\"708\":2,\"712\":2,\"725\":3,\"726\":2,\"731\":3,\"743\":2,\"751\":2,\"762\":1,\"763\":1,\"766\":6,\"787\":1,\"806\":5,\"824\":4,\"825\":27,\"838\":1,\"858\":1,\"890\":2,\"914\":1,\"984\":1,\"992\":1,\"995\":1,\"1004\":2,\"1005\":2,\"1015\":10,\"1028\":3,\"1030\":4,\"1046\":2,\"1048\":7,\"1052\":4,\"1053\":1,\"1057\":2,\"1058\":2,\"1060\":3,\"1064\":7,\"1066\":3,\"1068\":1,\"1069\":1,\"1073\":3,\"1075\":3,\"1083\":1,\"1086\":1,\"1093\":1,\"1099\":2,\"1133\":11,\"1138\":14,\"1139\":12,\"1153\":1,\"1171\":8,\"1176\":2,\"1190\":3,\"1198\":10,\"1204\":3,\"1205\":1,\"1206\":8,\"1214\":7,\"1219\":1,\"1244\":3,\"1270\":5,\"1273\":7,\"1305\":1,\"1370\":1,\"1378\":1,\"1462\":1,\"1478\":2,\"1480\":2,\"1506\":1,\"1508\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1522\":1,\"1543\":2,\"1545\":5,\"1546\":3,\"1552\":8,\"1564\":1,\"1572\":1,\"1576\":6,\"1577\":6,\"1578\":1,\"1579\":1,\"1580\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1655\":1,\"1656\":2,\"1663\":1,\"1664\":2,\"1665\":2,\"1689\":1,\"1690\":1,\"1691\":1,\"1719\":1,\"1731\":1,\"1732\":1,\"1752\":2,\"1757\":1,\"1765\":3,\"1767\":2,\"1768\":2,\"1769\":1,\"1771\":2,\"1778\":3,\"1782\":1,\"1788\":3,\"1789\":1,\"1800\":4,\"1801\":2,\"1803\":3,\"1805\":2,\"1830\":1,\"1831\":2,\"1832\":1,\"1844\":4,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":2,\"1849\":2,\"1850\":3,\"1851\":2,\"1852\":3,\"1856\":2,\"1857\":3,\"1858\":2,\"1861\":2,\"1862\":4,\"1863\":5,\"1870\":1,\"1871\":4,\"1877\":2,\"1880\":3,\"1917\":4,\"1969\":2,\"2001\":4,\"2040\":1,\"2099\":1,\"2102\":1,\"2459\":2,\"2573\":1}}],[\"our\",{\"0\":{\"2359\":1,\"2456\":1,\"2521\":1,\"2580\":1},\"1\":{\"13\":1,\"17\":1,\"20\":1,\"47\":1,\"48\":3,\"57\":2,\"60\":1,\"69\":1,\"75\":1,\"82\":1,\"97\":1,\"112\":1,\"120\":1,\"126\":2,\"133\":1,\"141\":1,\"144\":1,\"201\":1,\"295\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2168\":1,\"2373\":1,\"2400\":1,\"2403\":1,\"2414\":2,\"2415\":1,\"2418\":1,\"2450\":1,\"2468\":1,\"2536\":1,\"2539\":1,\"2543\":1,\"2567\":1,\"2573\":1,\"2584\":3,\"2585\":1,\"2637\":1}}],[\"opcodes\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"opcode\",{\"1\":{\"2500\":5,\"2617\":5,\"2635\":5}}],[\"opportunity\",{\"1\":{\"2473\":1}}],[\"ops\",{\"1\":{\"1688\":1,\"1735\":2,\"1756\":1}}],[\"opid\",{\"1\":{\"1143\":1,\"1144\":1,\"1227\":2,\"1228\":2}}],[\"op\",{\"0\":{\"1194\":1,\"1226\":1,\"2161\":1,\"2162\":1,\"2163\":1},\"1\":{\"1031\":1,\"1144\":1,\"1194\":1,\"1226\":1,\"1227\":2,\"1228\":2,\"1688\":1,\"1756\":1,\"1761\":3,\"1763\":3,\"1768\":3,\"1805\":3,\"2161\":1,\"2162\":1,\"2163\":1}}],[\"opt=none\",{\"1\":{\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2}}],[\"opt\",{\"0\":{\"888\":1,\"1957\":1},\"1\":{\"251\":1,\"253\":2,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"888\":1,\"1957\":2,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1}}],[\"opts\",{\"1\":{\"144\":2,\"275\":1,\"276\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"294\":1,\"2099\":1}}],[\"optimmodule\",{\"0\":{\"1217\":1},\"1\":{\"1217\":2,\"1247\":1,\"1248\":1}}],[\"optimizations\",{\"1\":{\"119\":1}}],[\"optimized\",{\"1\":{\"1860\":1}}],[\"optimizer=none\",{\"1\":{\"834\":1}}],[\"optimizers\",{\"0\":{\"1972\":1,\"1973\":1,\"1974\":1,\"2698\":1},\"1\":{\"727\":1,\"728\":1,\"1972\":2,\"1973\":1,\"1974\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":2,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":2,\"2104\":2,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":2,\"2185\":1,\"2201\":7,\"2203\":1}}],[\"optimizer|dict\",{\"1\":{\"727\":1,\"728\":1}}],[\"optimizerfactoryinterface\",{\"0\":{\"661\":1},\"1\":{\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"665\":1}}],[\"optimizer\",{\"0\":{\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"665\":2,\"666\":1,\"792\":1,\"888\":1,\"1973\":1,\"1974\":1,\"2674\":1},\"1\":{\"62\":1,\"65\":1,\"80\":1,\"240\":1,\"604\":2,\"627\":4,\"659\":3,\"660\":3,\"661\":5,\"662\":3,\"663\":2,\"664\":2,\"665\":3,\"666\":2,\"667\":4,\"671\":4,\"727\":6,\"728\":6,\"742\":2,\"754\":1,\"792\":2,\"826\":1,\"834\":6,\"888\":1,\"976\":4,\"1043\":4,\"1217\":1,\"1245\":2,\"1298\":1,\"1299\":1,\"1302\":1,\"1303\":1,\"1327\":1,\"1773\":1,\"1778\":1,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1972\":1,\"1973\":1,\"1974\":1,\"2018\":5,\"2019\":2,\"2020\":2,\"2021\":5,\"2022\":7,\"2023\":6,\"2170\":3,\"2185\":1,\"2201\":4,\"2203\":1,\"2264\":1,\"2558\":1}}],[\"optimize\",{\"1\":{\"22\":1}}],[\"optim\",{\"0\":{\"1973\":1,\"1974\":1},\"1\":{\"62\":3,\"64\":4,\"80\":2,\"87\":2,\"92\":2,\"110\":3,\"174\":4,\"627\":1,\"792\":1,\"976\":1,\"1043\":1,\"1773\":1,\"1778\":1,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1972\":1,\"1973\":1,\"1974\":3,\"2170\":9,\"2294\":1,\"2357\":1,\"2378\":1,\"2437\":1,\"2440\":2,\"2558\":2,\"2563\":1,\"2578\":1,\"2584\":2}}],[\"optimum\",{\"1\":{\"24\":1,\"150\":2,\"1705\":1}}],[\"optimal\",{\"1\":{\"23\":1,\"119\":1,\"885\":1,\"1218\":1,\"1508\":1,\"1706\":1,\"1707\":1}}],[\"optionnaly\",{\"1\":{\"113\":1}}],[\"optional\",{\"0\":{\"134\":1,\"136\":1,\"2644\":1},\"1\":{\"21\":19,\"22\":6,\"56\":1,\"59\":4,\"108\":1,\"114\":1,\"115\":30,\"117\":1,\"135\":1,\"144\":1,\"271\":1,\"272\":1,\"641\":1,\"676\":2,\"696\":1,\"701\":7,\"735\":1,\"737\":7,\"738\":1,\"742\":3,\"747\":2,\"754\":6,\"762\":3,\"763\":6,\"774\":2,\"801\":4,\"802\":5,\"804\":2,\"815\":2,\"821\":5,\"826\":6,\"835\":2,\"838\":1,\"899\":2,\"901\":2,\"987\":2,\"996\":1,\"1011\":1,\"1025\":1,\"1141\":2,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1170\":2,\"1177\":2,\"1198\":1,\"1209\":1,\"1211\":1,\"1221\":1,\"1224\":2,\"1252\":2,\"1253\":3,\"1254\":2,\"1279\":1,\"1336\":1,\"1348\":2,\"1462\":1,\"1464\":1,\"1516\":11,\"1581\":2,\"1603\":1,\"1604\":1,\"1671\":3,\"1765\":1,\"1766\":3,\"1771\":1,\"1772\":2,\"1773\":41,\"1777\":1,\"1778\":12,\"1781\":4,\"1785\":4,\"1803\":1,\"1804\":12,\"1805\":13,\"1808\":2,\"1810\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1837\":18,\"1844\":2,\"1850\":3,\"1851\":9,\"1859\":5,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1868\":2,\"1876\":1,\"1877\":5,\"1878\":11,\"1880\":3,\"1927\":3,\"2001\":6,\"2002\":10,\"2004\":6,\"2078\":14,\"2079\":2,\"2081\":2,\"2082\":46,\"2083\":9,\"2086\":18,\"2087\":18,\"2090\":20,\"2095\":19,\"2096\":4,\"2097\":3,\"2098\":4,\"2099\":4,\"2100\":4,\"2101\":4,\"2102\":4,\"2103\":4,\"2104\":4,\"2105\":4,\"2107\":4,\"2108\":4,\"2109\":4,\"2110\":4,\"2111\":4,\"2112\":4,\"2113\":4,\"2114\":4,\"2115\":4,\"2116\":4,\"2117\":4,\"2118\":4,\"2176\":1,\"2240\":24,\"2243\":11,\"2244\":13,\"2255\":13,\"2257\":7,\"2259\":4,\"2260\":15,\"2261\":10,\"2262\":5,\"2263\":10,\"2264\":10,\"2274\":3,\"2278\":24,\"2279\":12,\"2349\":4,\"2584\":1}}],[\"optionally\",{\"1\":{\"1\":1,\"84\":1,\"136\":1,\"1255\":1,\"2638\":1}}],[\"option\",{\"1\":{\"18\":1,\"19\":2,\"25\":2,\"26\":3,\"28\":2,\"29\":2,\"32\":1,\"49\":1,\"52\":1,\"58\":1,\"62\":2,\"70\":1,\"73\":1,\"80\":1,\"90\":1,\"113\":1,\"118\":1,\"135\":8,\"144\":11,\"240\":2,\"398\":22,\"506\":1,\"727\":1,\"728\":1,\"952\":1,\"1245\":1,\"1406\":1,\"1773\":6,\"1967\":1,\"2082\":6,\"2099\":5,\"2102\":2,\"2185\":2,\"2186\":1,\"2198\":2,\"2201\":3,\"2203\":2,\"2204\":1,\"2309\":1,\"2313\":1,\"2377\":1,\"2436\":1,\"2558\":1,\"2562\":1,\"2585\":3}}],[\"options=\",{\"1\":{\"98\":1}}],[\"options\",{\"0\":{\"18\":1},\"1\":{\"3\":1,\"18\":2,\"22\":1,\"24\":1,\"26\":1,\"27\":2,\"28\":2,\"62\":1,\"90\":2,\"96\":1,\"98\":3,\"117\":1,\"118\":1,\"144\":3,\"235\":1,\"274\":2,\"275\":2,\"276\":1,\"277\":1,\"279\":2,\"280\":1,\"281\":2,\"282\":2,\"283\":2,\"284\":2,\"286\":2,\"294\":2,\"296\":2,\"297\":2,\"298\":1,\"377\":1,\"676\":2,\"742\":1,\"781\":2,\"812\":2,\"889\":1,\"894\":1,\"1243\":2,\"1245\":1,\"1269\":2,\"1659\":1,\"1665\":1,\"2099\":9,\"2131\":2,\"2176\":1,\"2180\":1,\"2185\":4,\"2198\":2,\"2201\":6,\"2203\":4,\"2377\":1,\"2436\":1,\"2562\":1,\"2568\":1,\"2583\":2}}],[\"operands\",{\"1\":{\"1694\":1}}],[\"operator\",{\"1\":{\"1144\":1,\"1228\":2}}],[\"operation\",{\"0\":{\"1737\":1},\"1\":{\"628\":1,\"1011\":1,\"1025\":1,\"1143\":1,\"1186\":1,\"1187\":2,\"1194\":1,\"1202\":2,\"1210\":1,\"1226\":1,\"1269\":2,\"1286\":1,\"1287\":1,\"1522\":1,\"1523\":1,\"1572\":1,\"1737\":1,\"2264\":1}}],[\"operations\",{\"1\":{\"27\":1,\"82\":1,\"1688\":1,\"1756\":1,\"2637\":1}}],[\"openslr53\",{\"1\":{\"2584\":1,\"2585\":5}}],[\"openslr\",{\"1\":{\"2512\":1,\"2657\":1}}],[\"openssl\",{\"1\":{\"38\":1,\"40\":1,\"41\":1,\"42\":1}}],[\"openmp\",{\"1\":{\"1337\":1,\"1349\":1,\"1350\":1}}],[\"openreview\",{\"1\":{\"668\":1,\"1841\":1}}],[\"openblas\",{\"1\":{\"134\":6}}],[\"openaiwhispertokenizer\",{\"0\":{\"2129\":1},\"1\":{\"2129\":1}}],[\"openaiwhispertokenidconverter\",{\"0\":{\"2128\":1},\"1\":{\"2128\":1}}],[\"openaiwhisperencoder\",{\"0\":{\"1215\":1},\"1\":{\"1215\":1}}],[\"openaiwhisperdecoder\",{\"0\":{\"1214\":1},\"1\":{\"1214\":1}}],[\"openai\",{\"0\":{\"98\":1},\"1\":{\"98\":1,\"697\":1,\"1214\":2,\"1215\":2,\"1284\":2}}],[\"open\",{\"1\":{\"12\":1,\"20\":1,\"112\":1,\"130\":3,\"171\":2,\"175\":2,\"185\":1,\"193\":1,\"194\":2,\"199\":1,\"200\":1,\"203\":1,\"206\":1,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"222\":2,\"223\":2,\"224\":1,\"229\":2,\"230\":2,\"231\":1,\"277\":1,\"609\":1,\"611\":1,\"2046\":1,\"2198\":1,\"2357\":2,\"2360\":1,\"2363\":1,\"2386\":2,\"2452\":2,\"2458\":1,\"2468\":2,\"2502\":1,\"2506\":1,\"2514\":1,\"2523\":1,\"2568\":3,\"2578\":2,\"2582\":1,\"2584\":1,\"2592\":1,\"2596\":1,\"2601\":1,\"2618\":1,\"2650\":1,\"2653\":1,\"2659\":1}}],[\"osize\",{\"1\":{\"1103\":1}}],[\"os\",{\"1\":{\"6\":1,\"134\":1,\"167\":2,\"178\":2,\"194\":4,\"196\":2,\"200\":2,\"201\":2,\"210\":2,\"211\":2,\"212\":2,\"214\":2,\"215\":2,\"216\":2,\"222\":2,\"223\":2,\"229\":2,\"230\":2,\"234\":2,\"235\":2,\"2367\":1,\"2372\":1,\"2386\":3,\"2470\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2497\":1,\"2514\":2,\"2568\":8,\"2604\":1,\"2614\":1,\"2621\":1,\"2632\":1,\"2647\":1,\"2659\":2}}],[\"owns\",{\"1\":{\"610\":1}}],[\"own\",{\"0\":{\"2360\":1,\"2458\":1,\"2522\":1,\"2523\":1,\"2581\":1,\"2582\":1,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1},\"1\":{\"5\":1,\"115\":1,\"133\":1,\"243\":1,\"760\":1,\"778\":1,\"831\":1,\"1476\":1,\"1598\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2084\":1,\"2089\":1,\"2360\":1,\"2393\":1,\"2458\":1,\"2481\":1,\"2501\":1,\"2514\":1,\"2522\":1,\"2523\":1,\"2529\":1,\"2581\":1,\"2582\":1,\"2587\":1,\"2593\":1,\"2600\":2,\"2618\":1,\"2659\":1}}],[\"offers\",{\"1\":{\"2452\":1}}],[\"officials\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"official\",{\"1\":{\"628\":1,\"1015\":1,\"1765\":1,\"1778\":1,\"1800\":1,\"1801\":3,\"1803\":1,\"1805\":1,\"1844\":1,\"1846\":3,\"1847\":3,\"1850\":1,\"1852\":1,\"1857\":1,\"1858\":1,\"1877\":1,\"2430\":1,\"2555\":1}}],[\"offsets\",{\"1\":{\"1735\":1}}],[\"offsetting\",{\"1\":{\"1735\":1}}],[\"offset=1\",{\"1\":{\"736\":1,\"738\":1}}],[\"offset\",{\"1\":{\"249\":2,\"737\":2,\"738\":2,\"1011\":1,\"1143\":2,\"1398\":1,\"2431\":1}}],[\"offline\",{\"0\":{\"2453\":1,\"2518\":1},\"1\":{\"122\":1,\"124\":1,\"836\":3,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"2451\":1,\"2452\":2,\"2461\":1}}],[\"off\",{\"1\":{\"59\":1,\"82\":1,\"940\":1,\"1142\":1,\"1186\":1,\"1210\":2,\"1712\":1,\"1715\":1,\"1860\":1,\"1883\":1,\"2090\":1}}],[\"often\",{\"1\":{\"17\":1,\"85\":1,\"1398\":1}}],[\"of\",{\"0\":{\"16\":1,\"19\":1,\"45\":1,\"69\":1,\"72\":1,\"93\":1,\"95\":1,\"100\":1,\"143\":1,\"205\":1,\"2359\":1,\"2432\":1,\"2452\":1,\"2468\":1,\"2498\":1,\"2598\":1,\"2616\":1,\"2634\":1},\"1\":{\"2\":1,\"5\":1,\"11\":1,\"14\":1,\"15\":1,\"16\":2,\"17\":1,\"20\":1,\"21\":20,\"22\":6,\"23\":5,\"25\":2,\"26\":5,\"28\":2,\"29\":1,\"31\":1,\"32\":1,\"33\":4,\"35\":1,\"36\":1,\"37\":3,\"45\":2,\"46\":5,\"47\":3,\"48\":4,\"49\":10,\"51\":1,\"52\":3,\"56\":5,\"57\":5,\"58\":1,\"59\":2,\"60\":16,\"62\":4,\"65\":3,\"68\":1,\"69\":5,\"72\":7,\"73\":6,\"74\":5,\"75\":5,\"76\":7,\"77\":3,\"78\":5,\"79\":3,\"80\":4,\"82\":1,\"84\":2,\"85\":13,\"88\":1,\"92\":4,\"93\":2,\"95\":4,\"99\":1,\"102\":3,\"108\":4,\"109\":1,\"112\":7,\"113\":4,\"114\":1,\"115\":22,\"116\":12,\"117\":3,\"118\":3,\"119\":4,\"121\":2,\"122\":1,\"126\":1,\"128\":1,\"130\":7,\"134\":1,\"137\":1,\"141\":1,\"142\":1,\"143\":4,\"144\":1,\"148\":7,\"150\":3,\"156\":1,\"161\":10,\"162\":2,\"163\":2,\"171\":1,\"188\":1,\"195\":1,\"197\":1,\"198\":1,\"203\":1,\"206\":1,\"233\":2,\"235\":4,\"237\":5,\"238\":6,\"239\":7,\"240\":5,\"274\":2,\"275\":4,\"276\":2,\"277\":1,\"279\":2,\"280\":1,\"281\":2,\"282\":2,\"283\":2,\"284\":2,\"286\":6,\"294\":1,\"295\":4,\"296\":5,\"297\":1,\"503\":1,\"600\":2,\"601\":1,\"606\":1,\"610\":3,\"612\":5,\"614\":3,\"618\":5,\"619\":5,\"621\":3,\"623\":1,\"625\":1,\"626\":2,\"627\":5,\"628\":1,\"629\":2,\"630\":1,\"631\":1,\"632\":5,\"633\":2,\"635\":2,\"647\":1,\"648\":1,\"652\":2,\"656\":1,\"668\":4,\"669\":1,\"670\":2,\"672\":4,\"676\":21,\"677\":3,\"678\":5,\"679\":8,\"680\":3,\"681\":7,\"682\":8,\"683\":7,\"684\":6,\"685\":9,\"686\":5,\"687\":5,\"688\":8,\"689\":8,\"691\":26,\"692\":7,\"693\":13,\"694\":1,\"695\":1,\"696\":2,\"697\":28,\"700\":4,\"701\":21,\"702\":5,\"703\":3,\"704\":1,\"705\":2,\"706\":4,\"709\":3,\"710\":1,\"711\":3,\"712\":7,\"713\":6,\"723\":3,\"725\":5,\"726\":1,\"727\":1,\"728\":3,\"730\":1,\"731\":5,\"732\":8,\"734\":7,\"735\":10,\"736\":3,\"737\":11,\"738\":2,\"740\":4,\"741\":4,\"742\":18,\"743\":2,\"745\":16,\"746\":13,\"747\":13,\"748\":5,\"749\":8,\"753\":2,\"754\":37,\"755\":10,\"757\":2,\"758\":7,\"759\":4,\"760\":6,\"761\":2,\"762\":3,\"763\":3,\"764\":4,\"765\":1,\"766\":3,\"767\":5,\"768\":1,\"771\":3,\"773\":4,\"774\":6,\"775\":4,\"776\":4,\"778\":7,\"779\":2,\"781\":16,\"782\":1,\"784\":8,\"785\":3,\"786\":6,\"787\":1,\"793\":1,\"794\":5,\"796\":1,\"797\":18,\"798\":1,\"799\":1,\"801\":2,\"802\":11,\"803\":4,\"804\":6,\"805\":4,\"806\":4,\"807\":3,\"808\":4,\"809\":3,\"812\":2,\"814\":1,\"815\":3,\"816\":2,\"817\":5,\"820\":6,\"821\":63,\"822\":10,\"824\":6,\"825\":5,\"826\":47,\"828\":7,\"831\":1,\"832\":2,\"833\":1,\"834\":5,\"835\":8,\"836\":3,\"837\":1,\"838\":6,\"852\":4,\"856\":1,\"857\":8,\"858\":1,\"861\":1,\"866\":1,\"870\":2,\"875\":6,\"878\":1,\"880\":1,\"889\":2,\"890\":1,\"892\":6,\"895\":2,\"899\":4,\"901\":4,\"903\":3,\"905\":5,\"906\":2,\"911\":1,\"913\":1,\"914\":2,\"917\":2,\"918\":1,\"920\":2,\"921\":1,\"922\":4,\"923\":2,\"924\":2,\"926\":1,\"927\":1,\"933\":2,\"934\":1,\"940\":2,\"943\":1,\"950\":4,\"952\":3,\"955\":1,\"965\":1,\"968\":4,\"972\":1,\"981\":1,\"987\":3,\"997\":3,\"999\":1,\"1001\":1,\"1003\":4,\"1004\":6,\"1005\":3,\"1007\":1,\"1010\":2,\"1011\":6,\"1013\":2,\"1015\":2,\"1019\":2,\"1025\":3,\"1028\":12,\"1031\":1,\"1034\":1,\"1037\":4,\"1039\":2,\"1046\":3,\"1048\":11,\"1049\":3,\"1050\":3,\"1051\":2,\"1052\":8,\"1053\":2,\"1054\":2,\"1055\":2,\"1056\":3,\"1057\":8,\"1058\":3,\"1059\":2,\"1060\":1,\"1062\":2,\"1064\":2,\"1065\":3,\"1066\":8,\"1068\":6,\"1069\":1,\"1071\":4,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":4,\"1076\":5,\"1077\":2,\"1078\":1,\"1079\":2,\"1081\":2,\"1093\":4,\"1096\":2,\"1098\":1,\"1101\":3,\"1110\":2,\"1112\":2,\"1118\":2,\"1120\":2,\"1122\":2,\"1124\":2,\"1126\":2,\"1127\":1,\"1128\":2,\"1131\":2,\"1132\":2,\"1133\":11,\"1135\":2,\"1137\":2,\"1138\":8,\"1139\":4,\"1141\":4,\"1142\":21,\"1144\":2,\"1145\":11,\"1148\":9,\"1149\":8,\"1150\":8,\"1152\":2,\"1154\":3,\"1155\":6,\"1157\":2,\"1159\":2,\"1160\":6,\"1161\":6,\"1162\":3,\"1163\":2,\"1164\":6,\"1165\":4,\"1170\":3,\"1173\":1,\"1175\":2,\"1177\":3,\"1178\":1,\"1180\":1,\"1181\":5,\"1185\":2,\"1186\":19,\"1187\":3,\"1189\":2,\"1190\":5,\"1198\":19,\"1200\":3,\"1202\":3,\"1203\":9,\"1208\":3,\"1209\":3,\"1210\":16,\"1211\":7,\"1213\":2,\"1214\":3,\"1216\":2,\"1221\":4,\"1222\":4,\"1223\":2,\"1224\":6,\"1228\":6,\"1230\":2,\"1232\":2,\"1234\":2,\"1236\":2,\"1238\":2,\"1240\":2,\"1241\":2,\"1243\":1,\"1244\":7,\"1245\":13,\"1246\":2,\"1248\":9,\"1250\":2,\"1251\":1,\"1252\":9,\"1253\":12,\"1254\":9,\"1255\":4,\"1256\":3,\"1257\":1,\"1258\":2,\"1260\":2,\"1262\":2,\"1264\":2,\"1266\":2,\"1268\":2,\"1269\":17,\"1270\":4,\"1272\":8,\"1273\":3,\"1275\":2,\"1277\":2,\"1279\":6,\"1281\":2,\"1282\":4,\"1283\":2,\"1285\":2,\"1286\":3,\"1287\":8,\"1298\":12,\"1299\":12,\"1301\":16,\"1302\":16,\"1303\":16,\"1304\":20,\"1327\":1,\"1334\":6,\"1336\":7,\"1337\":11,\"1345\":6,\"1347\":6,\"1348\":6,\"1349\":10,\"1350\":10,\"1351\":1,\"1352\":2,\"1354\":1,\"1361\":2,\"1363\":2,\"1365\":2,\"1367\":2,\"1375\":6,\"1377\":3,\"1379\":6,\"1381\":1,\"1390\":1,\"1392\":2,\"1394\":1,\"1396\":1,\"1398\":3,\"1400\":3,\"1406\":1,\"1407\":2,\"1412\":2,\"1417\":1,\"1422\":1,\"1427\":4,\"1429\":1,\"1430\":8,\"1432\":2,\"1434\":2,\"1436\":2,\"1438\":2,\"1440\":2,\"1442\":2,\"1444\":2,\"1446\":2,\"1448\":2,\"1450\":2,\"1451\":2,\"1452\":2,\"1453\":2,\"1454\":4,\"1457\":2,\"1459\":2,\"1461\":2,\"1462\":7,\"1463\":4,\"1464\":3,\"1467\":2,\"1469\":2,\"1470\":3,\"1471\":2,\"1475\":2,\"1476\":1,\"1477\":2,\"1479\":2,\"1481\":2,\"1483\":2,\"1486\":2,\"1488\":2,\"1490\":2,\"1492\":2,\"1494\":2,\"1496\":2,\"1498\":2,\"1500\":2,\"1502\":2,\"1504\":2,\"1505\":9,\"1507\":2,\"1509\":2,\"1513\":2,\"1514\":2,\"1515\":5,\"1516\":9,\"1517\":3,\"1519\":2,\"1521\":2,\"1522\":18,\"1523\":20,\"1524\":1,\"1525\":1,\"1528\":5,\"1529\":5,\"1530\":1,\"1531\":7,\"1532\":4,\"1533\":2,\"1534\":3,\"1535\":4,\"1536\":2,\"1537\":8,\"1538\":2,\"1539\":6,\"1541\":2,\"1543\":5,\"1544\":2,\"1545\":4,\"1548\":2,\"1550\":2,\"1551\":11,\"1553\":12,\"1556\":2,\"1557\":2,\"1558\":3,\"1559\":1,\"1560\":1,\"1562\":2,\"1565\":2,\"1572\":4,\"1574\":2,\"1576\":2,\"1577\":3,\"1581\":6,\"1582\":2,\"1584\":2,\"1585\":2,\"1587\":2,\"1589\":2,\"1591\":2,\"1593\":2,\"1594\":3,\"1595\":1,\"1597\":2,\"1598\":4,\"1599\":2,\"1600\":2,\"1602\":4,\"1603\":5,\"1604\":2,\"1606\":2,\"1608\":2,\"1610\":2,\"1612\":2,\"1614\":2,\"1615\":2,\"1618\":4,\"1619\":5,\"1621\":2,\"1622\":4,\"1623\":2,\"1625\":2,\"1626\":3,\"1628\":2,\"1630\":2,\"1632\":2,\"1634\":2,\"1636\":2,\"1637\":2,\"1638\":5,\"1639\":7,\"1640\":1,\"1642\":2,\"1645\":6,\"1646\":4,\"1647\":2,\"1648\":3,\"1649\":2,\"1650\":2,\"1651\":2,\"1652\":5,\"1653\":2,\"1654\":4,\"1655\":12,\"1656\":2,\"1657\":2,\"1658\":4,\"1659\":9,\"1660\":9,\"1661\":9,\"1662\":11,\"1664\":9,\"1665\":11,\"1669\":8,\"1670\":21,\"1671\":22,\"1673\":2,\"1675\":2,\"1677\":2,\"1679\":1,\"1683\":1,\"1688\":5,\"1693\":7,\"1696\":2,\"1697\":1,\"1698\":2,\"1701\":5,\"1702\":1,\"1704\":3,\"1705\":2,\"1706\":1,\"1707\":2,\"1708\":1,\"1712\":3,\"1713\":3,\"1714\":1,\"1715\":3,\"1718\":1,\"1719\":19,\"1735\":2,\"1737\":1,\"1739\":2,\"1741\":3,\"1752\":10,\"1755\":7,\"1756\":5,\"1758\":1,\"1759\":1,\"1762\":2,\"1764\":2,\"1765\":14,\"1766\":13,\"1767\":8,\"1768\":4,\"1770\":2,\"1771\":13,\"1772\":4,\"1773\":5,\"1775\":2,\"1776\":6,\"1777\":3,\"1778\":22,\"1780\":2,\"1783\":2,\"1785\":5,\"1786\":8,\"1787\":15,\"1788\":12,\"1790\":2,\"1791\":1,\"1792\":2,\"1794\":2,\"1796\":2,\"1797\":5,\"1798\":6,\"1800\":13,\"1801\":3,\"1802\":2,\"1803\":14,\"1804\":82,\"1805\":25,\"1807\":2,\"1808\":25,\"1810\":7,\"1829\":3,\"1833\":4,\"1834\":5,\"1835\":2,\"1836\":7,\"1837\":1,\"1838\":1,\"1839\":9,\"1841\":6,\"1843\":4,\"1844\":13,\"1845\":4,\"1846\":6,\"1847\":7,\"1848\":11,\"1849\":14,\"1850\":1,\"1851\":49,\"1856\":8,\"1857\":10,\"1858\":13,\"1859\":2,\"1860\":2,\"1861\":8,\"1862\":12,\"1863\":9,\"1864\":9,\"1865\":7,\"1866\":3,\"1867\":3,\"1868\":5,\"1870\":6,\"1871\":10,\"1872\":2,\"1873\":3,\"1874\":6,\"1876\":2,\"1877\":2,\"1878\":41,\"1879\":11,\"1880\":13,\"1883\":4,\"1889\":1,\"1891\":2,\"1895\":3,\"1897\":2,\"1900\":3,\"1903\":2,\"1904\":1,\"1905\":11,\"1906\":1,\"1908\":2,\"1910\":1,\"1912\":5,\"1913\":2,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":8,\"1918\":1,\"1919\":1,\"1920\":1,\"1925\":2,\"1928\":2,\"1929\":2,\"1930\":1,\"1932\":3,\"1937\":1,\"1941\":3,\"1946\":1,\"1947\":1,\"1951\":2,\"1952\":2,\"1954\":2,\"1956\":2,\"1957\":3,\"1958\":2,\"1959\":3,\"1960\":3,\"1962\":1,\"1963\":2,\"1968\":1,\"1971\":2,\"1972\":2,\"1973\":1,\"1977\":2,\"1979\":2,\"1982\":2,\"1985\":2,\"1986\":1,\"1988\":2,\"1990\":2,\"1992\":2,\"1995\":2,\"1998\":2,\"2000\":8,\"2001\":23,\"2002\":35,\"2003\":1,\"2004\":20,\"2012\":1,\"2018\":2,\"2022\":2,\"2023\":1,\"2025\":2,\"2029\":8,\"2030\":5,\"2031\":2,\"2032\":1,\"2033\":2,\"2035\":2,\"2037\":2,\"2039\":2,\"2040\":7,\"2041\":2,\"2043\":2,\"2044\":2,\"2045\":2,\"2046\":2,\"2048\":2,\"2049\":3,\"2051\":2,\"2052\":1,\"2053\":2,\"2054\":9,\"2055\":4,\"2056\":2,\"2058\":2,\"2060\":2,\"2062\":2,\"2064\":3,\"2065\":2,\"2067\":2,\"2068\":2,\"2069\":2,\"2070\":1,\"2071\":2,\"2073\":2,\"2078\":26,\"2079\":9,\"2081\":9,\"2082\":5,\"2083\":14,\"2084\":1,\"2086\":52,\"2087\":55,\"2088\":5,\"2089\":1,\"2090\":57,\"2091\":16,\"2095\":69,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":8,\"2100\":1,\"2101\":1,\"2102\":7,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2121\":1,\"2122\":1,\"2125\":1,\"2131\":1,\"2142\":2,\"2150\":2,\"2151\":5,\"2155\":1,\"2156\":2,\"2166\":1,\"2169\":2,\"2170\":1,\"2177\":1,\"2188\":1,\"2197\":11,\"2199\":2,\"2201\":2,\"2209\":2,\"2217\":1,\"2234\":2,\"2238\":2,\"2239\":1,\"2240\":2,\"2242\":2,\"2243\":48,\"2244\":63,\"2245\":16,\"2247\":2,\"2249\":2,\"2251\":2,\"2255\":62,\"2256\":16,\"2257\":10,\"2258\":4,\"2259\":6,\"2260\":12,\"2261\":13,\"2262\":4,\"2263\":51,\"2264\":54,\"2265\":8,\"2267\":5,\"2268\":1,\"2270\":2,\"2271\":1,\"2272\":2,\"2274\":2,\"2276\":2,\"2278\":2,\"2279\":49,\"2280\":16,\"2282\":2,\"2284\":2,\"2286\":2,\"2289\":2,\"2291\":2,\"2293\":2,\"2298\":2,\"2300\":2,\"2301\":2,\"2303\":1,\"2305\":2,\"2309\":2,\"2317\":3,\"2322\":1,\"2324\":1,\"2325\":2,\"2330\":2,\"2343\":2,\"2344\":4,\"2349\":1,\"2354\":1,\"2361\":1,\"2363\":3,\"2367\":1,\"2372\":10,\"2373\":5,\"2375\":4,\"2383\":1,\"2384\":6,\"2385\":10,\"2387\":6,\"2389\":3,\"2390\":1,\"2391\":1,\"2393\":3,\"2394\":10,\"2395\":1,\"2396\":1,\"2398\":1,\"2399\":1,\"2400\":3,\"2403\":1,\"2406\":1,\"2407\":1,\"2408\":1,\"2410\":2,\"2411\":6,\"2412\":2,\"2414\":1,\"2418\":2,\"2422\":3,\"2424\":1,\"2426\":1,\"2427\":2,\"2429\":8,\"2430\":5,\"2431\":3,\"2432\":1,\"2433\":1,\"2440\":1,\"2441\":3,\"2447\":1,\"2449\":1,\"2450\":1,\"2451\":1,\"2452\":4,\"2457\":1,\"2461\":1,\"2462\":1,\"2465\":1,\"2467\":6,\"2468\":1,\"2473\":3,\"2479\":1,\"2480\":1,\"2481\":2,\"2482\":1,\"2485\":2,\"2487\":3,\"2490\":1,\"2491\":3,\"2497\":4,\"2498\":1,\"2500\":2,\"2501\":2,\"2502\":1,\"2503\":1,\"2506\":3,\"2508\":4,\"2512\":2,\"2515\":2,\"2516\":1,\"2518\":2,\"2525\":3,\"2526\":1,\"2527\":1,\"2529\":3,\"2530\":10,\"2531\":1,\"2532\":1,\"2534\":1,\"2535\":1,\"2536\":3,\"2539\":1,\"2543\":1,\"2545\":3,\"2547\":1,\"2549\":1,\"2550\":2,\"2552\":1,\"2554\":8,\"2555\":6,\"2558\":4,\"2559\":2,\"2564\":1,\"2565\":1,\"2568\":4,\"2570\":1,\"2572\":1,\"2573\":2,\"2574\":1,\"2576\":1,\"2583\":1,\"2584\":11,\"2585\":9,\"2586\":1,\"2587\":1,\"2592\":2,\"2596\":2,\"2597\":1,\"2600\":2,\"2601\":1,\"2604\":1,\"2618\":1,\"2621\":1,\"2638\":1,\"2641\":1,\"2646\":1,\"2650\":1,\"2653\":3,\"2657\":2}}],[\"onstop\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"onset\",{\"1\":{\"249\":2}}],[\"ondataavailable\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"onreim\",{\"0\":{\"1620\":1},\"1\":{\"1620\":2}}],[\"onto\",{\"1\":{\"1143\":1}}],[\"onnx\",{\"0\":{\"930\":1,\"2597\":1},\"1\":{\"140\":1,\"159\":2,\"244\":1,\"930\":1,\"2597\":3,\"2598\":6,\"2599\":4,\"2600\":6}}],[\"onloadend\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"onlineprocessor\",{\"1\":{\"1071\":1}}],[\"onlineaudioprocessor\",{\"0\":{\"1071\":1},\"1\":{\"1071\":2}}],[\"online\",{\"0\":{\"692\":1,\"693\":1,\"1071\":1},\"1\":{\"105\":1,\"124\":2,\"130\":1,\"235\":1,\"236\":1,\"692\":2,\"693\":2,\"836\":2,\"1071\":1,\"2586\":1}}],[\"only=0\",{\"1\":{\"135\":1}}],[\"only\",{\"0\":{\"1343\":1},\"1\":{\"7\":1,\"22\":1,\"28\":1,\"45\":1,\"47\":1,\"56\":1,\"57\":1,\"66\":2,\"74\":1,\"75\":2,\"98\":1,\"111\":1,\"115\":2,\"116\":22,\"117\":1,\"133\":2,\"134\":1,\"135\":1,\"136\":1,\"148\":2,\"209\":1,\"213\":1,\"217\":2,\"240\":1,\"251\":1,\"286\":2,\"429\":2,\"597\":1,\"613\":1,\"650\":1,\"684\":1,\"709\":3,\"740\":1,\"741\":1,\"745\":1,\"746\":1,\"775\":1,\"776\":1,\"785\":3,\"821\":2,\"826\":1,\"868\":1,\"875\":1,\"900\":1,\"902\":1,\"987\":3,\"1003\":1,\"1004\":1,\"1061\":1,\"1075\":1,\"1115\":2,\"1179\":1,\"1181\":1,\"1198\":1,\"1252\":1,\"1253\":1,\"1254\":2,\"1337\":1,\"1340\":1,\"1343\":3,\"1419\":1,\"1438\":1,\"1462\":2,\"1464\":2,\"1466\":1,\"1531\":1,\"1545\":1,\"1551\":1,\"1553\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":2,\"1571\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1655\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1666\":1,\"1667\":2,\"1668\":1,\"1670\":1,\"1671\":2,\"1688\":1,\"1704\":1,\"1712\":1,\"1715\":1,\"1719\":1,\"1746\":1,\"1756\":1,\"1804\":5,\"1805\":1,\"1864\":3,\"1865\":3,\"1877\":1,\"1878\":5,\"1905\":2,\"1927\":1,\"1932\":3,\"1933\":1,\"1956\":1,\"1972\":1,\"1973\":1,\"2095\":1,\"2148\":1,\"2168\":1,\"2263\":1,\"2264\":1,\"2355\":1,\"2364\":4,\"2384\":1,\"2397\":1,\"2433\":1,\"2437\":1,\"2440\":3,\"2506\":1,\"2507\":4,\"2510\":8,\"2512\":1,\"2513\":4,\"2533\":1,\"2563\":1,\"2564\":3,\"2565\":1,\"2568\":2,\"2584\":4,\"2597\":1,\"2654\":4,\"2657\":1,\"2658\":4}}],[\"ony\",{\"1\":{\"80\":1}}],[\"oneseded\",{\"1\":{\"1859\":1}}],[\"onesided\",{\"1\":{\"1158\":1,\"1643\":1,\"1644\":1,\"1859\":2,\"1918\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1}}],[\"ones\",{\"1\":{\"907\":3,\"928\":2,\"1016\":1,\"1847\":1,\"2394\":2,\"2530\":2,\"2584\":1}}],[\"onehot\",{\"0\":{\"793\":1},\"1\":{\"793\":2}}],[\"one\",{\"0\":{\"919\":1,\"1759\":1},\"1\":{\"21\":1,\"23\":1,\"26\":1,\"49\":1,\"56\":1,\"64\":2,\"79\":1,\"80\":1,\"92\":1,\"99\":1,\"112\":2,\"114\":1,\"116\":1,\"119\":1,\"120\":1,\"121\":1,\"130\":1,\"142\":1,\"143\":1,\"204\":1,\"209\":1,\"213\":1,\"249\":1,\"251\":2,\"253\":2,\"255\":2,\"257\":1,\"259\":2,\"261\":1,\"263\":1,\"265\":2,\"267\":1,\"269\":2,\"299\":1,\"560\":1,\"564\":2,\"612\":1,\"684\":1,\"692\":4,\"725\":2,\"734\":1,\"743\":1,\"745\":3,\"746\":3,\"749\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"793\":2,\"794\":1,\"806\":2,\"824\":2,\"832\":1,\"875\":1,\"889\":2,\"905\":1,\"919\":2,\"927\":1,\"976\":1,\"1043\":1,\"1046\":2,\"1066\":2,\"1069\":1,\"1073\":2,\"1075\":2,\"1083\":2,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1130\":1,\"1131\":1,\"1133\":3,\"1135\":1,\"1137\":1,\"1148\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1160\":2,\"1161\":2,\"1162\":1,\"1164\":2,\"1165\":2,\"1175\":1,\"1177\":2,\"1185\":1,\"1189\":1,\"1203\":1,\"1208\":1,\"1209\":2,\"1213\":1,\"1214\":2,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1243\":1,\"1246\":1,\"1248\":1,\"1250\":1,\"1252\":3,\"1253\":3,\"1254\":3,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1270\":2,\"1273\":2,\"1275\":1,\"1277\":1,\"1279\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1352\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1375\":1,\"1406\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1451\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1519\":1,\"1521\":1,\"1523\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1557\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1577\":1,\"1582\":1,\"1584\":1,\"1585\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":2,\"1606\":1,\"1608\":1,\"1610\":1,\"1612\":1,\"1614\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1621\":1,\"1623\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1637\":1,\"1638\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1671\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1703\":1,\"1704\":1,\"1713\":1,\"1739\":1,\"1759\":3,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1859\":2,\"1890\":1,\"1891\":1,\"1895\":1,\"1900\":1,\"1903\":1,\"1905\":1,\"1908\":1,\"1913\":1,\"1917\":1,\"1932\":1,\"1951\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2001\":2,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2046\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2054\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2154\":1,\"2169\":1,\"2185\":4,\"2198\":1,\"2201\":4,\"2203\":4,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2367\":1,\"2372\":1,\"2373\":2,\"2384\":2,\"2385\":1,\"2400\":1,\"2411\":1,\"2414\":1,\"2415\":1,\"2428\":1,\"2430\":2,\"2441\":2,\"2462\":2,\"2485\":1,\"2515\":1,\"2536\":1,\"2543\":2,\"2551\":1,\"2555\":2,\"2564\":1,\"2569\":1,\"2573\":1,\"2584\":1,\"2585\":2,\"2604\":1,\"2621\":1}}],[\"once\",{\"1\":{\"16\":1,\"235\":1,\"627\":1,\"727\":1,\"728\":1,\"1484\":2,\"1601\":2,\"1688\":1,\"1756\":1}}],[\"on\",{\"0\":{\"126\":1,\"2384\":1,\"2394\":1,\"2428\":1,\"2530\":1,\"2551\":1,\"2585\":1},\"1\":{\"1\":1,\"5\":6,\"6\":1,\"16\":1,\"18\":1,\"22\":1,\"23\":2,\"28\":1,\"44\":1,\"49\":2,\"56\":1,\"57\":1,\"62\":1,\"79\":1,\"80\":1,\"83\":1,\"84\":2,\"90\":1,\"97\":1,\"98\":1,\"102\":1,\"107\":1,\"110\":1,\"112\":2,\"113\":1,\"115\":2,\"119\":2,\"120\":3,\"124\":1,\"130\":2,\"133\":1,\"135\":1,\"144\":1,\"150\":1,\"155\":1,\"161\":2,\"162\":1,\"164\":1,\"167\":1,\"178\":1,\"201\":2,\"235\":1,\"240\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":3,\"267\":1,\"269\":3,\"628\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"691\":6,\"692\":1,\"693\":2,\"697\":8,\"700\":5,\"704\":1,\"705\":1,\"740\":1,\"741\":1,\"758\":1,\"762\":1,\"774\":2,\"775\":1,\"776\":1,\"797\":2,\"802\":1,\"803\":1,\"804\":1,\"835\":1,\"875\":1,\"885\":1,\"892\":1,\"920\":1,\"943\":1,\"944\":1,\"950\":2,\"955\":1,\"965\":1,\"968\":2,\"972\":1,\"987\":1,\"988\":2,\"1001\":2,\"1025\":1,\"1031\":1,\"1032\":1,\"1033\":1,\"1048\":3,\"1066\":1,\"1071\":1,\"1075\":1,\"1130\":1,\"1138\":7,\"1139\":5,\"1142\":1,\"1154\":1,\"1156\":1,\"1187\":1,\"1194\":1,\"1202\":1,\"1226\":1,\"1244\":1,\"1252\":1,\"1255\":1,\"1256\":1,\"1276\":1,\"1343\":1,\"1409\":2,\"1529\":1,\"1531\":2,\"1564\":1,\"1566\":2,\"1567\":2,\"1568\":3,\"1569\":2,\"1570\":1,\"1571\":2,\"1645\":1,\"1662\":1,\"1695\":1,\"1703\":1,\"1704\":1,\"1706\":1,\"1707\":1,\"1719\":1,\"1735\":1,\"1766\":1,\"1781\":1,\"1791\":1,\"1860\":1,\"1861\":1,\"1883\":1,\"1911\":1,\"1917\":3,\"1925\":2,\"1964\":1,\"1971\":1,\"2030\":1,\"2046\":1,\"2097\":3,\"2121\":1,\"2122\":1,\"2142\":1,\"2236\":1,\"2237\":1,\"2263\":1,\"2354\":2,\"2355\":2,\"2368\":1,\"2371\":2,\"2372\":5,\"2373\":3,\"2384\":2,\"2387\":1,\"2388\":2,\"2389\":2,\"2395\":2,\"2400\":2,\"2403\":1,\"2408\":2,\"2410\":1,\"2412\":1,\"2422\":3,\"2424\":1,\"2429\":2,\"2430\":1,\"2433\":1,\"2438\":1,\"2440\":2,\"2441\":1,\"2449\":2,\"2450\":1,\"2457\":1,\"2461\":1,\"2465\":2,\"2467\":1,\"2468\":1,\"2471\":1,\"2473\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2481\":4,\"2486\":1,\"2490\":1,\"2494\":2,\"2497\":1,\"2500\":2,\"2503\":2,\"2508\":1,\"2515\":1,\"2518\":1,\"2524\":2,\"2525\":2,\"2531\":2,\"2536\":2,\"2539\":1,\"2545\":3,\"2547\":1,\"2554\":2,\"2555\":1,\"2564\":1,\"2568\":1,\"2569\":1,\"2572\":1,\"2573\":1,\"2574\":1,\"2583\":1,\"2584\":3,\"2585\":3,\"2586\":1,\"2587\":1,\"2605\":1,\"2609\":1,\"2612\":2,\"2618\":1,\"2622\":1,\"2626\":1,\"2630\":2,\"2635\":1,\"2638\":1}}],[\"1lxoxcofgx3u8cvkb1loybgftarkkpcah\",{\"1\":{\"2647\":1}}],[\"1l71zunq6qqk95t54h0th\",{\"1\":{\"2593\":1}}],[\"1l8w93r8bs5ftc3a\",{\"1\":{\"2593\":1}}],[\"1ydeqfqdp4k6fiul\",{\"1\":{\"2593\":1}}],[\"1wnaeebjddcgi8rzhnikkemfv15ktcnre\",{\"1\":{\"2519\":1}}],[\"1fzyyjlvrt\",{\"1\":{\"2512\":1}}],[\"1fohdfbloa7ipc9v2luy\",{\"1\":{\"2490\":1,\"2609\":1,\"2626\":1}}],[\"1pxssaulipn31hnq8ywwsi9ndb3b2my\",{\"1\":{\"2510\":1}}],[\"1pjt9fx13d7mv6locs\",{\"1\":{\"2506\":1}}],[\"1phn\",{\"1\":{\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1}}],[\"1h7\",{\"1\":{\"2500\":1}}],[\"1hrdjjmgxbtxoqoq9iijuxpca4y\",{\"1\":{\"2396\":1,\"2532\":1}}],[\"1taszxzsnbspsk\",{\"1\":{\"2494\":1}}],[\"1`\",{\"1\":{\"2441\":1}}],[\"1zcukd\",{\"1\":{\"2372\":1,\"2497\":1,\"2614\":1,\"2632\":1}}],[\"1sn2razxvsm1hrcj5oilq61egbjkxngdq\",{\"1\":{\"2454\":1}}],[\"1st\",{\"1\":{\"2368\":1,\"2486\":1,\"2490\":1,\"2568\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"1smrn5nfsg6juqss2sfy3ehd8oicqk6ws\",{\"1\":{\"2367\":1,\"2485\":1,\"2604\":1,\"2621\":1}}],[\"1best\",{\"1\":{\"2193\":2}}],[\"1$\",{\"1\":{\"1517\":2}}],[\"1x1\",{\"1\":{\"1375\":3,\"1664\":1,\"1665\":1,\"1690\":1,\"1731\":1,\"1831\":2}}],[\"1|4\",{\"1\":{\"2440\":1,\"2441\":1}}],[\"1|4|9\",{\"1\":{\"794\":1}}],[\"1|1\",{\"1\":{\"2440\":3}}],[\"1|16\",{\"1\":{\"2440\":1}}],[\"1|0\",{\"1\":{\"2440\":2,\"2572\":1}}],[\"1|22\",{\"1\":{\"2441\":1}}],[\"1|2\",{\"1\":{\"2440\":1}}],[\"1|28\",{\"1\":{\"2440\":1}}],[\"1|2|3\",{\"1\":{\"794\":1}}],[\"1czzmpmliwszja9tdbv7wmidlgepzbeui\",{\"1\":{\"2478\":1}}],[\"1ch\",{\"1\":{\"942\":1,\"1551\":2,\"1553\":2}}],[\"1char\",{\"1\":{\"175\":1,\"194\":1,\"239\":2,\"276\":1,\"298\":1}}],[\"1c|c0=c0\",{\"1\":{\"52\":2}}],[\"17\",{\"1\":{\"2445\":1}}],[\"17dmwdw84wf3fz3t7ia1zssdzhkpvqgzm\",{\"1\":{\"2368\":1,\"2486\":1,\"2605\":1,\"2622\":1}}],[\"1710\",{\"1\":{\"1084\":1}}],[\"17196\",{\"1\":{\"130\":1}}],[\"1703\",{\"1\":{\"729\":1}}],[\"1706\",{\"1\":{\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"2019\":1}}],[\"1704\",{\"1\":{\"678\":1}}],[\"173\",{\"1\":{\"87\":1}}],[\"14706\",{\"1\":{\"2467\":1}}],[\"146\",{\"1\":{\"2384\":1}}],[\"1445\",{\"1\":{\"2040\":1}}],[\"142\",{\"1\":{\"1778\":1,\"1852\":1,\"1860\":1,\"1870\":1,\"1883\":1}}],[\"14941\",{\"1\":{\"692\":1,\"693\":1,\"706\":2,\"2586\":1}}],[\"1456\",{\"1\":{\"130\":2}}],[\"14\",{\"1\":{\"115\":1,\"706\":2,\"2482\":1}}],[\"14x\",{\"1\":{\"115\":1}}],[\"199epoch\",{\"1\":{\"2510\":1}}],[\"19\",{\"1\":{\"2446\":1}}],[\"1915\",{\"1\":{\"2445\":1}}],[\"1910\",{\"1\":{\"1072\":1,\"1080\":1,\"1150\":1}}],[\"1980\",{\"1\":{\"1904\":1}}],[\"198329952\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1}}],[\"195\",{\"1\":{\"1860\":1}}],[\"1909\",{\"1\":{\"1371\":1}}],[\"1908\",{\"1\":{\"1067\":1}}],[\"1904\",{\"1\":{\"950\":1,\"968\":1,\"1037\":1}}],[\"1901\",{\"1\":{\"771\":1,\"772\":1,\"809\":1,\"810\":1}}],[\"19452\",{\"1\":{\"110\":1}}],[\"1920928955078125e\",{\"1\":{\"1641\":1}}],[\"192khz\",{\"1\":{\"49\":2}}],[\"192\",{\"1\":{\"44\":1,\"87\":1,\"1761\":1,\"1763\":1,\"1771\":2,\"1776\":1,\"1777\":1,\"1787\":2,\"1788\":2,\"1798\":1,\"1804\":1,\"1805\":2,\"1863\":2,\"1864\":2,\"1865\":2,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1}}],[\"11th\",{\"1\":{\"2583\":1}}],[\"11epoch\",{\"1\":{\"2490\":1,\"2609\":1,\"2626\":1}}],[\"11692\",{\"0\":{\"2380\":1,\"2388\":1,\"2406\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1}}],[\"11492\",{\"0\":{\"2380\":1,\"2388\":1,\"2406\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1}}],[\"114348\",{\"1\":{\"1973\":1}}],[\"11751\",{\"0\":{\"2354\":1,\"2421\":1,\"2524\":1,\"2544\":1}}],[\"1109\",{\"1\":{\"1696\":1,\"1698\":1}}],[\"11273\",{\"1\":{\"1566\":1}}],[\"11\",{\"1\":{\"51\":1,\"87\":1,\"1082\":1,\"1167\":6,\"1168\":6,\"1196\":6,\"1197\":6,\"1398\":2,\"1761\":11,\"1763\":11,\"1765\":2,\"1778\":2,\"1800\":2,\"1801\":1,\"1803\":1,\"1804\":2,\"1805\":14,\"1844\":1,\"1845\":1,\"1847\":1,\"1850\":2,\"1851\":1,\"1852\":2,\"1871\":1,\"1877\":2,\"1878\":1,\"2259\":2,\"2375\":3,\"2429\":1,\"2441\":2,\"2442\":2,\"2554\":1,\"2558\":3,\"2583\":1,\"2584\":7,\"2585\":4,\"2598\":1}}],[\"1khz\",{\"1\":{\"48\":1}}],[\"1gpu\",{\"0\":{\"39\":1},\"1\":{\"2500\":4,\"2617\":1,\"2635\":1}}],[\"129\",{\"1\":{\"1778\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1}}],[\"12943\",{\"1\":{\"1084\":1}}],[\"12433\",{\"1\":{\"1660\":1}}],[\"1211\",{\"1\":{\"700\":2,\"1048\":1,\"1138\":2,\"1139\":2}}],[\"1200\",{\"1\":{\"295\":8}}],[\"128\",{\"1\":{\"115\":1,\"116\":2,\"1065\":1,\"1066\":1,\"1106\":1,\"1108\":1,\"1115\":2,\"1158\":1,\"1269\":1,\"1373\":1,\"1375\":1,\"1377\":1,\"1516\":2,\"1517\":1,\"1522\":1,\"1523\":1,\"1643\":1,\"1644\":1,\"1645\":2,\"1658\":1,\"1659\":3,\"1665\":1,\"1671\":1,\"1761\":6,\"1763\":6,\"1778\":1,\"1785\":2,\"1786\":1,\"1801\":1,\"1805\":7,\"1846\":1,\"1847\":1,\"1849\":1,\"1850\":4,\"1851\":3,\"1852\":4,\"1862\":1,\"1871\":1,\"1877\":1,\"1880\":1,\"1897\":1,\"1910\":1,\"1918\":1,\"1960\":1,\"2003\":2,\"2059\":1,\"2064\":1,\"2084\":1,\"2089\":1,\"2095\":3,\"2243\":3,\"2244\":3,\"2255\":3,\"2257\":3,\"2261\":3,\"2262\":1,\"2263\":3,\"2264\":3,\"2304\":1,\"2440\":1,\"2600\":1}}],[\"123\",{\"1\":{\"987\":1,\"1385\":2,\"1387\":2,\"2154\":2}}],[\"1230\",{\"1\":{\"75\":2}}],[\"1234\",{\"1\":{\"58\":1,\"1015\":2,\"2154\":1}}],[\"12\",{\"1\":{\"31\":1,\"38\":1,\"40\":1,\"41\":1,\"42\":1,\"51\":1,\"84\":1,\"105\":1,\"110\":1,\"111\":1,\"769\":1,\"1115\":4,\"1140\":1,\"1169\":1,\"1181\":2,\"1269\":2,\"1735\":1,\"1941\":1,\"2267\":1,\"2375\":3,\"2387\":1,\"2394\":2,\"2409\":2,\"2429\":3,\"2431\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2498\":1,\"2530\":2,\"2552\":3,\"2553\":1,\"2558\":2,\"2559\":3,\"2564\":1,\"2584\":1,\"2616\":1,\"2634\":1}}],[\"15a6dc1501b65211725a4fb514fcf5dd24f7ae95\",{\"1\":{\"2442\":1}}],[\"155\",{\"1\":{\"2432\":1}}],[\"156\",{\"1\":{\"75\":2}}],[\"15\",{\"1\":{\"51\":1,\"885\":1,\"887\":1,\"1198\":1,\"1256\":1,\"1643\":1,\"1644\":1,\"1698\":1,\"1711\":1,\"1761\":1,\"1763\":1,\"1778\":1,\"1801\":1,\"1805\":2,\"1846\":1,\"1847\":1,\"1849\":1,\"1850\":1,\"1852\":1,\"1856\":1,\"1877\":1,\"2002\":1,\"2095\":1,\"2263\":1,\"2372\":1,\"2409\":1,\"2466\":1,\"2558\":1}}],[\"1500\",{\"1\":{\"2070\":1}}],[\"15003\",{\"1\":{\"1466\":1}}],[\"150\",{\"1\":{\"26\":2}}],[\"1536\",{\"1\":{\"21\":1,\"1505\":1,\"1669\":1,\"1778\":2,\"1852\":2,\"2044\":1,\"2049\":1,\"2052\":1,\"2055\":1,\"2064\":1,\"2068\":1,\"2090\":2,\"2243\":2,\"2244\":2,\"2255\":1,\"2279\":2}}],[\"1d\",{\"0\":{\"1306\":1},\"1\":{\"21\":1,\"115\":3,\"697\":1,\"706\":2,\"708\":1,\"712\":1,\"714\":1,\"715\":1,\"716\":1,\"734\":1,\"773\":1,\"777\":1,\"796\":1,\"815\":1,\"817\":1,\"828\":1,\"1190\":1,\"1221\":1,\"1244\":1,\"1306\":1,\"1517\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"2264\":1}}],[\"13jek\",{\"1\":{\"2510\":1}}],[\"1330\",{\"1\":{\"2012\":1}}],[\"130\",{\"1\":{\"1947\":1,\"2429\":1,\"2444\":1,\"2445\":1,\"2446\":1,\"2554\":1}}],[\"13048\",{\"1\":{\"1075\":1}}],[\"13404\",{\"1\":{\"1767\":1,\"1793\":1}}],[\"13\",{\"1\":{\"1761\":1,\"1763\":1,\"1805\":1,\"2375\":3,\"2384\":1,\"2440\":1,\"2444\":1,\"2450\":1,\"2482\":2,\"2504\":1,\"2517\":1,\"2559\":3,\"2564\":1,\"2569\":1,\"2571\":1}}],[\"137762\",{\"1\":{\"110\":1}}],[\"137936\",{\"1\":{\"17\":1}}],[\"13876\",{\"1\":{\"130\":1}}],[\"138175\",{\"1\":{\"17\":1}}],[\"138049\",{\"1\":{\"17\":1}}],[\"1386\",{\"1\":{\"17\":1}}],[\"136473\",{\"1\":{\"17\":1}}],[\"136320\",{\"1\":{\"17\":1}}],[\"136184\",{\"1\":{\"17\":1}}],[\"1ezvm3yujtvzsytoewtd0msxdqw19aaxa\",{\"1\":{\"2519\":1}}],[\"1ezs8iputlr\",{\"1\":{\"2476\":1}}],[\"1ekuemvmab3zhaiy\",{\"1\":{\"2460\":1}}],[\"1e\",{\"1\":{\"17\":6,\"115\":7,\"609\":1,\"611\":1,\"760\":1,\"831\":1,\"885\":1,\"887\":1,\"932\":1,\"1072\":1,\"1080\":1,\"1132\":1,\"1465\":1,\"1524\":3,\"1525\":3,\"1528\":1,\"1611\":4,\"1671\":1,\"1696\":2,\"1697\":2,\"1698\":2,\"1703\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1711\":1,\"1712\":2,\"1714\":2,\"1715\":2,\"1746\":2,\"1759\":1,\"1835\":1,\"1855\":1,\"1906\":1,\"1920\":1,\"1949\":1,\"2271\":1}}],[\"18ant62ittt7ai2e8bqrlvt0zvxxsf1ee\",{\"1\":{\"2470\":1,\"2647\":1}}],[\"18lyr\",{\"1\":{\"2460\":3,\"2461\":2}}],[\"18781\",{\"0\":{\"2354\":1,\"2421\":1,\"2524\":1,\"2544\":1}}],[\"1812\",{\"1\":{\"1061\":1}}],[\"1801\",{\"1\":{\"1084\":1}}],[\"1809\",{\"1\":{\"813\":1}}],[\"1807\",{\"1\":{\"681\":1,\"682\":1}}],[\"18\",{\"0\":{\"8\":1},\"1\":{\"5\":1,\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2375\":1,\"2444\":1}}],[\"10best\",{\"1\":{\"2357\":2,\"2455\":1,\"2460\":1,\"2461\":1,\"2472\":2,\"2474\":1,\"2476\":2,\"2478\":1,\"2500\":1,\"2507\":1,\"2513\":1,\"2578\":2,\"2648\":2,\"2649\":1}}],[\"10batch\",{\"1\":{\"87\":1}}],[\"1011\",{\"1\":{\"2032\":1}}],[\"1016\",{\"1\":{\"1715\":1}}],[\"1023\",{\"1\":{\"1400\":1}}],[\"1024\",{\"1\":{\"49\":1,\"102\":1,\"115\":1,\"116\":5,\"295\":15,\"1065\":1,\"1066\":2,\"1075\":1,\"1179\":1,\"1181\":2,\"1761\":9,\"1763\":9,\"1777\":2,\"1778\":3,\"1801\":2,\"1804\":2,\"1805\":12,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":1,\"1849\":1,\"1850\":5,\"1851\":2,\"1852\":3,\"1856\":1,\"1858\":1,\"1859\":1,\"1870\":1,\"1877\":3,\"1896\":1,\"1960\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"2002\":1,\"2003\":1,\"2049\":1,\"2055\":1,\"2064\":1,\"2084\":1,\"2086\":2,\"2087\":2,\"2089\":1,\"2095\":1,\"2210\":1,\"2236\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2263\":1,\"2264\":2,\"2432\":1,\"2440\":3,\"2558\":2,\"2564\":1}}],[\"10447\",{\"1\":{\"1829\":1}}],[\"104\",{\"1\":{\"1179\":1}}],[\"1075\",{\"1\":{\"734\":1}}],[\"10ms\",{\"1\":{\"109\":2}}],[\"10096020\",{\"1\":{\"1462\":1,\"1463\":1}}],[\"1000epoch\",{\"1\":{\"2510\":1}}],[\"10000\",{\"1\":{\"77\":1,\"652\":1,\"2023\":1}}],[\"10000000000\",{\"1\":{\"609\":1,\"611\":1}}],[\"1000000\",{\"1\":{\"282\":1}}],[\"1000000steps\",{\"1\":{\"216\":1}}],[\"100000\",{\"1\":{\"26\":1}}],[\"1000\",{\"1\":{\"69\":1,\"109\":1,\"1897\":1,\"1905\":2,\"1936\":1,\"1939\":1,\"2151\":1,\"2210\":1,\"2255\":1,\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1,\"2584\":1}}],[\"100\",{\"1\":{\"26\":3,\"68\":1,\"689\":1,\"959\":1,\"987\":1,\"989\":2,\"1171\":1,\"1181\":1,\"1203\":6,\"1206\":1,\"1220\":2,\"1269\":1,\"1289\":2,\"1408\":1,\"1552\":1,\"1567\":1,\"1799\":1,\"1928\":1,\"1953\":1,\"1955\":1,\"2079\":1,\"2151\":2,\"2444\":1,\"2445\":1,\"2446\":1,\"2558\":1,\"2592\":1}}],[\"10654\",{\"1\":{\"1371\":1}}],[\"10655\",{\"1\":{\"1066\":1}}],[\"106\",{\"1\":{\"17\":1}}],[\"10\",{\"1\":{\"5\":4,\"8\":2,\"17\":1,\"58\":1,\"62\":1,\"72\":4,\"74\":1,\"75\":1,\"93\":2,\"98\":1,\"109\":1,\"130\":2,\"132\":1,\"134\":1,\"135\":2,\"150\":6,\"174\":2,\"175\":1,\"185\":2,\"193\":2,\"196\":1,\"217\":1,\"224\":1,\"231\":1,\"234\":1,\"609\":1,\"611\":1,\"652\":1,\"835\":2,\"880\":1,\"947\":1,\"954\":1,\"959\":1,\"967\":1,\"971\":1,\"1067\":1,\"1115\":8,\"1180\":1,\"1219\":1,\"1220\":1,\"1269\":3,\"1289\":1,\"1398\":1,\"1639\":1,\"1696\":1,\"1698\":1,\"1703\":1,\"1715\":1,\"1759\":2,\"1761\":7,\"1763\":7,\"1805\":8,\"1833\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1859\":1,\"1861\":1,\"1890\":1,\"1985\":1,\"1989\":1,\"2002\":1,\"2079\":7,\"2095\":1,\"2149\":1,\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2197\":1,\"2200\":1,\"2243\":1,\"2244\":1,\"2248\":1,\"2255\":1,\"2261\":1,\"2262\":1,\"2263\":2,\"2264\":2,\"2372\":1,\"2375\":3,\"2377\":2,\"2378\":2,\"2385\":4,\"2387\":2,\"2419\":1,\"2436\":2,\"2437\":2,\"2440\":4,\"2441\":1,\"2461\":1,\"2558\":4,\"2562\":2,\"2563\":2,\"2564\":1,\"2569\":1,\"2576\":1,\"2584\":4,\"2602\":1,\"2619\":1}}],[\"16k\",{\"1\":{\"2490\":2,\"2492\":1,\"2609\":2,\"2626\":2,\"2628\":1}}],[\"16khz\",{\"1\":{\"48\":2}}],[\"1609\",{\"1\":{\"1782\":1}}],[\"160\",{\"1\":{\"1207\":1,\"1255\":1}}],[\"1608\",{\"1\":{\"668\":1}}],[\"16000\",{\"1\":{\"109\":1,\"286\":1,\"296\":1,\"752\":1,\"778\":1,\"959\":1,\"989\":1,\"992\":1,\"995\":1,\"1071\":1,\"1115\":2,\"1158\":1,\"1181\":1,\"1198\":1,\"1239\":1,\"1284\":1,\"1408\":3,\"1558\":1,\"1643\":1,\"1644\":1,\"1785\":1,\"1791\":1,\"1912\":1,\"1917\":1,\"1928\":1,\"1989\":1,\"2194\":1,\"2197\":1,\"2248\":1,\"2357\":5,\"2455\":1,\"2460\":1,\"2501\":1,\"2520\":1,\"2578\":5}}],[\"16xlarge\",{\"1\":{\"44\":1}}],[\"16852\",{\"1\":{\"1850\":1}}],[\"168\",{\"1\":{\"44\":1}}],[\"1616\",{\"1\":{\"17\":1}}],[\"16\",{\"1\":{\"5\":1,\"6\":2,\"17\":1,\"87\":3,\"104\":2,\"509\":1,\"512\":1,\"525\":1,\"585\":1,\"838\":1,\"1115\":2,\"1132\":1,\"1149\":2,\"1150\":2,\"1179\":1,\"1244\":1,\"1269\":1,\"1463\":1,\"1522\":1,\"1523\":1,\"1605\":1,\"1761\":8,\"1763\":10,\"1765\":2,\"1766\":2,\"1778\":3,\"1786\":2,\"1800\":4,\"1801\":3,\"1803\":2,\"1804\":5,\"1805\":12,\"1844\":2,\"1846\":1,\"1847\":1,\"1849\":1,\"1850\":3,\"1851\":2,\"1852\":3,\"1856\":1,\"1858\":1,\"1863\":1,\"1870\":1,\"1877\":4,\"1878\":3,\"2188\":1,\"2372\":1,\"2429\":1,\"2441\":2,\"2442\":2,\"2445\":1,\"2446\":2,\"2521\":1,\"2554\":1,\"2592\":1,\"2596\":1,\"2600\":2}}],[\"1\",{\"0\":{\"134\":1,\"171\":1,\"236\":1,\"238\":1,\"2398\":1,\"2414\":2,\"2432\":1,\"2438\":1,\"2453\":1,\"2454\":2,\"2455\":1,\"2456\":1,\"2457\":1,\"2458\":1,\"2459\":1,\"2461\":1,\"2462\":1,\"2469\":1,\"2470\":2,\"2471\":2,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2479\":1,\"2485\":2,\"2490\":1,\"2497\":1,\"2500\":1,\"2501\":1,\"2508\":1,\"2510\":1,\"2534\":2,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2542\":2,\"2543\":1,\"2554\":2,\"2556\":1,\"2559\":1,\"2560\":1,\"2564\":2,\"2565\":1,\"2566\":1,\"2589\":1,\"2637\":1,\"2642\":1},\"1\":{\"1\":3,\"3\":4,\"4\":1,\"8\":6,\"17\":1,\"19\":5,\"21\":9,\"23\":3,\"28\":2,\"29\":1,\"32\":2,\"36\":1,\"39\":4,\"41\":2,\"44\":1,\"53\":2,\"58\":1,\"60\":1,\"62\":3,\"64\":2,\"76\":1,\"78\":1,\"87\":1,\"92\":2,\"109\":1,\"115\":21,\"116\":9,\"118\":1,\"119\":1,\"135\":3,\"143\":2,\"150\":5,\"167\":2,\"168\":1,\"169\":1,\"171\":1,\"173\":2,\"174\":2,\"175\":2,\"178\":2,\"180\":1,\"181\":1,\"182\":1,\"185\":1,\"186\":1,\"187\":1,\"193\":1,\"194\":1,\"196\":2,\"200\":1,\"203\":2,\"207\":2,\"217\":6,\"218\":1,\"224\":2,\"225\":1,\"231\":2,\"232\":1,\"234\":2,\"235\":3,\"236\":3,\"237\":1,\"238\":14,\"239\":1,\"240\":3,\"249\":1,\"251\":1,\"286\":2,\"294\":3,\"296\":2,\"509\":1,\"512\":1,\"525\":1,\"585\":1,\"605\":1,\"623\":1,\"625\":2,\"629\":1,\"689\":1,\"691\":1,\"693\":1,\"694\":1,\"697\":1,\"698\":2,\"699\":1,\"700\":6,\"710\":1,\"711\":4,\"712\":11,\"714\":2,\"715\":3,\"716\":3,\"717\":2,\"718\":2,\"719\":3,\"720\":3,\"721\":3,\"722\":2,\"725\":5,\"726\":4,\"728\":3,\"729\":1,\"730\":1,\"732\":1,\"736\":1,\"742\":1,\"744\":12,\"747\":2,\"748\":1,\"749\":3,\"754\":2,\"756\":1,\"758\":1,\"760\":2,\"765\":1,\"766\":2,\"768\":1,\"771\":1,\"772\":1,\"777\":1,\"778\":3,\"784\":1,\"785\":2,\"794\":12,\"798\":1,\"799\":1,\"800\":1,\"801\":1,\"806\":8,\"809\":3,\"825\":2,\"827\":1,\"835\":1,\"837\":1,\"838\":1,\"857\":1,\"858\":1,\"865\":1,\"869\":3,\"876\":11,\"877\":3,\"878\":2,\"885\":2,\"899\":4,\"900\":169,\"901\":4,\"902\":124,\"904\":8,\"906\":1,\"907\":15,\"921\":6,\"925\":1,\"928\":9,\"940\":2,\"944\":1,\"952\":1,\"961\":1,\"987\":1,\"1005\":2,\"1011\":2,\"1028\":3,\"1048\":2,\"1049\":4,\"1050\":4,\"1051\":2,\"1052\":10,\"1053\":2,\"1054\":1,\"1055\":1,\"1056\":4,\"1057\":2,\"1058\":2,\"1062\":2,\"1064\":2,\"1065\":9,\"1066\":2,\"1067\":1,\"1068\":3,\"1069\":8,\"1071\":7,\"1073\":10,\"1075\":13,\"1076\":24,\"1077\":1,\"1081\":5,\"1082\":7,\"1083\":1,\"1084\":4,\"1096\":6,\"1099\":1,\"1106\":2,\"1107\":1,\"1108\":2,\"1115\":20,\"1116\":1,\"1129\":1,\"1133\":11,\"1134\":1,\"1138\":6,\"1139\":5,\"1140\":3,\"1141\":4,\"1145\":1,\"1148\":3,\"1149\":3,\"1150\":4,\"1158\":1,\"1167\":2,\"1168\":2,\"1169\":3,\"1170\":3,\"1171\":2,\"1172\":1,\"1179\":7,\"1180\":2,\"1181\":2,\"1182\":2,\"1191\":1,\"1192\":1,\"1194\":1,\"1195\":1,\"1196\":2,\"1197\":2,\"1198\":2,\"1200\":2,\"1203\":9,\"1204\":2,\"1206\":1,\"1209\":2,\"1210\":1,\"1211\":1,\"1214\":4,\"1215\":1,\"1216\":1,\"1219\":1,\"1220\":5,\"1222\":2,\"1224\":1,\"1225\":1,\"1226\":1,\"1239\":2,\"1241\":1,\"1245\":2,\"1248\":1,\"1255\":1,\"1269\":6,\"1270\":9,\"1271\":2,\"1272\":4,\"1273\":8,\"1274\":1,\"1276\":1,\"1282\":1,\"1289\":3,\"1302\":1,\"1303\":1,\"1304\":2,\"1337\":3,\"1355\":2,\"1371\":2,\"1375\":2,\"1376\":6,\"1379\":2,\"1400\":1,\"1410\":1,\"1412\":1,\"1418\":2,\"1423\":7,\"1426\":1,\"1427\":3,\"1444\":1,\"1462\":1,\"1463\":2,\"1465\":1,\"1476\":1,\"1478\":4,\"1480\":4,\"1505\":5,\"1506\":2,\"1508\":2,\"1516\":3,\"1517\":3,\"1522\":10,\"1523\":9,\"1524\":3,\"1525\":2,\"1529\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1545\":5,\"1551\":2,\"1554\":2,\"1568\":1,\"1576\":1,\"1577\":1,\"1581\":1,\"1600\":2,\"1603\":1,\"1605\":4,\"1607\":1,\"1611\":2,\"1619\":2,\"1622\":1,\"1631\":3,\"1633\":1,\"1635\":1,\"1643\":2,\"1644\":1,\"1660\":3,\"1661\":2,\"1662\":2,\"1664\":2,\"1665\":2,\"1669\":4,\"1670\":1,\"1671\":2,\"1685\":1,\"1686\":1,\"1688\":2,\"1693\":2,\"1695\":1,\"1696\":3,\"1698\":2,\"1702\":1,\"1705\":3,\"1706\":2,\"1707\":2,\"1708\":1,\"1712\":7,\"1713\":3,\"1715\":5,\"1719\":1,\"1735\":2,\"1736\":1,\"1755\":2,\"1756\":2,\"1761\":63,\"1763\":69,\"1765\":9,\"1766\":12,\"1767\":2,\"1771\":5,\"1772\":2,\"1773\":7,\"1776\":4,\"1777\":4,\"1778\":37,\"1786\":13,\"1787\":1,\"1788\":4,\"1791\":2,\"1797\":4,\"1798\":2,\"1800\":9,\"1801\":15,\"1803\":7,\"1804\":23,\"1805\":92,\"1808\":3,\"1810\":1,\"1811\":2,\"1833\":1,\"1834\":1,\"1835\":2,\"1837\":6,\"1842\":1,\"1844\":8,\"1845\":5,\"1846\":5,\"1847\":9,\"1848\":4,\"1849\":5,\"1850\":39,\"1851\":33,\"1852\":35,\"1853\":1,\"1855\":1,\"1856\":3,\"1857\":2,\"1858\":4,\"1859\":3,\"1860\":2,\"1861\":5,\"1862\":4,\"1863\":5,\"1864\":3,\"1865\":4,\"1866\":2,\"1867\":1,\"1868\":6,\"1870\":4,\"1871\":3,\"1874\":2,\"1876\":1,\"1877\":28,\"1878\":20,\"1879\":4,\"1880\":7,\"1883\":1,\"1890\":1,\"1892\":2,\"1893\":1,\"1905\":8,\"1911\":1,\"1917\":2,\"1926\":1,\"1935\":2,\"1940\":1,\"1943\":2,\"1947\":1,\"1960\":3,\"1970\":1,\"1971\":1,\"1972\":1,\"1975\":1,\"1984\":1,\"1985\":1,\"1993\":1,\"1996\":2,\"1997\":1,\"1999\":2,\"2000\":1,\"2001\":10,\"2002\":8,\"2003\":2,\"2004\":6,\"2006\":2,\"2007\":1,\"2008\":1,\"2009\":1,\"2012\":1,\"2018\":8,\"2019\":1,\"2020\":1,\"2021\":3,\"2022\":3,\"2023\":5,\"2026\":3,\"2027\":1,\"2029\":4,\"2032\":2,\"2054\":2,\"2059\":1,\"2063\":1,\"2070\":7,\"2074\":4,\"2075\":4,\"2076\":3,\"2078\":2,\"2079\":2,\"2081\":1,\"2082\":6,\"2086\":9,\"2087\":10,\"2090\":21,\"2091\":4,\"2095\":12,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":6,\"2100\":1,\"2101\":1,\"2102\":4,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2149\":1,\"2151\":2,\"2153\":1,\"2154\":1,\"2170\":4,\"2178\":4,\"2179\":4,\"2184\":6,\"2188\":1,\"2191\":4,\"2193\":1,\"2194\":3,\"2195\":4,\"2196\":7,\"2197\":2,\"2200\":6,\"2201\":1,\"2240\":6,\"2243\":23,\"2244\":35,\"2245\":4,\"2255\":34,\"2256\":4,\"2257\":1,\"2259\":2,\"2260\":4,\"2261\":1,\"2263\":12,\"2264\":19,\"2265\":1,\"2267\":1,\"2268\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2275\":1,\"2278\":6,\"2279\":34,\"2280\":5,\"2290\":1,\"2292\":2,\"2294\":2,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":2,\"2305\":1,\"2325\":1,\"2330\":1,\"2355\":1,\"2360\":1,\"2362\":2,\"2365\":1,\"2372\":6,\"2373\":7,\"2375\":2,\"2378\":1,\"2385\":1,\"2386\":3,\"2394\":4,\"2395\":2,\"2397\":1,\"2398\":1,\"2403\":1,\"2409\":1,\"2410\":1,\"2412\":2,\"2429\":3,\"2430\":7,\"2432\":3,\"2437\":1,\"2440\":15,\"2441\":5,\"2442\":2,\"2444\":1,\"2445\":1,\"2446\":2,\"2450\":1,\"2452\":1,\"2454\":1,\"2455\":2,\"2458\":1,\"2461\":1,\"2472\":3,\"2474\":2,\"2476\":10,\"2481\":2,\"2487\":1,\"2492\":1,\"2497\":5,\"2498\":9,\"2499\":1,\"2500\":6,\"2508\":3,\"2510\":1,\"2514\":2,\"2515\":1,\"2517\":1,\"2521\":2,\"2522\":2,\"2523\":3,\"2530\":4,\"2531\":2,\"2533\":1,\"2534\":1,\"2539\":1,\"2542\":1,\"2552\":3,\"2553\":1,\"2554\":1,\"2555\":7,\"2558\":10,\"2559\":2,\"2563\":1,\"2564\":8,\"2568\":11,\"2569\":3,\"2571\":1,\"2582\":1,\"2584\":12,\"2585\":2,\"2592\":3,\"2593\":1,\"2598\":1,\"2600\":3,\"2602\":1,\"2614\":5,\"2615\":1,\"2616\":10,\"2617\":6,\"2618\":3,\"2619\":1,\"2628\":1,\"2632\":5,\"2633\":1,\"2634\":10,\"2635\":6,\"2638\":3,\"2648\":3,\"2649\":2,\"2655\":1,\"2659\":2,\"2660\":1}}],[\"egg=fairseq\",{\"1\":{\"2466\":1,\"2646\":1}}],[\"egs2\",{\"1\":{\"2\":2,\"47\":1,\"49\":1,\"85\":9,\"110\":2,\"134\":1,\"1551\":1,\"1553\":1,\"2361\":1,\"2366\":1,\"2372\":3,\"2375\":5,\"2377\":6,\"2379\":1,\"2382\":1,\"2384\":2,\"2385\":3,\"2387\":2,\"2394\":1,\"2396\":1,\"2398\":1,\"2401\":1,\"2411\":1,\"2429\":3,\"2430\":1,\"2432\":2,\"2436\":5,\"2440\":1,\"2461\":1,\"2530\":1,\"2532\":1,\"2537\":1,\"2542\":1,\"2554\":3,\"2555\":1,\"2558\":6,\"2559\":2,\"2562\":5,\"2564\":1,\"2566\":4,\"2567\":3,\"2571\":1,\"2574\":1,\"2576\":1,\"2584\":2,\"2585\":3,\"2587\":1,\"2601\":1,\"2618\":1,\"2637\":3,\"2638\":1,\"2640\":1,\"2646\":1,\"2650\":1}}],[\"egs\",{\"0\":{\"179\":1},\"1\":{\"1\":2,\"2\":1,\"3\":5,\"4\":1,\"15\":1,\"16\":2,\"28\":1,\"85\":1,\"134\":1,\"168\":2,\"169\":1,\"171\":1,\"175\":1,\"179\":2,\"180\":1,\"181\":2,\"183\":1,\"184\":1,\"185\":1,\"186\":2,\"187\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"193\":1,\"194\":1,\"197\":2,\"198\":2,\"201\":1,\"204\":1,\"235\":3,\"243\":2,\"878\":1,\"2359\":7,\"2456\":6,\"2460\":5,\"2461\":2,\"2521\":9,\"2580\":7}}],[\"err\",{\"1\":{\"2444\":2,\"2445\":2,\"2446\":2,\"2500\":12,\"2617\":12,\"2635\":12}}],[\"err|\",{\"1\":{\"2440\":3,\"2441\":3,\"2564\":3,\"2572\":3}}],[\"errorcalculatortransducer\",{\"0\":{\"1173\":1},\"1\":{\"1059\":1,\"1173\":2}}],[\"errorcalculator\",{\"0\":{\"750\":1,\"1059\":1},\"1\":{\"750\":3,\"1059\":1}}],[\"error\",{\"0\":{\"530\":1,\"1059\":1,\"1173\":1},\"1\":{\"44\":1,\"59\":1,\"98\":2,\"136\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"530\":3,\"593\":2,\"594\":3,\"702\":1,\"738\":1,\"822\":1,\"823\":2,\"1057\":2,\"1059\":1,\"1085\":1,\"1095\":1,\"1173\":1,\"1371\":1,\"2000\":1,\"2088\":1,\"2271\":1,\"2368\":1,\"2375\":3,\"2403\":1,\"2418\":1,\"2467\":1,\"2486\":1,\"2490\":1,\"2539\":1,\"2542\":1,\"2558\":1,\"2559\":3,\"2564\":1,\"2568\":3,\"2569\":2,\"2584\":1,\"2585\":1,\"2598\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"errors\",{\"1\":{\"34\":1,\"150\":2,\"2362\":1,\"2393\":1,\"2427\":1,\"2467\":2,\"2504\":1,\"2529\":1,\"2550\":1,\"2576\":1,\"2651\":1}}],[\"e18\",{\"1\":{\"2436\":1,\"2562\":1}}],[\"ez\",{\"0\":{\"2352\":1,\"2353\":1},\"1\":{\"2352\":2,\"2353\":2}}],[\"eer\",{\"0\":{\"2310\":1,\"2311\":1,\"2341\":1},\"1\":{\"2198\":1,\"2310\":2,\"2311\":2,\"2341\":2,\"2403\":1,\"2405\":1,\"2539\":1,\"2541\":1,\"2542\":1,\"2543\":1}}],[\"eend\",{\"1\":{\"1371\":6,\"1375\":1}}],[\"ebidirectional\",{\"1\":{\"2086\":2,\"2087\":2}}],[\"ebranchformerencoderlayer\",{\"0\":{\"1170\":1},\"1\":{\"1170\":1}}],[\"ebranchformerencoder\",{\"0\":{\"1169\":1},\"1\":{\"1169\":1}}],[\"ebranchformer\",{\"0\":{\"1056\":2,\"1091\":1},\"1\":{\"115\":1,\"1056\":2,\"1091\":1}}],[\"eigenvectors\",{\"1\":{\"1695\":2}}],[\"eigenvalues\",{\"1\":{\"1695\":2}}],[\"eigenvalue\",{\"0\":{\"1695\":1},\"1\":{\"1695\":2,\"1704\":3,\"1713\":1}}],[\"einsum\",{\"0\":{\"1694\":1},\"1\":{\"1694\":2}}],[\"either\",{\"1\":{\"21\":1,\"113\":1,\"115\":2,\"745\":2,\"746\":2,\"858\":1,\"862\":1,\"886\":1,\"910\":1,\"934\":1,\"1186\":1,\"1187\":1,\"1198\":1,\"1202\":1,\"1210\":1,\"1811\":1,\"1905\":1,\"2083\":2,\"2385\":1,\"2441\":1,\"2585\":1}}],[\"ey\",{\"1\":{\"1221\":1}}],[\"eye\",{\"1\":{\"88\":1}}],[\"ecapatdnnencoder\",{\"0\":{\"2049\":1},\"1\":{\"2049\":1}}],[\"ecapablock\",{\"0\":{\"2047\":1},\"1\":{\"2047\":2,\"2049\":1}}],[\"ecapa\",{\"0\":{\"2047\":1,\"2049\":1},\"1\":{\"2044\":1,\"2047\":2,\"2049\":4}}],[\"eccv\",{\"1\":{\"2040\":1}}],[\"econv\",{\"1\":{\"821\":3,\"2083\":6,\"2095\":6,\"2263\":6}}],[\"ecoder\",{\"1\":{\"731\":1}}],[\"echo\",{\"1\":{\"18\":2,\"90\":2,\"143\":5,\"167\":1,\"178\":1,\"196\":1,\"198\":1,\"200\":1,\"234\":1,\"295\":3,\"2385\":1,\"2387\":4,\"2567\":2,\"2568\":2}}],[\"eqauls\",{\"1\":{\"760\":1}}],[\"eq\",{\"1\":{\"706\":2,\"880\":1,\"2267\":3}}],[\"equation\",{\"1\":{\"1694\":1,\"1742\":1,\"2268\":1,\"2270\":1,\"2272\":1}}],[\"equals\",{\"1\":{\"2021\":1,\"2022\":1,\"2023\":1,\"2099\":1,\"2102\":1,\"2216\":1,\"2217\":1}}],[\"equalization\",{\"0\":{\"1936\":1},\"1\":{\"1905\":1,\"1936\":2}}],[\"equal\",{\"1\":{\"56\":1,\"76\":1,\"77\":1,\"78\":1,\"917\":1,\"1048\":1,\"1418\":1,\"1423\":1,\"1735\":1,\"1870\":1,\"1929\":1,\"1941\":1,\"1947\":1,\"2403\":1,\"2500\":1,\"2539\":1,\"2542\":1,\"2617\":1,\"2635\":1}}],[\"equiped\",{\"1\":{\"1065\":1}}],[\"equivalently\",{\"1\":{\"1187\":1,\"1202\":1}}],[\"equivalent\",{\"1\":{\"38\":1,\"63\":2,\"143\":1,\"1144\":2,\"1339\":1,\"1377\":1}}],[\"efficient\",{\"1\":{\"743\":1,\"875\":1,\"1141\":1,\"1144\":1,\"1170\":1,\"1228\":1,\"1345\":1,\"1347\":1,\"1462\":1,\"1463\":1,\"1572\":1,\"1670\":1,\"1671\":1,\"1688\":1,\"1697\":1,\"1756\":1,\"2373\":2,\"2375\":1,\"2430\":1,\"2431\":1,\"2433\":1,\"2555\":2,\"2558\":1}}],[\"efficiently\",{\"1\":{\"704\":1,\"705\":1,\"762\":1,\"2415\":1}}],[\"effects\",{\"1\":{\"188\":1,\"1905\":8,\"1927\":1,\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2195\":1,\"2200\":1}}],[\"effectively\",{\"1\":{\"2467\":1}}],[\"effectiveness\",{\"1\":{\"150\":1}}],[\"effective\",{\"1\":{\"80\":1,\"1462\":2,\"1464\":2,\"1923\":1,\"2521\":1}}],[\"effect\",{\"1\":{\"63\":1,\"685\":1,\"1905\":4,\"1928\":1}}],[\"e5n\",{\"1\":{\"231\":1}}],[\"euler\",{\"1\":{\"1638\":1,\"1646\":1}}],[\"eulermaruyamapredictor\",{\"0\":{\"1557\":1},\"1\":{\"1557\":1}}],[\"eunits=512\",{\"1\":{\"2083\":1}}],[\"eunits=100\",{\"1\":{\"187\":1}}],[\"eunits\",{\"1\":{\"682\":3,\"754\":1,\"821\":1,\"826\":1,\"1778\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"2083\":3,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2279\":2}}],[\"europe\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"euro\",{\"1\":{\"130\":1}}],[\"eos=\",{\"1\":{\"2348\":1}}],[\"eos=0\",{\"1\":{\"612\":1}}],[\"eos>\",{\"1\":{\"1171\":2,\"1955\":1,\"2076\":2,\"2348\":1}}],[\"eos\",{\"0\":{\"852\":2},\"1\":{\"175\":2,\"194\":2,\"217\":1,\"224\":1,\"231\":1,\"691\":2,\"693\":2,\"697\":2,\"704\":1,\"705\":2,\"706\":2,\"734\":1,\"797\":2,\"798\":3,\"815\":1,\"829\":1,\"852\":4,\"857\":2,\"870\":2,\"905\":2,\"1057\":1,\"1171\":1,\"1219\":1,\"1955\":1,\"1975\":1,\"2076\":1,\"2294\":1,\"2373\":1,\"2555\":1}}],[\"eog\",{\"1\":{\"88\":3}}],[\"edist\",{\"1\":{\"2500\":2,\"2617\":2,\"2635\":2}}],[\"editdistance\",{\"1\":{\"2500\":2,\"2617\":2,\"2635\":2}}],[\"editing\",{\"1\":{\"2378\":1}}],[\"edit\",{\"1\":{\"25\":1,\"2500\":4,\"2617\":4,\"2635\":4}}],[\"ed\",{\"1\":{\"2387\":1}}],[\"edropout\",{\"1\":{\"2086\":2,\"2087\":2}}],[\"edu\",{\"1\":{\"1400\":1,\"1523\":1,\"2380\":1,\"2388\":1,\"2406\":1,\"2421\":2,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":1,\"2544\":1,\"2583\":1,\"2618\":4}}],[\"eda\",{\"1\":{\"1371\":3,\"1375\":1}}],[\"edge\",{\"1\":{\"130\":1}}],[\"evluate\",{\"0\":{\"2617\":1,\"2635\":1}}],[\"evd\",{\"1\":{\"1704\":2,\"1713\":2}}],[\"evaltypes\",{\"1\":{\"528\":1}}],[\"evaludation\",{\"1\":{\"239\":1}}],[\"evaluator\",{\"0\":{\"979\":1},\"1\":{\"607\":1,\"626\":1,\"975\":1,\"979\":3,\"1042\":1}}],[\"evaluated\",{\"1\":{\"1001\":1,\"1031\":1}}],[\"evaluates\",{\"1\":{\"607\":1}}],[\"evaluate\",{\"0\":{\"2499\":1},\"1\":{\"96\":1,\"98\":2,\"528\":1,\"530\":1,\"607\":2,\"626\":4,\"975\":2,\"1001\":2,\"1042\":2,\"2481\":1,\"2618\":1}}],[\"evaluation\",{\"0\":{\"96\":1,\"97\":1,\"98\":1,\"189\":1},\"1\":{\"85\":1,\"91\":2,\"168\":1,\"179\":1,\"237\":1,\"238\":1,\"607\":3,\"676\":1,\"781\":1,\"804\":1,\"812\":1,\"975\":1,\"1042\":1,\"1603\":1,\"1895\":1,\"1900\":1,\"2197\":1,\"2373\":2,\"2385\":1,\"2418\":2,\"2430\":1,\"2452\":1,\"2462\":1,\"2468\":1,\"2555\":1}}],[\"eval92\",{\"1\":{\"98\":1}}],[\"eval\",{\"0\":{\"280\":1,\"528\":1,\"530\":1},\"1\":{\"91\":1,\"174\":1,\"175\":1,\"194\":1,\"217\":2,\"224\":2,\"231\":2,\"265\":2,\"269\":2,\"280\":3,\"429\":2,\"528\":5,\"530\":2,\"607\":2,\"608\":2,\"942\":1,\"979\":2,\"1531\":1,\"1640\":1,\"2185\":1,\"2193\":2,\"2197\":2,\"2201\":1,\"2203\":1,\"2360\":1,\"2397\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2431\":1,\"2458\":1,\"2500\":1,\"2523\":1,\"2533\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2582\":1,\"2617\":1,\"2635\":1}}],[\"even\",{\"1\":{\"745\":1,\"746\":1,\"804\":1,\"836\":1,\"1245\":1,\"1406\":1,\"1522\":2,\"1523\":1,\"2467\":1,\"2564\":1}}],[\"events\",{\"1\":{\"17\":1}}],[\"everyday\",{\"1\":{\"940\":1}}],[\"every\",{\"1\":{\"65\":1,\"69\":1,\"80\":1,\"116\":2,\"752\":1,\"756\":1,\"760\":1,\"778\":1,\"811\":1,\"831\":1,\"1003\":1,\"1004\":1,\"1075\":3,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1134\":1,\"1136\":1,\"1151\":1,\"1156\":1,\"1158\":1,\"1174\":1,\"1178\":1,\"1184\":1,\"1188\":1,\"1207\":1,\"1212\":1,\"1215\":1,\"1220\":1,\"1222\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1244\":2,\"1245\":1,\"1249\":1,\"1252\":2,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":1,\"1518\":1,\"1520\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1543\":1,\"1547\":1,\"1549\":1,\"1555\":1,\"1561\":1,\"1564\":1,\"1573\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1596\":1,\"1598\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1613\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1641\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1656\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1761\":1,\"1763\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1801\":1,\"1806\":1,\"1890\":1,\"1902\":1,\"1907\":1,\"1912\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1976\":1,\"1978\":1,\"1981\":1,\"1984\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1994\":1,\"1997\":1,\"2024\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2050\":1,\"2052\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2099\":1,\"2102\":1,\"2149\":1,\"2168\":1,\"2233\":1,\"2237\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2275\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1,\"2467\":1,\"2585\":1}}],[\"emotion\",{\"1\":{\"2467\":1}}],[\"emrys365\",{\"1\":{\"2366\":1,\"2601\":1,\"2618\":1}}],[\"emulating\",{\"1\":{\"1242\":1}}],[\"emulate\",{\"1\":{\"150\":1,\"1927\":1}}],[\"emebedding\",{\"1\":{\"1174\":1}}],[\"embs\",{\"1\":{\"2262\":2}}],[\"emb\",{\"1\":{\"771\":2,\"806\":1,\"807\":1,\"809\":2,\"929\":1,\"1073\":1,\"1083\":2,\"1106\":4,\"1108\":4,\"1141\":3,\"1148\":1,\"1169\":1,\"1170\":3,\"1181\":1,\"1270\":1,\"1515\":2,\"1528\":2,\"1529\":2,\"1578\":3,\"1579\":3,\"1580\":3,\"1659\":6,\"1660\":6,\"1661\":8,\"1662\":6,\"1665\":2,\"2054\":1,\"2290\":1}}],[\"embd\",{\"1\":{\"429\":2,\"2038\":1,\"2046\":3}}],[\"embed=none\",{\"1\":{\"807\":1}}],[\"embedded\",{\"1\":{\"749\":2,\"875\":1,\"1149\":3,\"1150\":3,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1200\":1,\"1269\":1,\"1272\":1,\"2029\":1}}],[\"embedding=false\",{\"1\":{\"987\":1,\"1214\":1,\"1518\":1,\"1520\":1}}],[\"embeddings\",{\"1\":{\"161\":1,\"736\":1,\"774\":1,\"875\":3,\"1079\":4,\"1115\":2,\"1269\":2,\"1529\":1,\"1568\":1,\"1573\":1,\"1659\":2,\"1778\":1,\"1804\":2,\"1805\":1,\"1851\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2044\":1,\"2046\":1,\"2049\":1,\"2052\":1,\"2055\":1,\"2064\":1,\"2068\":2,\"2070\":2,\"2086\":2,\"2087\":2,\"2090\":1,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2261\":2,\"2262\":3,\"2263\":2,\"2264\":2,\"2279\":1,\"2416\":1,\"2420\":1}}],[\"embedding\",{\"0\":{\"770\":1,\"772\":1,\"800\":1,\"810\":1,\"813\":1,\"818\":1,\"1971\":2,\"2416\":1},\"1\":{\"28\":1,\"29\":3,\"116\":2,\"255\":2,\"265\":2,\"269\":2,\"429\":1,\"437\":1,\"712\":3,\"726\":1,\"735\":1,\"743\":2,\"747\":1,\"749\":1,\"754\":5,\"770\":2,\"771\":1,\"772\":3,\"774\":1,\"800\":2,\"806\":2,\"809\":1,\"810\":2,\"813\":2,\"818\":2,\"821\":7,\"826\":6,\"858\":1,\"861\":1,\"987\":1,\"1049\":4,\"1050\":4,\"1052\":7,\"1056\":4,\"1066\":2,\"1068\":2,\"1073\":3,\"1075\":2,\"1076\":4,\"1077\":1,\"1079\":1,\"1083\":2,\"1106\":2,\"1108\":2,\"1115\":2,\"1133\":1,\"1149\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1204\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1462\":1,\"1484\":1,\"1517\":2,\"1529\":4,\"1530\":2,\"1547\":1,\"1551\":2,\"1554\":2,\"1568\":1,\"1573\":1,\"1601\":1,\"1605\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1665\":1,\"1773\":3,\"1778\":1,\"1798\":1,\"1804\":3,\"1805\":1,\"1829\":1,\"1837\":2,\"1850\":1,\"1851\":10,\"1877\":2,\"1878\":5,\"1971\":8,\"2001\":5,\"2002\":6,\"2004\":5,\"2029\":1,\"2030\":1,\"2046\":4,\"2049\":1,\"2055\":1,\"2064\":1,\"2070\":1,\"2078\":1,\"2082\":3,\"2086\":6,\"2087\":6,\"2090\":5,\"2095\":6,\"2200\":1,\"2240\":3,\"2243\":5,\"2244\":9,\"2255\":9,\"2257\":1,\"2261\":1,\"2262\":2,\"2263\":6,\"2264\":7,\"2278\":3,\"2279\":9,\"2415\":5,\"2416\":2,\"2419\":2,\"2441\":1,\"2543\":1}}],[\"embedidgrad\",{\"0\":{\"746\":1},\"1\":{\"746\":1}}],[\"embedidfunction\",{\"0\":{\"745\":1},\"1\":{\"745\":1}}],[\"embedid\",{\"0\":{\"743\":1},\"1\":{\"743\":1,\"744\":1,\"875\":1}}],[\"embed\",{\"0\":{\"429\":1,\"743\":1,\"745\":1,\"746\":1,\"875\":2},\"1\":{\"28\":1,\"29\":3,\"66\":4,\"116\":5,\"429\":2,\"712\":4,\"725\":1,\"735\":1,\"743\":3,\"745\":1,\"746\":1,\"754\":5,\"806\":4,\"821\":5,\"826\":6,\"875\":2,\"876\":2,\"1066\":4,\"1073\":6,\"1075\":4,\"1076\":2,\"1083\":8,\"1115\":10,\"1133\":2,\"1149\":3,\"1150\":3,\"1167\":1,\"1168\":1,\"1172\":2,\"1179\":2,\"1196\":1,\"1197\":1,\"1200\":1,\"1204\":2,\"1214\":1,\"1244\":2,\"1269\":3,\"1270\":5,\"1271\":1,\"1272\":1,\"1273\":2,\"1517\":1,\"1518\":2,\"1520\":2,\"1547\":1,\"1778\":4,\"1804\":4,\"1805\":3,\"1850\":7,\"1851\":14,\"1852\":6,\"1877\":3,\"1878\":4,\"1960\":1,\"1970\":2,\"1971\":2,\"2001\":6,\"2002\":8,\"2003\":2,\"2004\":6,\"2029\":1,\"2081\":2,\"2083\":3,\"2086\":10,\"2087\":10,\"2090\":6,\"2095\":8,\"2158\":1,\"2198\":1,\"2243\":6,\"2244\":14,\"2255\":14,\"2262\":3,\"2263\":8,\"2264\":8,\"2279\":14,\"2584\":1}}],[\"ema\",{\"0\":{\"1069\":1},\"1\":{\"116\":4,\"1065\":5,\"1066\":5,\"1069\":10,\"1327\":1}}],[\"emiru\",{\"1\":{\"2357\":2,\"2578\":2,\"2589\":1}}],[\"emits\",{\"1\":{\"1154\":1}}],[\"emitted\",{\"1\":{\"113\":1}}],[\"emission\",{\"1\":{\"23\":1,\"1142\":1,\"1186\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1287\":1,\"1301\":1,\"1304\":1,\"1337\":1,\"1349\":1,\"1350\":1}}],[\"emphasis\",{\"1\":{\"1935\":1,\"1943\":1}}],[\"emphasized\",{\"1\":{\"1935\":1,\"1943\":1,\"2044\":1,\"2049\":1}}],[\"emphasize\",{\"1\":{\"1935\":1,\"1943\":1}}],[\"employs\",{\"1\":{\"2090\":1}}],[\"employ\",{\"1\":{\"104\":1,\"106\":1,\"2467\":1}}],[\"empty\",{\"1\":{\"1\":1,\"6\":1,\"595\":1,\"1612\":1,\"1615\":1,\"2212\":1,\"2584\":1}}],[\"em\",{\"1\":{\"45\":3}}],[\"ellos\",{\"1\":{\"2457\":1}}],[\"el\",{\"1\":{\"2049\":1,\"2457\":1}}],[\"elimination\",{\"1\":{\"1639\":1}}],[\"elif\",{\"1\":{\"217\":1,\"224\":1,\"2500\":4,\"2617\":4,\"2635\":4}}],[\"elu\",{\"1\":{\"1506\":1,\"1543\":2,\"1564\":1,\"1655\":2,\"1656\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":2,\"2155\":4}}],[\"elemwise=false\",{\"1\":{\"1188\":1}}],[\"elementwise\",{\"1\":{\"1838\":1,\"1911\":1}}],[\"elementwiseaffineflow\",{\"0\":{\"1838\":1},\"1\":{\"1838\":2}}],[\"element\",{\"1\":{\"57\":1,\"710\":2,\"745\":2,\"746\":2,\"760\":2,\"875\":1,\"981\":2,\"1327\":1,\"1739\":2,\"1905\":1}}],[\"elements\",{\"0\":{\"2009\":1},\"1\":{\"21\":2,\"78\":1,\"115\":1,\"710\":1,\"734\":1,\"745\":1,\"746\":1,\"760\":2,\"767\":1,\"817\":1,\"828\":1,\"922\":1,\"1079\":2,\"1905\":1,\"2009\":1}}],[\"elaborated\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"elan\",{\"1\":{\"2387\":3}}],[\"eladhoffer\",{\"1\":{\"837\":1}}],[\"elayers=1\",{\"1\":{\"2083\":1}}],[\"elayers\",{\"1\":{\"638\":6,\"754\":1,\"805\":2,\"808\":2,\"821\":1,\"826\":1,\"1778\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"2083\":1,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2279\":2}}],[\"elapsed=$\",{\"1\":{\"2568\":1}}],[\"elapsed\",{\"1\":{\"17\":1}}],[\"elsewhere\",{\"1\":{\"2565\":1}}],[\"else\",{\"1\":{\"59\":1,\"217\":2,\"224\":1,\"231\":1,\"943\":1,\"950\":1,\"955\":1,\"965\":1,\"968\":1,\"972\":1,\"1228\":1,\"1243\":1,\"1345\":1,\"1347\":1,\"1371\":1,\"1516\":1,\"1741\":1,\"1880\":2,\"2022\":2,\"2023\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2170\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2472\":1,\"2476\":1,\"2500\":5,\"2592\":4,\"2596\":2,\"2617\":5,\"2635\":5,\"2648\":1}}],[\"et05\",{\"1\":{\"2492\":5,\"2628\":5}}],[\"eta\",{\"1\":{\"632\":1,\"2151\":2}}],[\"eta=1\",{\"1\":{\"632\":1}}],[\"etal\",{\"1\":{\"130\":1}}],[\"eth\",{\"1\":{\"45\":2}}],[\"eth0\",{\"1\":{\"45\":1}}],[\"ethernets\",{\"1\":{\"45\":1}}],[\"ethernet\",{\"0\":{\"44\":1},\"1\":{\"45\":3}}],[\"et\",{\"1\":{\"23\":4,\"24\":1,\"113\":2,\"119\":4,\"120\":1,\"692\":1,\"693\":1,\"704\":1,\"705\":2,\"729\":1,\"880\":1,\"885\":1,\"1150\":1,\"1198\":1,\"1257\":1,\"1393\":1,\"1466\":1,\"1515\":1,\"1523\":1,\"1524\":1,\"1528\":1,\"1529\":2,\"1547\":1,\"1566\":1,\"1568\":2,\"1572\":1,\"1581\":1,\"1645\":1,\"1706\":1,\"1707\":1,\"1712\":1,\"1715\":2,\"1917\":1,\"2030\":1,\"2044\":1,\"2054\":1,\"2055\":1,\"2064\":1,\"2068\":1,\"2070\":1,\"2438\":1,\"2564\":1}}],[\"etype\",{\"1\":{\"21\":5}}],[\"etc\",{\"0\":{\"126\":1,\"127\":1},\"1\":{\"5\":1,\"48\":3,\"56\":1,\"57\":1,\"85\":2,\"96\":1,\"98\":1,\"126\":1,\"134\":1,\"135\":1,\"136\":1,\"144\":1,\"149\":1,\"167\":1,\"168\":1,\"178\":1,\"179\":1,\"196\":1,\"197\":1,\"198\":1,\"200\":1,\"234\":1,\"989\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"1626\":1,\"2372\":1,\"2375\":1,\"2385\":3,\"2394\":1,\"2429\":1,\"2481\":1,\"2530\":1,\"2554\":1,\"2558\":1,\"2559\":1,\"2568\":2,\"2618\":1}}],[\"e2e\",{\"0\":{\"206\":1,\"742\":2,\"750\":1,\"754\":1,\"755\":1,\"762\":1,\"763\":1,\"794\":1,\"811\":1,\"821\":1,\"822\":1,\"826\":1,\"880\":1,\"890\":1,\"896\":1,\"1528\":1,\"2469\":1,\"2472\":1,\"2473\":1,\"2475\":1,\"2477\":1},\"1\":{\"20\":2,\"24\":1,\"173\":4,\"194\":3,\"197\":1,\"198\":2,\"203\":1,\"206\":1,\"217\":1,\"224\":1,\"231\":1,\"235\":1,\"285\":2,\"286\":1,\"296\":1,\"629\":1,\"650\":1,\"735\":1,\"742\":9,\"750\":2,\"754\":1,\"755\":1,\"762\":1,\"763\":1,\"794\":1,\"811\":1,\"816\":5,\"821\":1,\"822\":1,\"826\":1,\"836\":5,\"880\":2,\"890\":2,\"896\":1,\"977\":1,\"978\":1,\"1017\":1,\"1044\":1,\"1045\":1,\"1528\":1,\"2361\":1,\"2377\":2,\"2436\":2,\"2467\":6,\"2468\":1,\"2471\":1,\"2472\":1,\"2473\":2,\"2476\":1,\"2478\":1,\"2479\":1,\"2562\":2,\"2576\":1,\"2650\":1}}],[\"energy\",{\"0\":{\"2241\":2},\"1\":{\"1773\":17,\"1837\":14,\"1850\":10,\"1851\":32,\"1852\":7,\"1879\":3,\"2049\":1,\"2082\":17,\"2240\":17,\"2241\":6,\"2243\":1,\"2244\":34,\"2245\":3,\"2255\":33,\"2256\":3,\"2278\":17,\"2279\":33,\"2280\":3}}],[\"enrollment\",{\"1\":{\"1484\":1,\"1551\":2,\"1554\":2,\"1601\":1,\"1659\":1}}],[\"enroll\",{\"1\":{\"1484\":2,\"1551\":2,\"1554\":4,\"1601\":2,\"1659\":5,\"1665\":6,\"2200\":1}}],[\"enrolldim\",{\"1\":{\"1484\":1,\"1601\":1,\"1725\":1}}],[\"enrique\",{\"1\":{\"130\":1}}],[\"enfrente\",{\"1\":{\"2457\":1}}],[\"enforcing\",{\"1\":{\"1551\":1,\"1553\":1}}],[\"enforced\",{\"1\":{\"1187\":1,\"1202\":1}}],[\"enffiles\",{\"1\":{\"280\":1}}],[\"enumeration\",{\"1\":{\"1225\":1}}],[\"enumerate\",{\"1\":{\"80\":1,\"2500\":2,\"2592\":1,\"2617\":2,\"2635\":2}}],[\"enum\",{\"1\":{\"1186\":1,\"1194\":1,\"1210\":1,\"1225\":1,\"1226\":1}}],[\"ensure\",{\"1\":{\"1369\":1,\"1473\":1}}],[\"ensures\",{\"1\":{\"1032\":1,\"1033\":1}}],[\"ensemble\",{\"1\":{\"812\":1}}],[\"enabling\",{\"1\":{\"988\":1}}],[\"enables\",{\"1\":{\"745\":1,\"746\":1,\"997\":1,\"2584\":1,\"2585\":1}}],[\"enabled\",{\"1\":{\"22\":1,\"29\":1,\"118\":1,\"429\":2,\"1269\":3}}],[\"enable\",{\"0\":{\"35\":1},\"1\":{\"19\":1,\"56\":1,\"70\":1,\"79\":1,\"91\":1,\"105\":1,\"113\":2,\"120\":1,\"812\":1,\"988\":1,\"1343\":1,\"1895\":1,\"1900\":1,\"2559\":1,\"2585\":1}}],[\"entities\",{\"1\":{\"2467\":1}}],[\"entity\",{\"1\":{\"429\":2,\"2476\":9}}],[\"entire\",{\"1\":{\"692\":1,\"693\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1,\"1352\":1,\"1436\":1,\"1511\":1,\"1644\":1,\"1719\":1,\"2372\":2,\"2429\":2,\"2430\":1,\"2473\":1,\"2554\":2,\"2555\":1,\"2565\":1}}],[\"entries\",{\"1\":{\"996\":2,\"1138\":1}}],[\"entries=none\",{\"1\":{\"996\":1}}],[\"entry\",{\"0\":{\"2569\":1},\"1\":{\"85\":1,\"175\":2,\"194\":2,\"1138\":1,\"1551\":1,\"1553\":1,\"2372\":1,\"2385\":1,\"2394\":1,\"2429\":1,\"2431\":1,\"2530\":1,\"2554\":1,\"2569\":1,\"2640\":1}}],[\"entropy>\",{\"1\":{\"605\":1}}],[\"entropy\",{\"1\":{\"22\":1,\"118\":1,\"173\":1,\"822\":1,\"1567\":1,\"2000\":1}}],[\"enhancing\",{\"1\":{\"2473\":1}}],[\"enhanced\",{\"1\":{\"280\":1,\"528\":1,\"636\":1,\"729\":1,\"1524\":2,\"1525\":1,\"1611\":1,\"1646\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"1736\":1,\"1758\":1,\"1759\":1,\"2369\":2,\"2481\":1,\"2487\":3,\"2491\":2,\"2492\":2,\"2501\":2,\"2606\":2,\"2607\":1,\"2610\":1,\"2623\":2,\"2624\":1,\"2627\":1,\"2628\":2}}],[\"enhance\",{\"0\":{\"247\":1,\"636\":1,\"2369\":1,\"2487\":1,\"2491\":1,\"2606\":1,\"2607\":1,\"2610\":1,\"2623\":1,\"2624\":1,\"2627\":1},\"1\":{\"247\":3,\"636\":2,\"1433\":1,\"1551\":2,\"1553\":2,\"1554\":1,\"1646\":2,\"2481\":1,\"2492\":1,\"2501\":1,\"2618\":1,\"2628\":1}}],[\"enhancementtask\",{\"0\":{\"2102\":1},\"1\":{\"2102\":2}}],[\"enhancement\",{\"0\":{\"156\":1,\"2366\":1,\"2367\":1,\"2480\":1,\"2483\":1,\"2484\":1,\"2488\":1,\"2492\":1,\"2601\":1,\"2603\":1,\"2604\":1,\"2608\":1,\"2618\":1,\"2620\":1,\"2621\":1,\"2625\":1,\"2628\":1,\"2641\":1},\"1\":{\"130\":2,\"156\":1,\"161\":1,\"1437\":1,\"1443\":1,\"1454\":1,\"1462\":2,\"1463\":2,\"1510\":1,\"1511\":1,\"1523\":1,\"1551\":2,\"1552\":1,\"1553\":3,\"1570\":1,\"1604\":1,\"1643\":1,\"1644\":1,\"1655\":1,\"1667\":1,\"1670\":2,\"1671\":2,\"1719\":2,\"1928\":6,\"2090\":1,\"2184\":1,\"2480\":1,\"2481\":4,\"2485\":1,\"2490\":2,\"2601\":1,\"2609\":1,\"2618\":6,\"2626\":1,\"2635\":1}}],[\"enhpreprocessor\",{\"0\":{\"2184\":1},\"1\":{\"2184\":2,\"2200\":1}}],[\"enhs2ttask\",{\"0\":{\"2101\":1},\"1\":{\"2101\":2}}],[\"enh+asr\",{\"1\":{\"1552\":1}}],[\"enhfiles\",{\"1\":{\"528\":2}}],[\"enh2\",{\"1\":{\"280\":1,\"528\":1}}],[\"enh1\",{\"1\":{\"280\":1,\"1551\":1,\"1553\":1,\"2366\":1,\"2385\":1,\"2601\":1,\"2618\":1,\"2637\":3,\"2638\":1,\"2640\":1}}],[\"enh\",{\"0\":{\"349\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"1430\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":2,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1452\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1505\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1522\":1,\"1523\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1534\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1551\":2,\"1552\":2,\"1553\":1,\"1554\":1,\"1555\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1585\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1626\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1637\":1,\"1638\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"2101\":1,\"2102\":1,\"2117\":1,\"2689\":1},\"1\":{\"247\":3,\"307\":2,\"343\":2,\"350\":4,\"357\":4,\"363\":2,\"368\":2,\"397\":2,\"398\":8,\"443\":2,\"528\":5,\"1113\":1,\"1218\":1,\"1371\":1,\"1377\":1,\"1430\":2,\"1431\":2,\"1433\":1,\"1435\":2,\"1437\":1,\"1439\":4,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1452\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":2,\"1462\":2,\"1463\":1,\"1464\":2,\"1465\":2,\"1466\":1,\"1468\":1,\"1470\":2,\"1471\":2,\"1472\":1,\"1473\":2,\"1474\":1,\"1476\":1,\"1478\":2,\"1480\":1,\"1482\":2,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":2,\"1503\":1,\"1505\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":2,\"1518\":1,\"1520\":1,\"1522\":2,\"1523\":1,\"1524\":1,\"1525\":2,\"1526\":2,\"1527\":2,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":2,\"1532\":2,\"1534\":1,\"1535\":2,\"1537\":2,\"1539\":1,\"1540\":1,\"1542\":1,\"1543\":2,\"1545\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1551\":5,\"1552\":6,\"1553\":4,\"1554\":1,\"1555\":1,\"1557\":1,\"1558\":1,\"1559\":2,\"1560\":2,\"1561\":2,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":2,\"1573\":1,\"1575\":2,\"1576\":2,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1585\":1,\"1586\":2,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":2,\"1595\":1,\"1596\":1,\"1598\":2,\"1600\":1,\"1601\":1,\"1602\":2,\"1603\":1,\"1604\":1,\"1605\":2,\"1607\":2,\"1609\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":2,\"1619\":2,\"1620\":2,\"1622\":1,\"1623\":1,\"1624\":1,\"1626\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1637\":1,\"1638\":2,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":2,\"1650\":2,\"1652\":2,\"1654\":1,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":2,\"1664\":2,\"1665\":2,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":2,\"1671\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1678\":1,\"1679\":2,\"1680\":1,\"1681\":2,\"1682\":2,\"1683\":2,\"1684\":2,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":2,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":2,\"1701\":2,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":2,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":2,\"1719\":1,\"1720\":1,\"1721\":2,\"1722\":2,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":2,\"1728\":2,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":2,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":2,\"1739\":1,\"1740\":2,\"1741\":1,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1747\":1,\"1748\":2,\"1749\":2,\"1750\":1,\"1751\":2,\"1752\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":1,\"1758\":2,\"1759\":2,\"2101\":2,\"2102\":2,\"2117\":1,\"2184\":1,\"2368\":9,\"2369\":1,\"2371\":9,\"2486\":9,\"2487\":1,\"2490\":9,\"2491\":1,\"2492\":5,\"2494\":12,\"2501\":1,\"2605\":11,\"2606\":1,\"2607\":1,\"2609\":11,\"2610\":1,\"2612\":11,\"2622\":9,\"2623\":1,\"2624\":1,\"2626\":9,\"2627\":1,\"2628\":5,\"2630\":13,\"2638\":1,\"2642\":4,\"2643\":1,\"2644\":1,\"2645\":3}}],[\"en5\",{\"1\":{\"231\":1}}],[\"engaging\",{\"1\":{\"2467\":1}}],[\"english\",{\"0\":{\"201\":1,\"202\":1,\"208\":1,\"2590\":1},\"1\":{\"201\":1,\"202\":1,\"217\":2,\"218\":1,\"461\":1,\"2357\":2,\"2363\":3,\"2373\":1,\"2433\":1,\"2506\":3,\"2510\":2,\"2512\":4,\"2518\":1,\"2555\":1,\"2578\":2,\"2585\":2,\"2593\":1,\"2653\":3,\"2657\":4}}],[\"engine\",{\"1\":{\"142\":1,\"2354\":1,\"2388\":1,\"2421\":1,\"2524\":1,\"2544\":1}}],[\"endangered\",{\"1\":{\"2387\":1}}],[\"end=none\",{\"1\":{\"2315\":1}}],[\"end=false\",{\"1\":{\"1491\":1,\"1627\":1}}],[\"end=\",{\"1\":{\"880\":1}}],[\"ended\",{\"1\":{\"691\":4,\"692\":1,\"693\":1,\"697\":4,\"797\":4,\"880\":2}}],[\"endine\",{\"1\":{\"142\":1}}],[\"endian\",{\"1\":{\"48\":2}}],[\"end\",{\"0\":{\"880\":1,\"2375\":2,\"2558\":2,\"2574\":1},\"1\":{\"51\":1,\"65\":1,\"108\":3,\"109\":2,\"110\":1,\"130\":8,\"168\":2,\"179\":2,\"226\":1,\"501\":1,\"691\":1,\"692\":1,\"693\":2,\"697\":2,\"701\":2,\"704\":2,\"705\":3,\"706\":1,\"729\":2,\"758\":2,\"797\":2,\"857\":2,\"880\":6,\"1132\":2,\"1136\":1,\"1145\":1,\"1198\":4,\"1269\":1,\"1274\":2,\"1398\":3,\"1400\":3,\"1423\":3,\"1426\":1,\"1524\":2,\"1528\":3,\"1581\":2,\"1618\":1,\"1619\":1,\"1638\":1,\"1778\":2,\"1798\":2,\"1805\":4,\"1850\":4,\"1852\":2,\"1863\":2,\"1864\":2,\"1868\":2,\"1874\":2,\"1877\":4,\"1878\":2,\"2078\":2,\"2081\":2,\"2083\":4,\"2095\":6,\"2199\":1,\"2243\":2,\"2244\":2,\"2257\":2,\"2261\":2,\"2262\":2,\"2263\":2,\"2265\":2,\"2266\":1,\"2354\":6,\"2363\":2,\"2373\":1,\"2385\":2,\"2387\":1,\"2388\":2,\"2421\":2,\"2467\":2,\"2506\":2,\"2516\":2,\"2524\":2,\"2544\":2,\"2555\":1,\"2558\":1,\"2600\":4,\"2646\":2,\"2653\":2}}],[\"en\",{\"0\":{\"2121\":1},\"1\":{\"45\":1,\"98\":3,\"110\":2,\"201\":1,\"207\":1,\"210\":4,\"211\":4,\"212\":4,\"214\":3,\"215\":3,\"216\":3,\"217\":1,\"274\":1,\"294\":1,\"295\":15,\"296\":4,\"461\":3,\"1031\":1,\"1921\":1,\"1922\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"2121\":5,\"2128\":1,\"2129\":1,\"2357\":4,\"2454\":1,\"2455\":2,\"2457\":1,\"2460\":3,\"2461\":2,\"2472\":3,\"2474\":2,\"2476\":3,\"2478\":2,\"2507\":2,\"2510\":4,\"2513\":2,\"2519\":1,\"2520\":4,\"2569\":1,\"2572\":3,\"2578\":4,\"2590\":1,\"2600\":2,\"2648\":3,\"2649\":2}}],[\"enough\",{\"1\":{\"45\":1,\"84\":1,\"102\":1,\"148\":1}}],[\"enc=\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"enc=none\",{\"1\":{\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1}}],[\"encapsulation\",{\"1\":{\"1087\":1}}],[\"encapsulating\",{\"1\":{\"115\":1}}],[\"encloses\",{\"1\":{\"608\":1}}],[\"encs=2\",{\"1\":{\"638\":1}}],[\"encs\",{\"1\":{\"245\":2,\"249\":2,\"251\":2,\"1220\":2,\"1289\":1}}],[\"enc\",{\"0\":{\"886\":1},\"1\":{\"21\":3,\"22\":1,\"28\":5,\"29\":1,\"30\":1,\"67\":1,\"104\":1,\"115\":3,\"251\":4,\"255\":4,\"259\":4,\"265\":4,\"269\":4,\"677\":7,\"678\":7,\"679\":7,\"681\":6,\"682\":4,\"684\":7,\"685\":7,\"686\":6,\"687\":6,\"688\":6,\"689\":6,\"698\":2,\"699\":2,\"700\":15,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"726\":12,\"747\":1,\"749\":2,\"754\":5,\"758\":7,\"766\":3,\"825\":21,\"826\":5,\"858\":3,\"861\":2,\"886\":4,\"911\":3,\"933\":2,\"1048\":8,\"1049\":6,\"1050\":6,\"1052\":10,\"1056\":6,\"1057\":1,\"1058\":2,\"1064\":4,\"1068\":4,\"1076\":6,\"1077\":1,\"1093\":4,\"1106\":3,\"1108\":3,\"1133\":2,\"1138\":18,\"1139\":15,\"1140\":1,\"1148\":2,\"1149\":4,\"1150\":4,\"1167\":1,\"1168\":1,\"1169\":1,\"1190\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1203\":2,\"1204\":1,\"1205\":1,\"1219\":1,\"1271\":1,\"1272\":2,\"1273\":1,\"1360\":1,\"1376\":2,\"1505\":4,\"1522\":14,\"1523\":14,\"1539\":1,\"1555\":2,\"1558\":2,\"1560\":1,\"1645\":2,\"1669\":2,\"1671\":2,\"1771\":2,\"1778\":7,\"1850\":7,\"1851\":14,\"1852\":7,\"1890\":1,\"1960\":1,\"1971\":2,\"2001\":6,\"2002\":7,\"2003\":3,\"2004\":6,\"2026\":1,\"2029\":2,\"2054\":2,\"2076\":1,\"2090\":12,\"2095\":1,\"2243\":14,\"2244\":14,\"2255\":14,\"2263\":1,\"2264\":12,\"2279\":14,\"2440\":1,\"2564\":1}}],[\"encontrarlo\",{\"1\":{\"2457\":1}}],[\"encouraged\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1,\"2542\":1}}],[\"encourages\",{\"1\":{\"99\":1}}],[\"encountered\",{\"1\":{\"106\":1}}],[\"encounter\",{\"1\":{\"20\":1,\"112\":1,\"2368\":1,\"2389\":1,\"2393\":1,\"2408\":1,\"2422\":1,\"2423\":1,\"2427\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2486\":1,\"2490\":1,\"2503\":1,\"2525\":1,\"2529\":1,\"2545\":1,\"2546\":1,\"2550\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"encoded\",{\"1\":{\"272\":1,\"327\":2,\"449\":2,\"676\":1,\"691\":5,\"692\":5,\"693\":3,\"696\":1,\"697\":6,\"706\":3,\"711\":2,\"731\":2,\"770\":1,\"772\":1,\"797\":3,\"810\":1,\"813\":1,\"815\":1,\"817\":1,\"818\":1,\"829\":2,\"857\":1,\"1116\":2,\"1133\":4,\"1141\":1,\"1170\":1,\"1190\":1,\"1204\":1,\"1214\":2,\"1221\":1,\"1244\":1,\"1273\":5,\"1377\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":2,\"1669\":1,\"1798\":3,\"1863\":1,\"1874\":1,\"2001\":1,\"2260\":1,\"2592\":1}}],[\"encode\",{\"0\":{\"272\":1,\"877\":1},\"1\":{\"272\":2,\"676\":2,\"725\":1,\"726\":1,\"749\":2,\"806\":2,\"877\":1,\"1046\":1,\"1049\":2,\"1050\":2,\"1052\":2,\"1053\":1,\"1056\":2,\"1057\":1,\"1058\":2,\"1066\":2,\"1073\":2,\"1075\":2,\"1083\":1,\"1113\":1,\"1171\":1,\"1172\":1,\"1216\":1,\"1270\":2,\"1285\":1,\"1371\":1,\"1552\":2,\"1892\":1,\"1893\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2046\":1,\"2076\":1,\"2132\":1,\"2137\":1,\"2191\":1,\"2294\":1}}],[\"encoders\",{\"0\":{\"805\":1,\"808\":1,\"833\":1,\"878\":1},\"1\":{\"805\":1,\"808\":1,\"833\":1,\"878\":1,\"1517\":2,\"1752\":3,\"1932\":1,\"2468\":1,\"2643\":1}}],[\"encodermix\",{\"0\":{\"749\":1},\"1\":{\"749\":1}}],[\"encoderlayer\",{\"0\":{\"748\":1},\"1\":{\"711\":1,\"748\":2}}],[\"encoder\",{\"0\":{\"115\":1,\"124\":1,\"711\":1,\"726\":1,\"747\":2,\"748\":1,\"749\":1,\"842\":1,\"846\":1,\"849\":1,\"851\":1,\"878\":1,\"879\":2,\"915\":1,\"933\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1058\":3,\"1068\":1,\"1076\":1,\"1077\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1115\":2,\"1116\":2,\"1119\":2,\"1134\":2,\"1140\":2,\"1141\":2,\"1148\":2,\"1149\":2,\"1150\":2,\"1169\":2,\"1170\":2,\"1178\":2,\"1179\":2,\"1180\":2,\"1181\":2,\"1187\":2,\"1191\":2,\"1200\":2,\"1203\":2,\"1215\":2,\"1222\":2,\"1229\":2,\"1231\":2,\"1249\":2,\"1263\":2,\"1269\":2,\"1272\":2,\"1282\":2,\"1305\":2,\"1308\":2,\"1309\":2,\"1310\":2,\"1312\":2,\"1313\":2,\"1326\":2,\"1330\":2,\"1354\":2,\"1435\":2,\"1511\":2,\"1555\":1,\"1617\":2,\"1644\":2,\"1726\":1,\"1798\":1,\"1863\":1,\"1874\":1,\"2042\":2,\"2049\":2,\"2050\":2,\"2054\":2,\"2055\":2,\"2059\":2,\"2061\":2,\"2063\":2,\"2064\":2,\"2070\":2,\"2074\":2,\"2075\":2,\"2081\":2,\"2083\":2,\"2093\":2,\"2253\":1,\"2257\":1,\"2261\":1,\"2262\":1,\"2400\":1,\"2536\":1},\"1\":{\"21\":4,\"22\":2,\"28\":4,\"29\":3,\"30\":4,\"31\":1,\"66\":2,\"67\":2,\"102\":2,\"104\":4,\"114\":1,\"115\":10,\"116\":1,\"117\":1,\"121\":2,\"124\":2,\"625\":2,\"635\":7,\"638\":1,\"676\":1,\"677\":4,\"678\":4,\"679\":4,\"680\":1,\"681\":4,\"682\":4,\"683\":1,\"684\":4,\"685\":4,\"686\":4,\"687\":4,\"688\":4,\"689\":4,\"691\":1,\"692\":1,\"693\":4,\"695\":1,\"696\":1,\"697\":1,\"700\":6,\"703\":2,\"706\":2,\"711\":2,\"712\":1,\"726\":8,\"732\":1,\"734\":2,\"736\":1,\"747\":8,\"748\":2,\"749\":4,\"754\":15,\"758\":4,\"766\":5,\"773\":2,\"781\":1,\"784\":1,\"796\":1,\"797\":1,\"805\":1,\"808\":1,\"812\":1,\"815\":1,\"817\":1,\"821\":6,\"825\":8,\"826\":16,\"828\":2,\"836\":1,\"842\":2,\"846\":2,\"848\":1,\"849\":2,\"851\":2,\"858\":2,\"859\":1,\"860\":1,\"862\":2,\"878\":3,\"879\":3,\"886\":1,\"892\":1,\"909\":1,\"910\":1,\"915\":3,\"929\":1,\"933\":5,\"934\":1,\"1048\":4,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":9,\"1058\":16,\"1064\":5,\"1068\":5,\"1071\":3,\"1076\":1,\"1077\":1,\"1087\":4,\"1088\":2,\"1089\":2,\"1090\":1,\"1091\":2,\"1092\":2,\"1093\":4,\"1094\":1,\"1099\":3,\"1103\":6,\"1104\":1,\"1105\":5,\"1106\":2,\"1108\":2,\"1113\":3,\"1114\":1,\"1115\":14,\"1116\":4,\"1119\":4,\"1127\":1,\"1133\":3,\"1134\":2,\"1138\":7,\"1139\":6,\"1140\":3,\"1141\":3,\"1145\":3,\"1148\":8,\"1149\":3,\"1150\":4,\"1167\":1,\"1168\":1,\"1169\":3,\"1170\":3,\"1171\":12,\"1172\":5,\"1178\":4,\"1179\":9,\"1180\":4,\"1181\":5,\"1187\":2,\"1190\":5,\"1191\":2,\"1196\":1,\"1197\":1,\"1198\":3,\"1200\":3,\"1203\":8,\"1204\":1,\"1206\":11,\"1214\":2,\"1215\":3,\"1220\":1,\"1221\":1,\"1222\":4,\"1229\":2,\"1231\":2,\"1244\":4,\"1249\":2,\"1253\":1,\"1255\":1,\"1263\":2,\"1269\":36,\"1271\":1,\"1272\":3,\"1273\":2,\"1282\":2,\"1284\":1,\"1305\":2,\"1308\":2,\"1309\":2,\"1310\":2,\"1312\":2,\"1313\":2,\"1326\":2,\"1330\":2,\"1354\":2,\"1371\":3,\"1374\":1,\"1376\":2,\"1435\":4,\"1454\":2,\"1505\":3,\"1511\":4,\"1516\":1,\"1522\":7,\"1523\":7,\"1551\":4,\"1552\":11,\"1553\":4,\"1554\":3,\"1555\":1,\"1558\":1,\"1617\":3,\"1644\":3,\"1645\":2,\"1671\":1,\"1726\":2,\"1752\":1,\"1771\":1,\"1778\":3,\"1787\":1,\"1788\":1,\"1798\":5,\"1804\":68,\"1805\":21,\"1850\":3,\"1851\":19,\"1852\":3,\"1853\":3,\"1854\":4,\"1863\":3,\"1874\":5,\"1877\":21,\"1878\":69,\"1892\":3,\"1893\":3,\"1932\":1,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"1970\":5,\"1975\":4,\"1983\":3,\"1984\":3,\"1986\":5,\"2001\":2,\"2004\":2,\"2027\":4,\"2029\":1,\"2042\":2,\"2044\":1,\"2046\":5,\"2049\":4,\"2050\":3,\"2052\":1,\"2054\":9,\"2055\":4,\"2059\":2,\"2061\":2,\"2063\":2,\"2064\":4,\"2068\":1,\"2070\":3,\"2074\":2,\"2075\":2,\"2076\":7,\"2078\":1,\"2079\":1,\"2081\":6,\"2083\":10,\"2086\":4,\"2087\":4,\"2090\":16,\"2093\":3,\"2095\":5,\"2243\":17,\"2244\":19,\"2253\":1,\"2255\":19,\"2257\":10,\"2261\":11,\"2262\":1,\"2263\":5,\"2264\":18,\"2279\":19,\"2399\":1,\"2400\":2,\"2401\":2,\"2439\":1,\"2440\":2,\"2468\":1,\"2535\":1,\"2536\":2,\"2537\":2,\"2558\":4,\"2564\":3,\"2584\":3,\"2600\":2,\"2641\":5,\"2642\":2,\"2643\":1,\"2645\":1}}],[\"encoding=none\",{\"1\":{\"2315\":1}}],[\"encoding=\",{\"1\":{\"2099\":1}}],[\"encodings\",{\"1\":{\"770\":2,\"810\":1,\"818\":1}}],[\"encoding\",{\"0\":{\"1077\":1,\"1094\":1},\"1\":{\"21\":2,\"104\":1,\"115\":2,\"272\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"725\":3,\"726\":3,\"748\":1,\"749\":1,\"754\":5,\"770\":3,\"771\":3,\"772\":2,\"800\":3,\"809\":3,\"810\":2,\"813\":2,\"818\":2,\"826\":5,\"858\":3,\"861\":1,\"877\":1,\"886\":3,\"935\":1,\"1076\":2,\"1077\":4,\"1093\":2,\"1094\":4,\"1148\":4,\"1149\":2,\"1150\":2,\"1203\":4,\"1272\":1,\"1505\":2,\"1669\":2,\"1771\":1,\"1787\":4,\"1788\":4,\"1798\":4,\"1804\":4,\"1805\":1,\"1851\":9,\"1874\":4,\"1877\":1,\"1878\":4,\"1926\":1,\"1927\":2,\"1971\":1,\"2029\":1,\"2054\":4,\"2090\":7,\"2243\":9,\"2244\":9,\"2255\":9,\"2264\":7,\"2279\":9}}],[\"envs\",{\"1\":{\"2431\":1,\"2432\":1}}],[\"environnement\",{\"1\":{\"2584\":2}}],[\"environ\",{\"1\":{\"2386\":2}}],[\"environmet\",{\"1\":{\"196\":1,\"234\":1}}],[\"environments\",{\"0\":{\"2442\":1},\"1\":{\"133\":1,\"940\":1,\"1712\":1,\"1715\":1,\"2441\":1}}],[\"environment\",{\"0\":{\"2384\":1,\"2394\":1,\"2428\":1,\"2530\":1,\"2551\":1},\"1\":{\"1\":1,\"3\":1,\"5\":1,\"15\":1,\"37\":1,\"45\":3,\"85\":3,\"94\":1,\"126\":1,\"132\":1,\"135\":5,\"136\":1,\"141\":1,\"2372\":2,\"2384\":1,\"2385\":1,\"2426\":1,\"2429\":1,\"2549\":1,\"2554\":1}}],[\"envfile\",{\"1\":{\"377\":2}}],[\"envrionments\",{\"0\":{\"2517\":1}}],[\"envrionment\",{\"0\":{\"196\":1,\"234\":1}}],[\"env\",{\"1\":{\"1\":2,\"3\":1,\"18\":1,\"45\":1,\"90\":1,\"98\":1,\"135\":2,\"136\":1,\"2180\":2,\"2568\":1,\"2569\":1}}],[\"eaf\",{\"1\":{\"2387\":1}}],[\"earlier\",{\"1\":{\"684\":1}}],[\"early\",{\"0\":{\"1007\":1,\"1034\":1},\"1\":{\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"1007\":2,\"1034\":2,\"2186\":1,\"2193\":1,\"2202\":2,\"2204\":1}}],[\"easy\",{\"1\":{\"2030\":1,\"2040\":1,\"2400\":1,\"2468\":1,\"2536\":1,\"2585\":1}}],[\"easiest\",{\"1\":{\"2599\":1}}],[\"easier\",{\"1\":{\"1245\":1,\"2399\":1,\"2535\":1}}],[\"easily\",{\"1\":{\"17\":1,\"150\":1,\"185\":1,\"188\":1,\"2410\":1,\"2420\":1,\"2640\":1}}],[\"ease\",{\"1\":{\"21\":1}}],[\"eachlayer\",{\"1\":{\"1846\":1}}],[\"each\",{\"0\":{\"36\":1,\"40\":1,\"42\":1,\"69\":1},\"1\":{\"15\":1,\"21\":2,\"22\":2,\"23\":5,\"28\":2,\"32\":3,\"36\":1,\"37\":1,\"56\":3,\"57\":1,\"58\":1,\"59\":2,\"60\":2,\"69\":2,\"72\":2,\"74\":1,\"77\":1,\"78\":2,\"79\":1,\"80\":2,\"82\":1,\"83\":1,\"85\":1,\"99\":1,\"113\":1,\"114\":2,\"115\":6,\"118\":1,\"119\":3,\"144\":1,\"150\":4,\"169\":1,\"181\":1,\"235\":3,\"237\":7,\"238\":1,\"239\":1,\"240\":1,\"564\":1,\"594\":1,\"625\":1,\"627\":1,\"686\":1,\"688\":1,\"689\":3,\"691\":2,\"693\":1,\"697\":2,\"700\":2,\"701\":2,\"702\":1,\"725\":2,\"727\":1,\"728\":1,\"735\":2,\"736\":1,\"742\":1,\"747\":2,\"754\":4,\"755\":2,\"774\":2,\"787\":1,\"794\":1,\"797\":5,\"806\":2,\"816\":1,\"821\":4,\"822\":1,\"824\":2,\"826\":5,\"834\":1,\"836\":1,\"857\":1,\"875\":2,\"914\":1,\"1001\":2,\"1019\":1,\"1037\":2,\"1039\":1,\"1048\":2,\"1057\":1,\"1059\":1,\"1068\":3,\"1093\":1,\"1138\":4,\"1139\":2,\"1143\":1,\"1171\":2,\"1187\":3,\"1198\":2,\"1202\":3,\"1206\":2,\"1211\":2,\"1224\":2,\"1244\":3,\"1252\":4,\"1254\":1,\"1269\":4,\"1270\":2,\"1286\":3,\"1287\":5,\"1336\":2,\"1348\":2,\"1377\":1,\"1379\":1,\"1392\":2,\"1398\":1,\"1427\":1,\"1430\":1,\"1454\":1,\"1462\":1,\"1516\":2,\"1522\":2,\"1523\":2,\"1531\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1551\":3,\"1552\":2,\"1553\":3,\"1559\":1,\"1560\":1,\"1572\":4,\"1577\":1,\"1603\":2,\"1604\":1,\"1658\":1,\"1659\":1,\"1661\":1,\"1664\":1,\"1665\":1,\"1670\":4,\"1671\":4,\"1693\":1,\"1714\":1,\"1718\":1,\"1719\":2,\"1741\":2,\"1755\":1,\"1766\":3,\"1767\":1,\"1768\":1,\"1773\":3,\"1778\":2,\"1786\":2,\"1804\":4,\"1805\":2,\"1810\":1,\"1839\":1,\"1841\":2,\"1845\":2,\"1846\":1,\"1847\":2,\"1848\":1,\"1849\":1,\"1856\":1,\"1858\":2,\"1861\":1,\"1879\":1,\"1895\":1,\"1897\":1,\"1900\":1,\"1901\":1,\"1905\":3,\"1953\":2,\"1955\":2,\"1956\":1,\"1962\":1,\"2000\":1,\"2001\":2,\"2002\":2,\"2004\":2,\"2012\":1,\"2032\":1,\"2040\":2,\"2078\":1,\"2081\":1,\"2082\":3,\"2083\":2,\"2086\":2,\"2087\":2,\"2088\":1,\"2090\":2,\"2091\":2,\"2095\":2,\"2099\":3,\"2102\":3,\"2168\":1,\"2170\":1,\"2197\":2,\"2239\":1,\"2243\":2,\"2244\":2,\"2245\":2,\"2255\":2,\"2256\":2,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":2,\"2264\":3,\"2279\":1,\"2280\":2,\"2343\":1,\"2363\":1,\"2372\":3,\"2375\":1,\"2385\":4,\"2387\":2,\"2403\":1,\"2429\":2,\"2440\":1,\"2452\":1,\"2506\":1,\"2512\":1,\"2539\":1,\"2554\":2,\"2559\":1,\"2638\":1,\"2653\":1,\"2657\":1}}],[\"estén\",{\"1\":{\"2457\":1}}],[\"est\",{\"1\":{\"1516\":1,\"1540\":1,\"1600\":1,\"1639\":2,\"1640\":2,\"1664\":1,\"1665\":1,\"1719\":1,\"2500\":23,\"2617\":23,\"2635\":23}}],[\"estimate\",{\"1\":{\"1516\":1,\"1604\":2,\"1719\":5,\"1864\":1,\"1865\":1,\"2375\":1,\"2558\":1}}],[\"estimates\",{\"1\":{\"1377\":1}}],[\"estimated\",{\"1\":{\"17\":1,\"87\":1,\"1375\":1,\"1454\":1,\"1505\":1,\"1516\":2,\"1523\":1,\"1528\":1,\"1534\":1,\"1539\":1,\"1558\":1,\"1626\":1,\"1639\":1,\"1640\":1,\"1658\":1,\"1669\":1}}],[\"estimation\",{\"1\":{\"1377\":1,\"1505\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1626\":1,\"1654\":1,\"1658\":1,\"1659\":2,\"1669\":1,\"1705\":1,\"2236\":1}}],[\"estimator\",{\"0\":{\"782\":1,\"1595\":1},\"1\":{\"782\":1,\"1595\":2}}],[\"estoi\",{\"1\":{\"528\":2}}],[\"essentially\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"essential\",{\"0\":{\"182\":1},\"1\":{\"83\":1,\"2384\":2,\"2467\":1}}],[\"esp400\",{\"1\":{\"2387\":1}}],[\"espeak\",{\"1\":{\"461\":14}}],[\"especially\",{\"1\":{\"48\":1,\"49\":1,\"2574\":1}}],[\"esp\",{\"1\":{\"130\":1}}],[\"espnet==202308\",{\"1\":{\"2651\":1}}],[\"espnet==0\",{\"1\":{\"2576\":1,\"2602\":1,\"2619\":1}}],[\"espnet=202207\",{\"1\":{\"2553\":1}}],[\"espnet=0\",{\"1\":{\"2372\":1}}],[\"espnetuasrmodel\",{\"0\":{\"2294\":1},\"1\":{\"2118\":1,\"2294\":2}}],[\"espnetttsmodel\",{\"0\":{\"2240\":1},\"1\":{\"2116\":1,\"2240\":3,\"2278\":1}}],[\"espnettts2model\",{\"0\":{\"2278\":1},\"1\":{\"2115\":1,\"2278\":2}}],[\"espnetsvsmodel\",{\"0\":{\"2082\":1},\"1\":{\"2082\":3,\"2113\":1}}],[\"espnetspeakermodel\",{\"0\":{\"2046\":1},\"1\":{\"2046\":1}}],[\"espnetslumodel\",{\"0\":{\"2027\":1},\"1\":{\"2027\":2}}],[\"espnets2stmodel\",{\"0\":{\"1984\":1},\"1\":{\"1984\":2,\"2109\":1}}],[\"espnets2tmodel\",{\"0\":{\"1975\":1},\"1\":{\"1975\":2}}],[\"espnetstmodel\",{\"0\":{\"2076\":1},\"1\":{\"1552\":1,\"2076\":2}}],[\"espnetlanguagemodel\",{\"0\":{\"1953\":1},\"1\":{\"1953\":1}}],[\"espnetganttsmodel\",{\"0\":{\"1837\":1},\"1\":{\"1837\":2,\"2104\":1}}],[\"espnetgansvsmodel\",{\"0\":{\"1773\":1},\"1\":{\"1773\":2,\"2103\":1}}],[\"espnetezdataset\",{\"0\":{\"2346\":1},\"1\":{\"2346\":2}}],[\"espnetez\",{\"0\":{\"2306\":1,\"2307\":1,\"2308\":1,\"2343\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2347\":1,\"2348\":1,\"2349\":1,\"2350\":1,\"2351\":1,\"2352\":1,\"2353\":1,\"2715\":1},\"1\":{\"2306\":2,\"2307\":2,\"2308\":2,\"2343\":2,\"2344\":2,\"2345\":2,\"2346\":2,\"2347\":1,\"2348\":2,\"2349\":1,\"2350\":2,\"2351\":2,\"2352\":2,\"2353\":2}}],[\"espnetextractionmodel\",{\"0\":{\"1554\":1},\"1\":{\"1554\":1}}],[\"espnetenhs2tmodel\",{\"0\":{\"1552\":1},\"1\":{\"1552\":1}}],[\"espnetenhancementmodel\",{\"0\":{\"1553\":1},\"1\":{\"1551\":1,\"1552\":1,\"1553\":1}}],[\"espnetmultitasklanguagemodel\",{\"0\":{\"1955\":1},\"1\":{\"1955\":1}}],[\"espnetmtmodel\",{\"0\":{\"1970\":1},\"1\":{\"1172\":1,\"1970\":2}}],[\"espnetmodel\",{\"1\":{\"56\":1,\"2168\":1,\"2170\":1}}],[\"espnetdiffusionmodel\",{\"0\":{\"1551\":1},\"1\":{\"1551\":1}}],[\"espnetdiarizationmodel\",{\"0\":{\"1371\":1},\"1\":{\"1371\":1,\"1552\":1}}],[\"espnetdiscreteasrmodel\",{\"0\":{\"1172\":1},\"1\":{\"1172\":1}}],[\"espnetdataset\",{\"0\":{\"2182\":1},\"1\":{\"60\":1,\"2182\":2,\"2183\":1,\"2209\":2}}],[\"espnetasvspoofmodel\",{\"0\":{\"1113\":1},\"1\":{\"1113\":1}}],[\"espnetasrmodel\",{\"0\":{\"1171\":1},\"1\":{\"112\":1,\"1171\":2,\"1206\":1,\"1552\":1,\"2027\":1}}],[\"espnetasrtransducermodel\",{\"0\":{\"1057\":1},\"1\":{\"112\":1,\"1057\":2}}],[\"espnet1\",{\"0\":{\"20\":1,\"84\":1,\"139\":1,\"146\":1,\"164\":1,\"165\":1},\"1\":{\"14\":1,\"33\":1,\"56\":1,\"72\":2,\"76\":1,\"77\":1,\"78\":1,\"83\":1,\"85\":3,\"95\":1,\"107\":1,\"108\":3,\"109\":2,\"115\":1,\"134\":1,\"146\":1,\"148\":2,\"150\":4}}],[\"espnet2asrtransducermodel\",{\"1\":{\"1057\":1}}],[\"espnet2\",{\"0\":{\"49\":1,\"83\":1,\"85\":1,\"140\":1,\"147\":1,\"153\":1,\"244\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1086\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1115\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1129\":1,\"1130\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":1,\"1142\":1,\"1143\":1,\"1145\":1,\"1146\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1154\":1,\"1155\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1176\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1186\":1,\"1187\":1,\"1188\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1204\":1,\"1205\":1,\"1206\":1,\"1207\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1227\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1249\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":1,\"1288\":1,\"1289\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1346\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":1,\"1382\":1,\"1384\":1,\"1386\":1,\"1388\":1,\"1390\":1,\"1393\":1,\"1394\":1,\"1396\":1,\"1398\":1,\"1399\":1,\"1401\":1,\"1403\":1,\"1405\":1,\"1407\":1,\"1409\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1427\":1,\"1428\":1,\"1429\":1,\"1430\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1452\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1505\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1522\":1,\"1523\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1534\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1555\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1585\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1626\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1637\":1,\"1638\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1774\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1781\":1,\"1782\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1806\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1822\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1827\":1,\"1828\":1,\"1829\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1882\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"1902\":1,\"1904\":1,\"1905\":1,\"1906\":1,\"1907\":1,\"1909\":1,\"1910\":1,\"1911\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1922\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1926\":1,\"1928\":1,\"1929\":1,\"1930\":1,\"1931\":1,\"1932\":1,\"1933\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1944\":1,\"1945\":1,\"1946\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1961\":1,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":1,\"1968\":1,\"1970\":1,\"1971\":1,\"1972\":1,\"1973\":1,\"1974\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1,\"2124\":1,\"2125\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2133\":1,\"2135\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":1,\"2166\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2171\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2176\":1,\"2177\":1,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":1,\"2184\":1,\"2185\":1,\"2186\":1,\"2187\":1,\"2188\":1,\"2189\":1,\"2191\":1,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2202\":1,\"2203\":1,\"2204\":1,\"2205\":1,\"2206\":1,\"2207\":1,\"2208\":1,\"2210\":1,\"2211\":1,\"2212\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":1,\"2223\":1,\"2224\":1,\"2225\":1,\"2226\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2230\":1,\"2231\":1,\"2232\":1,\"2233\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2274\":1,\"2275\":1,\"2277\":1,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2309\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2313\":1,\"2315\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2321\":1,\"2322\":1,\"2324\":1,\"2325\":1,\"2326\":1,\"2327\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2336\":1,\"2338\":1,\"2339\":1,\"2341\":1,\"2342\":1,\"2576\":1,\"2583\":1,\"2586\":1,\"2650\":1,\"2666\":1,\"2683\":1},\"1\":{\"14\":2,\"29\":2,\"32\":1,\"33\":2,\"34\":2,\"35\":1,\"36\":2,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"49\":1,\"56\":3,\"57\":2,\"58\":1,\"60\":3,\"62\":6,\"63\":2,\"64\":2,\"65\":1,\"66\":6,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":2,\"72\":3,\"79\":1,\"83\":1,\"84\":1,\"85\":5,\"90\":1,\"92\":1,\"95\":2,\"98\":1,\"107\":2,\"108\":3,\"109\":2,\"110\":1,\"111\":1,\"112\":2,\"115\":1,\"118\":1,\"124\":1,\"130\":1,\"134\":1,\"147\":1,\"148\":3,\"150\":4,\"161\":1,\"162\":1,\"163\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1061\":2,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":2,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":2,\"1083\":1,\"1084\":2,\"1085\":1,\"1086\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":2,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1115\":1,\"1116\":1,\"1117\":2,\"1119\":2,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":2,\"1129\":1,\"1130\":2,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":2,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":1,\"1142\":1,\"1143\":1,\"1145\":2,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1154\":1,\"1155\":1,\"1156\":1,\"1158\":1,\"1160\":2,\"1161\":1,\"1162\":2,\"1163\":2,\"1164\":2,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":2,\"1172\":1,\"1173\":1,\"1174\":1,\"1176\":1,\"1177\":2,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":2,\"1186\":1,\"1187\":1,\"1188\":2,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1204\":2,\"1205\":2,\"1206\":2,\"1207\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":2,\"1218\":2,\"1219\":1,\"1220\":2,\"1222\":2,\"1224\":1,\"1225\":1,\"1226\":1,\"1227\":1,\"1229\":1,\"1231\":1,\"1233\":2,\"1235\":1,\"1237\":1,\"1239\":2,\"1241\":2,\"1244\":2,\"1245\":2,\"1247\":2,\"1248\":2,\"1249\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":2,\"1257\":2,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1279\":2,\"1280\":2,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":1,\"1288\":1,\"1289\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":2,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":2,\"1312\":1,\"1313\":1,\"1314\":2,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":2,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":3,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":2,\"1328\":2,\"1329\":2,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1339\":2,\"1340\":1,\"1341\":2,\"1342\":1,\"1343\":2,\"1344\":1,\"1346\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1351\":2,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":2,\"1356\":2,\"1357\":2,\"1358\":2,\"1359\":1,\"1360\":1,\"1362\":1,\"1364\":2,\"1366\":2,\"1368\":1,\"1369\":2,\"1370\":1,\"1371\":2,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":2,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":2,\"1382\":1,\"1384\":1,\"1386\":1,\"1388\":2,\"1390\":1,\"1393\":2,\"1394\":2,\"1396\":2,\"1398\":2,\"1399\":2,\"1401\":1,\"1403\":1,\"1405\":2,\"1407\":2,\"1409\":2,\"1411\":2,\"1413\":2,\"1415\":2,\"1417\":1,\"1419\":2,\"1420\":1,\"1422\":2,\"1424\":1,\"1426\":2,\"1427\":1,\"1428\":2,\"1429\":1,\"1430\":2,\"1431\":2,\"1433\":1,\"1435\":2,\"1437\":1,\"1439\":2,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1452\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":2,\"1462\":2,\"1463\":1,\"1464\":2,\"1465\":2,\"1466\":1,\"1468\":1,\"1470\":2,\"1471\":2,\"1472\":1,\"1473\":2,\"1474\":1,\"1476\":1,\"1478\":2,\"1480\":1,\"1482\":2,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":2,\"1503\":1,\"1505\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":2,\"1518\":1,\"1520\":1,\"1522\":2,\"1523\":1,\"1524\":1,\"1525\":2,\"1526\":2,\"1527\":2,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":2,\"1532\":2,\"1534\":1,\"1535\":2,\"1537\":2,\"1539\":1,\"1540\":1,\"1542\":1,\"1543\":2,\"1545\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1551\":4,\"1552\":2,\"1553\":4,\"1554\":2,\"1555\":1,\"1557\":1,\"1558\":1,\"1559\":2,\"1560\":2,\"1561\":2,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":2,\"1573\":1,\"1575\":2,\"1576\":2,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1585\":1,\"1586\":2,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":2,\"1595\":1,\"1596\":1,\"1598\":2,\"1600\":1,\"1601\":1,\"1602\":2,\"1603\":1,\"1604\":1,\"1605\":2,\"1607\":2,\"1609\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":2,\"1619\":2,\"1620\":2,\"1622\":1,\"1623\":1,\"1624\":1,\"1626\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1637\":1,\"1638\":2,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":2,\"1650\":2,\"1652\":2,\"1654\":1,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":2,\"1664\":2,\"1665\":2,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":2,\"1671\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1678\":1,\"1679\":2,\"1680\":1,\"1681\":2,\"1682\":2,\"1683\":2,\"1684\":2,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":2,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":2,\"1701\":2,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":2,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":2,\"1719\":1,\"1720\":1,\"1721\":2,\"1722\":2,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":2,\"1728\":2,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":2,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":2,\"1739\":1,\"1740\":2,\"1741\":1,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1747\":1,\"1748\":2,\"1749\":2,\"1750\":1,\"1751\":2,\"1752\":1,\"1753\":1,\"1754\":1,\"1755\":1,\"1756\":1,\"1757\":1,\"1758\":2,\"1759\":2,\"1760\":2,\"1761\":1,\"1763\":1,\"1765\":1,\"1766\":1,\"1767\":2,\"1768\":2,\"1769\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1774\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1781\":1,\"1782\":2,\"1784\":2,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":2,\"1791\":1,\"1793\":2,\"1795\":2,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1805\":2,\"1806\":2,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":2,\"1812\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":2,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":2,\"1821\":1,\"1822\":1,\"1823\":2,\"1824\":2,\"1825\":1,\"1826\":2,\"1827\":2,\"1828\":2,\"1829\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":2,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":2,\"1841\":2,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":2,\"1851\":1,\"1852\":1,\"1853\":2,\"1854\":1,\"1855\":2,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":2,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":2,\"1876\":1,\"1877\":2,\"1878\":1,\"1879\":2,\"1880\":2,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"1902\":2,\"1904\":2,\"1905\":1,\"1906\":2,\"1907\":1,\"1909\":1,\"1910\":1,\"1911\":2,\"1912\":2,\"1914\":1,\"1915\":1,\"1916\":2,\"1917\":2,\"1918\":2,\"1919\":2,\"1920\":2,\"1921\":1,\"1922\":1,\"1923\":1,\"1924\":1,\"1925\":2,\"1926\":2,\"1928\":2,\"1929\":2,\"1930\":1,\"1931\":1,\"1932\":1,\"1933\":1,\"1934\":1,\"1935\":2,\"1936\":1,\"1937\":1,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":2,\"1942\":1,\"1943\":2,\"1944\":1,\"1945\":2,\"1946\":2,\"1947\":2,\"1948\":2,\"1949\":2,\"1950\":1,\"1951\":3,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":2,\"1960\":2,\"1961\":2,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":2,\"1968\":2,\"1970\":2,\"1971\":2,\"1972\":2,\"1973\":1,\"1974\":1,\"1975\":2,\"1976\":1,\"1978\":2,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":2,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":2,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":2,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":2,\"2020\":1,\"2021\":2,\"2022\":1,\"2023\":1,\"2024\":1,\"2026\":1,\"2027\":2,\"2028\":1,\"2029\":1,\"2030\":2,\"2032\":2,\"2034\":2,\"2036\":2,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":2,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":2,\"2077\":2,\"2078\":1,\"2081\":1,\"2082\":2,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":2,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":2,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":5,\"2097\":2,\"2098\":5,\"2099\":5,\"2100\":5,\"2101\":5,\"2102\":5,\"2103\":5,\"2104\":5,\"2105\":5,\"2106\":2,\"2107\":5,\"2108\":5,\"2109\":5,\"2110\":5,\"2111\":5,\"2112\":5,\"2113\":5,\"2114\":5,\"2115\":5,\"2116\":5,\"2117\":4,\"2118\":5,\"2119\":2,\"2120\":2,\"2121\":2,\"2122\":2,\"2123\":1,\"2124\":1,\"2125\":2,\"2126\":2,\"2127\":2,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":2,\"2132\":1,\"2133\":2,\"2135\":1,\"2136\":2,\"2137\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2143\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":2,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":2,\"2166\":2,\"2167\":2,\"2168\":2,\"2170\":2,\"2171\":2,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":2,\"2176\":2,\"2177\":2,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":2,\"2184\":2,\"2185\":2,\"2186\":1,\"2187\":2,\"2188\":2,\"2189\":1,\"2191\":1,\"2192\":2,\"2193\":2,\"2194\":2,\"2195\":2,\"2196\":2,\"2197\":2,\"2198\":2,\"2199\":2,\"2200\":2,\"2201\":2,\"2202\":2,\"2203\":2,\"2204\":1,\"2205\":2,\"2206\":2,\"2207\":2,\"2208\":2,\"2209\":3,\"2210\":1,\"2211\":2,\"2212\":2,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":2,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":2,\"2223\":2,\"2224\":2,\"2225\":1,\"2226\":2,\"2227\":1,\"2228\":2,\"2229\":2,\"2230\":2,\"2231\":1,\"2232\":2,\"2233\":1,\"2235\":2,\"2236\":2,\"2239\":1,\"2240\":2,\"2241\":2,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":2,\"2253\":1,\"2254\":1,\"2255\":2,\"2256\":2,\"2257\":1,\"2258\":2,\"2259\":2,\"2260\":1,\"2261\":2,\"2262\":1,\"2263\":2,\"2264\":1,\"2265\":1,\"2266\":2,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":2,\"2274\":1,\"2275\":1,\"2277\":2,\"2278\":2,\"2279\":1,\"2280\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":2,\"2290\":1,\"2292\":1,\"2294\":2,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2309\":1,\"2310\":2,\"2311\":2,\"2312\":2,\"2313\":1,\"2315\":1,\"2316\":2,\"2317\":1,\"2318\":1,\"2319\":2,\"2321\":2,\"2322\":1,\"2324\":2,\"2325\":2,\"2326\":1,\"2327\":2,\"2329\":2,\"2330\":2,\"2331\":2,\"2332\":2,\"2333\":2,\"2334\":2,\"2336\":2,\"2338\":2,\"2339\":2,\"2341\":2,\"2342\":1,\"2355\":1,\"2358\":1,\"2361\":2,\"2364\":2,\"2366\":1,\"2368\":2,\"2371\":1,\"2372\":4,\"2377\":1,\"2378\":1,\"2384\":2,\"2385\":3,\"2390\":2,\"2391\":2,\"2394\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2423\":1,\"2424\":1,\"2429\":4,\"2436\":1,\"2437\":1,\"2455\":1,\"2460\":1,\"2472\":1,\"2474\":2,\"2476\":1,\"2478\":1,\"2486\":2,\"2490\":2,\"2494\":1,\"2498\":1,\"2500\":1,\"2507\":2,\"2510\":4,\"2513\":2,\"2520\":2,\"2526\":2,\"2527\":2,\"2530\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2543\":2,\"2546\":1,\"2547\":1,\"2554\":4,\"2558\":1,\"2562\":1,\"2563\":1,\"2573\":1,\"2576\":2,\"2579\":1,\"2583\":2,\"2584\":1,\"2586\":1,\"2591\":1,\"2600\":3,\"2601\":2,\"2605\":2,\"2609\":2,\"2612\":1,\"2616\":1,\"2617\":1,\"2618\":2,\"2622\":2,\"2626\":2,\"2630\":1,\"2634\":1,\"2635\":1,\"2642\":4,\"2643\":1,\"2644\":1,\"2645\":3,\"2646\":1,\"2648\":1,\"2649\":2,\"2650\":2,\"2654\":2,\"2658\":2}}],[\"espnet\",{\"0\":{\"2\":1,\"9\":1,\"126\":1,\"127\":1,\"129\":1,\"135\":1,\"151\":1,\"168\":1,\"170\":1,\"179\":2,\"199\":1,\"205\":1,\"206\":1,\"593\":1,\"594\":1,\"595\":1,\"596\":1,\"598\":1,\"599\":1,\"600\":1,\"601\":1,\"602\":1,\"603\":1,\"604\":1,\"605\":1,\"606\":1,\"607\":1,\"609\":1,\"610\":1,\"611\":1,\"612\":1,\"614\":1,\"615\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"674\":1,\"675\":1,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"695\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"705\":1,\"706\":1,\"707\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"782\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"789\":1,\"790\":1,\"791\":1,\"792\":1,\"793\":1,\"794\":1,\"795\":1,\"797\":1,\"798\":1,\"799\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":1,\"813\":1,\"814\":1,\"816\":1,\"817\":1,\"818\":1,\"819\":1,\"820\":1,\"821\":1,\"822\":1,\"823\":1,\"824\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"857\":1,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":1,\"867\":1,\"868\":1,\"869\":1,\"870\":1,\"871\":1,\"872\":1,\"873\":1,\"874\":1,\"875\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"890\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"901\":1,\"903\":1,\"905\":1,\"906\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"929\":1,\"930\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":1,\"936\":1,\"937\":1,\"938\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"946\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"954\":1,\"955\":1,\"956\":1,\"957\":1,\"958\":1,\"960\":1,\"961\":1,\"962\":1,\"963\":1,\"964\":1,\"965\":1,\"966\":1,\"967\":1,\"968\":1,\"969\":1,\"970\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"985\":1,\"986\":1,\"987\":1,\"988\":1,\"989\":1,\"990\":1,\"991\":1,\"993\":1,\"994\":1,\"996\":1,\"997\":1,\"998\":1,\"999\":1,\"1000\":1,\"1001\":1,\"1002\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1006\":1,\"1007\":1,\"1008\":1,\"1009\":1,\"1010\":1,\"1011\":1,\"1012\":1,\"1013\":1,\"1015\":1,\"1016\":1,\"1018\":1,\"1019\":1,\"1020\":1,\"1021\":1,\"1022\":1,\"1023\":1,\"1025\":1,\"1026\":1,\"1027\":1,\"1028\":1,\"1029\":1,\"1031\":1,\"1032\":1,\"1033\":1,\"1034\":1,\"1035\":1,\"1036\":1,\"1037\":1,\"1038\":1,\"1039\":1,\"1040\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1044\":1,\"1045\":1,\"1057\":1,\"1113\":1,\"1171\":1,\"1172\":1,\"1218\":1,\"1371\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1773\":1,\"1837\":1,\"1892\":1,\"1893\":1,\"1953\":1,\"1955\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2046\":1,\"2076\":1,\"2082\":1,\"2168\":1,\"2170\":1,\"2240\":1,\"2278\":1,\"2294\":1,\"2354\":1,\"2383\":1,\"2388\":1,\"2393\":1,\"2394\":1,\"2409\":1,\"2421\":1,\"2427\":1,\"2429\":1,\"2450\":1,\"2452\":1,\"2466\":1,\"2468\":1,\"2516\":1,\"2524\":1,\"2529\":1,\"2530\":1,\"2544\":1,\"2550\":1,\"2552\":1,\"2575\":1,\"2585\":1,\"2597\":1,\"2599\":1,\"2601\":1,\"2618\":1,\"2646\":1,\"2662\":1,\"2669\":1},\"1\":{\"1\":1,\"2\":1,\"5\":7,\"7\":1,\"10\":1,\"11\":5,\"12\":1,\"13\":2,\"15\":2,\"19\":1,\"20\":2,\"21\":2,\"24\":4,\"26\":2,\"27\":1,\"28\":3,\"47\":1,\"49\":1,\"56\":1,\"72\":2,\"74\":2,\"75\":1,\"76\":2,\"77\":1,\"78\":1,\"79\":1,\"80\":1,\"81\":1,\"82\":2,\"85\":2,\"90\":1,\"97\":3,\"99\":1,\"100\":1,\"103\":1,\"110\":2,\"127\":1,\"128\":3,\"130\":8,\"134\":1,\"135\":21,\"136\":4,\"137\":1,\"140\":1,\"150\":1,\"156\":1,\"159\":1,\"161\":8,\"162\":1,\"166\":4,\"167\":15,\"168\":2,\"169\":1,\"170\":2,\"171\":1,\"172\":1,\"173\":2,\"175\":2,\"176\":1,\"177\":4,\"178\":14,\"179\":2,\"180\":1,\"181\":2,\"183\":1,\"184\":1,\"185\":2,\"186\":2,\"187\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":2,\"193\":1,\"194\":3,\"195\":4,\"196\":14,\"197\":3,\"198\":3,\"199\":7,\"200\":8,\"201\":2,\"202\":2,\"203\":2,\"204\":1,\"206\":4,\"207\":4,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":4,\"222\":2,\"223\":2,\"224\":4,\"229\":2,\"230\":2,\"231\":4,\"233\":4,\"234\":13,\"235\":1,\"237\":1,\"244\":1,\"295\":3,\"528\":2,\"564\":1,\"593\":1,\"594\":1,\"595\":1,\"596\":1,\"598\":1,\"599\":1,\"600\":2,\"601\":2,\"602\":2,\"603\":2,\"604\":2,\"605\":1,\"606\":2,\"607\":2,\"609\":1,\"610\":2,\"611\":1,\"612\":1,\"614\":2,\"615\":2,\"616\":2,\"617\":1,\"618\":2,\"619\":2,\"620\":2,\"621\":2,\"622\":2,\"623\":2,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":3,\"630\":2,\"631\":2,\"632\":2,\"633\":2,\"634\":2,\"635\":1,\"636\":2,\"637\":1,\"638\":2,\"639\":1,\"640\":1,\"641\":2,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":2,\"647\":2,\"648\":2,\"649\":2,\"650\":4,\"651\":2,\"652\":2,\"653\":2,\"654\":2,\"655\":2,\"656\":2,\"657\":2,\"658\":1,\"659\":2,\"660\":2,\"661\":1,\"662\":2,\"663\":2,\"664\":2,\"665\":1,\"666\":2,\"667\":2,\"668\":1,\"669\":2,\"670\":2,\"671\":2,\"672\":1,\"673\":2,\"674\":1,\"675\":1,\"676\":3,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"695\":1,\"696\":1,\"697\":2,\"698\":1,\"699\":1,\"700\":3,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"705\":1,\"706\":3,\"707\":1,\"708\":1,\"709\":2,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":2,\"728\":2,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":2,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"742\":3,\"743\":1,\"745\":1,\"746\":1,\"747\":2,\"748\":1,\"749\":2,\"750\":2,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":3,\"768\":1,\"769\":1,\"770\":1,\"771\":3,\"772\":3,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"781\":3,\"782\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"789\":1,\"790\":1,\"791\":1,\"792\":1,\"793\":2,\"794\":1,\"795\":1,\"796\":1,\"797\":1,\"798\":1,\"799\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":3,\"810\":3,\"811\":1,\"812\":3,\"813\":1,\"814\":1,\"815\":6,\"816\":1,\"817\":1,\"818\":1,\"819\":1,\"820\":3,\"821\":1,\"822\":1,\"823\":1,\"824\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"829\":2,\"830\":1,\"831\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"857\":2,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":1,\"867\":2,\"868\":1,\"869\":1,\"870\":2,\"871\":1,\"872\":1,\"873\":2,\"874\":2,\"875\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":2,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"890\":2,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"901\":1,\"903\":1,\"905\":1,\"906\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"929\":1,\"930\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":1,\"936\":1,\"937\":2,\"938\":2,\"939\":2,\"940\":1,\"941\":2,\"942\":1,\"943\":2,\"944\":2,\"945\":2,\"946\":1,\"947\":1,\"948\":2,\"949\":2,\"950\":2,\"951\":2,\"952\":2,\"953\":2,\"954\":1,\"955\":2,\"956\":2,\"957\":1,\"958\":1,\"960\":2,\"961\":1,\"962\":2,\"963\":2,\"964\":2,\"965\":2,\"966\":2,\"967\":1,\"968\":2,\"969\":2,\"970\":2,\"971\":1,\"972\":2,\"973\":2,\"974\":1,\"975\":1,\"976\":1,\"977\":2,\"978\":2,\"979\":2,\"980\":2,\"981\":2,\"982\":2,\"983\":2,\"985\":2,\"986\":2,\"987\":2,\"988\":1,\"989\":2,\"990\":2,\"991\":2,\"993\":2,\"994\":2,\"996\":2,\"997\":1,\"998\":1,\"999\":2,\"1000\":2,\"1001\":1,\"1002\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1006\":1,\"1007\":1,\"1008\":2,\"1009\":1,\"1010\":1,\"1011\":2,\"1012\":3,\"1013\":1,\"1015\":1,\"1016\":1,\"1017\":1,\"1018\":1,\"1019\":2,\"1020\":1,\"1021\":1,\"1022\":1,\"1023\":1,\"1025\":1,\"1026\":1,\"1027\":2,\"1028\":2,\"1029\":2,\"1031\":2,\"1032\":1,\"1033\":1,\"1034\":1,\"1035\":1,\"1036\":1,\"1037\":2,\"1038\":2,\"1039\":2,\"1040\":2,\"1041\":1,\"1042\":1,\"1043\":2,\"1044\":2,\"1045\":2,\"1057\":1,\"1113\":1,\"1133\":1,\"1138\":2,\"1139\":2,\"1148\":2,\"1149\":1,\"1150\":1,\"1153\":1,\"1167\":1,\"1168\":1,\"1171\":2,\"1172\":1,\"1182\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1203\":2,\"1204\":1,\"1209\":1,\"1218\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1371\":1,\"1400\":1,\"1409\":1,\"1551\":1,\"1552\":1,\"1553\":2,\"1554\":1,\"1604\":1,\"1655\":1,\"1719\":1,\"1773\":2,\"1837\":2,\"1892\":1,\"1893\":1,\"1917\":1,\"1953\":1,\"1955\":1,\"1970\":2,\"1971\":1,\"1975\":2,\"1984\":3,\"2001\":1,\"2004\":1,\"2027\":2,\"2029\":1,\"2046\":1,\"2054\":2,\"2076\":2,\"2082\":3,\"2086\":1,\"2087\":1,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2155\":1,\"2156\":1,\"2168\":2,\"2170\":1,\"2182\":1,\"2185\":2,\"2189\":1,\"2203\":2,\"2240\":3,\"2278\":3,\"2294\":2,\"2350\":1,\"2354\":9,\"2355\":9,\"2357\":3,\"2358\":1,\"2359\":4,\"2361\":2,\"2366\":2,\"2371\":2,\"2372\":23,\"2375\":5,\"2377\":10,\"2378\":2,\"2379\":2,\"2380\":4,\"2381\":1,\"2382\":5,\"2383\":3,\"2384\":14,\"2385\":1,\"2386\":1,\"2387\":3,\"2388\":7,\"2389\":1,\"2390\":7,\"2393\":6,\"2394\":12,\"2396\":3,\"2398\":2,\"2400\":4,\"2401\":3,\"2403\":1,\"2406\":3,\"2407\":1,\"2409\":6,\"2411\":1,\"2414\":2,\"2415\":1,\"2419\":1,\"2421\":5,\"2422\":1,\"2424\":7,\"2427\":4,\"2428\":2,\"2429\":18,\"2430\":2,\"2431\":5,\"2432\":4,\"2436\":5,\"2437\":2,\"2440\":2,\"2441\":1,\"2442\":2,\"2446\":7,\"2447\":5,\"2450\":6,\"2451\":1,\"2452\":4,\"2456\":3,\"2460\":2,\"2461\":3,\"2463\":3,\"2466\":6,\"2467\":1,\"2468\":5,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2480\":4,\"2482\":5,\"2492\":1,\"2498\":1,\"2500\":1,\"2502\":3,\"2504\":3,\"2506\":2,\"2514\":1,\"2516\":1,\"2517\":3,\"2520\":1,\"2521\":6,\"2524\":6,\"2525\":1,\"2526\":7,\"2529\":6,\"2530\":12,\"2532\":3,\"2536\":4,\"2537\":3,\"2539\":1,\"2542\":3,\"2543\":2,\"2544\":5,\"2545\":1,\"2547\":7,\"2550\":5,\"2551\":2,\"2552\":4,\"2553\":1,\"2554\":8,\"2555\":2,\"2558\":11,\"2559\":2,\"2562\":5,\"2563\":2,\"2564\":2,\"2566\":1,\"2567\":3,\"2571\":1,\"2572\":1,\"2573\":2,\"2574\":3,\"2575\":3,\"2576\":3,\"2578\":1,\"2579\":1,\"2580\":4,\"2583\":1,\"2584\":11,\"2585\":8,\"2587\":2,\"2588\":1,\"2590\":1,\"2591\":3,\"2597\":5,\"2598\":8,\"2599\":4,\"2600\":7,\"2601\":2,\"2602\":1,\"2612\":2,\"2616\":1,\"2617\":2,\"2618\":3,\"2619\":1,\"2628\":1,\"2630\":3,\"2634\":1,\"2635\":4,\"2641\":1,\"2644\":1,\"2646\":7,\"2648\":1,\"2649\":1,\"2650\":2,\"2651\":1,\"2659\":1}}],[\"es\",{\"1\":{\"22\":1,\"201\":1,\"296\":1,\"1057\":1,\"1879\":2,\"2245\":2,\"2256\":2,\"2280\":2,\"2357\":1,\"2454\":1,\"2455\":3,\"2460\":4,\"2461\":2,\"2519\":1,\"2520\":5,\"2578\":1}}],[\"eprenet\",{\"1\":{\"826\":4,\"2086\":8,\"2087\":8,\"2264\":8}}],[\"eprojs\",{\"1\":{\"677\":2,\"678\":2,\"679\":2,\"680\":2,\"681\":2,\"683\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"703\":2,\"758\":2,\"892\":2,\"1145\":3,\"1289\":1}}],[\"ep\",{\"1\":{\"28\":1,\"240\":3,\"241\":2,\"656\":1}}],[\"eps=1\",{\"1\":{\"1641\":1}}],[\"eps=1e\",{\"1\":{\"769\":1,\"947\":1,\"954\":1,\"967\":1,\"971\":1,\"1430\":1,\"1447\":1,\"1449\":1,\"1455\":1,\"1470\":1,\"1471\":1,\"1476\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1604\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":1,\"1680\":1,\"1695\":1,\"1719\":1,\"1739\":1,\"1779\":1,\"2022\":1,\"2268\":1,\"2269\":1}}],[\"eps=none\",{\"1\":{\"1640\":1}}],[\"epsilon\",{\"1\":{\"115\":7,\"1093\":1,\"1427\":7,\"1430\":1,\"1470\":1,\"1471\":1,\"1604\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":1,\"1671\":1,\"1835\":1,\"1855\":1}}],[\"eps\",{\"0\":{\"630\":1},\"1\":{\"17\":1,\"62\":2,\"115\":7,\"251\":5,\"255\":4,\"259\":4,\"265\":2,\"269\":2,\"630\":6,\"631\":2,\"760\":1,\"831\":1,\"885\":2,\"887\":2,\"932\":2,\"1047\":2,\"1072\":2,\"1080\":2,\"1093\":2,\"1098\":2,\"1430\":1,\"1465\":2,\"1470\":1,\"1471\":1,\"1524\":2,\"1525\":2,\"1604\":1,\"1611\":2,\"1640\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":1,\"1671\":2,\"1680\":1,\"1696\":4,\"1697\":4,\"1698\":4,\"1703\":2,\"1704\":4,\"1705\":4,\"1706\":4,\"1707\":4,\"1708\":4,\"1711\":2,\"1712\":4,\"1714\":2,\"1715\":4,\"1719\":3,\"1739\":1,\"1746\":2,\"1759\":2,\"1835\":2,\"1855\":2,\"1906\":2,\"1920\":1,\"1949\":2}}],[\"epoch=none\",{\"1\":{\"2018\":1,\"2022\":1}}],[\"epoch=0\",{\"1\":{\"996\":1}}],[\"epochs=10\",{\"1\":{\"25\":1}}],[\"epochs10\",{\"1\":{\"25\":1}}],[\"epochs\",{\"1\":{\"17\":1,\"240\":3,\"251\":2,\"255\":2,\"259\":2,\"265\":6,\"269\":6,\"997\":1,\"998\":1,\"1007\":3,\"1034\":1,\"1895\":1,\"1897\":1,\"1900\":1,\"2022\":1,\"2099\":2,\"2102\":2,\"2193\":3,\"2199\":1,\"2441\":1,\"2564\":1,\"2584\":1}}],[\"epoch\",{\"0\":{\"69\":1},\"1\":{\"17\":3,\"65\":2,\"69\":16,\"82\":1,\"174\":6,\"240\":2,\"253\":2,\"499\":2,\"612\":2,\"623\":1,\"656\":1,\"981\":2,\"996\":2,\"1034\":1,\"1894\":1,\"1895\":4,\"1896\":1,\"1897\":4,\"1898\":1,\"1900\":4,\"1962\":1,\"2006\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":3,\"2019\":1,\"2020\":1,\"2021\":1,\"2023\":3,\"2099\":14,\"2102\":14,\"2106\":2,\"2185\":4,\"2186\":1,\"2193\":19,\"2198\":1,\"2199\":2,\"2201\":4,\"2202\":2,\"2203\":4,\"2204\":1,\"2294\":1,\"2345\":1,\"2440\":1,\"2558\":1,\"2584\":1}}],[\"e\",{\"0\":{\"1169\":1,\"1170\":1},\"1\":{\"1\":2,\"3\":1,\"5\":1,\"10\":1,\"18\":1,\"19\":2,\"21\":5,\"24\":1,\"25\":7,\"26\":1,\"27\":1,\"28\":2,\"30\":1,\"37\":1,\"38\":1,\"45\":3,\"48\":1,\"49\":1,\"57\":2,\"58\":1,\"59\":1,\"60\":2,\"63\":1,\"64\":3,\"75\":2,\"76\":1,\"77\":1,\"78\":1,\"80\":1,\"84\":1,\"85\":2,\"90\":1,\"91\":1,\"92\":2,\"98\":3,\"99\":1,\"102\":1,\"108\":3,\"112\":1,\"115\":3,\"116\":1,\"121\":1,\"127\":1,\"135\":2,\"136\":5,\"143\":1,\"144\":2,\"150\":1,\"167\":2,\"168\":1,\"169\":4,\"178\":2,\"179\":1,\"181\":4,\"196\":1,\"200\":1,\"234\":1,\"235\":2,\"240\":1,\"274\":1,\"275\":1,\"276\":1,\"277\":1,\"279\":1,\"280\":2,\"281\":1,\"283\":1,\"284\":1,\"286\":1,\"294\":3,\"296\":1,\"297\":1,\"298\":1,\"528\":1,\"564\":1,\"665\":1,\"689\":1,\"691\":1,\"693\":1,\"697\":1,\"711\":2,\"727\":1,\"728\":1,\"731\":3,\"732\":2,\"747\":2,\"748\":1,\"749\":2,\"784\":2,\"794\":2,\"797\":1,\"800\":1,\"835\":1,\"838\":1,\"857\":1,\"872\":1,\"873\":1,\"874\":1,\"909\":1,\"952\":1,\"1012\":1,\"1015\":3,\"1056\":6,\"1081\":1,\"1084\":2,\"1091\":3,\"1132\":2,\"1133\":2,\"1148\":2,\"1149\":2,\"1150\":2,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1169\":2,\"1170\":2,\"1177\":1,\"1187\":1,\"1202\":1,\"1203\":2,\"1211\":1,\"1244\":1,\"1245\":2,\"1246\":1,\"1252\":2,\"1253\":3,\"1254\":2,\"1255\":1,\"1269\":1,\"1272\":2,\"1279\":2,\"1286\":2,\"1287\":1,\"1327\":2,\"1336\":1,\"1337\":1,\"1339\":1,\"1352\":1,\"1356\":1,\"1375\":2,\"1406\":1,\"1429\":1,\"1454\":2,\"1463\":1,\"1505\":3,\"1515\":2,\"1516\":3,\"1523\":1,\"1528\":2,\"1529\":1,\"1530\":1,\"1534\":1,\"1539\":1,\"1543\":1,\"1551\":1,\"1553\":1,\"1558\":1,\"1572\":1,\"1618\":1,\"1619\":1,\"1626\":1,\"1645\":2,\"1654\":1,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":3,\"1670\":1,\"1671\":2,\"1695\":6,\"1704\":1,\"1719\":2,\"1739\":2,\"1785\":1,\"1862\":1,\"1878\":1,\"1879\":2,\"1880\":1,\"1905\":1,\"1930\":1,\"1932\":1,\"1946\":1,\"1947\":1,\"1962\":1,\"1971\":1,\"2001\":2,\"2004\":2,\"2011\":1,\"2012\":1,\"2029\":2,\"2046\":2,\"2050\":1,\"2090\":1,\"2099\":1,\"2102\":1,\"2155\":5,\"2193\":4,\"2245\":2,\"2256\":2,\"2275\":1,\"2280\":2,\"2314\":2,\"2360\":4,\"2373\":1,\"2378\":1,\"2385\":2,\"2394\":1,\"2410\":1,\"2414\":1,\"2430\":1,\"2437\":1,\"2450\":1,\"2458\":4,\"2481\":1,\"2501\":1,\"2514\":1,\"2517\":1,\"2520\":1,\"2523\":4,\"2530\":1,\"2543\":1,\"2555\":2,\"2563\":1,\"2568\":3,\"2569\":2,\"2574\":1,\"2582\":4,\"2584\":1,\"2585\":3,\"2618\":1,\"2638\":1,\"2639\":1,\"2641\":1,\"2643\":1,\"2659\":1}}],[\"exhaustive\",{\"1\":{\"2584\":1}}],[\"exhausted\",{\"1\":{\"44\":1}}],[\"exercise\",{\"0\":{\"2438\":1,\"2542\":1,\"2543\":1,\"2564\":1,\"2565\":1},\"1\":{\"2440\":3,\"2542\":1,\"2543\":1,\"2564\":2,\"2572\":1}}],[\"executing\",{\"1\":{\"85\":1,\"1964\":1,\"2372\":1,\"2429\":1,\"2554\":1,\"2564\":1}}],[\"execution\",{\"0\":{\"16\":1}}],[\"executable\",{\"1\":{\"15\":2,\"2467\":1}}],[\"executes\",{\"1\":{\"2372\":1}}],[\"executed\",{\"1\":{\"1\":1,\"3\":1}}],[\"execute\",{\"0\":{\"1\":1,\"2571\":1},\"1\":{\"1\":2,\"7\":2,\"16\":2,\"19\":1,\"99\":1,\"698\":1,\"699\":1,\"2387\":1}}],[\"exists\",{\"0\":{\"1924\":1},\"1\":{\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"1924\":1}}],[\"exist\",{\"1\":{\"112\":1,\"135\":1,\"922\":1}}],[\"existing\",{\"1\":{\"11\":1,\"38\":1,\"124\":1,\"144\":1,\"161\":1,\"1670\":1,\"2423\":1,\"2468\":1,\"2473\":1,\"2546\":1}}],[\"exiting\",{\"1\":{\"2568\":1}}],[\"exitcode\",{\"1\":{\"594\":3}}],[\"exit\",{\"1\":{\"98\":1,\"108\":1,\"2568\":5,\"2569\":1}}],[\"exaplin\",{\"1\":{\"2419\":1}}],[\"exact\",{\"1\":{\"628\":1,\"2395\":1,\"2531\":1,\"2543\":1}}],[\"exactly\",{\"1\":{\"49\":1,\"235\":1,\"2600\":1}}],[\"examples>\",{\"1\":{\"979\":1}}],[\"examples\",{\"0\":{\"33\":1,\"617\":1,\"744\":1,\"796\":1,\"815\":1,\"838\":1,\"876\":1,\"900\":1,\"902\":1,\"904\":1,\"907\":1,\"928\":1,\"959\":1,\"984\":1,\"992\":1,\"995\":1,\"1014\":1,\"1017\":1,\"1024\":1,\"1030\":1,\"1383\":1,\"1385\":1,\"1387\":1,\"1389\":1,\"1391\":1,\"1395\":1,\"1397\":1,\"1400\":1,\"1402\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1897\":1,\"1969\":1,\"2134\":1,\"2143\":1,\"2149\":1,\"2158\":1,\"2183\":1,\"2190\":1,\"2209\":1,\"2314\":1,\"2320\":1,\"2323\":1,\"2328\":1,\"2335\":1,\"2337\":1,\"2340\":1,\"2359\":1},\"1\":{\"7\":1,\"38\":1,\"124\":1,\"157\":1,\"161\":4,\"617\":2,\"727\":1,\"728\":1,\"817\":1,\"1355\":1,\"1398\":1,\"1958\":1,\"2024\":1,\"2028\":1,\"2154\":1,\"2294\":1,\"2385\":1,\"2429\":1,\"2457\":1,\"2459\":2,\"2467\":1,\"2508\":1,\"2552\":1,\"2558\":2}}],[\"example\",{\"0\":{\"16\":1,\"110\":1,\"2367\":1,\"2372\":1,\"2456\":1,\"2484\":1,\"2496\":1,\"2521\":1,\"2580\":1,\"2604\":1,\"2614\":1,\"2621\":1,\"2632\":1},\"1\":{\"1\":1,\"3\":2,\"5\":2,\"16\":2,\"28\":2,\"29\":3,\"45\":1,\"47\":1,\"48\":1,\"57\":1,\"62\":1,\"76\":1,\"79\":1,\"85\":2,\"98\":1,\"102\":2,\"104\":1,\"109\":1,\"115\":1,\"116\":2,\"135\":2,\"144\":2,\"149\":1,\"150\":1,\"161\":1,\"166\":2,\"168\":1,\"169\":1,\"177\":2,\"180\":1,\"195\":1,\"197\":1,\"198\":4,\"233\":1,\"235\":1,\"282\":1,\"285\":1,\"286\":7,\"295\":8,\"296\":6,\"652\":1,\"706\":1,\"899\":1,\"901\":1,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1177\":2,\"1209\":1,\"1211\":1,\"1224\":1,\"1252\":2,\"1253\":3,\"1254\":2,\"1255\":1,\"1279\":1,\"1287\":1,\"1336\":1,\"1348\":1,\"1856\":1,\"1861\":1,\"1905\":1,\"2099\":1,\"2168\":1,\"2170\":1,\"2176\":1,\"2193\":2,\"2372\":1,\"2373\":1,\"2380\":1,\"2384\":1,\"2385\":2,\"2386\":1,\"2392\":1,\"2425\":1,\"2431\":2,\"2432\":2,\"2438\":1,\"2457\":1,\"2518\":1,\"2528\":1,\"2548\":1,\"2558\":1,\"2564\":1,\"2572\":1,\"2573\":1,\"2584\":4,\"2587\":1,\"2600\":1}}],[\"excersices\",{\"1\":{\"2422\":1,\"2525\":1,\"2545\":1}}],[\"exception\",{\"1\":{\"607\":1,\"823\":1,\"1085\":1}}],[\"except\",{\"1\":{\"23\":1,\"80\":1,\"119\":1,\"710\":1,\"754\":2,\"826\":2,\"1180\":1,\"1377\":1,\"1679\":1,\"1851\":2,\"1895\":1,\"1900\":1,\"1972\":1,\"2020\":1,\"2021\":1,\"2090\":2,\"2148\":1,\"2243\":2,\"2244\":2,\"2255\":2,\"2264\":2,\"2279\":2}}],[\"excitation\",{\"1\":{\"1800\":1}}],[\"excitation=none\",{\"1\":{\"1800\":2}}],[\"exclusive\",{\"1\":{\"1905\":1}}],[\"excluding\",{\"1\":{\"66\":2,\"760\":1}}],[\"excluded\",{\"1\":{\"1896\":1}}],[\"exclude\",{\"1\":{\"66\":1,\"538\":1,\"2099\":1,\"2584\":4}}],[\"exclude=$\",{\"1\":{\"11\":1}}],[\"excl\",{\"1\":{\"116\":1}}],[\"extact\",{\"1\":{\"2266\":1}}],[\"exteded\",{\"1\":{\"706\":1}}],[\"extension\",{\"1\":{\"277\":1,\"610\":4,\"629\":1,\"630\":2,\"631\":2,\"651\":2,\"652\":2,\"656\":2,\"706\":2,\"834\":5,\"988\":2,\"996\":3,\"1551\":1,\"1553\":1,\"2046\":1,\"2389\":1,\"2400\":1,\"2401\":1,\"2440\":1,\"2525\":1,\"2536\":1,\"2537\":1,\"2558\":1,\"2571\":1,\"2584\":1}}],[\"extensions\",{\"1\":{\"132\":1,\"2046\":1}}],[\"extendedhypothesis\",{\"0\":{\"751\":1,\"1060\":1,\"1176\":1},\"1\":{\"700\":2,\"725\":1,\"751\":1,\"806\":1,\"824\":1,\"917\":1,\"922\":2,\"1048\":1,\"1060\":1,\"1138\":2,\"1139\":2,\"1176\":1,\"1270\":1}}],[\"extended\",{\"1\":{\"684\":1,\"685\":1,\"692\":2,\"693\":2,\"704\":1,\"705\":1,\"751\":1,\"917\":1,\"1048\":1,\"1060\":1,\"1176\":1,\"2354\":1}}],[\"extending\",{\"1\":{\"130\":1}}],[\"extend\",{\"1\":{\"11\":2,\"692\":2,\"693\":2,\"698\":1,\"699\":1,\"705\":3,\"706\":4,\"770\":1,\"810\":1,\"818\":1,\"1077\":1,\"1400\":1,\"2542\":1}}],[\"extlm\",{\"0\":{\"609\":1,\"611\":1},\"1\":{\"609\":1,\"611\":1}}],[\"ext\",{\"0\":{\"1882\":1},\"1\":{\"240\":1,\"277\":2,\"2401\":1,\"2440\":1,\"2537\":1,\"2558\":1,\"2571\":1}}],[\"extration\",{\"0\":{\"238\":1}}],[\"extra\",{\"1\":{\"137\":1,\"274\":2,\"760\":3,\"778\":3,\"831\":3,\"1429\":1,\"1476\":3,\"1598\":3,\"1906\":3,\"1910\":3,\"1912\":3,\"1914\":3,\"1915\":3,\"1918\":3,\"1919\":3,\"1920\":3,\"2076\":3,\"2084\":3,\"2086\":1,\"2087\":1,\"2089\":3,\"2482\":1,\"2542\":1}}],[\"extras=none\",{\"1\":{\"754\":2}}],[\"extras\",{\"1\":{\"134\":2,\"167\":2,\"178\":2,\"196\":2,\"234\":2,\"754\":2}}],[\"extracting\",{\"1\":{\"2467\":1}}],[\"extraction\",{\"1\":{\"16\":1,\"84\":1,\"235\":1,\"238\":3,\"429\":1,\"437\":1,\"1269\":1,\"1551\":1,\"1554\":1,\"2046\":2,\"2200\":1,\"2354\":1,\"2388\":1,\"2415\":2,\"2421\":1,\"2524\":1,\"2544\":1}}],[\"extractfile\",{\"1\":{\"1961\":1}}],[\"extracts\",{\"1\":{\"692\":1,\"693\":1,\"834\":1,\"2049\":1,\"2055\":1,\"2064\":1,\"2070\":1,\"2467\":1}}],[\"extractors\",{\"1\":{\"2468\":1}}],[\"extractor\",{\"0\":{\"1441\":2,\"1659\":2},\"1\":{\"134\":1,\"1071\":3,\"1132\":1,\"1269\":8,\"1441\":2,\"1554\":1,\"1659\":3,\"2046\":1,\"2236\":1,\"2241\":1}}],[\"extracted\",{\"0\":{\"2433\":1,\"2443\":1},\"1\":{\"84\":1,\"727\":1,\"728\":1,\"797\":1,\"1179\":1,\"1484\":1,\"1601\":1,\"1889\":1,\"2415\":2,\"2416\":1,\"2433\":8,\"2436\":2,\"2440\":8,\"2441\":1,\"2574\":1}}],[\"extract\",{\"0\":{\"51\":1,\"52\":1,\"429\":1,\"476\":1,\"1317\":1,\"1812\":1,\"1813\":1,\"1981\":2,\"1987\":1,\"1989\":1,\"1991\":1,\"2084\":2,\"2085\":2,\"2089\":2,\"2094\":2,\"2233\":2,\"2236\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2275\":2,\"2281\":1,\"2416\":1,\"2431\":1},\"1\":{\"51\":1,\"161\":1,\"235\":1,\"429\":2,\"476\":2,\"643\":1,\"725\":1,\"797\":1,\"806\":1,\"824\":1,\"918\":1,\"1046\":1,\"1057\":3,\"1073\":1,\"1083\":1,\"1116\":1,\"1171\":1,\"1172\":1,\"1206\":1,\"1270\":1,\"1317\":1,\"1551\":1,\"1553\":2,\"1554\":1,\"1773\":8,\"1812\":1,\"1813\":1,\"1837\":3,\"1889\":1,\"1961\":1,\"1970\":1,\"1975\":1,\"1981\":2,\"1984\":2,\"1987\":1,\"1989\":1,\"1991\":1,\"2002\":1,\"2027\":1,\"2046\":4,\"2076\":1,\"2082\":8,\"2084\":2,\"2085\":2,\"2089\":2,\"2090\":1,\"2094\":2,\"2095\":1,\"2142\":1,\"2198\":1,\"2233\":2,\"2236\":2,\"2240\":3,\"2241\":2,\"2243\":1,\"2244\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2266\":2,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2275\":2,\"2278\":3,\"2281\":1,\"2415\":3,\"2416\":3,\"2431\":1}}],[\"expressivetacotron\",{\"1\":{\"1986\":2}}],[\"exponent\",{\"1\":{\"1643\":1,\"1644\":1}}],[\"exponentially\",{\"1\":{\"2440\":1,\"2558\":1}}],[\"exponential\",{\"0\":{\"1316\":1},\"1\":{\"834\":1,\"1069\":1,\"1194\":1,\"1316\":1}}],[\"exported\",{\"1\":{\"2600\":3}}],[\"exportation\",{\"1\":{\"2598\":1,\"2600\":1}}],[\"export\",{\"0\":{\"375\":1,\"491\":1,\"2599\":1,\"2600\":1},\"1\":{\"1\":1,\"3\":7,\"38\":1,\"70\":3,\"375\":3,\"491\":3,\"2597\":1,\"2598\":1,\"2599\":6,\"2600\":12,\"2638\":1}}],[\"explain\",{\"1\":{\"2420\":1}}],[\"explained\",{\"1\":{\"2355\":1}}],[\"explanations\",{\"1\":{\"1211\":1,\"1286\":1,\"1336\":1,\"1337\":1}}],[\"explanation\",{\"1\":{\"120\":1,\"162\":1,\"163\":1,\"164\":3,\"1398\":1,\"2461\":1,\"2508\":1}}],[\"exploding\",{\"1\":{\"1618\":1}}],[\"explore\",{\"0\":{\"185\":1},\"1\":{\"185\":1}}],[\"exploration\",{\"1\":{\"161\":1,\"2441\":1}}],[\"explicit\",{\"1\":{\"461\":1,\"2122\":1,\"2315\":2}}],[\"explicitely\",{\"1\":{\"79\":1}}],[\"explicitly\",{\"1\":{\"57\":1}}],[\"expired\",{\"1\":{\"49\":1}}],[\"experiencias\",{\"1\":{\"2457\":1}}],[\"experiment\",{\"0\":{\"96\":1},\"1\":{\"16\":1,\"19\":1,\"93\":1,\"2372\":1,\"2506\":1,\"2508\":1,\"2510\":2,\"2558\":1,\"2559\":1}}],[\"experiments\",{\"1\":{\"3\":1,\"16\":1,\"17\":1,\"85\":1,\"113\":1,\"133\":1,\"169\":1,\"181\":1,\"2354\":1,\"2372\":3,\"2387\":1,\"2388\":1,\"2421\":1,\"2429\":1,\"2524\":1,\"2542\":2,\"2544\":1,\"2554\":1}}],[\"experimental\",{\"1\":{\"3\":1,\"216\":1,\"1242\":1,\"1243\":1}}],[\"expect\",{\"1\":{\"27\":1,\"28\":1,\"29\":1,\"2403\":1,\"2539\":1}}],[\"expected\",{\"1\":{\"24\":1,\"30\":1,\"59\":1,\"119\":1,\"1047\":1,\"1048\":1,\"1072\":1,\"1080\":1,\"1222\":1,\"1245\":1,\"1282\":1,\"1409\":1,\"1517\":1,\"2050\":1,\"2381\":1,\"2391\":1,\"2407\":1,\"2423\":1,\"2432\":1,\"2448\":1,\"2464\":1,\"2527\":1,\"2546\":1,\"2584\":1}}],[\"expense\",{\"1\":{\"23\":1,\"119\":1,\"1712\":1,\"1715\":1}}],[\"expanding\",{\"1\":{\"2585\":1}}],[\"expand=2\",{\"1\":{\"1177\":1}}],[\"expand=none\",{\"1\":{\"1162\":1}}],[\"expand=1\",{\"1\":{\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1280\":1,\"1311\":1,\"1358\":1}}],[\"expand\",{\"0\":{\"1811\":2,\"2094\":1},\"1\":{\"785\":4,\"1741\":1,\"1781\":2,\"1804\":5,\"1805\":2,\"1811\":6,\"1877\":1,\"1878\":2,\"2094\":2}}],[\"expands\",{\"1\":{\"774\":1}}],[\"expandedtokenembedding\",{\"0\":{\"1174\":1},\"1\":{\"1174\":1}}],[\"expanded\",{\"1\":{\"23\":1,\"119\":1,\"700\":1,\"766\":2,\"1048\":1,\"1064\":2,\"1138\":1,\"1139\":1,\"1804\":2,\"1842\":2,\"1853\":2,\"1854\":2,\"1878\":2}}],[\"expansions\",{\"0\":{\"917\":1},\"1\":{\"23\":1,\"113\":1,\"119\":1,\"700\":1,\"917\":3,\"1048\":3,\"1057\":1,\"1059\":1,\"1138\":1,\"1139\":1}}],[\"expansion\",{\"1\":{\"23\":6,\"113\":1,\"119\":5,\"249\":4,\"700\":7,\"917\":2,\"1048\":9,\"1134\":1,\"1138\":7,\"1139\":7,\"1804\":1,\"1878\":1}}],[\"expname\",{\"1\":{\"17\":1}}],[\"expdir\",{\"1\":{\"17\":1}}],[\"exp\",{\"0\":{\"1332\":1,\"1333\":1,\"1344\":1,\"2443\":1},\"1\":{\"17\":1,\"23\":2,\"28\":1,\"87\":1,\"88\":2,\"89\":1,\"96\":8,\"110\":2,\"119\":1,\"169\":1,\"181\":1,\"191\":1,\"192\":1,\"194\":1,\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"240\":14,\"241\":2,\"242\":2,\"249\":2,\"275\":1,\"279\":1,\"282\":1,\"283\":1,\"284\":1,\"285\":2,\"297\":1,\"700\":2,\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"1048\":2,\"1136\":1,\"1138\":2,\"1139\":2,\"1145\":1,\"1247\":1,\"1248\":1,\"1332\":3,\"1333\":1,\"1344\":2,\"2368\":2,\"2375\":5,\"2401\":1,\"2431\":1,\"2440\":9,\"2441\":1,\"2455\":2,\"2460\":2,\"2461\":2,\"2472\":2,\"2474\":2,\"2476\":2,\"2478\":2,\"2486\":2,\"2490\":2,\"2492\":1,\"2494\":2,\"2500\":3,\"2507\":2,\"2510\":6,\"2513\":2,\"2520\":4,\"2537\":1,\"2558\":4,\"2559\":9,\"2560\":1,\"2564\":3,\"2571\":1,\"2572\":2,\"2584\":1,\"2585\":2,\"2605\":2,\"2609\":2,\"2622\":2,\"2626\":2,\"2628\":1,\"2648\":2,\"2649\":2}}],[\"0m\",{\"1\":{\"2500\":9,\"2617\":9,\"2635\":9}}],[\"0+cu117\",{\"1\":{\"2482\":2}}],[\"0|62\",{\"1\":{\"2564\":1}}],[\"0|7\",{\"1\":{\"2564\":1}}],[\"0|1\",{\"1\":{\"2441\":1}}],[\"0|16\",{\"1\":{\"2440\":1}}],[\"0|\",{\"1\":{\"2440\":3,\"2441\":3,\"2564\":3}}],[\"0|0\",{\"1\":{\"2440\":1,\"2572\":2}}],[\"0|5\",{\"1\":{\"2440\":1}}],[\"0|46\",{\"1\":{\"2564\":1}}],[\"0|4\",{\"1\":{\"2440\":1,\"2441\":1}}],[\"09\",{\"1\":{\"2387\":1}}],[\"09685\",{\"1\":{\"1932\":1}}],[\"09921\",{\"1\":{\"1371\":1}}],[\"0db\",{\"1\":{\"1921\":1}}],[\"028\",{\"1\":{\"1715\":1}}],[\"02860\",{\"1\":{\"771\":1,\"772\":1,\"809\":1,\"810\":1}}],[\"02\",{\"1\":{\"1065\":1,\"1070\":1,\"1078\":1,\"1079\":1,\"2194\":1,\"2441\":1}}],[\"02971\",{\"1\":{\"1049\":1}}],[\"0296\",{\"1\":{\"17\":1}}],[\"02795\",{\"1\":{\"770\":2}}],[\"033\",{\"1\":{\"2500\":10,\"2592\":2,\"2617\":10,\"2635\":10}}],[\"032\",{\"1\":{\"1929\":1,\"1941\":1,\"1947\":1}}],[\"03\",{\"1\":{\"1761\":1,\"1763\":1,\"1805\":1,\"2441\":1}}],[\"03952\",{\"1\":{\"1660\":1}}],[\"03983\",{\"1\":{\"668\":1}}],[\"03541\",{\"1\":{\"1138\":1,\"1210\":1,\"1211\":1,\"1286\":2,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":2,\"1337\":2}}],[\"03577\",{\"1\":{\"700\":1,\"1138\":1,\"1139\":1}}],[\"03762\",{\"1\":{\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"2019\":1}}],[\"01634449\",{\"1\":{\"1712\":1,\"1715\":1}}],[\"01531\",{\"1\":{\"1645\":1}}],[\"01\",{\"1\":{\"152\":1,\"297\":1,\"1096\":1,\"1778\":2,\"2090\":2,\"2151\":1,\"2210\":1,\"2274\":2,\"2514\":1,\"2659\":1}}],[\"06670\",{\"1\":{\"2467\":1,\"2646\":1}}],[\"06389\",{\"1\":{\"2260\":1}}],[\"06247\",{\"1\":{\"1061\":1,\"1371\":1}}],[\"0625\",{\"1\":{\"109\":1,\"110\":1}}],[\"06736\",{\"1\":{\"681\":1,\"682\":1}}],[\"06\",{\"1\":{\"152\":1,\"1132\":1,\"1455\":1,\"1524\":2,\"1525\":2,\"1604\":1,\"1611\":2,\"1695\":1,\"1739\":1,\"2040\":1}}],[\"068\",{\"1\":{\"87\":1}}],[\"0=1\",{\"1\":{\"87\":2}}],[\"07949\",{\"1\":{\"1870\":1}}],[\"07907v1\",{\"1\":{\"2078\":2,\"2095\":1}}],[\"07907\",{\"1\":{\"758\":1,\"2081\":1,\"2083\":1}}],[\"07093\",{\"1\":{\"1782\":1}}],[\"07597\",{\"1\":{\"1198\":1,\"1917\":1}}],[\"07145\",{\"1\":{\"1084\":1}}],[\"07467\",{\"1\":{\"1072\":1}}],[\"07\",{\"1\":{\"195\":1,\"233\":1,\"1524\":1,\"1525\":1,\"1611\":2,\"1641\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1714\":1,\"1715\":2}}],[\"07840\",{\"1\":{\"130\":1}}],[\"07204\",{\"1\":{\"1150\":1}}],[\"072\",{\"1\":{\"87\":1}}],[\"076\",{\"1\":{\"87\":1}}],[\"0500\",{\"1\":{\"2442\":1}}],[\"0500`\",{\"1\":{\"2441\":1}}],[\"05042\",{\"1\":{\"2040\":1}}],[\"05941v1\",{\"1\":{\"1084\":1}}],[\"05895\",{\"1\":{\"1080\":1}}],[\"05420\",{\"1\":{\"1048\":1}}],[\"05k\",{\"1\":{\"295\":12}}],[\"055\",{\"1\":{\"87\":1}}],[\"05\",{\"1\":{\"84\":1,\"87\":4,\"115\":7,\"1057\":1,\"1072\":1,\"1080\":1,\"1171\":1,\"1179\":1,\"1211\":1,\"1269\":1,\"1286\":1,\"1336\":1,\"1430\":1,\"1447\":1,\"1449\":1,\"1465\":1,\"1470\":1,\"1471\":1,\"1476\":1,\"1528\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1618\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":1,\"1671\":1,\"1779\":1,\"1835\":1,\"1855\":1,\"1915\":1,\"2196\":1,\"2260\":2,\"2441\":1,\"2442\":1}}],[\"0025\",{\"1\":{\"2357\":1,\"2578\":1}}],[\"00>\",{\"1\":{\"2194\":2}}],[\"008\",{\"1\":{\"1929\":1,\"1941\":1,\"1947\":1,\"2274\":2}}],[\"003\",{\"1\":{\"1797\":3}}],[\"004\",{\"1\":{\"110\":1}}],[\"0002\",{\"1\":{\"2440\":1}}],[\"0000\",{\"1\":{\"1410\":3,\"1412\":3,\"2593\":1}}],[\"00000000210000000030\",{\"1\":{\"1398\":1}}],[\"00000000110000000020\",{\"1\":{\"1398\":1}}],[\"00000000000000000010\",{\"1\":{\"1398\":1}}],[\"00077\",{\"1\":{\"1056\":1}}],[\"000hz\",{\"1\":{\"940\":1}}],[\"000th\",{\"1\":{\"652\":1}}],[\"000\",{\"1\":{\"87\":2,\"940\":1,\"2154\":1}}],[\"0001\",{\"1\":{\"62\":1,\"609\":1,\"2022\":1}}],[\"006\",{\"1\":{\"87\":3}}],[\"001\",{\"1\":{\"62\":1,\"110\":3,\"174\":1,\"1245\":2,\"1248\":1,\"1886\":3,\"1887\":3,\"1888\":3,\"2018\":2,\"2020\":1,\"2558\":1}}],[\"0e\",{\"1\":{\"62\":2}}],[\"08661v5\",{\"1\":{\"2003\":1}}],[\"08661\",{\"1\":{\"1976\":1}}],[\"08681\",{\"1\":{\"1067\":1}}],[\"08779\",{\"1\":{\"950\":1,\"968\":1,\"1037\":1}}],[\"08895\",{\"1\":{\"813\":1}}],[\"082\",{\"1\":{\"110\":1}}],[\"08\",{\"1\":{\"17\":6,\"62\":2,\"1604\":1,\"1680\":1,\"1696\":1,\"1697\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1714\":1,\"1715\":1,\"1719\":1,\"1746\":2,\"2022\":1,\"2268\":1,\"2269\":1}}],[\"04301\",{\"1\":{\"1842\":1,\"1986\":1}}],[\"04368\",{\"1\":{\"678\":1}}],[\"04783\",{\"1\":{\"729\":1}}],[\"0475\",{\"1\":{\"17\":1}}],[\"046\",{\"1\":{\"87\":1}}],[\"04\",{\"0\":{\"8\":1},\"1\":{\"5\":1,\"6\":2,\"87\":3,\"133\":1,\"2441\":1,\"2442\":2}}],[\"0\",{\"0\":{\"180\":1,\"237\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2418\":1,\"2419\":1,\"2420\":1,\"2432\":1},\"1\":{\"1\":1,\"2\":1,\"3\":3,\"4\":1,\"5\":5,\"8\":3,\"17\":10,\"19\":1,\"21\":18,\"22\":11,\"23\":1,\"24\":2,\"26\":2,\"28\":1,\"29\":1,\"36\":1,\"39\":1,\"47\":1,\"51\":1,\"58\":1,\"60\":1,\"62\":8,\"64\":2,\"78\":1,\"98\":1,\"102\":1,\"109\":2,\"110\":1,\"113\":7,\"115\":42,\"116\":11,\"118\":9,\"121\":2,\"134\":1,\"150\":20,\"167\":1,\"171\":2,\"172\":2,\"173\":4,\"174\":7,\"175\":7,\"178\":1,\"185\":2,\"186\":2,\"187\":1,\"189\":1,\"191\":2,\"192\":1,\"193\":2,\"194\":6,\"196\":1,\"203\":3,\"207\":2,\"217\":7,\"224\":4,\"231\":6,\"234\":1,\"235\":1,\"237\":2,\"238\":3,\"239\":1,\"276\":1,\"281\":1,\"286\":3,\"294\":2,\"296\":3,\"298\":1,\"609\":1,\"611\":2,\"628\":1,\"632\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"688\":1,\"690\":1,\"691\":1,\"692\":5,\"693\":6,\"694\":1,\"697\":8,\"698\":6,\"699\":7,\"700\":2,\"705\":1,\"712\":2,\"725\":4,\"726\":4,\"728\":2,\"729\":1,\"730\":2,\"736\":1,\"738\":1,\"743\":1,\"744\":7,\"747\":1,\"749\":1,\"752\":2,\"756\":3,\"758\":1,\"762\":1,\"763\":1,\"765\":2,\"768\":2,\"770\":2,\"774\":2,\"778\":9,\"787\":1,\"797\":3,\"798\":1,\"806\":5,\"807\":1,\"818\":1,\"822\":1,\"825\":14,\"834\":1,\"838\":4,\"857\":6,\"858\":5,\"869\":1,\"875\":1,\"876\":9,\"877\":1,\"880\":1,\"900\":123,\"902\":168,\"904\":5,\"907\":6,\"914\":1,\"921\":3,\"925\":3,\"940\":1,\"989\":1,\"1011\":2,\"1019\":1,\"1026\":1,\"1028\":1,\"1036\":1,\"1037\":1,\"1039\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":3,\"1050\":3,\"1052\":3,\"1054\":2,\"1056\":3,\"1057\":10,\"1061\":5,\"1065\":9,\"1066\":12,\"1067\":2,\"1068\":3,\"1069\":4,\"1070\":5,\"1072\":2,\"1073\":5,\"1074\":4,\"1075\":8,\"1076\":6,\"1077\":4,\"1078\":3,\"1079\":3,\"1082\":6,\"1083\":3,\"1084\":1,\"1093\":6,\"1096\":10,\"1099\":1,\"1101\":1,\"1106\":2,\"1107\":1,\"1108\":4,\"1115\":60,\"1130\":1,\"1132\":1,\"1133\":2,\"1136\":1,\"1138\":1,\"1139\":4,\"1140\":9,\"1141\":6,\"1142\":2,\"1145\":7,\"1148\":8,\"1149\":4,\"1150\":4,\"1155\":1,\"1158\":4,\"1166\":1,\"1167\":6,\"1168\":6,\"1169\":6,\"1171\":6,\"1172\":6,\"1177\":1,\"1178\":5,\"1179\":14,\"1180\":14,\"1181\":6,\"1182\":1,\"1186\":2,\"1187\":1,\"1192\":1,\"1194\":1,\"1195\":2,\"1196\":6,\"1197\":6,\"1198\":1,\"1200\":1,\"1201\":2,\"1202\":1,\"1203\":4,\"1204\":6,\"1206\":5,\"1209\":1,\"1210\":2,\"1211\":7,\"1214\":2,\"1215\":2,\"1217\":1,\"1220\":4,\"1222\":2,\"1224\":5,\"1225\":1,\"1226\":1,\"1233\":2,\"1239\":4,\"1241\":1,\"1243\":1,\"1244\":5,\"1245\":1,\"1247\":1,\"1248\":1,\"1252\":3,\"1254\":2,\"1256\":1,\"1257\":1,\"1269\":16,\"1270\":5,\"1271\":8,\"1272\":6,\"1273\":6,\"1282\":2,\"1286\":2,\"1287\":1,\"1301\":2,\"1304\":2,\"1314\":2,\"1336\":5,\"1337\":2,\"1343\":1,\"1348\":5,\"1349\":2,\"1350\":2,\"1355\":2,\"1371\":2,\"1376\":1,\"1391\":1,\"1398\":1,\"1400\":1,\"1410\":1,\"1412\":1,\"1423\":2,\"1426\":1,\"1427\":2,\"1429\":1,\"1430\":2,\"1444\":1,\"1455\":1,\"1458\":1,\"1460\":1,\"1465\":1,\"1478\":2,\"1480\":4,\"1501\":1,\"1505\":3,\"1506\":1,\"1515\":3,\"1517\":2,\"1518\":2,\"1522\":4,\"1523\":4,\"1524\":3,\"1525\":2,\"1528\":4,\"1529\":3,\"1530\":1,\"1531\":4,\"1532\":1,\"1534\":3,\"1535\":1,\"1537\":1,\"1539\":3,\"1545\":4,\"1552\":1,\"1558\":3,\"1560\":1,\"1563\":1,\"1573\":1,\"1577\":2,\"1581\":2,\"1598\":2,\"1600\":2,\"1602\":4,\"1603\":2,\"1605\":2,\"1611\":4,\"1622\":2,\"1626\":3,\"1629\":1,\"1631\":1,\"1635\":1,\"1638\":1,\"1643\":2,\"1644\":2,\"1648\":2,\"1650\":1,\"1652\":2,\"1654\":3,\"1669\":3,\"1670\":2,\"1671\":3,\"1688\":2,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1696\":2,\"1704\":1,\"1712\":2,\"1713\":1,\"1714\":1,\"1715\":1,\"1719\":1,\"1731\":1,\"1732\":1,\"1741\":1,\"1753\":2,\"1755\":1,\"1756\":2,\"1761\":13,\"1763\":13,\"1765\":3,\"1771\":5,\"1773\":1,\"1776\":1,\"1777\":1,\"1778\":27,\"1781\":1,\"1785\":3,\"1787\":5,\"1788\":12,\"1791\":4,\"1797\":10,\"1798\":5,\"1800\":3,\"1801\":2,\"1803\":1,\"1804\":16,\"1805\":39,\"1808\":1,\"1833\":1,\"1834\":1,\"1835\":2,\"1837\":1,\"1841\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":1,\"1849\":1,\"1850\":23,\"1851\":15,\"1852\":25,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":2,\"1860\":2,\"1861\":1,\"1862\":4,\"1863\":2,\"1864\":2,\"1865\":2,\"1866\":1,\"1867\":1,\"1868\":2,\"1870\":7,\"1871\":1,\"1874\":5,\"1877\":22,\"1878\":14,\"1880\":4,\"1883\":2,\"1886\":1,\"1887\":4,\"1888\":1,\"1890\":4,\"1892\":7,\"1895\":2,\"1896\":3,\"1898\":1,\"1900\":2,\"1905\":11,\"1912\":6,\"1914\":1,\"1915\":3,\"1917\":1,\"1921\":1,\"1922\":1,\"1925\":3,\"1928\":3,\"1929\":3,\"1932\":5,\"1935\":4,\"1936\":3,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":2,\"1943\":4,\"1946\":1,\"1947\":3,\"1953\":1,\"1955\":3,\"1958\":3,\"1960\":3,\"1970\":3,\"1971\":1,\"1972\":7,\"1973\":1,\"1975\":5,\"1985\":4,\"1989\":1,\"1993\":2,\"1996\":3,\"1997\":1,\"1999\":3,\"2000\":2,\"2001\":9,\"2002\":7,\"2003\":4,\"2004\":9,\"2018\":8,\"2020\":4,\"2021\":4,\"2022\":4,\"2023\":5,\"2026\":4,\"2027\":5,\"2029\":4,\"2040\":1,\"2054\":8,\"2070\":2,\"2076\":12,\"2079\":3,\"2083\":1,\"2086\":5,\"2087\":6,\"2090\":14,\"2095\":10,\"2099\":1,\"2149\":1,\"2151\":6,\"2153\":1,\"2170\":4,\"2177\":2,\"2178\":6,\"2179\":6,\"2180\":2,\"2181\":2,\"2182\":3,\"2184\":5,\"2188\":2,\"2191\":6,\"2193\":3,\"2194\":7,\"2195\":6,\"2196\":7,\"2197\":3,\"2200\":5,\"2201\":2,\"2208\":2,\"2210\":1,\"2222\":1,\"2243\":12,\"2244\":16,\"2248\":1,\"2253\":1,\"2255\":15,\"2259\":2,\"2260\":3,\"2262\":2,\"2263\":10,\"2264\":20,\"2265\":1,\"2274\":8,\"2279\":17,\"2290\":2,\"2292\":3,\"2294\":3,\"2295\":1,\"2296\":1,\"2301\":3,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2349\":2,\"2358\":2,\"2359\":1,\"2360\":1,\"2364\":3,\"2368\":1,\"2369\":1,\"2371\":1,\"2372\":3,\"2373\":2,\"2377\":1,\"2384\":1,\"2395\":3,\"2430\":2,\"2440\":16,\"2441\":1,\"2442\":1,\"2444\":1,\"2445\":3,\"2446\":3,\"2456\":2,\"2460\":2,\"2461\":4,\"2472\":5,\"2474\":4,\"2476\":6,\"2478\":2,\"2482\":1,\"2486\":1,\"2487\":1,\"2490\":1,\"2491\":1,\"2494\":1,\"2497\":1,\"2498\":10,\"2500\":13,\"2501\":1,\"2504\":2,\"2507\":3,\"2510\":6,\"2513\":3,\"2514\":5,\"2520\":3,\"2521\":2,\"2522\":1,\"2523\":1,\"2531\":3,\"2555\":2,\"2558\":11,\"2564\":8,\"2568\":2,\"2569\":1,\"2574\":1,\"2576\":1,\"2579\":2,\"2580\":1,\"2581\":1,\"2582\":1,\"2584\":8,\"2592\":12,\"2596\":7,\"2598\":1,\"2600\":8,\"2605\":1,\"2606\":1,\"2607\":1,\"2609\":1,\"2610\":1,\"2612\":1,\"2614\":1,\"2615\":1,\"2616\":13,\"2617\":13,\"2622\":1,\"2623\":1,\"2624\":1,\"2626\":1,\"2627\":1,\"2630\":1,\"2632\":1,\"2633\":1,\"2634\":13,\"2635\":13,\"2648\":5,\"2649\":4,\"2651\":2,\"2654\":3,\"2658\":3,\"2659\":5}}],[\"cvss\",{\"1\":{\"2518\":2}}],[\"cvf\",{\"1\":{\"2030\":1}}],[\"c0pxkb6pfolb6xxfdxcpd\",{\"1\":{\"2476\":1}}],[\"c0466d9a356c1a33f671a546426d7bc33b5b17e8\",{\"1\":{\"200\":1}}],[\"cwskattention\",{\"0\":{\"2074\":1},\"1\":{\"2074\":1}}],[\"cwd\",{\"1\":{\"144\":2}}],[\"c=2\",{\"1\":{\"2314\":1}}],[\"c=4\",{\"1\":{\"2314\":1}}],[\"c=none\",{\"1\":{\"1800\":2}}],[\"c=1\",{\"1\":{\"1247\":1,\"1248\":1}}],[\"cn\",{\"1\":{\"2618\":3}}],[\"cnt\",{\"1\":{\"1773\":6,\"2082\":6}}],[\"cnn\",{\"1\":{\"833\":1,\"858\":1,\"1148\":4,\"1149\":2,\"1203\":4,\"1505\":2,\"1778\":1,\"1850\":1,\"1851\":3,\"1852\":1,\"2003\":1,\"2026\":2,\"2054\":4,\"2090\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2279\":3}}],[\"cg\",{\"1\":{\"1639\":2}}],[\"cgmlp\",{\"0\":{\"1151\":1,\"1153\":1},\"1\":{\"1140\":4,\"1141\":5,\"1151\":2,\"1153\":1,\"1169\":2,\"1170\":2,\"2440\":4,\"2564\":4}}],[\"cbn\",{\"1\":{\"1516\":2}}],[\"cbhg=true\",{\"1\":{\"821\":1}}],[\"cbhgloss\",{\"0\":{\"702\":1},\"1\":{\"702\":1}}],[\"cbhg\",{\"0\":{\"701\":2,\"702\":1,\"764\":1},\"1\":{\"701\":6,\"702\":6,\"764\":1,\"821\":16}}],[\"c3\",{\"1\":{\"1391\":1}}],[\"c1\",{\"1\":{\"1391\":1,\"1425\":2}}],[\"cfg=none\",{\"1\":{\"1263\":1}}],[\"cfg\",{\"1\":{\"1116\":2,\"1983\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2371\":1,\"2612\":1,\"2630\":1}}],[\"c++\",{\"1\":{\"944\":1}}],[\"cx\",{\"1\":{\"685\":1}}],[\"cstr\",{\"1\":{\"2512\":2,\"2657\":2}}],[\"cse\",{\"1\":{\"1523\":1}}],[\"csv\",{\"1\":{\"1384\":1,\"1386\":1,\"1417\":1,\"2359\":2,\"2456\":2,\"2460\":2,\"2521\":2,\"2580\":2}}],[\"csgu\",{\"1\":{\"1153\":1}}],[\"csj\",{\"1\":{\"286\":1}}],[\"csmsc\",{\"1\":{\"229\":1,\"230\":1,\"295\":8,\"2363\":13,\"2506\":1,\"2653\":13}}],[\"cp\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2429\":4,\"2432\":3,\"2520\":1}}],[\"cpurnnt\",{\"0\":{\"1142\":1,\"1154\":1,\"1155\":1},\"1\":{\"1142\":1,\"1154\":1,\"1155\":3}}],[\"cpu\",{\"0\":{\"4\":1,\"1142\":2,\"1154\":2,\"1155\":2,\"1202\":2,\"1349\":1},\"1\":{\"4\":1,\"5\":2,\"7\":2,\"8\":1,\"19\":2,\"134\":2,\"135\":3,\"148\":1,\"175\":1,\"194\":1,\"218\":1,\"225\":1,\"232\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"727\":1,\"728\":1,\"745\":2,\"746\":2,\"921\":2,\"1142\":3,\"1154\":3,\"1155\":4,\"1202\":2,\"1349\":3,\"1757\":1,\"1927\":1,\"2099\":2,\"2109\":1,\"2113\":1,\"2115\":1,\"2116\":1,\"2157\":1,\"2365\":1,\"2373\":2,\"2430\":2,\"2431\":2,\"2508\":1,\"2510\":1,\"2515\":1,\"2521\":1,\"2522\":1,\"2523\":1,\"2555\":2,\"2559\":1,\"2592\":1,\"2655\":1,\"2660\":1}}],[\"cycle\",{\"1\":{\"2018\":8,\"2255\":1,\"2260\":3}}],[\"cycles\",{\"1\":{\"1862\":1,\"1880\":1}}],[\"cyclic\",{\"1\":{\"668\":1}}],[\"cycliccosinescheduler\",{\"0\":{\"668\":1},\"1\":{\"668\":1}}],[\"cygwin\",{\"1\":{\"134\":1}}],[\"cyberciti\",{\"1\":{\"45\":1}}],[\"cta\",{\"1\":{\"1144\":1}}],[\"ctareduce\",{\"0\":{\"1143\":1},\"1\":{\"1143\":1}}],[\"ctm\",{\"1\":{\"546\":3,\"588\":2}}],[\"ctx=none\",{\"1\":{\"711\":6}}],[\"ctx\",{\"1\":{\"104\":1,\"711\":8,\"759\":2,\"1086\":2,\"1149\":2,\"1150\":2,\"1187\":9,\"1202\":9,\"1286\":5,\"1287\":5}}],[\"ctcsamp0\",{\"1\":{\"2454\":1,\"2455\":2}}],[\"ctcprefixscoreth\",{\"0\":{\"705\":1},\"1\":{\"705\":1}}],[\"ctcprefixscore\",{\"0\":{\"704\":1},\"1\":{\"704\":1,\"705\":1,\"706\":1,\"829\":1}}],[\"ctcprefixscorer\",{\"0\":{\"706\":1},\"1\":{\"691\":1,\"693\":1,\"697\":1,\"706\":2,\"796\":1,\"797\":1,\"829\":1,\"857\":1}}],[\"ctcweight1\",{\"1\":{\"189\":1,\"191\":1}}],[\"ctc=none\",{\"1\":{\"692\":1}}],[\"ctc=59\",{\"1\":{\"87\":1}}],[\"ctc=65\",{\"1\":{\"87\":1}}],[\"ctc\",{\"0\":{\"150\":2,\"305\":1,\"703\":2,\"704\":1,\"705\":1,\"706\":1,\"759\":1,\"867\":2,\"929\":1,\"1136\":1,\"1145\":2,\"1332\":1,\"1997\":1},\"1\":{\"8\":2,\"17\":2,\"22\":7,\"24\":1,\"112\":1,\"118\":6,\"150\":24,\"167\":5,\"173\":1,\"175\":1,\"178\":5,\"194\":1,\"196\":4,\"200\":1,\"234\":4,\"245\":1,\"249\":6,\"251\":5,\"259\":3,\"307\":2,\"315\":4,\"327\":2,\"391\":2,\"408\":2,\"422\":2,\"443\":6,\"449\":2,\"485\":4,\"499\":1,\"676\":5,\"698\":7,\"699\":7,\"703\":4,\"704\":5,\"705\":10,\"706\":7,\"742\":2,\"750\":2,\"759\":4,\"796\":1,\"811\":2,\"825\":13,\"836\":1,\"867\":6,\"880\":1,\"929\":3,\"996\":1,\"1057\":6,\"1136\":3,\"1145\":9,\"1148\":7,\"1169\":5,\"1171\":5,\"1172\":3,\"1203\":6,\"1205\":3,\"1206\":5,\"1219\":4,\"1272\":6,\"1332\":1,\"1428\":1,\"1429\":2,\"1975\":10,\"1984\":4,\"1997\":3,\"2027\":4,\"2076\":6,\"2194\":2,\"2358\":1,\"2373\":1,\"2378\":1,\"2437\":1,\"2440\":2,\"2452\":2,\"2454\":1,\"2455\":4,\"2460\":4,\"2461\":3,\"2520\":1,\"2555\":1,\"2558\":2,\"2563\":1,\"2569\":1,\"2572\":4,\"2579\":1,\"2584\":1,\"2592\":1}}],[\"clearly\",{\"1\":{\"2419\":1,\"2501\":1}}],[\"cleaness\",{\"1\":{\"2411\":1}}],[\"cleaned\",{\"1\":{\"217\":2,\"224\":1,\"231\":1,\"237\":1}}],[\"cleaners\",{\"1\":{\"217\":3}}],[\"cleaner\",{\"0\":{\"2127\":1,\"2133\":1},\"1\":{\"98\":4,\"217\":1,\"461\":2,\"2127\":2,\"2133\":4,\"2134\":2,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2474\":1,\"2649\":1}}],[\"cleaner=whisper\",{\"1\":{\"98\":2}}],[\"clean\",{\"0\":{\"274\":1},\"1\":{\"110\":3,\"134\":1,\"217\":1,\"224\":1,\"231\":1,\"274\":2,\"295\":1,\"2638\":1}}],[\"cln\",{\"1\":{\"1368\":2,\"1377\":1,\"1379\":1,\"1430\":1,\"1472\":2,\"1598\":3,\"1648\":3,\"1652\":2,\"1658\":1,\"1659\":1,\"1664\":1,\"1665\":1,\"1670\":1,\"1671\":1}}],[\"clase\",{\"1\":{\"2457\":1}}],[\"classfication\",{\"1\":{\"2395\":1,\"2531\":1}}],[\"classchoices\",{\"0\":{\"2176\":1},\"1\":{\"2096\":12,\"2097\":7,\"2098\":9,\"2099\":3,\"2100\":10,\"2101\":27,\"2102\":9,\"2103\":13,\"2104\":10,\"2105\":9,\"2107\":5,\"2108\":10,\"2109\":17,\"2110\":12,\"2111\":13,\"2112\":16,\"2113\":12,\"2114\":11,\"2115\":9,\"2116\":10,\"2117\":7,\"2118\":8,\"2176\":3,\"2643\":1}}],[\"class=<class\",{\"1\":{\"747\":1,\"749\":1,\"1133\":1,\"1149\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1204\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1971\":1,\"2001\":1,\"2004\":1,\"2029\":1}}],[\"classes=1000\",{\"1\":{\"1231\":1}}],[\"classes\",{\"1\":{\"79\":1,\"124\":1,\"768\":1,\"1269\":3,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"2176\":1}}],[\"classic\",{\"1\":{\"301\":1}}],[\"classically\",{\"1\":{\"45\":1}}],[\"classifierwithstate\",{\"0\":{\"605\":1,\"710\":1},\"1\":{\"605\":1,\"710\":1}}],[\"classifier\",{\"1\":{\"255\":2}}],[\"classification\",{\"1\":{\"79\":1,\"150\":1,\"796\":1,\"1132\":1,\"1797\":1,\"2011\":1,\"2046\":1,\"2198\":1,\"2399\":1,\"2401\":1,\"2410\":2,\"2471\":1,\"2535\":1,\"2537\":1}}],[\"classmethod\",{\"1\":{\"56\":6,\"59\":2,\"60\":3,\"661\":1,\"672\":2,\"676\":1,\"767\":1,\"781\":1,\"944\":2,\"957\":1,\"1116\":1,\"1218\":1,\"1904\":1,\"1916\":1,\"2096\":6,\"2097\":6,\"2098\":6,\"2099\":27,\"2100\":6,\"2101\":6,\"2102\":7,\"2103\":7,\"2104\":7,\"2105\":6,\"2107\":6,\"2108\":6,\"2109\":7,\"2110\":6,\"2111\":5,\"2112\":6,\"2113\":7,\"2114\":6,\"2115\":7,\"2116\":7,\"2117\":6,\"2118\":7,\"2127\":1,\"2168\":1,\"2170\":1,\"2185\":4,\"2198\":2,\"2201\":8,\"2203\":4}}],[\"class\",{\"0\":{\"55\":1,\"56\":1,\"886\":1,\"1319\":1,\"2176\":1},\"1\":{\"56\":4,\"57\":1,\"59\":2,\"60\":4,\"62\":2,\"124\":3,\"217\":2,\"224\":2,\"231\":2,\"593\":2,\"594\":2,\"599\":1,\"604\":1,\"605\":1,\"606\":1,\"607\":1,\"609\":1,\"610\":1,\"611\":1,\"612\":1,\"614\":1,\"615\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":2,\"629\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"665\":2,\"667\":2,\"668\":2,\"669\":2,\"670\":2,\"671\":2,\"672\":3,\"673\":1,\"674\":3,\"676\":6,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"695\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":2,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"705\":1,\"706\":2,\"707\":1,\"708\":1,\"709\":1,\"710\":2,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":3,\"750\":1,\"751\":1,\"752\":1,\"754\":3,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":2,\"768\":1,\"769\":1,\"770\":2,\"771\":1,\"772\":2,\"773\":2,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"781\":4,\"782\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"791\":1,\"792\":1,\"793\":1,\"794\":1,\"795\":1,\"797\":1,\"798\":1,\"799\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"806\":1,\"807\":2,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":2,\"813\":2,\"814\":1,\"816\":1,\"817\":2,\"818\":1,\"819\":1,\"820\":2,\"821\":1,\"822\":1,\"823\":1,\"824\":1,\"825\":1,\"826\":3,\"827\":1,\"828\":2,\"829\":2,\"830\":1,\"831\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"859\":1,\"861\":3,\"872\":2,\"873\":3,\"874\":2,\"886\":6,\"936\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":3,\"945\":1,\"946\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"954\":1,\"955\":1,\"956\":1,\"957\":1,\"958\":1,\"960\":1,\"961\":1,\"962\":1,\"974\":1,\"975\":1,\"976\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"985\":1,\"986\":1,\"987\":1,\"988\":1,\"989\":1,\"990\":1,\"991\":1,\"993\":1,\"994\":1,\"996\":1,\"997\":1,\"998\":1,\"999\":2,\"1000\":1,\"1012\":4,\"1041\":1,\"1042\":1,\"1043\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":4,\"1050\":4,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":4,\"1055\":1,\"1056\":4,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1068\":4,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":4,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1086\":1,\"1098\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":2,\"1111\":1,\"1113\":1,\"1114\":1,\"1115\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":2,\"1130\":1,\"1132\":1,\"1133\":3,\"1134\":1,\"1136\":1,\"1138\":2,\"1139\":2,\"1140\":1,\"1141\":1,\"1142\":2,\"1145\":1,\"1148\":1,\"1149\":2,\"1150\":2,\"1151\":1,\"1153\":1,\"1154\":2,\"1155\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1176\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1186\":2,\"1187\":1,\"1188\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1200\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1204\":1,\"1205\":1,\"1206\":1,\"1207\":1,\"1209\":1,\"1210\":2,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1220\":1,\"1222\":2,\"1224\":1,\"1225\":1,\"1226\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1249\":1,\"1251\":1,\"1252\":2,\"1253\":2,\"1254\":2,\"1255\":1,\"1256\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":2,\"1273\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1282\":2,\"1284\":1,\"1286\":4,\"1287\":1,\"1319\":2,\"1327\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1382\":2,\"1384\":1,\"1386\":1,\"1388\":2,\"1390\":2,\"1393\":1,\"1394\":2,\"1396\":2,\"1398\":2,\"1399\":2,\"1401\":2,\"1403\":2,\"1405\":2,\"1407\":2,\"1409\":2,\"1411\":2,\"1413\":2,\"1415\":2,\"1430\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":2,\"1439\":1,\"1441\":1,\"1443\":2,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1452\":2,\"1454\":2,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1505\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1514\":2,\"1515\":1,\"1516\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1522\":1,\"1523\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1534\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1555\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":2,\"1571\":1,\"1572\":1,\"1573\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1585\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":2,\"1620\":1,\"1622\":1,\"1623\":2,\"1624\":1,\"1626\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1637\":1,\"1638\":2,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":2,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1718\":1,\"1719\":1,\"1760\":2,\"1761\":1,\"1763\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1774\":1,\"1776\":1,\"1777\":1,\"1778\":2,\"1779\":1,\"1781\":1,\"1782\":1,\"1784\":1,\"1785\":2,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1806\":1,\"1808\":1,\"1828\":2,\"1829\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":2,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":2,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":2,\"1902\":1,\"1904\":1,\"1905\":1,\"1906\":2,\"1907\":1,\"1909\":1,\"1910\":1,\"1911\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1951\":4,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1961\":1,\"1970\":1,\"1971\":2,\"1972\":1,\"1974\":1,\"1975\":1,\"1976\":2,\"1978\":2,\"1980\":2,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2001\":2,\"2002\":1,\"2003\":1,\"2004\":2,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":3,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":2,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":2,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":2,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":2,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":2,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":2,\"2078\":2,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2095\":1,\"2096\":14,\"2097\":7,\"2098\":11,\"2099\":7,\"2100\":12,\"2101\":29,\"2102\":12,\"2103\":15,\"2104\":12,\"2105\":11,\"2106\":1,\"2107\":7,\"2108\":12,\"2109\":19,\"2110\":14,\"2111\":15,\"2112\":18,\"2113\":14,\"2114\":13,\"2115\":11,\"2116\":12,\"2117\":9,\"2118\":10,\"2119\":1,\"2120\":1,\"2121\":2,\"2122\":2,\"2123\":1,\"2124\":1,\"2125\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2133\":1,\"2135\":1,\"2136\":1,\"2148\":2,\"2149\":1,\"2167\":1,\"2168\":7,\"2170\":5,\"2171\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2176\":10,\"2177\":2,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":2,\"2184\":1,\"2185\":1,\"2186\":1,\"2187\":1,\"2188\":2,\"2189\":2,\"2191\":1,\"2192\":1,\"2193\":2,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":2,\"2200\":1,\"2201\":3,\"2202\":1,\"2203\":1,\"2204\":1,\"2205\":1,\"2233\":1,\"2235\":2,\"2236\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2253\":1,\"2254\":2,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2275\":1,\"2277\":2,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":2,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2309\":3,\"2312\":1,\"2313\":2,\"2315\":1,\"2316\":1,\"2317\":1,\"2345\":1,\"2346\":1,\"2350\":2,\"2389\":1,\"2390\":1,\"2394\":3,\"2408\":1,\"2410\":1,\"2422\":2,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":2,\"2526\":1,\"2530\":3,\"2543\":1,\"2545\":2}}],[\"clamp\",{\"1\":{\"1142\":5,\"1186\":5,\"1202\":1,\"1210\":5,\"1211\":5,\"1224\":5,\"1286\":1,\"1287\":1,\"1301\":5,\"1304\":5,\"1336\":1,\"1337\":5,\"1348\":1,\"1349\":5,\"1350\":5,\"1639\":5,\"1640\":6}}],[\"cli\",{\"0\":{\"980\":1,\"982\":1,\"983\":1,\"985\":1,\"986\":1,\"990\":1,\"991\":1,\"993\":1,\"994\":1,\"1002\":1,\"1013\":1,\"1015\":1,\"1020\":1,\"1023\":1,\"1027\":1,\"1029\":1,\"1038\":1},\"1\":{\"672\":1,\"980\":2,\"982\":2,\"983\":2,\"985\":2,\"986\":2,\"990\":2,\"991\":2,\"993\":2,\"994\":2,\"1002\":1,\"1013\":1,\"1015\":1,\"1020\":1,\"1023\":1,\"1027\":2,\"1029\":2,\"1038\":2}}],[\"clipped\",{\"1\":{\"1925\":3}}],[\"clipping\",{\"0\":{\"1925\":1},\"1\":{\"627\":1,\"976\":1,\"1043\":1,\"1925\":3}}],[\"clip\",{\"1\":{\"174\":3,\"251\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"627\":2,\"976\":2,\"1043\":2,\"2186\":2,\"2202\":4,\"2204\":2,\"2584\":1}}],[\"click\",{\"1\":{\"134\":1}}],[\"clustering\",{\"1\":{\"1528\":3,\"1529\":3,\"1568\":3}}],[\"cluster\",{\"1\":{\"144\":1,\"2372\":2,\"2429\":2,\"2554\":2}}],[\"cloning\",{\"1\":{\"2512\":1,\"2585\":1,\"2657\":1}}],[\"clone\",{\"1\":{\"134\":2,\"135\":2,\"167\":3,\"178\":3,\"196\":3,\"200\":1,\"207\":1,\"234\":3,\"2359\":1,\"2372\":1,\"2383\":2,\"2384\":1,\"2393\":2,\"2409\":1,\"2427\":2,\"2429\":1,\"2450\":3,\"2456\":1,\"2466\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2504\":1,\"2517\":3,\"2521\":1,\"2529\":2,\"2550\":2,\"2580\":1,\"2584\":1,\"2585\":1,\"2646\":1,\"2648\":1,\"2649\":1}}],[\"closed\",{\"1\":{\"2198\":1}}],[\"close\",{\"1\":{\"762\":1,\"763\":2,\"980\":1,\"989\":1,\"1383\":1,\"1397\":1,\"1404\":1,\"1408\":1,\"1412\":1,\"1416\":1,\"1961\":1,\"2199\":1,\"2410\":1}}],[\"cloud\",{\"0\":{\"126\":1}}],[\"clstm\",{\"1\":{\"1516\":2}}],[\"cls\",{\"1\":{\"56\":6,\"59\":2,\"60\":3,\"675\":1,\"1452\":2,\"1620\":1,\"2096\":2,\"2097\":6,\"2098\":2,\"2099\":5,\"2100\":2,\"2101\":2,\"2102\":3,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2168\":1,\"2170\":1,\"2201\":2}}],[\"c\",{\"0\":{\"212\":1,\"216\":1},\"1\":{\"53\":2,\"135\":2,\"136\":5,\"137\":1,\"204\":2,\"217\":6,\"218\":2,\"224\":6,\"225\":2,\"231\":24,\"232\":2,\"286\":1,\"296\":4,\"690\":3,\"729\":2,\"730\":3,\"782\":3,\"806\":1,\"830\":2,\"885\":6,\"887\":4,\"944\":1,\"1001\":1,\"1011\":2,\"1073\":1,\"1198\":6,\"1221\":2,\"1241\":1,\"1245\":5,\"1247\":2,\"1248\":8,\"1255\":3,\"1256\":1,\"1270\":1,\"1381\":2,\"1389\":1,\"1395\":1,\"1397\":1,\"1406\":1,\"1414\":1,\"1416\":1,\"1430\":2,\"1455\":3,\"1462\":1,\"1463\":2,\"1466\":1,\"1470\":2,\"1471\":2,\"1478\":2,\"1523\":1,\"1524\":19,\"1525\":6,\"1545\":2,\"1566\":2,\"1567\":2,\"1569\":2,\"1571\":2,\"1572\":2,\"1576\":2,\"1577\":2,\"1578\":2,\"1579\":2,\"1580\":2,\"1595\":3,\"1598\":1,\"1604\":1,\"1643\":1,\"1644\":1,\"1652\":1,\"1654\":1,\"1655\":2,\"1664\":3,\"1666\":2,\"1668\":2,\"1671\":2,\"1680\":3,\"1683\":2,\"1685\":1,\"1688\":4,\"1693\":5,\"1695\":6,\"1696\":6,\"1697\":6,\"1698\":6,\"1701\":5,\"1702\":5,\"1703\":6,\"1704\":6,\"1705\":4,\"1706\":6,\"1707\":8,\"1708\":6,\"1710\":1,\"1711\":4,\"1712\":6,\"1713\":6,\"1715\":6,\"1717\":1,\"1719\":6,\"1722\":1,\"1723\":1,\"1724\":1,\"1736\":2,\"1737\":1,\"1739\":3,\"1746\":4,\"1747\":1,\"1748\":1,\"1749\":1,\"1755\":5,\"1756\":4,\"1758\":2,\"1759\":2,\"1765\":2,\"1767\":2,\"1768\":3,\"1778\":1,\"1800\":2,\"1803\":2,\"1805\":2,\"1808\":10,\"1834\":4,\"1844\":4,\"1848\":1,\"1852\":1,\"1857\":4,\"1862\":6,\"1867\":2,\"1869\":2,\"1871\":4,\"1872\":2,\"1873\":2,\"1876\":4,\"1880\":2,\"1884\":2,\"1885\":2,\"1917\":2,\"2063\":5,\"2074\":5,\"2075\":4,\"2079\":1,\"2267\":1,\"2311\":2,\"2314\":5,\"2321\":1,\"2387\":1,\"2432\":1,\"2518\":1,\"2568\":1}}],[\"c|\",{\"1\":{\"52\":1}}],[\"c21c3476c024ad6d56d5f48b0bca92be\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"c2\",{\"1\":{\"40\":1,\"1391\":1}}],[\"cropped\",{\"1\":{\"1808\":3}}],[\"crop\",{\"1\":{\"1808\":2,\"2188\":2,\"2266\":1}}],[\"crossentropy\",{\"1\":{\"2280\":1}}],[\"cross\",{\"0\":{\"1010\":1},\"1\":{\"22\":1,\"118\":1,\"173\":1,\"605\":1,\"822\":1,\"887\":1,\"1010\":1,\"1567\":1,\"1711\":1,\"2000\":1}}],[\"crnseparator\",{\"0\":{\"1523\":1},\"1\":{\"1523\":1}}],[\"crn\",{\"0\":{\"1522\":2,\"1523\":1,\"1545\":1,\"1572\":1,\"1576\":1,\"1577\":1},\"1\":{\"1516\":1,\"1522\":6,\"1523\":3,\"1545\":1,\"1572\":2,\"1576\":2,\"1577\":1}}],[\"crpblock\",{\"0\":{\"1468\":1},\"1\":{\"1468\":1}}],[\"critics\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"critical\",{\"1\":{\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"478\":1,\"485\":1,\"491\":1,\"1904\":1}}],[\"criteria\",{\"1\":{\"1466\":1}}],[\"criterions\",{\"0\":{\"1437\":1,\"1466\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1667\":1,\"1668\":1},\"1\":{\"1437\":1,\"1466\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1719\":1,\"1962\":1}}],[\"criterion\",{\"1\":{\"20\":1,\"56\":1,\"150\":1,\"251\":3,\"253\":2,\"255\":3,\"259\":3,\"265\":2,\"269\":2,\"1218\":1,\"1530\":1,\"1551\":1,\"1553\":1,\"1563\":1,\"1600\":2,\"1603\":2,\"1622\":2,\"1890\":1,\"1962\":2,\"1996\":1,\"2186\":3,\"2202\":6,\"2204\":3,\"2440\":2,\"2558\":2,\"2584\":1}}],[\"creo\",{\"1\":{\"2457\":1}}],[\"crelu\",{\"1\":{\"1611\":1}}],[\"creation\",{\"1\":{\"2375\":1,\"2558\":1}}],[\"creating\",{\"0\":{\"2636\":1},\"1\":{\"276\":1,\"281\":1,\"1028\":1,\"2431\":1,\"2618\":1,\"2635\":1}}],[\"create\",{\"0\":{\"172\":1,\"635\":1,\"866\":1,\"1009\":1,\"1810\":1,\"1924\":1,\"1930\":2,\"1931\":2,\"1932\":2,\"1933\":2,\"1934\":2,\"1937\":1,\"1944\":1,\"2343\":1,\"2566\":1,\"2569\":1,\"2570\":1,\"2637\":1,\"2642\":1,\"2644\":1,\"2645\":1},\"1\":{\"51\":1,\"52\":1,\"53\":1,\"54\":1,\"84\":1,\"99\":1,\"135\":2,\"170\":2,\"172\":1,\"182\":1,\"237\":1,\"429\":2,\"582\":1,\"612\":1,\"614\":1,\"635\":2,\"640\":1,\"642\":1,\"694\":1,\"712\":4,\"725\":2,\"759\":1,\"765\":1,\"798\":1,\"806\":2,\"807\":1,\"824\":2,\"859\":1,\"860\":1,\"862\":1,\"866\":2,\"920\":1,\"921\":1,\"924\":1,\"987\":1,\"989\":1,\"999\":1,\"1009\":1,\"1015\":1,\"1046\":2,\"1048\":1,\"1052\":4,\"1066\":2,\"1073\":2,\"1075\":2,\"1083\":2,\"1101\":1,\"1102\":1,\"1162\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1270\":2,\"1279\":1,\"1351\":1,\"1371\":1,\"1382\":1,\"1516\":2,\"1570\":1,\"1638\":2,\"1716\":1,\"1810\":3,\"1924\":1,\"1930\":3,\"1931\":2,\"1932\":3,\"1933\":3,\"1934\":3,\"1937\":1,\"1944\":1,\"2186\":1,\"2202\":2,\"2204\":1,\"2343\":3,\"2344\":1,\"2347\":1,\"2391\":1,\"2401\":1,\"2423\":1,\"2440\":1,\"2441\":1,\"2492\":1,\"2527\":1,\"2537\":1,\"2546\":1,\"2558\":2,\"2564\":2,\"2566\":2,\"2568\":2,\"2569\":1,\"2628\":1,\"2637\":1,\"2639\":1,\"2642\":1}}],[\"creates\",{\"1\":{\"26\":2,\"74\":1,\"75\":1,\"76\":1,\"182\":1,\"237\":1,\"239\":1,\"1895\":1,\"1896\":1,\"1900\":1}}],[\"created\",{\"1\":{\"3\":1,\"59\":1,\"237\":2,\"239\":2,\"754\":1,\"820\":1,\"821\":1,\"826\":1,\"1015\":1,\"1248\":1,\"1383\":1,\"2430\":1,\"2555\":1,\"2618\":1}}],[\"cretead\",{\"1\":{\"236\":1}}],[\"ce\",{\"1\":{\"768\":1}}],[\"cepstral\",{\"1\":{\"506\":1}}],[\"cell\",{\"1\":{\"167\":1,\"204\":1,\"837\":2,\"838\":5,\"1430\":2,\"1598\":2,\"1652\":2,\"1654\":2,\"1670\":2,\"2368\":1,\"2486\":1,\"2490\":1,\"2572\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"cells\",{\"1\":{\"116\":1,\"209\":1,\"213\":1}}],[\"cer\",{\"0\":{\"2445\":1},\"1\":{\"113\":2,\"150\":1,\"251\":1,\"259\":1,\"499\":2,\"811\":2,\"825\":2,\"1057\":3,\"1059\":7,\"1171\":1,\"1173\":7,\"1206\":1,\"1892\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":1,\"2375\":1,\"2440\":1,\"2441\":1,\"2559\":1,\"2564\":2,\"2572\":1}}],[\"certify\",{\"0\":{\"1293\":1},\"1\":{\"1293\":1}}],[\"certificate\",{\"1\":{\"70\":1}}],[\"certain\",{\"1\":{\"5\":1,\"14\":1,\"1254\":1,\"2543\":1}}],[\"central\",{\"1\":{\"2387\":1}}],[\"centered\",{\"1\":{\"2211\":1}}],[\"centered=true\",{\"1\":{\"1605\":1}}],[\"centers\",{\"1\":{\"1001\":1,\"2040\":1}}],[\"center\",{\"1\":{\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"1001\":1,\"1158\":1,\"1373\":1,\"1643\":1,\"1644\":1,\"1859\":3,\"1905\":1,\"1910\":1,\"1918\":1,\"1921\":3,\"1922\":3,\"1936\":3,\"1987\":1,\"1989\":1,\"1991\":1,\"2040\":1,\"2084\":1,\"2089\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1}}],[\"center=true\",{\"1\":{\"945\":1,\"953\":1,\"966\":1,\"970\":1}}],[\"centos\",{\"1\":{\"132\":2,\"134\":1}}],[\"centos7\",{\"1\":{\"45\":1,\"133\":1}}],[\"cen1\",{\"1\":{\"240\":1}}],[\"census\",{\"1\":{\"16\":1}}],[\"cmnd\",{\"1\":{\"2268\":1}}],[\"cmndfs\",{\"1\":{\"2267\":3}}],[\"cmndf\",{\"1\":{\"2267\":1}}],[\"cmap\",{\"1\":{\"648\":1}}],[\"cmap=\",{\"1\":{\"648\":1}}],[\"cmake\",{\"1\":{\"132\":1,\"136\":1,\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2372\":1}}],[\"cmake3\",{\"1\":{\"132\":1}}],[\"cmvnark\",{\"1\":{\"278\":1}}],[\"cmvn\",{\"0\":{\"496\":1,\"506\":1,\"941\":2,\"960\":1},\"1\":{\"238\":11,\"285\":3,\"286\":4,\"296\":4,\"496\":2,\"506\":2,\"941\":4,\"959\":2,\"960\":2}}],[\"cmu\",{\"0\":{\"161\":1,\"162\":1,\"163\":1,\"2354\":1,\"2380\":1,\"2388\":1,\"2406\":1,\"2421\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":1,\"2544\":1,\"2723\":1,\"2725\":1,\"2726\":1},\"1\":{\"16\":1,\"2372\":2,\"2380\":1,\"2385\":2,\"2387\":2,\"2388\":1,\"2406\":1,\"2421\":2,\"2429\":2,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":1,\"2544\":1,\"2554\":2,\"2583\":1,\"2618\":1}}],[\"cmd=utils\",{\"1\":{\"47\":1}}],[\"cmd\",{\"1\":{\"3\":2,\"15\":1,\"47\":3,\"85\":2,\"142\":2,\"143\":2,\"235\":1,\"275\":1,\"276\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"294\":1,\"297\":2,\"377\":2,\"2099\":1,\"2372\":1,\"2385\":1,\"2394\":1,\"2429\":1,\"2530\":1,\"2554\":1,\"2566\":1,\"2568\":1}}],[\"citi\",{\"1\":{\"2618\":1}}],[\"citation\",{\"1\":{\"729\":1,\"1524\":1}}],[\"citations\",{\"0\":{\"130\":1}}],[\"cisdrloss\",{\"0\":{\"1466\":1},\"1\":{\"1466\":1}}],[\"circleci\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1}}],[\"circle\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1}}],[\"ci\",{\"1\":{\"11\":2,\"12\":1,\"13\":1,\"126\":1,\"133\":2,\"1466\":1,\"2638\":1,\"2642\":1}}],[\"ch=wavfile\",{\"1\":{\"2592\":1}}],[\"ch=none\",{\"1\":{\"1631\":1,\"1633\":1,\"1635\":1}}],[\"chnattnstatpooling\",{\"0\":{\"2044\":1},\"1\":{\"2044\":1}}],[\"chnage\",{\"1\":{\"93\":2}}],[\"chn\",{\"0\":{\"2044\":1},\"1\":{\"2044\":1}}],[\"chnn\",{\"1\":{\"1861\":1}}],[\"chinese\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"children\",{\"1\":{\"2567\":4,\"2568\":2}}],[\"child\",{\"1\":{\"997\":1,\"1944\":1}}],[\"chime\",{\"0\":{\"2367\":1,\"2484\":1,\"2604\":1,\"2621\":1},\"1\":{\"3\":1,\"16\":1,\"940\":3,\"2368\":2,\"2481\":2,\"2486\":2,\"2605\":2,\"2618\":2,\"2622\":2}}],[\"chime5\",{\"1\":{\"3\":2}}],[\"chime3\",{\"1\":{\"1\":1,\"3\":2}}],[\"chime4\",{\"0\":{\"2369\":1,\"2487\":1,\"2491\":1,\"2606\":1,\"2610\":1,\"2623\":1,\"2627\":1},\"1\":{\"1\":2,\"3\":8,\"2367\":1,\"2485\":1,\"2604\":1,\"2621\":1}}],[\"ch\",{\"1\":{\"689\":1,\"1407\":1,\"1430\":4,\"1508\":2,\"1551\":2,\"1553\":3,\"1559\":1,\"1560\":2,\"1605\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1670\":4,\"1671\":4,\"1718\":1,\"2143\":1}}],[\"chdir\",{\"1\":{\"194\":2,\"201\":1,\"235\":1}}],[\"chmod\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2405\":1,\"2429\":1,\"2541\":1,\"2568\":1,\"2569\":1}}],[\"chuang\",{\"1\":{\"130\":1}}],[\"chunk=2048\",{\"1\":{\"2596\":1}}],[\"chunkiterfactory\",{\"0\":{\"1896\":1},\"1\":{\"1896\":1,\"1897\":1}}],[\"chunked\",{\"1\":{\"1436\":1,\"1511\":1,\"1644\":1}}],[\"chunks=3\",{\"1\":{\"1719\":1}}],[\"chunks\",{\"1\":{\"121\":4,\"692\":1,\"693\":1,\"1058\":1,\"1093\":3,\"1101\":3,\"1203\":3,\"1432\":3,\"1436\":1,\"1510\":3,\"1511\":1,\"1560\":2,\"1643\":3,\"1644\":1,\"1671\":1,\"1719\":4,\"1896\":2,\"2360\":3,\"2458\":3,\"2523\":3,\"2582\":3}}],[\"chunk\",{\"0\":{\"1101\":1,\"1896\":1},\"1\":{\"116\":3,\"120\":3,\"121\":12,\"122\":5,\"124\":1,\"327\":2,\"449\":2,\"1048\":2,\"1049\":7,\"1050\":7,\"1052\":7,\"1056\":7,\"1058\":3,\"1065\":3,\"1066\":3,\"1068\":6,\"1071\":5,\"1076\":10,\"1077\":2,\"1093\":9,\"1101\":6,\"1142\":1,\"1186\":1,\"1210\":2,\"1246\":1,\"1371\":3,\"1515\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1537\":1,\"1538\":2,\"1539\":2,\"1551\":3,\"1552\":3,\"1553\":3,\"1554\":3,\"1581\":2,\"1626\":1,\"1671\":2,\"1719\":1,\"1896\":4,\"1897\":2,\"2099\":1,\"2461\":1,\"2592\":6,\"2596\":3}}],[\"christoph\",{\"1\":{\"130\":1}}],[\"chennels\",{\"1\":{\"1867\":1}}],[\"chenda\",{\"1\":{\"130\":2,\"2366\":1,\"2371\":3,\"2601\":1,\"2612\":3,\"2618\":2,\"2630\":4}}],[\"chen\",{\"1\":{\"130\":3,\"1462\":1,\"1463\":1,\"1515\":1,\"1581\":1}}],[\"checkpint\",{\"0\":{\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2556\":1}}],[\"checkpoint3\",{\"0\":{\"2515\":1}}],[\"checkpoint2\",{\"1\":{\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2459\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1}}],[\"checkpoint1\",{\"1\":{\"2398\":1,\"2457\":1,\"2534\":1}}],[\"checkpoints\",{\"0\":{\"499\":1},\"1\":{\"499\":2,\"2389\":1,\"2408\":1,\"2422\":1,\"2440\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1,\"2558\":1,\"2560\":1}}],[\"checkpoint\",{\"0\":{\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2414\":1,\"2418\":1,\"2419\":1,\"2420\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2462\":1,\"2471\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2479\":1,\"2485\":1,\"2490\":1,\"2497\":1,\"2500\":1,\"2501\":1,\"2508\":1,\"2510\":1,\"2554\":1,\"2559\":1,\"2560\":1},\"1\":{\"65\":2,\"69\":1,\"80\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"282\":1,\"455\":2,\"1181\":1,\"2201\":1,\"2394\":7,\"2398\":1,\"2400\":1,\"2401\":2,\"2403\":1,\"2405\":1,\"2414\":1,\"2418\":1,\"2419\":1,\"2420\":1,\"2461\":1,\"2462\":1,\"2520\":2,\"2530\":7,\"2534\":1,\"2536\":1,\"2537\":2,\"2539\":1,\"2541\":1,\"2554\":1,\"2556\":1,\"2559\":1,\"2560\":1}}],[\"checks\",{\"1\":{\"745\":1,\"1007\":1}}],[\"checkout\",{\"1\":{\"200\":1,\"207\":1,\"2529\":1,\"2550\":1,\"2585\":1}}],[\"checking\",{\"1\":{\"14\":1,\"59\":2,\"745\":1,\"2309\":1,\"2373\":1,\"2385\":1,\"2430\":1,\"2555\":1}}],[\"check\",{\"0\":{\"11\":1,\"137\":1,\"203\":1,\"863\":1,\"864\":1,\"865\":1,\"1007\":1,\"1008\":2,\"1095\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1380\":1,\"1682\":1,\"1924\":1,\"2351\":1,\"2426\":1,\"2432\":1,\"2549\":1,\"2553\":1},\"1\":{\"11\":2,\"45\":1,\"49\":1,\"59\":1,\"106\":1,\"128\":2,\"133\":1,\"137\":3,\"146\":1,\"147\":1,\"148\":1,\"167\":3,\"171\":1,\"178\":3,\"196\":2,\"234\":3,\"235\":2,\"237\":1,\"240\":1,\"725\":2,\"745\":1,\"806\":1,\"863\":2,\"864\":2,\"865\":2,\"895\":1,\"933\":1,\"1007\":1,\"1008\":5,\"1095\":2,\"1270\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1380\":1,\"1682\":2,\"1924\":2,\"2096\":2,\"2098\":2,\"2099\":5,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2176\":1,\"2193\":1,\"2351\":2,\"2355\":1,\"2372\":6,\"2373\":4,\"2375\":1,\"2377\":2,\"2379\":1,\"2384\":1,\"2385\":1,\"2387\":2,\"2394\":1,\"2411\":1,\"2414\":2,\"2426\":1,\"2429\":3,\"2430\":4,\"2432\":1,\"2436\":2,\"2530\":1,\"2549\":1,\"2552\":1,\"2554\":4,\"2555\":5,\"2558\":1,\"2559\":1,\"2562\":2,\"2572\":1,\"2574\":1,\"2638\":1}}],[\"cholesky\",{\"1\":{\"1695\":2}}],[\"choi\",{\"1\":{\"1660\":2,\"1661\":2,\"1662\":2}}],[\"choice\",{\"1\":{\"1198\":4}}],[\"choices=none\",{\"1\":{\"2313\":1}}],[\"choices\",{\"0\":{\"2176\":1},\"1\":{\"119\":1,\"1028\":2,\"2096\":11,\"2097\":6,\"2098\":8,\"2099\":2,\"2100\":9,\"2101\":26,\"2102\":8,\"2103\":12,\"2104\":9,\"2105\":8,\"2107\":4,\"2108\":9,\"2109\":16,\"2110\":11,\"2111\":12,\"2112\":15,\"2113\":11,\"2114\":10,\"2115\":8,\"2116\":9,\"2117\":6,\"2118\":7,\"2176\":6,\"2398\":1,\"2431\":3,\"2534\":1,\"2643\":3}}],[\"chose\",{\"0\":{\"1381\":1},\"1\":{\"1381\":2}}],[\"chosen\",{\"1\":{\"905\":1,\"1269\":1}}],[\"chomp\",{\"1\":{\"1369\":1,\"1473\":1}}],[\"chomp1d\",{\"0\":{\"1369\":1,\"1473\":1},\"1\":{\"1369\":2,\"1473\":2}}],[\"choose\",{\"0\":{\"1683\":1},\"1\":{\"49\":1,\"1203\":1,\"1243\":1,\"1269\":2,\"1377\":1,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1683\":2,\"1719\":1,\"1811\":1,\"2357\":5,\"2363\":3,\"2371\":1,\"2398\":1,\"2534\":1,\"2578\":5,\"2612\":1,\"2630\":1,\"2653\":3}}],[\"ch1\",{\"1\":{\"54\":1,\"1408\":1}}],[\"ch0\",{\"1\":{\"54\":1,\"1408\":1}}],[\"chapter\",{\"1\":{\"1705\":1,\"2600\":1}}],[\"challenge\",{\"1\":{\"1604\":1,\"1655\":1,\"1719\":1,\"2396\":1,\"2532\":1}}],[\"chain\",{\"1\":{\"605\":2,\"606\":1,\"609\":1,\"611\":1,\"614\":1,\"615\":1,\"634\":1,\"680\":1,\"683\":1,\"703\":1,\"709\":1,\"717\":1,\"731\":1,\"732\":1,\"747\":1,\"748\":1,\"768\":1,\"777\":1,\"784\":1,\"791\":1,\"800\":1,\"801\":1,\"805\":1,\"808\":1,\"811\":1,\"833\":1,\"855\":1,\"870\":1,\"975\":2,\"1042\":2}}],[\"chainerdataloader\",{\"0\":{\"981\":1},\"1\":{\"981\":2}}],[\"chainerasrinterface\",{\"0\":{\"709\":1},\"1\":{\"709\":1,\"742\":1}}],[\"chainerscheduler\",{\"0\":{\"667\":1},\"1\":{\"667\":2}}],[\"chainer|pytorch\",{\"1\":{\"286\":1}}],[\"chainer\",{\"0\":{\"31\":1,\"604\":1,\"605\":1,\"606\":1,\"607\":1,\"609\":1,\"611\":1,\"614\":1,\"622\":1,\"634\":1,\"649\":1,\"657\":1,\"667\":1,\"680\":1,\"683\":1,\"703\":1,\"709\":1,\"717\":1,\"724\":1,\"727\":1,\"728\":1,\"731\":1,\"732\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"768\":1,\"769\":1,\"777\":1,\"784\":1,\"791\":1,\"800\":1,\"801\":1,\"805\":1,\"808\":1,\"833\":1,\"834\":1,\"855\":1,\"867\":1,\"870\":1,\"875\":1,\"878\":1,\"898\":1,\"923\":1,\"1032\":1},\"1\":{\"11\":1,\"16\":3,\"31\":1,\"84\":4,\"247\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"286\":1,\"604\":4,\"605\":2,\"606\":2,\"607\":4,\"609\":1,\"611\":1,\"612\":1,\"614\":3,\"615\":1,\"622\":2,\"626\":1,\"627\":1,\"628\":3,\"629\":1,\"634\":5,\"649\":2,\"654\":2,\"657\":2,\"659\":1,\"660\":1,\"661\":2,\"662\":1,\"665\":1,\"667\":3,\"676\":9,\"680\":1,\"683\":1,\"703\":3,\"709\":4,\"717\":2,\"724\":1,\"727\":4,\"728\":4,\"731\":2,\"732\":4,\"742\":8,\"743\":1,\"745\":1,\"746\":1,\"747\":6,\"748\":1,\"754\":1,\"768\":3,\"769\":1,\"777\":2,\"781\":7,\"784\":5,\"791\":1,\"799\":1,\"800\":1,\"801\":1,\"805\":1,\"808\":1,\"811\":1,\"820\":1,\"821\":1,\"826\":1,\"833\":1,\"834\":1,\"855\":2,\"867\":2,\"870\":3,\"872\":1,\"873\":1,\"874\":1,\"875\":4,\"878\":2,\"898\":1,\"923\":1,\"975\":2,\"976\":1,\"981\":2,\"1032\":2,\"1042\":2,\"1043\":1}}],[\"chars\",{\"1\":{\"2126\":1}}],[\"charseq\",{\"1\":{\"217\":3,\"224\":2}}],[\"chartokenizer\",{\"0\":{\"2120\":1},\"1\":{\"2120\":2}}],[\"characteristics\",{\"1\":{\"2514\":1,\"2659\":1}}],[\"character\",{\"1\":{\"274\":1,\"676\":2,\"732\":1,\"735\":1,\"742\":2,\"747\":1,\"754\":2,\"781\":1,\"825\":2,\"826\":3,\"1057\":1,\"1059\":7,\"1145\":2,\"1173\":7,\"1778\":1,\"1804\":2,\"1805\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2083\":2,\"2086\":2,\"2087\":2,\"2090\":1,\"2095\":1,\"2243\":1,\"2263\":1,\"2264\":2,\"2349\":3,\"2373\":1,\"2375\":1,\"2414\":2,\"2559\":1,\"2564\":1}}],[\"characters\",{\"1\":{\"84\":1,\"600\":1,\"601\":1,\"633\":1,\"647\":1,\"676\":2,\"742\":1,\"754\":2,\"781\":2,\"812\":2,\"820\":1,\"821\":1,\"826\":3,\"2002\":2,\"2079\":1,\"2083\":1,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":3,\"2264\":2,\"2279\":2,\"2347\":3,\"2433\":1,\"2555\":1}}],[\"char\",{\"0\":{\"2120\":1},\"1\":{\"173\":1,\"194\":1,\"217\":5,\"224\":5,\"231\":4,\"285\":1,\"295\":10,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"422\":1,\"443\":2,\"449\":1,\"461\":1,\"478\":1,\"579\":1,\"600\":2,\"601\":2,\"633\":2,\"647\":2,\"676\":4,\"742\":2,\"750\":2,\"774\":2,\"781\":4,\"812\":4,\"1059\":11,\"1173\":11,\"2120\":2,\"2357\":4,\"2414\":1,\"2500\":4,\"2578\":4,\"2589\":1,\"2617\":1,\"2635\":1}}],[\"chan\",{\"1\":{\"1518\":2,\"1520\":2,\"1656\":4,\"1752\":4}}],[\"chans=32\",{\"1\":{\"1017\":1,\"1543\":1,\"1655\":1,\"1719\":1}}],[\"chans=384\",{\"1\":{\"736\":1}}],[\"chans=512\",{\"1\":{\"802\":1,\"2078\":1,\"2083\":1}}],[\"chans=256\",{\"1\":{\"701\":1}}],[\"chans=128\",{\"1\":{\"701\":1}}],[\"chans=100\",{\"1\":{\"689\":1}}],[\"chans\",{\"1\":{\"679\":2,\"681\":2,\"682\":2,\"683\":2,\"684\":2,\"685\":2,\"688\":2,\"689\":3,\"701\":2,\"713\":6,\"737\":1,\"754\":1,\"758\":2,\"786\":6,\"821\":5,\"826\":2,\"892\":2,\"1220\":2,\"1289\":2,\"1543\":1,\"1655\":3,\"1719\":3,\"1778\":2,\"1850\":4,\"1851\":8,\"1852\":5,\"2002\":4,\"2003\":1,\"2078\":1,\"2083\":1,\"2086\":4,\"2087\":6,\"2090\":4,\"2095\":8,\"2243\":6,\"2244\":10,\"2255\":10,\"2257\":2,\"2261\":2,\"2263\":8,\"2264\":6,\"2265\":2,\"2279\":8}}],[\"channles\",{\"1\":{\"1872\":2,\"1873\":2}}],[\"channnels\",{\"1\":{\"49\":1}}],[\"channeltac\",{\"0\":{\"1471\":1},\"1\":{\"1471\":3}}],[\"channelattention\",{\"0\":{\"1470\":1},\"1\":{\"1470\":3}}],[\"channelwiselayernorm\",{\"0\":{\"1368\":1,\"1472\":1},\"1\":{\"1368\":1,\"1472\":1}}],[\"channel=4\",{\"1\":{\"1580\":1,\"1662\":1,\"2368\":1,\"2486\":1,\"2490\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"channel=none\",{\"1\":{\"1430\":1,\"1470\":1,\"1471\":1,\"1664\":1,\"1665\":1,\"1670\":1,\"2371\":1,\"2494\":1,\"2612\":1,\"2630\":1}}],[\"channel=0\",{\"1\":{\"942\":1}}],[\"channel=\",{\"1\":{\"942\":1,\"1660\":1}}],[\"channel=3\",{\"1\":{\"890\":1}}],[\"channel=16\",{\"1\":{\"1462\":1}}],[\"channel=128\",{\"1\":{\"890\":1,\"2063\":1,\"2074\":1,\"2075\":1}}],[\"channel=1\",{\"1\":{\"833\":1}}],[\"channel\",{\"0\":{\"942\":1,\"2367\":1,\"2369\":1,\"2484\":1,\"2487\":1,\"2488\":1,\"2491\":1,\"2604\":1,\"2606\":1,\"2608\":1,\"2610\":1,\"2621\":1,\"2623\":1,\"2625\":1,\"2627\":1},\"1\":{\"47\":2,\"52\":7,\"251\":2,\"350\":2,\"357\":2,\"363\":2,\"368\":2,\"568\":1,\"729\":2,\"730\":1,\"756\":1,\"833\":1,\"887\":1,\"890\":4,\"942\":2,\"1062\":1,\"1081\":2,\"1086\":4,\"1115\":12,\"1158\":1,\"1180\":4,\"1198\":1,\"1239\":1,\"1269\":18,\"1282\":1,\"1368\":3,\"1372\":2,\"1377\":1,\"1381\":2,\"1406\":1,\"1407\":2,\"1430\":8,\"1432\":1,\"1456\":1,\"1458\":1,\"1462\":1,\"1463\":3,\"1466\":1,\"1470\":3,\"1471\":3,\"1472\":3,\"1510\":2,\"1511\":2,\"1523\":2,\"1524\":2,\"1525\":1,\"1528\":1,\"1551\":1,\"1553\":1,\"1575\":2,\"1604\":1,\"1611\":9,\"1643\":1,\"1655\":2,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":2,\"1664\":1,\"1665\":1,\"1670\":6,\"1671\":9,\"1683\":2,\"1708\":1,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":1,\"1715\":2,\"1719\":7,\"1791\":1,\"1905\":1,\"1927\":1,\"2044\":1,\"2049\":1,\"2059\":1,\"2184\":2,\"2200\":2,\"2210\":1,\"2369\":1,\"2481\":2,\"2485\":1,\"2487\":1,\"2490\":1,\"2606\":1,\"2618\":2,\"2623\":1}}],[\"channels=channels\",{\"1\":{\"2596\":1}}],[\"channels=\",{\"1\":{\"1786\":1}}],[\"channels=512\",{\"1\":{\"1766\":1,\"1800\":1}}],[\"channels=384\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"channels=2\",{\"1\":{\"1522\":1}}],[\"channels=80\",{\"1\":{\"1800\":1}}],[\"channels=8\",{\"1\":{\"1522\":1,\"1545\":1}}],[\"channels=128\",{\"1\":{\"1464\":1,\"1594\":1}}],[\"channels=1\",{\"1\":{\"1241\":1,\"1245\":2,\"1655\":1,\"1719\":1,\"1800\":1,\"2596\":1}}],[\"channels=0\",{\"1\":{\"47\":1,\"1772\":1,\"1806\":1,\"1808\":1}}],[\"channelselector\",{\"0\":{\"942\":1},\"1\":{\"942\":1}}],[\"channels\",{\"0\":{\"54\":1},\"1\":{\"21\":4,\"47\":3,\"48\":1,\"52\":1,\"54\":2,\"115\":2,\"679\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"688\":1,\"689\":1,\"701\":2,\"708\":4,\"712\":2,\"713\":2,\"717\":1,\"723\":5,\"737\":1,\"754\":1,\"758\":1,\"786\":2,\"802\":1,\"821\":5,\"826\":2,\"833\":1,\"835\":2,\"892\":1,\"1011\":2,\"1025\":2,\"1051\":3,\"1052\":2,\"1054\":1,\"1055\":1,\"1198\":14,\"1241\":1,\"1245\":2,\"1248\":1,\"1255\":3,\"1256\":1,\"1370\":2,\"1375\":1,\"1378\":2,\"1379\":2,\"1430\":1,\"1456\":1,\"1458\":1,\"1463\":2,\"1478\":4,\"1480\":4,\"1506\":2,\"1522\":10,\"1523\":8,\"1543\":8,\"1545\":8,\"1546\":3,\"1549\":1,\"1551\":3,\"1553\":2,\"1554\":3,\"1558\":1,\"1559\":1,\"1560\":1,\"1564\":3,\"1576\":6,\"1577\":6,\"1578\":1,\"1579\":1,\"1580\":1,\"1655\":6,\"1656\":2,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":3,\"1663\":3,\"1664\":4,\"1665\":4,\"1670\":1,\"1671\":3,\"1672\":1,\"1718\":1,\"1719\":7,\"1752\":1,\"1763\":1,\"1765\":15,\"1766\":7,\"1769\":3,\"1771\":6,\"1772\":10,\"1776\":5,\"1777\":4,\"1778\":13,\"1779\":1,\"1782\":2,\"1786\":2,\"1787\":3,\"1788\":8,\"1789\":2,\"1800\":13,\"1801\":9,\"1803\":15,\"1804\":15,\"1805\":16,\"1806\":1,\"1807\":1,\"1808\":6,\"1830\":2,\"1831\":4,\"1832\":2,\"1833\":9,\"1834\":3,\"1835\":6,\"1838\":5,\"1840\":2,\"1844\":18,\"1845\":4,\"1846\":4,\"1847\":8,\"1848\":13,\"1849\":12,\"1850\":14,\"1851\":15,\"1852\":13,\"1855\":2,\"1856\":12,\"1857\":12,\"1858\":12,\"1861\":9,\"1862\":20,\"1863\":17,\"1864\":12,\"1865\":12,\"1866\":5,\"1867\":4,\"1868\":8,\"1870\":3,\"1871\":17,\"1872\":8,\"1873\":8,\"1877\":14,\"1878\":14,\"1880\":26,\"1904\":3,\"1916\":3,\"1917\":7,\"1918\":2,\"2002\":2,\"2061\":1,\"2078\":1,\"2086\":2,\"2087\":3,\"2090\":2,\"2095\":4,\"2243\":3,\"2244\":5,\"2255\":6,\"2257\":1,\"2258\":3,\"2259\":3,\"2260\":3,\"2261\":1,\"2263\":4,\"2264\":3,\"2265\":1,\"2279\":4,\"2290\":1,\"2367\":1,\"2485\":1,\"2604\":1,\"2621\":1}}],[\"chang\",{\"1\":{\"130\":5,\"1604\":1,\"1655\":1,\"1719\":1}}],[\"changing\",{\"0\":{\"25\":1,\"84\":1},\"1\":{\"1245\":1,\"2153\":1}}],[\"changes\",{\"1\":{\"613\":1,\"1946\":2,\"1947\":1}}],[\"changeable\",{\"1\":{\"62\":1}}],[\"changed\",{\"1\":{\"33\":1,\"60\":1,\"72\":2,\"95\":1,\"102\":3,\"143\":1,\"148\":1,\"150\":1,\"952\":1,\"2019\":1,\"2377\":1,\"2436\":1,\"2562\":1,\"2564\":1}}],[\"change\",{\"0\":{\"18\":1,\"61\":1,\"64\":1,\"68\":1,\"69\":1,\"73\":1,\"92\":1,\"93\":1,\"187\":1,\"503\":1,\"1965\":1,\"2376\":1,\"2435\":1,\"2561\":1},\"1\":{\"18\":1,\"25\":2,\"47\":1,\"49\":1,\"64\":1,\"72\":2,\"73\":1,\"85\":5,\"90\":1,\"91\":1,\"92\":3,\"102\":2,\"144\":2,\"187\":2,\"189\":1,\"197\":1,\"198\":1,\"240\":4,\"491\":2,\"503\":3,\"632\":1,\"927\":1,\"1138\":1,\"1171\":1,\"1206\":1,\"1241\":1,\"1552\":1,\"1927\":2,\"1953\":1,\"1955\":1,\"1965\":1,\"2019\":1,\"2128\":1,\"2129\":1,\"2151\":1,\"2153\":1,\"2166\":1,\"2179\":1,\"2378\":1,\"2395\":1,\"2412\":2,\"2423\":1,\"2430\":1,\"2437\":2,\"2441\":2,\"2514\":1,\"2531\":1,\"2542\":1,\"2543\":1,\"2546\":1,\"2555\":1,\"2558\":1,\"2563\":2,\"2659\":1}}],[\"cu117\",{\"1\":{\"2482\":1}}],[\"cuando\",{\"1\":{\"2457\":1}}],[\"cuscuta\",{\"1\":{\"2387\":1}}],[\"customparallelupdater\",{\"0\":{\"727\":1},\"1\":{\"727\":1}}],[\"customencoder\",{\"0\":{\"726\":1},\"1\":{\"726\":2}}],[\"customevaluator\",{\"0\":{\"626\":1,\"975\":1,\"1042\":1},\"1\":{\"626\":2,\"975\":1,\"1042\":1}}],[\"customdecoder\",{\"0\":{\"725\":1},\"1\":{\"700\":1,\"725\":2}}],[\"customupdater\",{\"0\":{\"627\":1,\"728\":1,\"976\":1,\"1043\":1},\"1\":{\"627\":2,\"728\":1,\"976\":1,\"1043\":2}}],[\"customconvertermulenc\",{\"0\":{\"625\":1},\"1\":{\"625\":1}}],[\"customconverter\",{\"0\":{\"599\":1,\"624\":1,\"724\":1,\"936\":1,\"974\":1,\"1041\":1},\"1\":{\"599\":2,\"624\":2,\"629\":2,\"709\":1,\"724\":1,\"727\":1,\"728\":1,\"742\":1,\"936\":3,\"974\":1,\"1041\":1}}],[\"customized\",{\"1\":{\"760\":1,\"778\":1,\"831\":1,\"1476\":1,\"1598\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2084\":1,\"2089\":1}}],[\"customize\",{\"0\":{\"60\":1},\"1\":{\"60\":2,\"607\":1,\"2099\":1,\"2378\":1,\"2437\":1,\"2563\":1}}],[\"customizable\",{\"1\":{\"21\":1}}],[\"custom\",{\"0\":{\"124\":1,\"136\":1,\"725\":1,\"726\":1,\"845\":1,\"846\":1,\"847\":1,\"868\":1,\"2600\":1},\"1\":{\"21\":11,\"30\":2,\"115\":1,\"173\":1,\"217\":2,\"599\":1,\"607\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"650\":1,\"709\":5,\"712\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"724\":1,\"725\":2,\"726\":2,\"727\":4,\"728\":4,\"742\":5,\"827\":1,\"845\":2,\"846\":2,\"847\":2,\"858\":1,\"868\":1,\"936\":1,\"974\":1,\"975\":1,\"976\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1245\":1,\"2156\":2,\"2198\":1,\"2352\":1}}],[\"cut\",{\"1\":{\"940\":1,\"1860\":1,\"1883\":1}}],[\"cutoff\",{\"1\":{\"461\":2,\"582\":2,\"1778\":1,\"1852\":1,\"1860\":3,\"1883\":2,\"1905\":2,\"1938\":3,\"1939\":3}}],[\"cumulate\",{\"1\":{\"821\":2,\"2002\":3,\"2078\":3,\"2095\":3,\"2263\":3}}],[\"cumulativemeannormalizeddifferencefunctiontorch\",{\"0\":{\"2269\":1},\"1\":{\"2269\":1}}],[\"cumulativemeannormalizeddifferencefunction\",{\"0\":{\"2268\":1},\"1\":{\"2268\":1}}],[\"cumulative\",{\"1\":{\"759\":1,\"2267\":2,\"2268\":2}}],[\"cupy\",{\"1\":{\"605\":2}}],[\"cur\",{\"1\":{\"711\":2}}],[\"curve\",{\"1\":{\"240\":5}}],[\"current\",{\"1\":{\"85\":1,\"122\":1,\"597\":1,\"616\":1,\"672\":2,\"692\":1,\"693\":1,\"710\":1,\"711\":2,\"1046\":1,\"1049\":2,\"1050\":2,\"1052\":2,\"1056\":2,\"1058\":2,\"1066\":1,\"1068\":2,\"1071\":3,\"1073\":1,\"1075\":1,\"1076\":4,\"1077\":2,\"1083\":1,\"1154\":1,\"1198\":1,\"1429\":1,\"1451\":2,\"1514\":2,\"1557\":2,\"1585\":2,\"1600\":1,\"1603\":1,\"1612\":2,\"1615\":2,\"1622\":1,\"1623\":2,\"1637\":2,\"2099\":1,\"2387\":1,\"2392\":2,\"2425\":2,\"2441\":1,\"2487\":1,\"2491\":1,\"2497\":1,\"2528\":2,\"2548\":2,\"2558\":1,\"2573\":1,\"2635\":1,\"2641\":1}}],[\"currently\",{\"1\":{\"5\":1,\"21\":1,\"27\":1,\"111\":1,\"112\":1,\"117\":1,\"148\":1,\"235\":1,\"747\":1,\"1155\":1,\"1187\":1,\"1202\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":3,\"1660\":2,\"1661\":2,\"1662\":2,\"1719\":2,\"1905\":1,\"1917\":1,\"1933\":1,\"2597\":1}}],[\"curl\",{\"1\":{\"70\":2}}],[\"cudnn=8302\",{\"1\":{\"2553\":1}}],[\"cudnnctc\",{\"1\":{\"251\":1,\"259\":1}}],[\"cudnn7\",{\"1\":{\"8\":4}}],[\"cudnn\",{\"0\":{\"435\":1,\"2160\":1},\"1\":{\"1\":1,\"82\":3,\"429\":6,\"2160\":1}}],[\"cudatk\",{\"1\":{\"134\":1}}],[\"cuda=11\",{\"1\":{\"2553\":1}}],[\"cuda=10\",{\"1\":{\"2372\":1}}],[\"cuda=no\",{\"1\":{\"134\":2}}],[\"cuda=9\",{\"1\":{\"1\":1}}],[\"cuda10\",{\"1\":{\"8\":2}}],[\"cuda\",{\"0\":{\"1143\":1,\"1186\":1,\"1194\":1,\"1210\":1,\"1226\":1,\"1227\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1,\"1344\":1,\"1346\":1},\"1\":{\"1\":3,\"5\":3,\"19\":2,\"44\":2,\"65\":1,\"71\":2,\"82\":1,\"134\":3,\"135\":4,\"136\":5,\"174\":3,\"217\":1,\"224\":1,\"231\":1,\"921\":1,\"1100\":1,\"1143\":2,\"1144\":2,\"1154\":1,\"1186\":3,\"1194\":1,\"1210\":3,\"1226\":1,\"1227\":2,\"1228\":2,\"1257\":1,\"1274\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1,\"1337\":1,\"1344\":1,\"1345\":2,\"1346\":1,\"1347\":2,\"1350\":1,\"1427\":1,\"1428\":1,\"2099\":2,\"2153\":1,\"2358\":1,\"2364\":1,\"2368\":1,\"2371\":1,\"2372\":4,\"2394\":1,\"2409\":1,\"2429\":2,\"2455\":1,\"2460\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":2,\"2530\":1,\"2552\":2,\"2553\":1,\"2579\":1,\"2584\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2617\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2635\":1,\"2654\":1,\"2658\":1}}],[\"cada\",{\"1\":{\"2457\":1}}],[\"caclualte\",{\"1\":{\"1773\":1,\"2082\":3,\"2240\":3,\"2278\":3}}],[\"caching\",{\"1\":{\"707\":1}}],[\"cache=none\",{\"1\":{\"711\":3,\"749\":1,\"1141\":1,\"1170\":1}}],[\"cacheitem\",{\"0\":{\"707\":1},\"1\":{\"707\":1}}],[\"cached\",{\"1\":{\"698\":2,\"699\":2,\"827\":1,\"1133\":2,\"1214\":1,\"1273\":1,\"2001\":1}}],[\"cache\",{\"0\":{\"1966\":1},\"1\":{\"124\":2,\"207\":1,\"429\":6,\"698\":1,\"699\":4,\"711\":4,\"725\":4,\"749\":3,\"806\":4,\"824\":4,\"827\":2,\"1048\":2,\"1049\":3,\"1050\":3,\"1051\":5,\"1052\":3,\"1054\":3,\"1055\":3,\"1056\":3,\"1058\":2,\"1068\":3,\"1071\":2,\"1133\":8,\"1141\":2,\"1170\":2,\"1214\":5,\"1270\":4,\"1273\":4,\"1778\":3,\"1805\":3,\"1829\":3,\"1850\":3,\"1852\":3,\"1877\":3,\"1896\":1,\"1966\":1,\"1968\":1,\"2001\":4,\"2106\":4,\"2182\":2,\"2222\":1,\"2396\":1,\"2450\":1,\"2517\":1,\"2599\":2}}],[\"cascading\",{\"1\":{\"2467\":1}}],[\"cascaded\",{\"1\":{\"2467\":2,\"2479\":1}}],[\"cast\",{\"1\":{\"1181\":1}}],[\"caseoftoken\",{\"1\":{\"2046\":1}}],[\"case4\",{\"0\":{\"54\":1}}],[\"case3\",{\"0\":{\"53\":1}}],[\"case2\",{\"0\":{\"52\":1}}],[\"case1\",{\"0\":{\"51\":1}}],[\"case\",{\"0\":{\"44\":1,\"50\":1,\"291\":1},\"1\":{\"5\":1,\"40\":1,\"46\":1,\"48\":1,\"69\":1,\"79\":1,\"80\":1,\"92\":1,\"144\":1,\"237\":1,\"294\":1,\"295\":1,\"597\":1,\"625\":1,\"629\":2,\"728\":2,\"745\":2,\"746\":2,\"799\":2,\"999\":2,\"1007\":1,\"1015\":1,\"1198\":1,\"1356\":1,\"1406\":1,\"1622\":1,\"1639\":2,\"2155\":1,\"2216\":1,\"2217\":1,\"2373\":1,\"2385\":1,\"2430\":1,\"2431\":1,\"2543\":1,\"2555\":1,\"2585\":1}}],[\"cases\",{\"1\":{\"5\":1,\"56\":1,\"60\":1,\"100\":1,\"294\":1,\"1133\":1,\"1204\":1,\"1214\":1,\"1216\":1,\"1244\":1,\"1245\":1,\"1273\":1,\"1639\":1,\"2050\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2372\":1,\"2429\":1,\"2508\":3,\"2554\":1,\"2584\":1,\"2585\":1}}],[\"cartesian\",{\"1\":{\"1025\":1}}],[\"care\",{\"1\":{\"745\":1,\"746\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2362\":1,\"2504\":1,\"2576\":1,\"2651\":1}}],[\"carefully\",{\"1\":{\"2564\":1}}],[\"careful\",{\"1\":{\"83\":1,\"148\":1,\"238\":1,\"1603\":1,\"1622\":1}}],[\"calcuated\",{\"1\":{\"1839\":2}}],[\"calcualate\",{\"1\":{\"1836\":1,\"1843\":1}}],[\"calcualte\",{\"1\":{\"758\":1}}],[\"calculating\",{\"1\":{\"1604\":1,\"2415\":1}}],[\"calculations\",{\"1\":{\"608\":1}}],[\"calculation\",{\"1\":{\"110\":1,\"111\":1,\"238\":1,\"702\":1,\"737\":1,\"738\":1,\"742\":1,\"754\":2,\"755\":2,\"821\":1,\"822\":2,\"826\":3,\"1155\":1,\"1551\":2,\"1553\":2,\"1688\":1,\"1752\":1,\"1756\":1,\"1839\":1,\"1851\":2,\"1879\":2,\"1951\":1,\"2086\":2,\"2087\":2,\"2088\":2,\"2090\":2,\"2091\":2,\"2095\":2,\"2170\":2,\"2198\":1,\"2243\":2,\"2244\":2,\"2245\":2,\"2255\":2,\"2256\":2,\"2259\":1,\"2263\":2,\"2264\":3,\"2279\":2,\"2280\":2,\"2644\":1}}],[\"calculator\",{\"0\":{\"735\":1,\"1059\":1,\"1173\":1,\"2239\":1},\"1\":{\"735\":3,\"1059\":1,\"1173\":1,\"2239\":2,\"2267\":1}}],[\"calculated\",{\"1\":{\"238\":1,\"737\":2,\"738\":1,\"742\":1,\"923\":1,\"1255\":1,\"1551\":1,\"1553\":1,\"1698\":1,\"1705\":1,\"1707\":1,\"1785\":1,\"1836\":2,\"2267\":3,\"2600\":1}}],[\"calculates\",{\"1\":{\"107\":1,\"762\":1,\"1701\":1,\"1702\":1,\"1710\":1,\"2267\":1}}],[\"calculate\",{\"0\":{\"501\":1,\"562\":1,\"1963\":2},\"1\":{\"107\":2,\"108\":2,\"110\":2,\"203\":1,\"235\":1,\"238\":1,\"501\":3,\"562\":3,\"628\":1,\"629\":1,\"676\":4,\"681\":1,\"682\":1,\"698\":1,\"699\":1,\"701\":1,\"702\":1,\"708\":1,\"711\":1,\"713\":1,\"735\":1,\"737\":1,\"738\":1,\"742\":1,\"750\":5,\"754\":3,\"755\":1,\"762\":1,\"763\":1,\"764\":1,\"774\":1,\"781\":2,\"786\":1,\"793\":1,\"802\":1,\"804\":1,\"820\":3,\"821\":3,\"822\":1,\"826\":5,\"830\":1,\"835\":1,\"838\":1,\"923\":1,\"925\":1,\"1059\":5,\"1140\":1,\"1145\":1,\"1148\":1,\"1169\":1,\"1173\":5,\"1203\":1,\"1703\":1,\"1713\":1,\"1714\":1,\"1765\":1,\"1773\":1,\"1798\":1,\"1800\":1,\"1803\":1,\"1804\":1,\"1829\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1851\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1876\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1956\":1,\"1963\":2,\"1980\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2049\":1,\"2054\":1,\"2077\":1,\"2078\":1,\"2081\":1,\"2083\":1,\"2086\":2,\"2087\":2,\"2088\":1,\"2090\":2,\"2091\":1,\"2095\":1,\"2235\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2252\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":2,\"2260\":4,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":2,\"2265\":1,\"2277\":1,\"2279\":1,\"2280\":1}}],[\"calc\",{\"0\":{\"380\":1},\"1\":{\"203\":2,\"380\":3,\"1113\":1,\"1171\":1,\"1172\":1,\"1206\":1,\"1371\":2,\"1551\":1,\"1552\":2,\"1553\":1,\"1554\":1,\"1892\":1,\"1893\":1,\"1970\":1,\"1975\":1,\"2027\":1,\"2076\":1,\"2294\":1}}],[\"callable>\",{\"1\":{\"2106\":2}}],[\"callable\",{\"1\":{\"914\":1,\"1218\":1,\"1343\":1,\"1452\":1,\"1898\":1,\"2096\":1,\"2097\":2,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":2,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2182\":1,\"2189\":1}}],[\"calls\",{\"1\":{\"608\":1,\"1228\":2,\"1345\":2,\"1347\":2,\"2429\":1,\"2554\":1,\"2568\":1}}],[\"callhome\",{\"1\":{\"201\":3,\"296\":1}}],[\"calling\",{\"1\":{\"124\":1,\"1248\":1,\"2121\":1,\"2122\":1}}],[\"called\",{\"1\":{\"83\":1,\"112\":1,\"118\":1,\"137\":1,\"610\":1,\"627\":1,\"745\":1,\"875\":1,\"1143\":1,\"1171\":1,\"1206\":1,\"1245\":2,\"1248\":1,\"1343\":1,\"1452\":1,\"1552\":1,\"1954\":1,\"1956\":1}}],[\"call\",{\"1\":{\"44\":1,\"85\":1,\"110\":1,\"752\":1,\"753\":1,\"756\":1,\"757\":1,\"760\":1,\"761\":1,\"778\":1,\"779\":1,\"816\":1,\"831\":1,\"832\":1,\"834\":2,\"836\":1,\"1109\":1,\"1110\":1,\"1111\":1,\"1112\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1130\":1,\"1131\":1,\"1134\":1,\"1135\":1,\"1136\":1,\"1137\":1,\"1143\":1,\"1151\":1,\"1152\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1171\":1,\"1174\":1,\"1175\":1,\"1184\":1,\"1185\":1,\"1188\":1,\"1189\":1,\"1206\":1,\"1207\":1,\"1208\":1,\"1212\":1,\"1213\":1,\"1215\":1,\"1216\":1,\"1220\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1229\":1,\"1230\":1,\"1231\":1,\"1232\":1,\"1233\":1,\"1234\":1,\"1235\":1,\"1236\":1,\"1237\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1245\":1,\"1246\":1,\"1249\":1,\"1250\":1,\"1257\":1,\"1258\":1,\"1259\":1,\"1260\":1,\"1261\":1,\"1262\":1,\"1263\":1,\"1264\":1,\"1265\":1,\"1266\":1,\"1267\":1,\"1268\":1,\"1274\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1285\":1,\"1344\":1,\"1346\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1431\":1,\"1432\":1,\"1433\":1,\"1434\":1,\"1435\":1,\"1436\":1,\"1437\":1,\"1438\":1,\"1439\":1,\"1440\":1,\"1441\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1445\":1,\"1446\":1,\"1447\":1,\"1448\":1,\"1449\":1,\"1450\":1,\"1452\":1,\"1453\":1,\"1456\":1,\"1457\":1,\"1458\":1,\"1459\":1,\"1460\":1,\"1461\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1506\":1,\"1507\":1,\"1508\":1,\"1509\":1,\"1512\":1,\"1513\":1,\"1518\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1532\":1,\"1533\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1538\":1,\"1540\":1,\"1541\":1,\"1543\":1,\"1544\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1550\":1,\"1552\":1,\"1555\":1,\"1556\":1,\"1561\":1,\"1562\":1,\"1564\":1,\"1565\":1,\"1573\":1,\"1574\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1613\":1,\"1614\":1,\"1620\":1,\"1621\":1,\"1624\":1,\"1625\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1630\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1641\":1,\"1642\":1,\"1646\":1,\"1647\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"1652\":1,\"1653\":1,\"1656\":1,\"1657\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1769\":1,\"1770\":1,\"1774\":1,\"1775\":1,\"1779\":1,\"1780\":1,\"1782\":1,\"1783\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1792\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1801\":1,\"1802\":1,\"1806\":1,\"1807\":1,\"1890\":1,\"1891\":1,\"1902\":1,\"1903\":1,\"1907\":1,\"1908\":1,\"1912\":1,\"1913\":1,\"1951\":1,\"1952\":1,\"1953\":2,\"1954\":1,\"1955\":2,\"1956\":1,\"1958\":1,\"1959\":1,\"1976\":1,\"1977\":1,\"1978\":1,\"1979\":1,\"1981\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1989\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1994\":1,\"1995\":1,\"1997\":1,\"1998\":1,\"2024\":1,\"2025\":1,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2039\":1,\"2040\":1,\"2041\":1,\"2042\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2047\":1,\"2048\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2055\":1,\"2056\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2064\":1,\"2065\":1,\"2066\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2149\":1,\"2150\":1,\"2168\":1,\"2169\":1,\"2233\":1,\"2234\":1,\"2237\":1,\"2238\":1,\"2241\":1,\"2242\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2266\":1,\"2267\":1,\"2275\":1,\"2276\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2288\":1,\"2289\":1,\"2290\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2297\":1,\"2298\":1,\"2299\":1,\"2300\":1,\"2372\":1}}],[\"capable\",{\"1\":{\"1551\":1,\"1553\":1}}],[\"capabilities\",{\"1\":{\"120\":1}}],[\"capability\",{\"1\":{\"46\":1}}],[\"capacity\",{\"1\":{\"80\":1}}],[\"categories\",{\"1\":{\"1551\":4,\"1553\":5,\"2181\":1,\"2184\":1,\"2200\":1}}],[\"category2utt\",{\"1\":{\"2006\":1}}],[\"categorybalancedsampler\",{\"0\":{\"2006\":1},\"1\":{\"2006\":1}}],[\"categoryiterfactory\",{\"0\":{\"1895\":1},\"1\":{\"1895\":1}}],[\"category\",{\"0\":{\"1895\":1,\"1899\":1,\"1901\":1,\"2006\":1,\"2013\":1},\"1\":{\"79\":3,\"1028\":6,\"1551\":3,\"1553\":5,\"1895\":1,\"1899\":1,\"1901\":1,\"2006\":1,\"2013\":1,\"2099\":1,\"2410\":1}}],[\"catalog\",{\"1\":{\"1400\":1}}],[\"catch\",{\"1\":{\"595\":1,\"823\":1,\"2212\":1}}],[\"cat\",{\"0\":{\"1681\":1,\"1684\":1},\"1\":{\"167\":1,\"178\":1,\"189\":1,\"196\":1,\"197\":2,\"198\":1,\"200\":1,\"203\":2,\"234\":1,\"239\":1,\"240\":4,\"1474\":1,\"1681\":2,\"1684\":2,\"2086\":1,\"2087\":1,\"2372\":1,\"2373\":1,\"2387\":1,\"2415\":1,\"2416\":1,\"2417\":1,\"2418\":1,\"2429\":1,\"2430\":1,\"2554\":1,\"2555\":1,\"2556\":1,\"2560\":1,\"2572\":1}}],[\"catbel\",{\"1\":{\"73\":1,\"79\":2,\"2012\":2}}],[\"ca\",{\"1\":{\"70\":2}}],[\"cauchymultiplysymmetric\",{\"0\":{\"1147\":1}}],[\"cauchymultiply\",{\"0\":{\"1146\":1}}],[\"cauchy\",{\"0\":{\"1146\":1,\"1147\":1,\"1290\":2,\"1291\":2,\"1292\":2}}],[\"caught\",{\"1\":{\"607\":1}}],[\"cause\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"causes\",{\"1\":{\"594\":1}}],[\"causal=true\",{\"1\":{\"1462\":1,\"1478\":1}}],[\"causal=false\",{\"1\":{\"1249\":1,\"1370\":1,\"1378\":1,\"1379\":1,\"1480\":1,\"1546\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"2297\":1}}],[\"causalconv1d\",{\"0\":{\"708\":1},\"1\":{\"708\":1,\"860\":1}}],[\"causal\",{\"1\":{\"21\":5,\"121\":1,\"708\":1,\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"835\":1,\"860\":2,\"1051\":3,\"1052\":3,\"1054\":3,\"1055\":3,\"1090\":1,\"1190\":1,\"1377\":2,\"1379\":3,\"1462\":2,\"1463\":3,\"1478\":2,\"1598\":1,\"1648\":1,\"1652\":1,\"1654\":3,\"1658\":2,\"1659\":2,\"1664\":3,\"1665\":3,\"1800\":3,\"2290\":1}}],[\"cautions\",{\"1\":{\"83\":1}}],[\"caution\",{\"1\":{\"52\":1,\"1946\":1,\"1947\":1}}],[\"canonical=none\",{\"1\":{\"2315\":1}}],[\"cancel\",{\"1\":{\"204\":1}}],[\"cannot\",{\"1\":{\"148\":1,\"226\":1,\"823\":1,\"1085\":1,\"1551\":1,\"1553\":1,\"2387\":2,\"2467\":1,\"2523\":1,\"2568\":1}}],[\"candidates\",{\"1\":{\"23\":1,\"119\":1,\"698\":1,\"699\":1,\"700\":1,\"917\":4,\"1048\":6,\"1138\":1,\"1139\":1}}],[\"can\",{\"1\":{\"1\":3,\"3\":1,\"4\":1,\"5\":1,\"11\":3,\"12\":1,\"15\":1,\"16\":1,\"17\":2,\"18\":1,\"21\":1,\"22\":4,\"25\":3,\"26\":1,\"27\":1,\"28\":3,\"29\":1,\"34\":1,\"37\":1,\"38\":1,\"40\":1,\"45\":1,\"47\":2,\"48\":1,\"49\":4,\"51\":1,\"54\":1,\"57\":2,\"58\":1,\"59\":2,\"60\":4,\"62\":2,\"63\":1,\"65\":1,\"69\":3,\"72\":1,\"74\":1,\"75\":1,\"76\":3,\"82\":1,\"84\":1,\"85\":5,\"90\":2,\"91\":2,\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"102\":2,\"104\":1,\"107\":2,\"110\":1,\"113\":2,\"115\":2,\"117\":2,\"118\":1,\"121\":2,\"122\":1,\"124\":2,\"132\":1,\"133\":1,\"134\":2,\"135\":1,\"136\":1,\"137\":1,\"141\":1,\"143\":1,\"144\":4,\"148\":1,\"149\":1,\"150\":8,\"168\":2,\"173\":1,\"179\":1,\"180\":1,\"187\":1,\"188\":1,\"197\":2,\"198\":1,\"203\":2,\"209\":1,\"213\":1,\"228\":1,\"233\":1,\"235\":2,\"236\":1,\"237\":2,\"238\":5,\"240\":5,\"241\":1,\"295\":4,\"564\":1,\"607\":1,\"652\":1,\"711\":4,\"727\":2,\"728\":2,\"745\":1,\"746\":1,\"771\":1,\"772\":1,\"785\":1,\"809\":1,\"810\":1,\"999\":1,\"1011\":1,\"1013\":1,\"1015\":1,\"1049\":2,\"1050\":2,\"1052\":2,\"1056\":2,\"1058\":2,\"1068\":2,\"1077\":2,\"1093\":1,\"1101\":1,\"1132\":1,\"1148\":1,\"1156\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1187\":4,\"1198\":1,\"1202\":4,\"1203\":1,\"1233\":1,\"1241\":1,\"1245\":2,\"1252\":1,\"1253\":1,\"1254\":1,\"1274\":1,\"1276\":1,\"1279\":1,\"1286\":2,\"1287\":2,\"1343\":1,\"1463\":1,\"1543\":1,\"1551\":1,\"1553\":1,\"1639\":2,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1688\":1,\"1713\":1,\"1719\":1,\"1739\":1,\"1756\":1,\"1785\":1,\"1897\":2,\"1905\":4,\"1927\":1,\"1932\":1,\"1972\":1,\"2046\":1,\"2054\":1,\"2096\":1,\"2098\":1,\"2099\":2,\"2100\":1,\"2101\":1,\"2102\":2,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2121\":1,\"2122\":1,\"2131\":1,\"2148\":1,\"2156\":1,\"2355\":2,\"2363\":1,\"2368\":1,\"2371\":1,\"2372\":2,\"2373\":1,\"2375\":2,\"2377\":2,\"2378\":1,\"2384\":1,\"2385\":3,\"2386\":1,\"2387\":1,\"2389\":2,\"2393\":1,\"2400\":1,\"2401\":1,\"2405\":1,\"2408\":2,\"2410\":2,\"2415\":3,\"2416\":2,\"2418\":1,\"2419\":1,\"2422\":2,\"2429\":1,\"2430\":3,\"2431\":2,\"2436\":2,\"2437\":1,\"2440\":2,\"2449\":2,\"2450\":1,\"2457\":1,\"2459\":1,\"2465\":2,\"2467\":3,\"2468\":1,\"2481\":2,\"2482\":1,\"2486\":1,\"2490\":1,\"2494\":2,\"2500\":1,\"2501\":2,\"2503\":2,\"2506\":1,\"2514\":3,\"2525\":2,\"2529\":1,\"2536\":1,\"2537\":1,\"2541\":1,\"2542\":1,\"2543\":2,\"2545\":2,\"2554\":1,\"2555\":3,\"2558\":2,\"2559\":3,\"2562\":2,\"2563\":1,\"2564\":3,\"2569\":1,\"2570\":1,\"2571\":1,\"2573\":2,\"2584\":7,\"2585\":2,\"2586\":1,\"2587\":1,\"2588\":1,\"2593\":2,\"2599\":1,\"2600\":5,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":2,\"2638\":3,\"2640\":2,\"2641\":1,\"2644\":1,\"2653\":1,\"2659\":3}}],[\"cochlear\",{\"1\":{\"1712\":1,\"1715\":1}}],[\"coherence\",{\"1\":{\"1566\":1}}],[\"cohort\",{\"1\":{\"429\":4}}],[\"covost2\",{\"1\":{\"2518\":1}}],[\"covolution\",{\"1\":{\"1804\":1,\"1878\":1}}],[\"covariances\",{\"0\":{\"1702\":1},\"1\":{\"1702\":1}}],[\"covariance\",{\"1\":{\"1524\":5,\"1696\":1,\"1697\":1,\"1698\":3,\"1702\":1,\"1704\":2,\"1705\":1,\"1706\":2,\"1707\":3,\"1708\":2,\"1712\":2,\"1713\":2,\"1715\":2}}],[\"covers\",{\"1\":{\"2355\":1,\"2389\":1,\"2422\":1,\"2525\":1,\"2545\":1}}],[\"coverage\",{\"1\":{\"678\":1,\"679\":2,\"2349\":3}}],[\"covered\",{\"1\":{\"124\":1}}],[\"co\",{\"1\":{\"1454\":1,\"2355\":1,\"2357\":1,\"2446\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2584\":1,\"2585\":3,\"2648\":1,\"2649\":1}}],[\"coeifficient\",{\"1\":{\"1132\":1}}],[\"coeff\",{\"1\":{\"1935\":3,\"1943\":3}}],[\"coefficients\",{\"1\":{\"1001\":1,\"1069\":4,\"1639\":3,\"1738\":1}}],[\"coefficient\",{\"1\":{\"762\":1,\"763\":2,\"1069\":4,\"1778\":4,\"1805\":8,\"1850\":5,\"1852\":4,\"1860\":1,\"1877\":5,\"1883\":1,\"1935\":1,\"1943\":1,\"2090\":4}}],[\"coef\",{\"1\":{\"1132\":2}}],[\"coordinate\",{\"1\":{\"1031\":2}}],[\"coordinates\",{\"1\":{\"1025\":1}}],[\"cooldown=0\",{\"1\":{\"2022\":1}}],[\"cooldown\",{\"1\":{\"62\":1,\"2022\":1}}],[\"cos\",{\"1\":{\"1079\":1,\"1560\":1,\"1797\":1,\"2040\":1}}],[\"cosineannealingwarmuprestarts\",{\"0\":{\"2018\":1},\"1\":{\"2018\":1}}],[\"cosine\",{\"0\":{\"2018\":1},\"1\":{\"668\":4,\"1115\":2,\"1560\":1,\"1883\":1,\"2018\":2}}],[\"costs\",{\"0\":{\"1300\":1},\"1\":{\"1142\":2,\"1186\":7,\"1210\":7,\"1300\":1,\"1337\":3,\"1349\":3,\"1350\":3}}],[\"cost\",{\"1\":{\"102\":1,\"1013\":1,\"1142\":2,\"1186\":2,\"1210\":2}}],[\"copies\",{\"1\":{\"1245\":1,\"1248\":2,\"1351\":1}}],[\"copied\",{\"1\":{\"5\":1,\"2121\":1,\"2122\":1}}],[\"copy=false\",{\"1\":{\"2166\":1}}],[\"copy\",{\"0\":{\"522\":1,\"1306\":1},\"1\":{\"522\":3,\"1306\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"2431\":1,\"2432\":1,\"2558\":1,\"2568\":1,\"2569\":1,\"2570\":1}}],[\"cofiguration\",{\"1\":{\"240\":1}}],[\"coarse\",{\"1\":{\"69\":1}}],[\"colapse\",{\"1\":{\"2457\":1}}],[\"colab\",{\"1\":{\"135\":1,\"166\":1,\"167\":1,\"198\":1,\"199\":2,\"202\":1,\"206\":1,\"295\":2,\"2360\":1,\"2368\":1,\"2389\":2,\"2408\":2,\"2422\":2,\"2431\":1,\"2449\":2,\"2458\":1,\"2465\":2,\"2480\":1,\"2481\":2,\"2486\":1,\"2490\":1,\"2501\":1,\"2502\":2,\"2503\":2,\"2522\":1,\"2523\":1,\"2525\":2,\"2545\":2,\"2581\":1,\"2582\":1,\"2584\":2,\"2598\":1,\"2600\":2,\"2601\":1,\"2605\":1,\"2607\":1,\"2609\":1,\"2615\":1,\"2618\":1,\"2622\":1,\"2624\":1,\"2626\":1,\"2633\":1,\"2650\":1}}],[\"cols\",{\"1\":{\"1227\":1,\"1228\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1}}],[\"colors\",{\"1\":{\"2500\":10,\"2617\":10,\"2635\":10}}],[\"colormap\",{\"1\":{\"648\":1}}],[\"colorbar\",{\"1\":{\"238\":2}}],[\"column\",{\"1\":{\"75\":1,\"144\":1,\"564\":2,\"743\":1,\"875\":3,\"1025\":1,\"1714\":1,\"1810\":1}}],[\"columns=false\",{\"1\":{\"2229\":1}}],[\"columns=true\",{\"1\":{\"1406\":3,\"1408\":1}}],[\"columns\",{\"0\":{\"1424\":1,\"2225\":1,\"2231\":1},\"1\":{\"54\":2,\"58\":1,\"1405\":1,\"1407\":5,\"1420\":1,\"1424\":2,\"1425\":1,\"2225\":1,\"2231\":1}}],[\"collegue\",{\"1\":{\"2583\":1}}],[\"collected\",{\"1\":{\"1964\":1,\"2294\":1}}],[\"collection\",{\"1\":{\"1057\":1,\"1898\":1,\"1962\":1,\"2133\":1,\"2177\":1,\"2178\":2,\"2179\":2,\"2182\":1,\"2188\":1,\"2189\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2208\":2,\"2354\":1}}],[\"collecting\",{\"1\":{\"989\":1,\"1530\":1,\"1563\":1,\"1600\":1,\"1603\":1,\"1622\":1}}],[\"collect\",{\"0\":{\"1964\":2,\"2398\":1,\"2534\":1},\"1\":{\"102\":1,\"1057\":4,\"1113\":1,\"1171\":2,\"1172\":1,\"1206\":1,\"1371\":1,\"1551\":3,\"1552\":1,\"1553\":4,\"1554\":2,\"1773\":1,\"1837\":1,\"1892\":1,\"1893\":1,\"1953\":1,\"1955\":1,\"1964\":3,\"1970\":2,\"1975\":2,\"1984\":2,\"2027\":2,\"2046\":1,\"2076\":2,\"2082\":1,\"2168\":1,\"2170\":1,\"2240\":1,\"2278\":1,\"2294\":1,\"2350\":1,\"2375\":2,\"2398\":1,\"2400\":1,\"2534\":1,\"2536\":1,\"2558\":2,\"2584\":1}}],[\"collaborative\",{\"1\":{\"1767\":1,\"1768\":1}}],[\"collate\",{\"0\":{\"2177\":1,\"2188\":1,\"2208\":2},\"1\":{\"56\":1,\"60\":7,\"174\":3,\"1895\":1,\"1896\":1,\"1897\":1,\"1900\":1,\"2096\":5,\"2097\":3,\"2098\":5,\"2099\":6,\"2100\":5,\"2101\":5,\"2102\":5,\"2103\":5,\"2104\":5,\"2105\":5,\"2106\":2,\"2107\":5,\"2108\":5,\"2109\":5,\"2110\":5,\"2112\":5,\"2113\":5,\"2114\":5,\"2115\":5,\"2116\":5,\"2117\":5,\"2118\":5,\"2177\":3,\"2188\":3,\"2208\":4,\"2209\":1}}],[\"collcate\",{\"1\":{\"60\":5}}],[\"coupling\",{\"0\":{\"1864\":1,\"1865\":1},\"1\":{\"1864\":3,\"1865\":2}}],[\"coupled\",{\"1\":{\"1719\":1}}],[\"could\",{\"1\":{\"607\":1,\"1427\":1,\"2387\":1,\"2401\":1,\"2419\":2,\"2420\":3,\"2461\":1,\"2499\":1,\"2500\":1,\"2537\":1,\"2584\":1,\"2617\":2,\"2635\":2}}],[\"course\",{\"0\":{\"160\":1,\"2724\":1},\"1\":{\"48\":1,\"138\":1,\"152\":2,\"161\":1,\"2584\":1}}],[\"count=\",{\"1\":{\"1028\":1}}],[\"counting\",{\"1\":{\"77\":1,\"78\":1}}],[\"counts\",{\"1\":{\"26\":1}}],[\"count\",{\"0\":{\"618\":1,\"2154\":1},\"1\":{\"26\":6,\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"618\":3,\"619\":2,\"785\":1,\"1028\":3,\"1143\":1,\"1144\":1,\"1858\":1,\"2154\":8,\"2199\":2,\"2386\":3,\"2507\":1,\"2513\":1}}],[\"corr\",{\"1\":{\"2444\":1,\"2445\":1,\"2446\":1}}],[\"corrupted\",{\"1\":{\"1929\":1}}],[\"corrupt\",{\"0\":{\"1929\":1},\"1\":{\"1929\":2}}],[\"correponding\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"correlation\",{\"1\":{\"1701\":3,\"1702\":2,\"1703\":6}}],[\"correlations\",{\"0\":{\"1701\":1},\"1\":{\"1701\":3,\"1703\":1}}],[\"corresonding\",{\"1\":{\"1551\":1,\"1553\":1}}],[\"correspondences\",{\"1\":{\"1011\":1}}],[\"correspondence\",{\"1\":{\"237\":2,\"238\":1}}],[\"correspond\",{\"1\":{\"74\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1177\":1,\"1209\":1,\"1252\":1,\"1253\":1,\"1254\":1}}],[\"corresponds\",{\"1\":{\"57\":1,\"1071\":3,\"1375\":1,\"1670\":1,\"1671\":1,\"1688\":1,\"1693\":1,\"1712\":1,\"1755\":1,\"1756\":1,\"1766\":1,\"2268\":1,\"2270\":1,\"2272\":1}}],[\"corresponding\",{\"1\":{\"21\":1,\"75\":1,\"76\":1,\"116\":1,\"120\":1,\"237\":2,\"635\":1,\"691\":2,\"692\":1,\"697\":2,\"710\":1,\"745\":2,\"746\":2,\"774\":1,\"797\":1,\"855\":1,\"867\":1,\"870\":1,\"922\":1,\"1133\":1,\"1187\":1,\"1202\":1,\"1248\":1,\"1273\":2,\"1286\":1,\"1287\":1,\"1462\":1,\"1463\":1,\"1551\":1,\"1553\":1,\"1714\":1,\"2373\":1,\"2398\":1,\"2430\":1,\"2494\":1,\"2534\":1,\"2555\":1,\"2638\":1,\"2643\":1}}],[\"corrected\",{\"1\":{\"1717\":1}}],[\"corrector=\",{\"1\":{\"1646\":1}}],[\"corrector\",{\"0\":{\"1514\":1},\"1\":{\"1451\":2,\"1514\":3,\"1585\":2,\"1612\":3,\"1646\":7,\"1647\":1}}],[\"correctors\",{\"0\":{\"1451\":1,\"1514\":1,\"1585\":1,\"1612\":1},\"1\":{\"1451\":1,\"1514\":1,\"1585\":1,\"1612\":1}}],[\"correction\",{\"0\":{\"1342\":1,\"1717\":1},\"1\":{\"1245\":1,\"1342\":1,\"1717\":2}}],[\"correction=false\",{\"1\":{\"1188\":1}}],[\"correctness\",{\"1\":{\"1214\":1}}],[\"correctly\",{\"1\":{\"94\":1,\"2372\":1}}],[\"correct\",{\"1\":{\"11\":1,\"1892\":1,\"2398\":1,\"2472\":1,\"2474\":1,\"2534\":1,\"2648\":1,\"2649\":1}}],[\"cornell\",{\"1\":{\"130\":1,\"1132\":1,\"1604\":1,\"1655\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1719\":1}}],[\"core\",{\"1\":{\"112\":1,\"604\":1,\"627\":1,\"727\":1,\"728\":1,\"784\":1,\"976\":1,\"1043\":1,\"1137\":1,\"2046\":1,\"2154\":1,\"2394\":1,\"2530\":1}}],[\"corpora\",{\"1\":{\"15\":1,\"85\":2,\"2363\":1,\"2372\":1,\"2384\":1,\"2385\":1,\"2411\":1,\"2429\":1,\"2506\":1,\"2512\":1,\"2554\":1,\"2653\":1,\"2657\":1}}],[\"corporas\",{\"1\":{\"3\":1}}],[\"corpora4\",{\"1\":{\"1\":1,\"3\":1}}],[\"corpus=\",{\"1\":{\"3\":4}}],[\"corpus\",{\"0\":{\"274\":1},\"1\":{\"3\":7,\"15\":1,\"46\":1,\"47\":1,\"48\":2,\"49\":4,\"57\":1,\"69\":3,\"74\":1,\"84\":1,\"85\":3,\"169\":1,\"181\":1,\"274\":2,\"750\":4,\"2099\":1,\"2102\":1,\"2363\":8,\"2457\":2,\"2506\":8,\"2512\":4,\"2518\":2,\"2587\":1,\"2653\":8,\"2657\":4}}],[\"codes\",{\"1\":{\"2400\":1,\"2536\":1}}],[\"codebase\",{\"1\":{\"2384\":1,\"2394\":1,\"2403\":1,\"2450\":1,\"2530\":1,\"2539\":1}}],[\"codecs\",{\"0\":{\"1926\":1},\"1\":{\"49\":3,\"1926\":3}}],[\"codec\",{\"0\":{\"52\":1},\"1\":{\"47\":1,\"48\":1,\"49\":2,\"1927\":1}}],[\"code\",{\"1\":{\"5\":3,\"295\":1,\"607\":1,\"837\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1661\":1,\"1986\":1,\"2259\":1,\"2294\":1,\"2383\":1,\"2393\":2,\"2400\":1,\"2405\":1,\"2427\":1,\"2529\":2,\"2536\":1,\"2541\":1,\"2550\":1,\"2568\":2,\"2569\":1}}],[\"comunidad\",{\"1\":{\"2457\":1}}],[\"combdblock\",{\"0\":{\"1768\":1},\"1\":{\"1768\":2}}],[\"combd\",{\"0\":{\"1767\":1},\"1\":{\"1761\":10,\"1763\":10,\"1767\":4,\"1768\":2,\"1805\":10}}],[\"combining\",{\"1\":{\"2473\":1}}],[\"combine=\",{\"1\":{\"1605\":1}}],[\"combine\",{\"0\":{\"1474\":1},\"1\":{\"1171\":1,\"1206\":1,\"1255\":1,\"1474\":2,\"1552\":1,\"1917\":1,\"1953\":1,\"1955\":1,\"2542\":1}}],[\"combination\",{\"0\":{\"1297\":1},\"1\":{\"679\":1,\"1156\":1,\"1245\":2,\"1297\":2,\"2022\":1,\"2023\":1,\"2046\":1,\"2363\":1,\"2506\":1,\"2653\":1}}],[\"comes\",{\"1\":{\"940\":1,\"2593\":2}}],[\"come\",{\"1\":{\"82\":1,\"148\":1}}],[\"com\",{\"1\":{\"44\":1,\"45\":1,\"47\":1,\"70\":2,\"97\":1,\"134\":1,\"135\":1,\"166\":1,\"167\":4,\"177\":1,\"178\":4,\"195\":2,\"196\":4,\"199\":2,\"200\":2,\"202\":1,\"206\":2,\"207\":1,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":2,\"223\":2,\"229\":2,\"230\":2,\"233\":1,\"234\":4,\"277\":1,\"295\":2,\"628\":1,\"668\":1,\"734\":1,\"740\":1,\"741\":1,\"771\":1,\"772\":1,\"775\":1,\"776\":1,\"809\":1,\"810\":1,\"817\":1,\"952\":1,\"1037\":1,\"1047\":1,\"1076\":1,\"1082\":1,\"1101\":1,\"1102\":1,\"1144\":1,\"1148\":1,\"1180\":1,\"1203\":2,\"1214\":1,\"1215\":1,\"1228\":1,\"1284\":1,\"1337\":1,\"1345\":1,\"1347\":1,\"1349\":1,\"1350\":1,\"1454\":2,\"1605\":2,\"1695\":1,\"1717\":1,\"1735\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"1860\":1,\"1917\":1,\"1958\":1,\"1986\":1,\"2054\":1,\"2125\":1,\"2131\":2,\"2154\":1,\"2259\":1,\"2294\":1,\"2309\":1,\"2324\":1,\"2354\":2,\"2359\":1,\"2360\":1,\"2361\":2,\"2363\":6,\"2366\":1,\"2372\":2,\"2377\":2,\"2379\":1,\"2382\":1,\"2383\":1,\"2384\":3,\"2387\":1,\"2393\":1,\"2394\":1,\"2409\":1,\"2427\":1,\"2429\":4,\"2430\":1,\"2431\":2,\"2432\":2,\"2446\":2,\"2450\":3,\"2456\":1,\"2458\":1,\"2466\":2,\"2482\":1,\"2499\":1,\"2500\":1,\"2504\":1,\"2506\":6,\"2517\":3,\"2521\":1,\"2523\":1,\"2529\":1,\"2530\":1,\"2550\":1,\"2552\":1,\"2554\":1,\"2555\":1,\"2564\":1,\"2574\":1,\"2575\":1,\"2576\":1,\"2580\":1,\"2582\":1,\"2584\":1,\"2587\":1,\"2593\":2,\"2597\":2,\"2601\":1,\"2617\":1,\"2618\":1,\"2635\":2,\"2646\":3,\"2650\":2,\"2653\":6}}],[\"coming\",{\"1\":{\"43\":1}}],[\"comprises\",{\"1\":{\"2046\":1}}],[\"compressed\",{\"1\":{\"1927\":1,\"2567\":1}}],[\"compress=true\",{\"1\":{\"984\":1}}],[\"compress=false\",{\"1\":{\"983\":1,\"986\":1}}],[\"compress\",{\"1\":{\"496\":2,\"509\":2,\"512\":2,\"522\":2,\"525\":2,\"1015\":3}}],[\"compression\",{\"1\":{\"49\":2,\"496\":2,\"509\":2,\"512\":2,\"522\":2,\"525\":2,\"986\":1,\"1015\":3,\"1132\":1,\"1911\":2,\"1917\":1,\"1926\":1,\"1927\":1}}],[\"compensate\",{\"1\":{\"802\":1}}],[\"completion\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"completed\",{\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1,\"2560\":1}}],[\"completes\",{\"1\":{\"2372\":1,\"2429\":1,\"2554\":1}}],[\"completely\",{\"1\":{\"84\":1,\"1245\":1}}],[\"complete\",{\"1\":{\"15\":1,\"29\":1,\"120\":1,\"133\":1,\"1057\":1,\"2354\":1,\"2384\":1,\"2388\":1,\"2394\":1,\"2421\":1,\"2429\":1,\"2524\":1,\"2530\":1,\"2544\":1,\"2554\":1}}],[\"complex=false\",{\"1\":{\"1660\":1,\"1661\":1}}],[\"complexlinear\",{\"0\":{\"1482\":1},\"1\":{\"1482\":2}}],[\"complexconvtranspose2d\",{\"0\":{\"1480\":1},\"1\":{\"1480\":2}}],[\"complexconv2d\",{\"0\":{\"1478\":1},\"1\":{\"1478\":3}}],[\"complexbatchnorm\",{\"0\":{\"1476\":1},\"1\":{\"1476\":1}}],[\"complexnn\",{\"0\":{\"1476\":1,\"1478\":1,\"1480\":1,\"1609\":1,\"1684\":1},\"1\":{\"1476\":1,\"1478\":2,\"1480\":1,\"1609\":1,\"1684\":2}}],[\"complex64\",{\"1\":{\"1455\":1,\"1524\":10,\"1525\":1,\"1611\":2,\"1680\":3,\"1696\":3,\"1697\":3,\"1698\":4,\"1704\":3,\"1705\":3,\"1706\":3,\"1707\":4,\"1708\":3,\"1711\":2,\"1712\":3,\"1713\":3,\"1715\":3,\"1736\":1,\"1739\":1,\"1746\":2}}],[\"complex\",{\"0\":{\"1681\":1,\"1684\":1,\"1685\":2,\"1694\":1,\"1722\":1,\"1723\":2,\"1724\":2,\"1727\":1,\"1733\":2,\"1740\":1,\"1742\":1,\"1744\":1,\"1747\":2,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1},\"1\":{\"1247\":1,\"1269\":2,\"1452\":3,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":3,\"1516\":5,\"1517\":2,\"1518\":1,\"1520\":1,\"1523\":2,\"1543\":2,\"1547\":1,\"1561\":1,\"1644\":1,\"1655\":3,\"1660\":2,\"1661\":2,\"1662\":2,\"1671\":1,\"1681\":2,\"1684\":2,\"1685\":2,\"1694\":2,\"1695\":2,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1719\":5,\"1722\":2,\"1723\":2,\"1724\":2,\"1727\":2,\"1733\":2,\"1736\":1,\"1737\":1,\"1740\":2,\"1742\":2,\"1744\":2,\"1747\":2,\"1748\":2,\"1749\":2,\"1750\":1,\"1751\":2,\"1758\":1,\"1759\":1,\"1799\":1,\"2175\":2,\"2199\":3,\"2205\":6,\"2230\":2,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"complextensor\",{\"1\":{\"690\":2,\"729\":3,\"730\":1,\"752\":1,\"756\":1,\"782\":1,\"854\":2,\"885\":5,\"887\":3,\"927\":3,\"928\":1,\"1375\":2,\"1377\":1,\"1455\":2,\"1463\":2,\"1505\":2,\"1515\":2,\"1516\":5,\"1523\":2,\"1524\":12,\"1525\":3,\"1528\":2,\"1529\":2,\"1534\":2,\"1539\":2,\"1558\":1,\"1595\":1,\"1600\":2,\"1611\":4,\"1626\":2,\"1643\":3,\"1644\":1,\"1645\":1,\"1654\":2,\"1658\":2,\"1659\":2,\"1669\":2,\"1671\":2,\"1678\":2,\"1680\":3,\"1681\":1,\"1685\":1,\"1696\":5,\"1697\":5,\"1698\":7,\"1701\":1,\"1702\":1,\"1703\":3,\"1704\":5,\"1705\":5,\"1706\":3,\"1707\":7,\"1708\":3,\"1711\":2,\"1712\":3,\"1713\":3,\"1715\":3,\"1719\":3,\"1722\":1,\"1727\":2,\"1733\":1,\"1736\":3,\"1737\":2,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":2,\"1744\":1,\"1746\":2,\"1751\":1,\"1758\":1,\"1759\":1,\"1918\":2,\"2498\":4,\"2616\":4,\"2634\":4}}],[\"component\",{\"1\":{\"1452\":1,\"2467\":1,\"2584\":1}}],[\"components\",{\"0\":{\"1129\":1,\"1166\":1,\"1199\":1,\"1212\":1,\"1235\":1,\"1237\":1,\"1242\":1,\"1259\":1,\"1261\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1322\":1,\"1352\":1},\"1\":{\"207\":1,\"242\":1,\"778\":1,\"1129\":1,\"1166\":1,\"1199\":1,\"1212\":1,\"1235\":1,\"1237\":1,\"1259\":1,\"1261\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1322\":1,\"1352\":1,\"1798\":1,\"1804\":1,\"1874\":1,\"1912\":1,\"2385\":1,\"2394\":1,\"2530\":1}}],[\"composing\",{\"1\":{\"1429\":1}}],[\"composeing\",{\"1\":{\"1429\":1}}],[\"composed\",{\"1\":{\"114\":1,\"117\":1}}],[\"computated\",{\"1\":{\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1}}],[\"computations\",{\"1\":{\"627\":1,\"1210\":1}}],[\"computational\",{\"1\":{\"130\":2,\"608\":1,\"745\":1,\"746\":1,\"2237\":1}}],[\"computation\",{\"1\":{\"113\":4,\"115\":1,\"116\":1,\"745\":2,\"746\":2,\"752\":1,\"756\":1,\"759\":2,\"760\":1,\"766\":1,\"778\":1,\"831\":1,\"1061\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1071\":1,\"1074\":1,\"1075\":1,\"1076\":6,\"1082\":1,\"1084\":1,\"1093\":1,\"1097\":1,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1134\":1,\"1136\":1,\"1151\":1,\"1154\":1,\"1156\":1,\"1158\":1,\"1174\":1,\"1184\":1,\"1188\":1,\"1207\":1,\"1212\":1,\"1215\":1,\"1216\":1,\"1220\":1,\"1222\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1245\":1,\"1249\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":1,\"1518\":1,\"1520\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1543\":1,\"1547\":1,\"1549\":1,\"1555\":1,\"1561\":1,\"1564\":1,\"1573\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1596\":1,\"1598\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1613\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1639\":1,\"1641\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1656\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1697\":1,\"1719\":3,\"1761\":1,\"1763\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1801\":1,\"1806\":1,\"1890\":1,\"1902\":1,\"1907\":1,\"1912\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1976\":1,\"1978\":1,\"1981\":1,\"1984\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1994\":1,\"1997\":1,\"2024\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2050\":1,\"2052\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2079\":1,\"2080\":1,\"2149\":1,\"2168\":1,\"2233\":1,\"2237\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2275\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1}}],[\"computing\",{\"1\":{\"1171\":1,\"1206\":1,\"1269\":2,\"1552\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1953\":1,\"1955\":1}}],[\"computemindcf\",{\"0\":{\"2311\":1},\"1\":{\"2311\":2}}],[\"computeerrorrates\",{\"0\":{\"2310\":1},\"1\":{\"2310\":2}}],[\"computes\",{\"1\":{\"616\":1,\"691\":2,\"697\":3,\"710\":1,\"745\":3,\"746\":3,\"1010\":1,\"1186\":1,\"1245\":1,\"1246\":1,\"1248\":2}}],[\"computed\",{\"1\":{\"607\":1,\"732\":1,\"747\":1,\"785\":2,\"1071\":2,\"1695\":1,\"1719\":1,\"2574\":1}}],[\"compute\",{\"0\":{\"506\":1,\"509\":1,\"512\":1,\"616\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1427\":1},\"1\":{\"22\":1,\"113\":1,\"150\":1,\"506\":3,\"509\":3,\"512\":3,\"606\":1,\"616\":2,\"620\":1,\"628\":2,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"691\":2,\"697\":2,\"704\":2,\"705\":2,\"710\":1,\"711\":2,\"723\":1,\"725\":1,\"732\":1,\"734\":3,\"745\":2,\"746\":2,\"747\":1,\"758\":1,\"767\":1,\"771\":2,\"772\":1,\"781\":1,\"785\":2,\"791\":1,\"794\":2,\"797\":1,\"806\":1,\"809\":2,\"817\":1,\"824\":1,\"825\":11,\"827\":1,\"828\":1,\"1047\":1,\"1051\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":2,\"1062\":1,\"1065\":2,\"1069\":5,\"1070\":1,\"1071\":2,\"1072\":1,\"1074\":1,\"1076\":5,\"1077\":1,\"1078\":1,\"1079\":3,\"1080\":1,\"1081\":2,\"1141\":1,\"1142\":5,\"1170\":1,\"1171\":2,\"1173\":2,\"1182\":1,\"1186\":3,\"1206\":2,\"1209\":2,\"1210\":3,\"1270\":1,\"1298\":2,\"1299\":2,\"1300\":1,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":1,\"1341\":1,\"1427\":2,\"1428\":1,\"1531\":1,\"1551\":1,\"1552\":3,\"1553\":1,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1570\":1,\"1571\":2,\"1602\":1,\"1603\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1892\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1,\"1957\":1,\"1960\":1,\"2268\":1,\"2270\":1,\"2272\":1,\"2403\":1,\"2440\":2,\"2457\":2,\"2539\":1,\"2600\":1,\"2644\":1}}],[\"computer\",{\"1\":{\"1\":1,\"2030\":1}}],[\"compiled\",{\"1\":{\"134\":1,\"135\":2,\"200\":1}}],[\"compile\",{\"1\":{\"84\":1,\"134\":3}}],[\"compacts\",{\"1\":{\"2543\":1}}],[\"compatiblity\",{\"1\":{\"1145\":1}}],[\"compatible\",{\"0\":{\"635\":1},\"1\":{\"26\":1,\"635\":3,\"640\":2,\"791\":1,\"1255\":1,\"1428\":1}}],[\"compatibility\",{\"1\":{\"635\":1,\"740\":1,\"741\":1,\"754\":3,\"775\":1,\"776\":1,\"935\":1,\"1187\":1,\"1202\":1,\"1255\":1,\"1551\":4,\"1553\":4,\"1640\":1,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1877\":1}}],[\"comparte\",{\"1\":{\"2457\":1}}],[\"comparing\",{\"1\":{\"1209\":1}}],[\"comparison\",{\"1\":{\"623\":1,\"2433\":1,\"2555\":1}}],[\"comparable\",{\"1\":{\"83\":1}}],[\"comparevaluetrigger\",{\"0\":{\"623\":1},\"1\":{\"623\":2}}],[\"compared\",{\"1\":{\"21\":1,\"30\":1,\"754\":1,\"1661\":1,\"2243\":1,\"2461\":1,\"2479\":1,\"2569\":1}}],[\"compare\",{\"1\":{\"17\":1,\"623\":3,\"710\":1,\"836\":1,\"2394\":1,\"2441\":1,\"2530\":1,\"2564\":1}}],[\"commercial\",{\"1\":{\"2467\":1}}],[\"commented\",{\"1\":{\"2510\":2}}],[\"comments\",{\"1\":{\"2002\":1}}],[\"comment\",{\"1\":{\"1286\":1,\"2002\":1,\"2573\":1}}],[\"community\",{\"1\":{\"2400\":1,\"2411\":1,\"2452\":1,\"2536\":1}}],[\"communities\",{\"1\":{\"2357\":1}}],[\"commonpreprocessor\",{\"0\":{\"2178\":1,\"2179\":1},\"1\":{\"2178\":1,\"2179\":2,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2197\":1}}],[\"commonvoice\",{\"1\":{\"286\":1,\"2518\":1}}],[\"commoncollatefn\",{\"0\":{\"2177\":1},\"1\":{\"60\":3,\"2177\":2,\"2188\":1}}],[\"common\",{\"0\":{\"145\":1,\"750\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"880\":1,\"890\":1,\"896\":1,\"2208\":1},\"1\":{\"28\":1,\"56\":2,\"60\":1,\"112\":1,\"750\":2,\"839\":2,\"840\":2,\"841\":2,\"842\":2,\"843\":2,\"880\":2,\"890\":2,\"896\":1,\"1522\":10,\"1523\":10,\"2046\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2168\":1,\"2170\":1,\"2177\":1,\"2188\":1,\"2208\":2,\"2209\":1,\"2432\":1}}],[\"commit\",{\"1\":{\"5\":2,\"2383\":1,\"2393\":2,\"2427\":2,\"2441\":1,\"2442\":1,\"2529\":3,\"2550\":3}}],[\"comma\",{\"1\":{\"28\":1,\"29\":1}}],[\"commas\",{\"1\":{\"3\":1,\"57\":1}}],[\"commandline\",{\"0\":{\"1020\":1},\"1\":{\"1020\":1}}],[\"commands\",{\"0\":{\"398\":1},\"1\":{\"54\":1,\"98\":1,\"143\":1,\"2433\":1,\"2467\":1,\"2568\":1,\"2569\":1}}],[\"command\",{\"0\":{\"90\":1,\"179\":1,\"190\":1,\"2378\":1,\"2437\":1,\"2563\":1},\"1\":{\"1\":1,\"2\":1,\"4\":2,\"17\":1,\"18\":1,\"45\":1,\"49\":1,\"57\":2,\"59\":1,\"62\":3,\"63\":1,\"90\":1,\"92\":1,\"135\":4,\"143\":1,\"144\":2,\"166\":1,\"173\":1,\"177\":1,\"235\":1,\"237\":1,\"606\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"2099\":1,\"2378\":1,\"2437\":2,\"2467\":1,\"2558\":1,\"2563\":2,\"2569\":1,\"2570\":1,\"2585\":1,\"2637\":1}}],[\"conent\",{\"1\":{\"2537\":1}}],[\"con\",{\"1\":{\"2457\":1}}],[\"concepts\",{\"1\":{\"2573\":1}}],[\"concept\",{\"1\":{\"2410\":1}}],[\"conclude\",{\"1\":{\"2401\":1,\"2537\":1}}],[\"concatadaptlayer\",{\"0\":{\"1484\":1},\"1\":{\"1484\":2}}],[\"concate=true\",{\"1\":{\"2078\":1}}],[\"concatenation\",{\"1\":{\"1015\":1}}],[\"concatenate\",{\"1\":{\"515\":1,\"517\":1,\"617\":1,\"754\":2,\"821\":1,\"826\":2,\"1471\":1,\"1851\":2,\"2078\":1,\"2090\":2,\"2208\":1,\"2243\":2,\"2244\":2,\"2255\":2,\"2264\":2,\"2279\":2}}],[\"concatenated\",{\"1\":{\"238\":1,\"1406\":2,\"1522\":1,\"1847\":1}}],[\"concate\",{\"1\":{\"821\":1,\"2002\":2,\"2078\":1,\"2095\":2,\"2263\":2}}],[\"concatjson\",{\"0\":{\"517\":1},\"1\":{\"517\":2}}],[\"concat\",{\"0\":{\"515\":1,\"617\":1},\"1\":{\"515\":2,\"617\":2,\"711\":4,\"727\":1,\"728\":1,\"749\":4,\"754\":2,\"826\":2,\"979\":1,\"1115\":2,\"1133\":3,\"1140\":1,\"1141\":1,\"1148\":4,\"1149\":4,\"1150\":4,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1203\":4,\"1204\":1,\"1271\":1,\"1272\":4,\"1273\":1,\"1405\":1,\"1426\":1,\"1505\":4,\"1669\":4,\"1778\":2,\"1850\":2,\"1851\":4,\"1852\":2,\"2001\":5,\"2002\":2,\"2004\":5,\"2026\":1,\"2029\":4,\"2090\":4,\"2095\":2,\"2243\":4,\"2244\":4,\"2255\":3,\"2263\":2,\"2264\":4,\"2279\":4,\"2440\":1,\"2564\":1}}],[\"conj\",{\"0\":{\"1703\":1},\"1\":{\"1703\":2,\"1737\":1}}],[\"conjugate\",{\"1\":{\"1248\":1,\"1703\":1}}],[\"cond\",{\"1\":{\"1605\":1}}],[\"condrefineblock\",{\"0\":{\"1491\":1},\"1\":{\"1491\":1}}],[\"condrcublock\",{\"0\":{\"1489\":1},\"1\":{\"1489\":1}}],[\"condmsfblock\",{\"0\":{\"1487\":1},\"1\":{\"1487\":1}}],[\"condcrpblock\",{\"0\":{\"1485\":1},\"1\":{\"1485\":1}}],[\"conducted\",{\"1\":{\"235\":1}}],[\"conduct\",{\"1\":{\"133\":1,\"161\":1,\"2395\":1,\"2399\":2,\"2401\":2,\"2403\":1,\"2406\":1,\"2410\":1,\"2417\":1,\"2418\":1,\"2463\":1,\"2531\":1,\"2535\":2,\"2537\":2,\"2539\":1,\"2542\":1}}],[\"conda\",{\"1\":{\"127\":1,\"132\":1,\"135\":11,\"136\":2}}],[\"conditional=false\",{\"1\":{\"1709\":1}}],[\"conditional=true\",{\"1\":{\"1605\":1}}],[\"conditionalvariancenorm2d\",{\"0\":{\"1503\":1},\"1\":{\"1503\":1}}],[\"conditionalresidualblock\",{\"0\":{\"1501\":1},\"1\":{\"1501\":1}}],[\"conditionalnonenorm2d\",{\"0\":{\"1499\":1},\"1\":{\"1499\":1}}],[\"conditionalinstancenorm2dplus\",{\"0\":{\"1497\":1},\"1\":{\"1497\":1,\"1501\":1}}],[\"conditionalinstancenorm2d\",{\"0\":{\"1495\":1},\"1\":{\"1495\":1}}],[\"conditionalbatchnorm2d\",{\"0\":{\"1493\":1},\"1\":{\"1493\":1}}],[\"conditional\",{\"1\":{\"384\":1,\"835\":1,\"1798\":1,\"1805\":1,\"1862\":2,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1}}],[\"conditioning\",{\"1\":{\"235\":1,\"802\":1,\"803\":1,\"804\":1,\"1148\":1,\"1169\":1,\"1203\":1,\"1272\":1,\"1765\":2,\"1771\":2,\"1772\":1,\"1800\":1,\"1803\":2,\"1804\":1,\"1808\":3,\"1833\":1,\"1835\":1,\"1844\":3,\"1851\":1,\"1862\":2,\"1863\":2,\"1864\":1,\"1865\":1,\"1868\":2,\"1878\":1,\"1880\":5,\"2258\":1,\"2260\":2,\"2263\":1}}],[\"conditions\",{\"1\":{\"124\":1,\"1670\":1,\"1671\":1}}],[\"condition\",{\"1\":{\"124\":1,\"429\":2,\"1551\":1,\"1553\":1,\"1772\":1,\"1956\":1,\"2258\":2,\"2260\":4}}],[\"consequently\",{\"1\":{\"2467\":1}}],[\"conservative\",{\"1\":{\"48\":1}}],[\"consuming\",{\"1\":{\"2420\":1}}],[\"consumed\",{\"1\":{\"836\":1,\"2185\":1,\"2201\":1,\"2203\":1}}],[\"consumes\",{\"1\":{\"148\":1,\"1398\":1}}],[\"consumption\",{\"1\":{\"608\":1}}],[\"consistency\",{\"1\":{\"1551\":3,\"1553\":4}}],[\"consists\",{\"1\":{\"235\":1,\"238\":1,\"239\":1,\"1198\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1858\":1,\"2197\":1}}],[\"consider\",{\"1\":{\"1719\":2}}],[\"considerably\",{\"1\":{\"1688\":1,\"1756\":1}}],[\"considered\",{\"1\":{\"76\":1,\"2440\":1}}],[\"considering\",{\"1\":{\"73\":1,\"1243\":1,\"2373\":1,\"2430\":1,\"2431\":1,\"2555\":1}}],[\"const\",{\"1\":{\"1921\":2,\"2360\":3,\"2458\":3,\"2523\":3,\"2582\":3}}],[\"constantbatchsampler\",{\"1\":{\"2209\":2}}],[\"constants\",{\"0\":{\"1225\":1,\"1315\":1,\"1353\":1,\"1359\":1},\"1\":{\"1225\":1,\"1315\":1,\"1353\":1,\"1359\":1}}],[\"constant\",{\"1\":{\"74\":1,\"75\":1,\"150\":1,\"697\":2,\"797\":1,\"1233\":2,\"1693\":1,\"1739\":1,\"1755\":1,\"1921\":2,\"1932\":1,\"2011\":1,\"2019\":1,\"2209\":1}}],[\"constraining\",{\"1\":{\"758\":1}}],[\"constraint=false\",{\"1\":{\"2079\":1,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"constraint\",{\"1\":{\"217\":1,\"263\":2,\"267\":2,\"399\":2,\"464\":2,\"470\":2,\"681\":2,\"682\":2,\"758\":2,\"940\":1,\"1985\":1,\"2002\":5,\"2079\":4,\"2095\":5,\"2263\":5}}],[\"constraints\",{\"1\":{\"113\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"constrained\",{\"1\":{\"23\":3,\"113\":1,\"700\":1,\"1138\":1,\"1139\":1,\"1705\":1,\"1712\":1,\"1715\":1}}],[\"constructing\",{\"1\":{\"1739\":1,\"2384\":1}}],[\"construction\",{\"1\":{\"79\":1,\"83\":1,\"1618\":1,\"1619\":1}}],[\"constructor\",{\"1\":{\"124\":3,\"816\":1,\"836\":1,\"1243\":1,\"1327\":2,\"1452\":1}}],[\"construct\",{\"1\":{\"57\":1,\"599\":1,\"624\":1,\"705\":1,\"711\":1,\"712\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":1,\"726\":1,\"740\":1,\"741\":1,\"742\":1,\"749\":1,\"750\":1,\"771\":1,\"775\":1,\"776\":1,\"785\":1,\"792\":1,\"809\":1,\"810\":1,\"818\":1,\"819\":1,\"823\":1,\"827\":1,\"936\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1062\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1173\":1,\"1209\":1,\"1452\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1752\":3,\"2433\":1}}],[\"connected\",{\"1\":{\"1522\":1,\"1523\":1,\"1545\":1,\"1561\":1,\"1716\":1,\"2046\":1,\"2237\":1}}],[\"connect\",{\"1\":{\"44\":2,\"1880\":2,\"2422\":1,\"2545\":1}}],[\"connectionist\",{\"1\":{\"796\":1}}],[\"connection\",{\"1\":{\"44\":2,\"835\":1,\"1130\":1,\"1156\":1,\"1233\":1,\"1484\":2,\"1601\":2,\"1659\":1,\"1664\":1,\"1665\":3,\"1880\":1,\"2090\":1}}],[\"connections\",{\"1\":{\"21\":2,\"115\":1,\"712\":1,\"1052\":1,\"1474\":1,\"1752\":3}}],[\"connecting\",{\"1\":{\"17\":1}}],[\"contiguous\",{\"0\":{\"1294\":1},\"1\":{\"1294\":1,\"1354\":1}}],[\"continuous\",{\"1\":{\"929\":1,\"1255\":1,\"1436\":1,\"1511\":1,\"1644\":1,\"2236\":1}}],[\"continues\",{\"1\":{\"1143\":1}}],[\"continue\",{\"1\":{\"231\":1}}],[\"continuing\",{\"1\":{\"99\":1}}],[\"contact\",{\"1\":{\"198\":1}}],[\"contain\",{\"1\":{\"238\":1,\"1171\":1,\"1206\":1,\"1301\":1,\"1304\":1,\"1462\":1,\"1464\":1,\"1552\":1,\"1953\":1,\"1955\":1,\"2584\":1,\"2638\":1}}],[\"containes\",{\"1\":{\"172\":1}}],[\"containers\",{\"0\":{\"3\":1},\"1\":{\"1\":1,\"4\":1,\"5\":1,\"6\":2}}],[\"container\",{\"0\":{\"4\":1},\"1\":{\"1\":3,\"5\":3,\"6\":1,\"999\":1,\"1430\":1,\"1581\":1,\"1650\":1}}],[\"containing\",{\"1\":{\"15\":1,\"116\":1,\"235\":3,\"236\":1,\"676\":1,\"742\":1,\"781\":1,\"812\":1,\"889\":1,\"894\":1,\"899\":2,\"901\":2,\"1016\":1,\"1133\":1,\"1190\":1,\"1204\":1,\"1211\":4,\"1214\":1,\"1224\":4,\"1244\":1,\"1273\":1,\"1287\":4,\"1336\":4,\"1348\":4,\"1390\":1,\"1427\":1,\"1735\":1,\"1739\":1,\"1767\":1,\"1768\":1}}],[\"contains\",{\"1\":{\"5\":1,\"237\":1,\"239\":1,\"240\":2,\"619\":1,\"697\":1,\"745\":1,\"746\":1,\"1057\":2,\"1186\":2,\"1210\":2,\"1298\":3,\"1299\":3,\"1301\":5,\"1302\":3,\"1303\":3,\"1304\":5,\"1551\":1,\"1553\":1,\"1798\":1,\"1874\":1,\"1878\":1,\"1962\":1,\"2492\":1,\"2558\":1,\"2565\":1,\"2585\":1,\"2628\":1,\"2638\":1}}],[\"contexutal\",{\"1\":{\"711\":9}}],[\"context=0\",{\"1\":{\"962\":1}}],[\"contextualblocktransformerencoder\",{\"0\":{\"1150\":1},\"1\":{\"1150\":1}}],[\"contextualblockconformerencoder\",{\"0\":{\"1149\":1},\"1\":{\"1149\":1}}],[\"contextualblockencoderlayer\",{\"0\":{\"711\":1},\"1\":{\"711\":1}}],[\"contextual\",{\"0\":{\"711\":1,\"1149\":1,\"1150\":1},\"1\":{\"104\":3,\"711\":1,\"1149\":3,\"1150\":4}}],[\"context\",{\"1\":{\"21\":2,\"104\":2,\"116\":3,\"121\":2,\"122\":1,\"251\":2,\"255\":2,\"259\":2,\"333\":2,\"608\":1,\"785\":1,\"1049\":4,\"1050\":4,\"1052\":4,\"1056\":4,\"1058\":4,\"1065\":1,\"1066\":1,\"1068\":4,\"1074\":3,\"1075\":3,\"1076\":9,\"1077\":4,\"1081\":3,\"1093\":1,\"1100\":3,\"1101\":1,\"1149\":2,\"1150\":2,\"1187\":4,\"1202\":4,\"1209\":1,\"1220\":1,\"1286\":2,\"1287\":2,\"1558\":3,\"1560\":10,\"1581\":1,\"1719\":3,\"1834\":3,\"1862\":3,\"2142\":1}}],[\"contents\",{\"0\":{\"2598\":1}}],[\"content\",{\"1\":{\"196\":1,\"201\":3,\"202\":3,\"203\":1,\"204\":2,\"234\":1,\"235\":1,\"2367\":2,\"2368\":5,\"2372\":7,\"2373\":1,\"2375\":5,\"2377\":2,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2393\":1,\"2394\":2,\"2396\":1,\"2398\":1,\"2400\":1,\"2401\":2,\"2403\":1,\"2409\":2,\"2411\":1,\"2428\":1,\"2429\":3,\"2431\":1,\"2432\":3,\"2436\":2,\"2440\":1,\"2455\":2,\"2460\":3,\"2461\":4,\"2470\":2,\"2472\":3,\"2473\":1,\"2474\":3,\"2476\":5,\"2478\":5,\"2485\":2,\"2486\":5,\"2490\":5,\"2494\":3,\"2497\":1,\"2500\":4,\"2506\":3,\"2507\":2,\"2510\":12,\"2512\":3,\"2513\":2,\"2514\":2,\"2519\":2,\"2520\":5,\"2529\":1,\"2530\":2,\"2532\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2542\":1,\"2543\":2,\"2550\":1,\"2551\":1,\"2552\":2,\"2554\":2,\"2555\":1,\"2558\":6,\"2559\":2,\"2562\":2,\"2566\":1,\"2567\":3,\"2568\":1,\"2571\":1,\"2584\":4,\"2585\":2,\"2604\":2,\"2605\":5,\"2609\":5,\"2614\":1,\"2621\":2,\"2622\":5,\"2626\":5,\"2632\":1,\"2647\":3,\"2648\":3,\"2649\":3}}],[\"contrib\",{\"1\":{\"1735\":1}}],[\"contributing\",{\"1\":{\"2446\":1,\"2575\":1,\"2618\":1}}],[\"contribution\",{\"1\":{\"22\":1,\"118\":1}}],[\"contribute\",{\"0\":{\"2575\":1},\"1\":{\"118\":1,\"2446\":1,\"2635\":1}}],[\"contrast\",{\"0\":{\"1928\":1},\"1\":{\"1928\":4}}],[\"contract\",{\"0\":{\"1686\":1},\"1\":{\"1686\":1}}],[\"contraining\",{\"1\":{\"676\":1,\"781\":1,\"812\":1}}],[\"contrary\",{\"1\":{\"57\":1,\"116\":1}}],[\"controls\",{\"1\":{\"1245\":1,\"1253\":1,\"1598\":1,\"1652\":1,\"1654\":1,\"1895\":1,\"1900\":1,\"1928\":1}}],[\"controllable\",{\"1\":{\"235\":1,\"736\":1,\"754\":1,\"774\":1,\"786\":1,\"2243\":1}}],[\"controlled\",{\"1\":{\"113\":1,\"115\":1,\"632\":1,\"2099\":1,\"2102\":1,\"2151\":1}}],[\"controlable\",{\"1\":{\"116\":1}}],[\"control\",{\"1\":{\"21\":2,\"23\":2,\"28\":1,\"115\":1,\"119\":2,\"121\":2,\"122\":1,\"235\":1,\"464\":2,\"470\":2,\"632\":1,\"758\":2,\"762\":1,\"763\":2,\"774\":1,\"1036\":2,\"1245\":1,\"1269\":1,\"1778\":1,\"1785\":1,\"1804\":1,\"1805\":1,\"1877\":1,\"1878\":1,\"2083\":1,\"2090\":1,\"2095\":1,\"2142\":1,\"2151\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2257\":1,\"2261\":1,\"2262\":1,\"2279\":1,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"convgenerator\",{\"0\":{\"2292\":1},\"1\":{\"2292\":1}}],[\"convglu\",{\"1\":{\"1576\":1}}],[\"convdiscriminator\",{\"0\":{\"2290\":1},\"1\":{\"2290\":1}}],[\"convdecoder\",{\"0\":{\"1510\":1},\"1\":{\"1510\":1}}],[\"convinupsamplenetwork\",{\"0\":{\"1834\":1},\"1\":{\"1834\":2,\"1862\":1}}],[\"convinput\",{\"0\":{\"1053\":1,\"1097\":1},\"1\":{\"1053\":5,\"1092\":1,\"1097\":1}}],[\"convflow\",{\"0\":{\"1833\":1},\"1\":{\"1833\":3}}],[\"convrelunorm\",{\"0\":{\"1769\":1},\"1\":{\"1769\":1}}],[\"convs=true\",{\"1\":{\"1800\":1}}],[\"convs\",{\"1\":{\"1765\":2,\"1778\":1,\"1800\":1,\"1803\":2,\"1844\":2,\"1848\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"1866\":2,\"2063\":1}}],[\"conv=true\",{\"1\":{\"1605\":1}}],[\"conv=false\",{\"1\":{\"1116\":1,\"1549\":1,\"1672\":1,\"1800\":1}}],[\"convtransp2d\",{\"0\":{\"1689\":1},\"1\":{\"1689\":1}}],[\"convtranspose2d\",{\"1\":{\"1577\":5}}],[\"convtasnet\",{\"1\":{\"1454\":1}}],[\"convmeanpool\",{\"0\":{\"1512\":1},\"1\":{\"1512\":1}}],[\"conv1x1\",{\"0\":{\"1690\":1,\"1731\":1},\"1\":{\"1375\":1,\"1690\":1,\"1731\":1}}],[\"conv1d1x1\",{\"0\":{\"1831\":1},\"1\":{\"1831\":1}}],[\"conv1dsubsampling3\",{\"0\":{\"716\":1},\"1\":{\"716\":2}}],[\"conv1dsubsampling2\",{\"0\":{\"715\":1},\"1\":{\"715\":2}}],[\"conv1dsubsampling1\",{\"0\":{\"714\":1},\"1\":{\"714\":2}}],[\"conv1dlinear\",{\"0\":{\"713\":1},\"1\":{\"711\":2,\"713\":2}}],[\"conv1d\",{\"0\":{\"712\":2,\"860\":1,\"1052\":2,\"1090\":1,\"1830\":1},\"1\":{\"21\":9,\"115\":4,\"712\":3,\"713\":2,\"749\":2,\"786\":3,\"860\":5,\"1052\":9,\"1090\":4,\"1148\":3,\"1149\":2,\"1150\":2,\"1203\":3,\"1272\":2,\"1505\":3,\"1645\":1,\"1669\":3,\"1671\":1,\"1767\":1,\"1768\":1,\"1771\":1,\"1778\":1,\"1787\":1,\"1788\":2,\"1798\":1,\"1804\":3,\"1805\":1,\"1830\":4,\"1831\":3,\"1850\":1,\"1851\":1,\"1852\":1,\"1874\":1,\"1877\":1,\"1878\":3,\"1917\":3,\"2029\":2,\"2054\":3,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2264\":1,\"2279\":1}}],[\"conv3x3\",{\"0\":{\"1305\":1,\"1691\":1,\"1732\":1},\"1\":{\"1305\":1,\"1691\":1,\"1732\":1}}],[\"convlutionmodule\",{\"1\":{\"711\":1}}],[\"convenience\",{\"1\":{\"2400\":1,\"2536\":1}}],[\"convenient\",{\"1\":{\"2011\":1,\"2437\":1,\"2563\":1,\"2585\":1}}],[\"convencoder\",{\"0\":{\"1511\":1},\"1\":{\"1511\":1}}],[\"convencoderlayer\",{\"1\":{\"712\":1}}],[\"conventional\",{\"1\":{\"768\":1,\"1158\":1,\"1989\":1,\"1991\":1,\"2248\":1,\"2250\":1,\"2385\":1,\"2467\":1}}],[\"convet\",{\"1\":{\"536\":1}}],[\"conversation\",{\"1\":{\"2467\":1}}],[\"conversations\",{\"1\":{\"2467\":1}}],[\"conversational\",{\"1\":{\"2387\":1}}],[\"conversaciones\",{\"1\":{\"2387\":2}}],[\"conversion\",{\"1\":{\"140\":1,\"159\":1,\"244\":1,\"269\":1,\"803\":1,\"1785\":1,\"1810\":1,\"2317\":1}}],[\"conversely\",{\"1\":{\"54\":1}}],[\"converts\",{\"1\":{\"701\":1,\"856\":1,\"1551\":3,\"1553\":3,\"2001\":1,\"2002\":1,\"2004\":1,\"2081\":1,\"2083\":1,\"2263\":1,\"2267\":1}}],[\"converter=<function\",{\"1\":{\"979\":1}}],[\"converter\",{\"0\":{\"304\":1,\"313\":1,\"320\":1,\"326\":1,\"332\":1,\"338\":1,\"390\":1,\"396\":1,\"407\":1,\"414\":1,\"428\":1,\"448\":1,\"454\":1,\"484\":1,\"490\":1,\"1454\":1,\"2123\":1,\"2128\":1,\"2135\":1},\"1\":{\"301\":1,\"599\":1,\"624\":1,\"625\":2,\"629\":2,\"709\":1,\"724\":1,\"727\":3,\"728\":3,\"742\":3,\"799\":1,\"936\":1,\"974\":1,\"999\":3,\"1041\":1,\"1454\":1,\"2123\":1,\"2128\":1,\"2135\":1,\"2474\":1,\"2649\":1}}],[\"converted\",{\"1\":{\"144\":2,\"239\":1,\"619\":1,\"652\":1,\"927\":1,\"1355\":1,\"2384\":1}}],[\"convert\",{\"0\":{\"53\":1,\"275\":1,\"519\":1,\"2306\":1},\"1\":{\"46\":2,\"49\":1,\"159\":1,\"217\":1,\"224\":1,\"231\":1,\"242\":1,\"275\":2,\"295\":1,\"363\":2,\"519\":3,\"533\":1,\"546\":1,\"549\":1,\"551\":1,\"554\":1,\"557\":1,\"570\":1,\"574\":1,\"579\":1,\"588\":1,\"590\":1,\"624\":1,\"625\":1,\"629\":1,\"648\":2,\"691\":1,\"701\":1,\"765\":1,\"778\":1,\"793\":1,\"798\":1,\"821\":1,\"826\":1,\"936\":1,\"1059\":2,\"1173\":2,\"1355\":1,\"1356\":1,\"1454\":1,\"1526\":1,\"1551\":1,\"1553\":1,\"1904\":3,\"1912\":1,\"1916\":3,\"2024\":1,\"2028\":1,\"2208\":1,\"2239\":1,\"2264\":1,\"2306\":2,\"2325\":1,\"2330\":1,\"2399\":1,\"2535\":1}}],[\"converting\",{\"0\":{\"46\":1},\"1\":{\"48\":1,\"267\":1}}],[\"convolvulaceae\",{\"1\":{\"2387\":1}}],[\"convolve\",{\"0\":{\"1814\":1},\"1\":{\"1814\":1}}],[\"convolving\",{\"1\":{\"115\":5,\"712\":1,\"1051\":1,\"1052\":1,\"1054\":1,\"1055\":1}}],[\"convolutive\",{\"1\":{\"1466\":1}}],[\"convolutions\",{\"1\":{\"1198\":7,\"1255\":1,\"1917\":2}}],[\"convolutionmodule\",{\"0\":{\"723\":1},\"1\":{\"723\":3}}],[\"convolutionalgatingmlp\",{\"0\":{\"1151\":1},\"1\":{\"1141\":1,\"1151\":1,\"1170\":1}}],[\"convolutionalspationgatingunit\",{\"1\":{\"1054\":1}}],[\"convolutionalspatialgatingunit\",{\"0\":{\"1054\":1,\"1153\":1},\"1\":{\"115\":6,\"1054\":4,\"1056\":1,\"1153\":1}}],[\"convolutional\",{\"1\":{\"21\":2,\"683\":2,\"701\":2,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"726\":1,\"737\":3,\"762\":1,\"821\":1,\"858\":1,\"859\":1,\"1054\":1,\"1132\":1,\"1151\":1,\"1153\":1,\"1198\":1,\"1269\":3,\"1379\":3,\"1510\":1,\"1511\":1,\"1522\":1,\"1523\":1,\"1545\":1,\"1664\":3,\"1665\":3,\"1696\":2,\"1698\":2,\"1771\":2,\"1772\":1,\"1776\":1,\"1777\":1,\"1788\":1,\"1808\":3,\"1833\":1,\"2265\":3,\"2290\":1,\"2292\":1}}],[\"convolution\",{\"0\":{\"723\":1,\"1051\":1,\"1054\":1,\"1055\":1},\"1\":{\"21\":5,\"115\":6,\"121\":1,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"688\":2,\"689\":2,\"701\":2,\"708\":1,\"711\":1,\"712\":4,\"723\":2,\"740\":5,\"741\":5,\"758\":2,\"775\":5,\"776\":5,\"826\":3,\"835\":1,\"892\":2,\"1049\":2,\"1050\":2,\"1051\":3,\"1052\":4,\"1053\":1,\"1054\":3,\"1055\":4,\"1056\":1,\"1093\":2,\"1097\":3,\"1148\":2,\"1198\":4,\"1203\":2,\"1241\":1,\"1245\":2,\"1247\":1,\"1248\":1,\"1269\":3,\"1375\":1,\"1377\":2,\"1505\":2,\"1516\":1,\"1543\":2,\"1564\":1,\"1656\":2,\"1658\":2,\"1659\":1,\"1688\":1,\"1690\":1,\"1691\":1,\"1716\":1,\"1731\":1,\"1732\":1,\"1756\":1,\"1765\":1,\"1782\":1,\"1787\":2,\"1788\":2,\"1798\":1,\"1800\":1,\"1803\":1,\"1804\":1,\"1834\":1,\"1844\":1,\"1848\":1,\"1849\":1,\"1851\":1,\"1856\":2,\"1857\":2,\"1858\":2,\"1862\":1,\"1866\":4,\"1867\":4,\"1871\":1,\"1874\":1,\"1878\":2,\"1880\":1,\"1917\":5,\"2054\":2,\"2264\":3}}],[\"convoluional\",{\"1\":{\"821\":1}}],[\"conv2dactnorm\",{\"0\":{\"1506\":1},\"1\":{\"1506\":1}}],[\"conv2dsubsamplingwoposenc\",{\"0\":{\"722\":1},\"1\":{\"722\":2}}],[\"conv2dsubsampling8\",{\"0\":{\"721\":1},\"1\":{\"721\":2}}],[\"conv2dsubsampling6\",{\"0\":{\"720\":1},\"1\":{\"720\":2}}],[\"conv2dsubsampling2\",{\"0\":{\"719\":1},\"1\":{\"719\":2}}],[\"conv2dsubsampling1\",{\"0\":{\"718\":1},\"1\":{\"718\":2}}],[\"conv2dsubsampling\",{\"0\":{\"717\":1},\"1\":{\"717\":2,\"718\":1,\"1095\":1}}],[\"conv2d\",{\"0\":{\"741\":1,\"1508\":1,\"1687\":1,\"1832\":1},\"1\":{\"115\":4,\"718\":1,\"741\":1,\"747\":2,\"749\":1,\"1097\":1,\"1140\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1169\":1,\"1200\":1,\"1203\":1,\"1272\":1,\"1506\":1,\"1508\":2,\"1543\":2,\"1576\":5,\"1662\":1,\"1687\":1,\"1688\":1,\"1756\":1,\"1832\":4,\"2558\":1,\"2564\":1,\"2584\":1}}],[\"conv2d2\",{\"1\":{\"102\":1,\"2054\":1,\"2439\":1,\"2440\":1}}],[\"conv\",{\"0\":{\"713\":1,\"740\":1,\"786\":1,\"1053\":1,\"1510\":1,\"1511\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1734\":1,\"1756\":1,\"1904\":1,\"1911\":1,\"1916\":1,\"1917\":1,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1,\"2368\":1,\"2486\":1,\"2605\":1,\"2622\":1},\"1\":{\"21\":4,\"115\":23,\"701\":8,\"711\":2,\"713\":2,\"723\":2,\"726\":2,\"740\":1,\"749\":2,\"786\":1,\"821\":7,\"826\":3,\"858\":3,\"859\":2,\"893\":1,\"1049\":2,\"1050\":2,\"1051\":2,\"1053\":3,\"1054\":1,\"1055\":1,\"1056\":4,\"1093\":4,\"1097\":2,\"1115\":8,\"1132\":1,\"1140\":2,\"1148\":2,\"1149\":2,\"1150\":2,\"1151\":1,\"1153\":1,\"1167\":3,\"1168\":3,\"1169\":3,\"1170\":3,\"1196\":3,\"1197\":3,\"1203\":2,\"1220\":2,\"1269\":8,\"1272\":2,\"1289\":2,\"1375\":3,\"1379\":1,\"1505\":2,\"1510\":1,\"1511\":1,\"1522\":7,\"1523\":7,\"1545\":4,\"1633\":1,\"1635\":1,\"1655\":1,\"1664\":2,\"1665\":2,\"1669\":2,\"1687\":1,\"1688\":1,\"1689\":1,\"1719\":1,\"1734\":2,\"1756\":1,\"1765\":3,\"1771\":5,\"1778\":1,\"1787\":4,\"1788\":4,\"1798\":6,\"1800\":4,\"1803\":3,\"1804\":6,\"1805\":2,\"1834\":2,\"1835\":1,\"1844\":3,\"1848\":5,\"1849\":4,\"1850\":5,\"1851\":16,\"1852\":5,\"1856\":2,\"1857\":4,\"1858\":2,\"1861\":6,\"1862\":6,\"1863\":1,\"1868\":4,\"1871\":4,\"1872\":1,\"1873\":1,\"1874\":6,\"1877\":3,\"1878\":9,\"1880\":13,\"1904\":2,\"1911\":2,\"1916\":2,\"1917\":2,\"1973\":1,\"2002\":2,\"2026\":1,\"2029\":2,\"2054\":2,\"2086\":9,\"2087\":9,\"2090\":1,\"2095\":17,\"2243\":13,\"2244\":13,\"2255\":13,\"2257\":12,\"2261\":12,\"2263\":17,\"2264\":21,\"2279\":1,\"2290\":5,\"2292\":4,\"2297\":1,\"2299\":1,\"2368\":2,\"2371\":2,\"2440\":2,\"2486\":2,\"2564\":2,\"2605\":2,\"2612\":2,\"2622\":2,\"2630\":3,\"2641\":2}}],[\"confused\",{\"1\":{\"2573\":1}}],[\"confusing\",{\"1\":{\"1013\":1}}],[\"confusion\",{\"1\":{\"62\":1}}],[\"confformer\",{\"1\":{\"2377\":1}}],[\"conffile=none\",{\"1\":{\"958\":1}}],[\"confwarmup\",{\"1\":{\"2357\":1,\"2578\":1}}],[\"conf=\",{\"1\":{\"2314\":4}}],[\"conf=none\",{\"1\":{\"692\":1,\"987\":1}}],[\"confirm=t\",{\"1\":{\"2506\":1,\"2510\":2,\"2512\":1}}],[\"confidently\",{\"1\":{\"929\":1}}],[\"configs\",{\"0\":{\"2376\":1,\"2435\":1,\"2561\":1},\"1\":{\"2377\":1,\"2435\":1,\"2436\":1,\"2558\":2,\"2561\":1,\"2562\":1,\"2564\":1,\"2570\":1}}],[\"config=cfg\",{\"1\":{\"2371\":1,\"2612\":1,\"2630\":1}}],[\"config=\",{\"1\":{\"2368\":1,\"2455\":1,\"2460\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2648\":1,\"2649\":1}}],[\"configurable\",{\"1\":{\"1217\":2}}],[\"configurations\",{\"1\":{\"25\":1,\"109\":1,\"115\":3,\"133\":1,\"150\":1,\"169\":1,\"181\":1,\"240\":1,\"606\":1,\"734\":1,\"817\":1,\"828\":1,\"2423\":1,\"2546\":1}}],[\"configuration\",{\"0\":{\"25\":1,\"61\":1,\"63\":1,\"64\":1,\"92\":1,\"144\":1,\"303\":1,\"310\":1,\"318\":1,\"324\":1,\"330\":1,\"336\":1,\"342\":1,\"346\":1,\"354\":1,\"360\":1,\"372\":1,\"383\":1,\"387\":1,\"394\":1,\"402\":1,\"411\":1,\"419\":1,\"425\":1,\"432\":1,\"440\":1,\"446\":1,\"452\":1,\"458\":1,\"467\":1,\"473\":1,\"481\":1,\"488\":1,\"2639\":1},\"1\":{\"1\":1,\"15\":1,\"48\":1,\"62\":2,\"63\":3,\"85\":3,\"92\":3,\"102\":2,\"104\":1,\"115\":5,\"116\":1,\"117\":1,\"122\":1,\"126\":1,\"142\":2,\"144\":2,\"235\":3,\"240\":1,\"286\":2,\"295\":1,\"296\":2,\"1058\":3,\"1071\":1,\"1087\":3,\"1088\":3,\"1089\":3,\"1090\":3,\"1091\":3,\"1092\":3,\"1094\":3,\"1103\":2,\"1104\":3,\"1105\":4,\"1115\":1,\"1198\":1,\"1269\":2,\"1559\":1,\"1560\":1,\"1718\":1,\"1930\":1,\"2099\":1,\"2176\":1,\"2372\":2,\"2378\":1,\"2385\":2,\"2429\":2,\"2437\":1,\"2542\":1,\"2554\":2,\"2563\":1,\"2584\":2,\"2600\":3,\"2639\":2}}],[\"configured\",{\"1\":{\"121\":1,\"1243\":1}}],[\"configure\",{\"0\":{\"1974\":1},\"1\":{\"64\":1,\"114\":1,\"134\":3,\"1245\":1,\"1974\":1}}],[\"configargparse\",{\"1\":{\"63\":1,\"207\":1,\"2309\":1}}],[\"config3\",{\"1\":{\"25\":1,\"247\":2,\"249\":2,\"251\":2,\"253\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2}}],[\"config2\",{\"1\":{\"25\":2,\"247\":2,\"249\":2,\"251\":2,\"253\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2}}],[\"config\",{\"0\":{\"2306\":1,\"2307\":1,\"2308\":2,\"2309\":1,\"2377\":1,\"2436\":1,\"2439\":1,\"2562\":1,\"2570\":1,\"2714\":1},\"1\":{\"20\":1,\"21\":1,\"22\":1,\"23\":1,\"25\":7,\"26\":2,\"27\":1,\"62\":5,\"63\":3,\"92\":1,\"114\":1,\"119\":1,\"173\":3,\"186\":1,\"187\":2,\"188\":1,\"189\":1,\"240\":4,\"241\":2,\"242\":1,\"245\":2,\"247\":2,\"249\":5,\"251\":2,\"253\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2,\"286\":2,\"296\":2,\"301\":4,\"307\":8,\"315\":10,\"321\":4,\"327\":8,\"333\":8,\"339\":4,\"343\":4,\"350\":6,\"357\":6,\"363\":2,\"368\":6,\"380\":4,\"384\":6,\"391\":8,\"398\":28,\"399\":6,\"408\":8,\"416\":4,\"422\":8,\"429\":4,\"437\":4,\"443\":12,\"449\":8,\"455\":6,\"464\":6,\"470\":6,\"476\":2,\"478\":8,\"485\":10,\"641\":4,\"668\":1,\"693\":5,\"742\":2,\"747\":1,\"1244\":4,\"1245\":2,\"1252\":4,\"1254\":4,\"1269\":2,\"1327\":3,\"1603\":1,\"1622\":1,\"1699\":2,\"1709\":2,\"1761\":2,\"1763\":2,\"1773\":1,\"1805\":2,\"2082\":1,\"2099\":6,\"2109\":1,\"2113\":1,\"2115\":1,\"2116\":1,\"2240\":1,\"2254\":1,\"2278\":1,\"2306\":2,\"2307\":2,\"2308\":5,\"2309\":4,\"2350\":1,\"2368\":1,\"2371\":1,\"2377\":4,\"2398\":1,\"2400\":1,\"2401\":2,\"2403\":1,\"2405\":1,\"2429\":1,\"2436\":4,\"2437\":1,\"2438\":2,\"2440\":3,\"2455\":1,\"2460\":1,\"2461\":2,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2514\":1,\"2520\":1,\"2534\":1,\"2536\":1,\"2537\":2,\"2539\":1,\"2541\":1,\"2543\":1,\"2558\":4,\"2559\":4,\"2562\":4,\"2563\":1,\"2564\":5,\"2569\":1,\"2570\":2,\"2573\":1,\"2584\":5,\"2585\":2,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2639\":1,\"2648\":1,\"2649\":1,\"2659\":1}}],[\"conference\",{\"1\":{\"130\":2,\"2030\":1}}],[\"conflr0\",{\"1\":{\"110\":3,\"2357\":1,\"2578\":1}}],[\"conformers\",{\"1\":{\"2468\":1}}],[\"conformerseparator\",{\"0\":{\"1505\":1},\"1\":{\"1505\":1}}],[\"conformer10\",{\"1\":{\"2436\":1,\"2562\":1}}],[\"conformer7\",{\"1\":{\"2377\":2}}],[\"conformer2\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"conformer6\",{\"1\":{\"2357\":3,\"2578\":3}}],[\"conformerpostencoder\",{\"0\":{\"2026\":1},\"1\":{\"2026\":1}}],[\"conformerencoder\",{\"0\":{\"1148\":1},\"1\":{\"1148\":1,\"1203\":1}}],[\"conformerconvolution\",{\"0\":{\"1051\":1},\"1\":{\"115\":3,\"1051\":7}}],[\"conformer\",{\"0\":{\"711\":1,\"723\":1,\"819\":1,\"839\":2,\"859\":1,\"935\":1,\"1050\":2,\"1089\":1,\"1148\":1,\"1149\":1,\"1505\":1,\"2026\":1,\"2054\":1},\"1\":{\"21\":4,\"30\":2,\"103\":1,\"104\":2,\"110\":3,\"115\":7,\"711\":1,\"723\":2,\"819\":1,\"839\":2,\"859\":4,\"935\":1,\"1050\":8,\"1068\":2,\"1089\":4,\"1093\":1,\"1148\":2,\"1149\":2,\"1203\":1,\"1505\":14,\"1771\":6,\"1778\":8,\"1787\":6,\"1788\":6,\"1798\":7,\"1804\":20,\"1805\":2,\"1850\":8,\"1851\":25,\"1852\":10,\"1874\":7,\"1877\":2,\"1878\":21,\"2003\":8,\"2026\":1,\"2054\":5,\"2090\":10,\"2243\":25,\"2244\":25,\"2255\":24,\"2279\":25,\"2363\":8,\"2377\":1,\"2400\":3,\"2436\":1,\"2454\":1,\"2455\":2,\"2460\":3,\"2461\":2,\"2472\":3,\"2474\":2,\"2476\":3,\"2478\":3,\"2506\":2,\"2512\":4,\"2536\":3,\"2558\":1,\"2562\":1,\"2574\":1,\"2648\":3,\"2649\":2,\"2653\":10,\"2657\":4}}],[\"conf\",{\"0\":{\"641\":1},\"1\":{\"15\":1,\"25\":12,\"27\":1,\"62\":2,\"63\":3,\"64\":6,\"85\":1,\"92\":3,\"102\":3,\"104\":1,\"113\":6,\"115\":11,\"116\":4,\"117\":3,\"118\":2,\"121\":2,\"124\":1,\"142\":6,\"144\":2,\"150\":3,\"169\":1,\"181\":1,\"186\":2,\"187\":1,\"189\":1,\"194\":3,\"217\":2,\"224\":2,\"231\":2,\"235\":1,\"240\":7,\"241\":2,\"242\":1,\"245\":4,\"247\":4,\"249\":8,\"251\":4,\"255\":2,\"257\":4,\"259\":4,\"261\":2,\"263\":4,\"265\":2,\"267\":4,\"269\":2,\"276\":1,\"281\":1,\"285\":6,\"286\":1,\"296\":1,\"307\":4,\"422\":2,\"429\":2,\"443\":2,\"449\":2,\"506\":2,\"522\":2,\"525\":2,\"533\":2,\"641\":4,\"987\":1,\"1058\":6,\"1071\":2,\"1103\":4,\"1105\":2,\"1158\":1,\"1215\":1,\"1220\":1,\"1239\":1,\"1791\":1,\"1930\":2,\"1974\":2,\"2176\":3,\"2191\":1,\"2314\":7,\"2372\":1,\"2377\":8,\"2378\":2,\"2385\":1,\"2394\":1,\"2398\":1,\"2400\":1,\"2401\":2,\"2403\":1,\"2405\":1,\"2429\":3,\"2436\":7,\"2437\":2,\"2440\":8,\"2530\":1,\"2534\":1,\"2536\":1,\"2537\":2,\"2539\":1,\"2541\":1,\"2554\":1,\"2558\":9,\"2559\":3,\"2562\":7,\"2563\":2,\"2564\":3,\"2566\":1,\"2569\":1,\"2570\":1,\"2584\":9,\"2585\":2,\"2639\":3}}],[\"cdim\",{\"1\":{\"805\":2,\"808\":3}}],[\"cd\",{\"1\":{\"1\":1,\"2\":1,\"3\":3,\"4\":1,\"10\":1,\"12\":1,\"16\":1,\"47\":1,\"49\":1,\"85\":4,\"127\":1,\"134\":5,\"135\":11,\"136\":4,\"137\":1,\"167\":6,\"168\":1,\"178\":6,\"180\":1,\"186\":1,\"187\":1,\"190\":1,\"196\":7,\"197\":1,\"198\":1,\"200\":2,\"204\":1,\"207\":1,\"234\":6,\"2372\":5,\"2384\":1,\"2385\":1,\"2387\":1,\"2393\":1,\"2394\":2,\"2396\":1,\"2409\":2,\"2411\":1,\"2428\":1,\"2429\":3,\"2450\":3,\"2504\":1,\"2517\":3,\"2529\":1,\"2530\":2,\"2532\":1,\"2550\":1,\"2551\":1,\"2552\":2,\"2554\":2,\"2566\":2,\"2567\":2,\"2584\":3,\"2585\":1}}],[\"$$\",{\"1\":{\"2600\":2}}],[\"$p\",{\"1\":{\"1618\":2,\"1619\":2,\"1638\":2}}],[\"$t\",{\"1\":{\"1517\":2}}],[\"$time\",{\"1\":{\"1517\":1}}],[\"$f\",{\"1\":{\"1517\":2}}],[\"$nfreqs\",{\"1\":{\"1517\":1}}],[\"$b\",{\"1\":{\"875\":1}}],[\"$b$\",{\"1\":{\"875\":1}}],[\"$v\",{\"1\":{\"875\":1}}],[\"$0\",{\"1\":{\"144\":3,\"2568\":1}}],[\"$cuda\",{\"1\":{\"136\":1}}],[\"$\",{\"1\":{\"1\":2,\"2\":2,\"3\":6,\"4\":2,\"5\":1,\"10\":3,\"11\":2,\"12\":2,\"16\":3,\"17\":4,\"18\":2,\"19\":4,\"25\":2,\"26\":3,\"37\":2,\"38\":2,\"40\":2,\"41\":2,\"42\":2,\"47\":5,\"90\":2,\"98\":13,\"128\":3,\"132\":5,\"134\":13,\"135\":31,\"143\":5,\"149\":1,\"187\":2,\"295\":1,\"1245\":2,\"1517\":2,\"1618\":2,\"1619\":2,\"1638\":2,\"2143\":1,\"2568\":23,\"2569\":1,\"2599\":1}}],[\"b0\",{\"1\":{\"2500\":9,\"2617\":9,\"2635\":9}}],[\"b64decode\",{\"1\":{\"2360\":2,\"2458\":2,\"2523\":2,\"2582\":2}}],[\"bw2\",{\"1\":{\"2309\":1}}],[\"btaps=5\",{\"1\":{\"1739\":1}}],[\"btaps+1\",{\"1\":{\"1524\":3,\"1696\":5,\"1697\":3,\"1702\":2}}],[\"btaps\",{\"1\":{\"1524\":1,\"1696\":1,\"1702\":2,\"1736\":2,\"1739\":1}}],[\"btype=\",{\"1\":{\"729\":1}}],[\"btype\",{\"1\":{\"251\":1,\"756\":1,\"1158\":1,\"1239\":1,\"1524\":1,\"1791\":1}}],[\"bdelay=3\",{\"1\":{\"1739\":1}}],[\"bdelay\",{\"1\":{\"1524\":1,\"1702\":1,\"1736\":1,\"1739\":1,\"1741\":3}}],[\"bdt\",{\"1\":{\"1472\":1,\"1575\":1,\"1683\":1}}],[\"bdropout\",{\"1\":{\"251\":2,\"756\":1,\"1158\":1,\"1239\":1,\"1611\":1,\"1791\":1}}],[\"bf\",{\"0\":{\"1460\":1},\"1\":{\"1460\":2,\"1611\":2}}],[\"b3\",{\"1\":{\"1425\":2}}],[\"bb\",{\"1\":{\"1397\":1,\"1404\":1,\"1408\":3,\"1412\":1,\"1416\":1}}],[\"b2text\",{\"1\":{\"2360\":2,\"2458\":2,\"2523\":2,\"2582\":2}}],[\"b2\",{\"1\":{\"1391\":1,\"1406\":1,\"1425\":2}}],[\"b1\",{\"1\":{\"1391\":1,\"1425\":2,\"2500\":9,\"2617\":9,\"2635\":9}}],[\"bnonlinear\",{\"1\":{\"1611\":1}}],[\"bnet\",{\"1\":{\"1611\":1}}],[\"bn\",{\"1\":{\"1377\":1,\"1379\":1,\"1516\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1658\":1,\"1659\":1,\"1664\":1,\"1665\":1,\"2292\":1,\"2584\":1,\"2585\":5}}],[\"bnmask=2\",{\"1\":{\"729\":1}}],[\"bnmask\",{\"1\":{\"251\":2,\"756\":1,\"1158\":1,\"1239\":1,\"1791\":1}}],[\"b^dt\",{\"1\":{\"1248\":1}}],[\"b=3\",{\"1\":{\"2323\":1}}],[\"b=b\",{\"1\":{\"2176\":1}}],[\"b=batch\",{\"1\":{\"1243\":1}}],[\"b=false\",{\"1\":{\"1314\":1}}],[\"b=2\",{\"1\":{\"944\":1}}],[\"bsrnnseparator\",{\"0\":{\"1463\":1},\"1\":{\"1463\":1}}],[\"bsrnn\",{\"0\":{\"1462\":2,\"1463\":1,\"1464\":1,\"1594\":1},\"1\":{\"1462\":6,\"1463\":3,\"1464\":2,\"1594\":2}}],[\"bs\",{\"1\":{\"959\":2,\"1071\":2,\"2198\":1}}],[\"bss\",{\"1\":{\"528\":3,\"1640\":1}}],[\"bpttupdater\",{\"0\":{\"604\":1},\"1\":{\"604\":2}}],[\"bprojs=320\",{\"1\":{\"729\":1}}],[\"bprojs\",{\"1\":{\"251\":2,\"756\":1,\"1158\":1,\"1239\":1,\"1524\":1,\"1611\":1,\"1791\":1}}],[\"bpe500\",{\"1\":{\"2590\":1,\"2600\":2}}],[\"bpe5000\",{\"1\":{\"110\":2,\"2357\":3,\"2578\":3}}],[\"bpes\",{\"1\":{\"2585\":1}}],[\"bpe1000\",{\"1\":{\"2585\":2}}],[\"bpe30\",{\"0\":{\"2443\":1},\"1\":{\"2375\":6,\"2440\":6,\"2441\":1,\"2558\":3,\"2559\":6,\"2560\":1,\"2564\":3,\"2584\":1}}],[\"bpe7000\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"bpemodel=none\",{\"1\":{\"2592\":1}}],[\"bpemodel\",{\"1\":{\"301\":2,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"422\":2,\"443\":4,\"449\":2,\"461\":2,\"478\":2,\"485\":2,\"2137\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1}}],[\"bpe\",{\"1\":{\"110\":3,\"150\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"422\":1,\"443\":2,\"449\":1,\"461\":1,\"478\":1,\"546\":2,\"2349\":2,\"2373\":2,\"2414\":1,\"2433\":1,\"2454\":1,\"2455\":2,\"2460\":3,\"2461\":2,\"2555\":1,\"2556\":1,\"2569\":1}}],[\"bgrup\",{\"1\":{\"251\":2}}],[\"bgru\",{\"1\":{\"251\":2}}],[\"bce\",{\"1\":{\"240\":2,\"821\":1,\"822\":2,\"826\":2,\"2000\":1,\"2095\":2,\"2263\":2,\"2264\":3}}],[\"bc\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"200\":1,\"234\":1}}],[\"b\",{\"0\":{\"211\":1,\"215\":1,\"223\":1,\"230\":1},\"1\":{\"47\":3,\"52\":2,\"53\":2,\"58\":6,\"135\":1,\"167\":1,\"178\":1,\"196\":1,\"207\":1,\"234\":1,\"240\":1,\"629\":3,\"676\":9,\"677\":5,\"678\":4,\"679\":4,\"681\":5,\"682\":6,\"684\":6,\"685\":10,\"686\":5,\"687\":5,\"688\":6,\"689\":6,\"690\":4,\"700\":1,\"701\":4,\"702\":3,\"705\":4,\"708\":2,\"712\":12,\"713\":2,\"725\":12,\"726\":6,\"729\":5,\"730\":5,\"731\":3,\"732\":2,\"735\":6,\"737\":6,\"738\":2,\"742\":5,\"747\":2,\"754\":12,\"755\":7,\"758\":7,\"759\":1,\"762\":3,\"763\":3,\"764\":2,\"766\":3,\"772\":1,\"774\":3,\"781\":7,\"782\":5,\"786\":2,\"793\":2,\"794\":3,\"799\":3,\"802\":2,\"804\":2,\"806\":22,\"810\":1,\"812\":2,\"820\":1,\"821\":12,\"822\":6,\"825\":35,\"826\":10,\"827\":5,\"830\":2,\"835\":3,\"838\":5,\"852\":3,\"863\":2,\"884\":2,\"898\":2,\"899\":1,\"901\":1,\"903\":3,\"905\":3,\"906\":2,\"920\":2,\"924\":2,\"925\":2,\"932\":2,\"944\":2,\"1001\":4,\"1010\":3,\"1011\":7,\"1028\":2,\"1047\":2,\"1049\":11,\"1050\":11,\"1051\":3,\"1052\":17,\"1053\":4,\"1054\":3,\"1055\":3,\"1056\":11,\"1057\":14,\"1058\":5,\"1059\":8,\"1060\":1,\"1062\":4,\"1064\":6,\"1065\":7,\"1066\":7,\"1068\":7,\"1069\":9,\"1070\":2,\"1072\":2,\"1073\":18,\"1074\":4,\"1075\":12,\"1076\":26,\"1077\":3,\"1080\":2,\"1081\":8,\"1083\":3,\"1086\":6,\"1099\":6,\"1102\":2,\"1116\":5,\"1132\":3,\"1136\":1,\"1138\":1,\"1139\":1,\"1142\":8,\"1145\":10,\"1149\":6,\"1150\":6,\"1155\":2,\"1160\":4,\"1161\":4,\"1164\":4,\"1165\":5,\"1173\":8,\"1177\":4,\"1178\":2,\"1179\":3,\"1180\":2,\"1181\":2,\"1186\":7,\"1198\":3,\"1200\":2,\"1209\":4,\"1210\":5,\"1228\":3,\"1243\":8,\"1245\":5,\"1246\":3,\"1247\":3,\"1248\":6,\"1251\":2,\"1252\":4,\"1253\":4,\"1254\":4,\"1255\":3,\"1256\":1,\"1269\":2,\"1270\":22,\"1272\":2,\"1274\":1,\"1276\":1,\"1298\":7,\"1299\":7,\"1301\":9,\"1302\":7,\"1303\":7,\"1304\":9,\"1332\":2,\"1333\":1,\"1334\":2,\"1337\":6,\"1339\":5,\"1345\":3,\"1347\":3,\"1349\":6,\"1350\":6,\"1357\":1,\"1370\":1,\"1375\":1,\"1377\":3,\"1378\":2,\"1379\":3,\"1383\":1,\"1389\":1,\"1395\":1,\"1397\":1,\"1406\":2,\"1414\":1,\"1416\":1,\"1421\":2,\"1432\":3,\"1436\":2,\"1452\":3,\"1454\":4,\"1455\":4,\"1462\":2,\"1463\":3,\"1464\":2,\"1478\":2,\"1505\":3,\"1510\":3,\"1511\":2,\"1515\":4,\"1516\":8,\"1522\":3,\"1523\":1,\"1524\":23,\"1525\":11,\"1528\":3,\"1529\":4,\"1530\":1,\"1534\":3,\"1539\":3,\"1545\":2,\"1546\":1,\"1558\":2,\"1560\":2,\"1572\":2,\"1576\":3,\"1577\":3,\"1578\":2,\"1579\":2,\"1580\":2,\"1594\":3,\"1595\":5,\"1626\":3,\"1643\":3,\"1644\":3,\"1645\":3,\"1654\":3,\"1655\":2,\"1658\":3,\"1659\":5,\"1660\":6,\"1661\":6,\"1662\":6,\"1663\":2,\"1664\":2,\"1665\":2,\"1669\":3,\"1671\":4,\"1695\":5,\"1696\":4,\"1697\":4,\"1702\":4,\"1719\":8,\"1727\":1,\"1736\":3,\"1742\":2,\"1765\":3,\"1766\":1,\"1767\":4,\"1768\":3,\"1771\":5,\"1772\":4,\"1773\":38,\"1776\":4,\"1777\":3,\"1778\":20,\"1781\":8,\"1786\":1,\"1787\":3,\"1788\":5,\"1798\":8,\"1800\":4,\"1803\":4,\"1804\":41,\"1805\":23,\"1807\":1,\"1808\":15,\"1811\":2,\"1829\":6,\"1833\":5,\"1834\":2,\"1835\":4,\"1837\":20,\"1838\":4,\"1840\":3,\"1841\":3,\"1842\":5,\"1844\":3,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":11,\"1851\":35,\"1852\":6,\"1853\":5,\"1854\":4,\"1855\":4,\"1856\":1,\"1857\":2,\"1858\":1,\"1859\":3,\"1860\":4,\"1861\":2,\"1862\":3,\"1863\":7,\"1864\":4,\"1865\":5,\"1866\":2,\"1867\":2,\"1868\":6,\"1869\":2,\"1870\":1,\"1871\":3,\"1872\":4,\"1873\":4,\"1874\":6,\"1876\":2,\"1877\":11,\"1878\":34,\"1879\":7,\"1880\":7,\"1881\":5,\"1884\":4,\"1885\":3,\"1889\":4,\"1906\":2,\"1917\":2,\"1920\":2,\"1949\":2,\"1971\":3,\"2000\":6,\"2001\":7,\"2002\":7,\"2004\":7,\"2029\":2,\"2044\":1,\"2049\":1,\"2063\":5,\"2074\":5,\"2075\":5,\"2078\":8,\"2081\":3,\"2082\":46,\"2083\":5,\"2086\":18,\"2087\":18,\"2088\":4,\"2090\":19,\"2091\":11,\"2095\":18,\"2154\":2,\"2162\":1,\"2176\":1,\"2240\":20,\"2243\":9,\"2244\":13,\"2245\":11,\"2255\":13,\"2256\":11,\"2257\":2,\"2261\":2,\"2262\":2,\"2263\":7,\"2264\":7,\"2265\":3,\"2278\":24,\"2279\":13,\"2280\":11,\"2321\":1,\"2323\":1,\"2360\":2,\"2409\":1,\"2432\":1,\"2450\":1,\"2458\":2,\"2504\":1,\"2517\":1,\"2523\":2,\"2529\":1,\"2582\":2,\"2592\":1}}],[\"broadening\",{\"1\":{\"2452\":1}}],[\"broadcasts\",{\"1\":{\"1679\":1}}],[\"broadcast\",{\"0\":{\"1679\":1},\"1\":{\"1679\":2}}],[\"brain\",{\"1\":{\"2417\":1}}],[\"branches\",{\"1\":{\"1352\":1}}],[\"branchformerencoderlayer\",{\"0\":{\"1141\":1},\"1\":{\"1141\":1}}],[\"branchformerencoder\",{\"0\":{\"1140\":1},\"1\":{\"1140\":1}}],[\"branchformer\",{\"0\":{\"1049\":2,\"1088\":1,\"1140\":1,\"1141\":1,\"1169\":1,\"1170\":1,\"2443\":1},\"1\":{\"115\":6,\"1049\":8,\"1056\":6,\"1068\":2,\"1088\":4,\"1091\":3,\"1140\":2,\"1141\":2,\"1169\":2,\"1170\":2,\"2429\":1,\"2436\":2,\"2438\":1,\"2440\":9,\"2441\":1,\"2520\":1,\"2558\":1,\"2562\":2,\"2564\":6,\"2569\":1,\"2570\":3,\"2572\":3}}],[\"branch\",{\"1\":{\"5\":1,\"13\":1,\"1130\":1,\"1140\":1,\"1141\":4,\"1377\":1,\"2393\":1,\"2440\":1,\"2450\":1,\"2529\":1,\"2564\":1}}],[\"brctc\",{\"1\":{\"1145\":3}}],[\"brnn\",{\"1\":{\"915\":1}}],[\"break=none\",{\"1\":{\"2315\":1}}],[\"break\",{\"1\":{\"238\":2,\"2375\":1,\"2386\":1,\"2559\":1,\"2596\":1}}],[\"briefly\",{\"1\":{\"2387\":1,\"2394\":1,\"2418\":1,\"2419\":1,\"2530\":1}}],[\"brian\",{\"1\":{\"130\":3}}],[\"bridgettesong\",{\"1\":{\"1986\":2}}],[\"bridge\",{\"1\":{\"45\":1}}],[\"buf\",{\"1\":{\"2592\":2}}],[\"buffer=chunk\",{\"1\":{\"2596\":1}}],[\"buffer=none\",{\"1\":{\"1446\":1,\"1658\":1}}],[\"buffers\",{\"1\":{\"1217\":1}}],[\"buffer\",{\"1\":{\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"1432\":1,\"1436\":1,\"1510\":1,\"1511\":1,\"1643\":1,\"1644\":1,\"1800\":1,\"1957\":1,\"1960\":1}}],[\"buffered\",{\"1\":{\"710\":1}}],[\"buff\",{\"1\":{\"710\":1}}],[\"bunits=300\",{\"1\":{\"729\":1}}],[\"bunits\",{\"1\":{\"251\":2,\"756\":1,\"1158\":1,\"1239\":1,\"1524\":1,\"1611\":1,\"1791\":1}}],[\"bundle=\",{\"1\":{\"70\":1}}],[\"bundle=your\",{\"1\":{\"70\":1}}],[\"buy03\",{\"1\":{\"211\":1}}],[\"buggy\",{\"1\":{\"2520\":1}}],[\"bug\",{\"1\":{\"84\":1,\"2368\":1,\"2486\":1,\"2490\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"bugs\",{\"1\":{\"83\":1,\"226\":1}}],[\"but\",{\"1\":{\"44\":1,\"46\":1,\"49\":1,\"62\":1,\"69\":1,\"76\":1,\"84\":1,\"85\":1,\"106\":1,\"107\":1,\"115\":1,\"117\":1,\"122\":1,\"126\":1,\"132\":1,\"235\":1,\"238\":1,\"684\":1,\"704\":1,\"705\":1,\"718\":1,\"737\":1,\"739\":1,\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"1245\":1,\"1406\":2,\"1545\":2,\"1618\":1,\"1619\":1,\"1778\":2,\"1805\":2,\"1850\":2,\"1852\":2,\"1877\":2,\"2019\":1,\"2279\":1,\"2309\":1,\"2354\":1,\"2362\":1,\"2372\":1,\"2373\":1,\"2384\":2,\"2385\":2,\"2387\":1,\"2395\":1,\"2412\":1,\"2414\":1,\"2428\":1,\"2430\":1,\"2433\":1,\"2467\":1,\"2504\":1,\"2506\":1,\"2514\":1,\"2531\":1,\"2551\":1,\"2555\":2,\"2558\":1,\"2568\":1,\"2576\":1,\"2584\":3,\"2600\":1,\"2651\":1,\"2659\":1}}],[\"builtin\",{\"1\":{\"251\":1,\"259\":1,\"944\":1,\"1067\":2,\"1084\":2,\"1145\":2,\"1516\":2,\"1644\":1,\"1660\":3,\"1661\":3,\"1662\":2,\"1704\":1}}],[\"built\",{\"1\":{\"1\":2,\"5\":2,\"57\":1,\"69\":1,\"2468\":1}}],[\"buildmodel\",{\"1\":{\"2401\":1,\"2537\":1}}],[\"building\",{\"0\":{\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1},\"1\":{\"5\":1,\"57\":1,\"124\":2,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1198\":1,\"1506\":1,\"1551\":1,\"1553\":1,\"2468\":1}}],[\"builds\",{\"0\":{\"5\":1},\"1\":{\"5\":3,\"126\":1}}],[\"build\",{\"0\":{\"126\":1,\"173\":1,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1289\":1,\"1882\":1,\"2012\":2,\"2137\":2,\"2318\":2},\"1\":{\"1\":1,\"4\":1,\"5\":6,\"6\":1,\"12\":1,\"56\":5,\"57\":1,\"60\":3,\"69\":2,\"115\":1,\"124\":2,\"133\":1,\"134\":1,\"167\":2,\"170\":1,\"178\":2,\"196\":2,\"234\":2,\"528\":2,\"564\":1,\"661\":1,\"672\":1,\"676\":1,\"727\":1,\"728\":1,\"759\":1,\"767\":1,\"781\":1,\"858\":2,\"859\":2,\"860\":2,\"861\":2,\"862\":2,\"1087\":2,\"1088\":2,\"1089\":2,\"1090\":2,\"1091\":2,\"1092\":2,\"1093\":2,\"1094\":2,\"1116\":2,\"1289\":1,\"1894\":1,\"1895\":2,\"1897\":2,\"1898\":2,\"1900\":2,\"2012\":2,\"2096\":4,\"2097\":5,\"2098\":4,\"2099\":23,\"2100\":4,\"2101\":4,\"2102\":9,\"2103\":5,\"2104\":5,\"2105\":4,\"2107\":4,\"2108\":4,\"2109\":5,\"2110\":4,\"2111\":2,\"2112\":4,\"2113\":5,\"2114\":4,\"2115\":5,\"2116\":5,\"2117\":4,\"2118\":5,\"2137\":2,\"2168\":1,\"2170\":1,\"2185\":2,\"2201\":2,\"2203\":2,\"2318\":3,\"2345\":1,\"2350\":1,\"2358\":1,\"2468\":1,\"2474\":1,\"2520\":1,\"2579\":1,\"2584\":1,\"2649\":1}}],[\"boy\",{\"1\":{\"2567\":4,\"2568\":2}}],[\"boyer\",{\"1\":{\"119\":1}}],[\"bouns\",{\"1\":{\"2530\":4,\"2543\":1}}],[\"bound=1\",{\"1\":{\"1886\":1,\"1888\":1}}],[\"bound\",{\"1\":{\"1517\":1,\"1833\":3,\"1925\":2}}],[\"boundaries\",{\"1\":{\"1036\":1}}],[\"boundary\",{\"1\":{\"1011\":1}}],[\"boldface\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"bosswissam\",{\"1\":{\"2324\":1}}],[\"boxcar\",{\"1\":{\"2210\":1}}],[\"box\",{\"1\":{\"1254\":3,\"2414\":1,\"2418\":1,\"2419\":1,\"2420\":1,\"2461\":1,\"2462\":1}}],[\"bonus\",{\"0\":{\"773\":1,\"2542\":1,\"2543\":1,\"2564\":1},\"1\":{\"773\":2,\"815\":1,\"2542\":1,\"2564\":1}}],[\"bonding\",{\"1\":{\"45\":1}}],[\"bond\",{\"1\":{\"45\":2}}],[\"bots\",{\"1\":{\"2467\":1}}],[\"botan\",{\"1\":{\"2387\":1}}],[\"botany\",{\"1\":{\"2387\":2}}],[\"bottle2neck\",{\"0\":{\"2042\":1},\"1\":{\"2042\":1,\"2055\":1,\"2064\":1}}],[\"bottleneck=128\",{\"1\":{\"2061\":1}}],[\"bottleneck=none\",{\"1\":{\"1241\":1}}],[\"bottleneck\",{\"1\":{\"148\":1,\"1113\":1,\"1243\":1,\"1366\":1,\"1371\":3,\"1375\":5,\"1377\":5,\"1379\":2,\"1658\":3,\"1659\":3,\"1664\":1,\"1665\":1,\"1670\":3,\"1671\":3,\"1907\":1,\"1931\":1,\"1933\":1}}],[\"bottom=false\",{\"1\":{\"2498\":3,\"2616\":3,\"2634\":3}}],[\"bottom=0\",{\"1\":{\"1887\":1}}],[\"bottom=true\",{\"1\":{\"648\":1}}],[\"bottom\",{\"1\":{\"648\":1}}],[\"both\",{\"1\":{\"28\":1,\"72\":1,\"84\":1,\"99\":1,\"107\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"701\":1,\"708\":1,\"729\":1,\"730\":1,\"752\":1,\"756\":1,\"758\":1,\"760\":2,\"778\":2,\"782\":1,\"793\":1,\"819\":1,\"830\":1,\"831\":2,\"835\":1,\"952\":1,\"1046\":1,\"1061\":1,\"1067\":1,\"1082\":1,\"1084\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1140\":1,\"1141\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1174\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1186\":1,\"1188\":1,\"1190\":1,\"1196\":1,\"1197\":1,\"1200\":1,\"1203\":1,\"1204\":1,\"1206\":1,\"1207\":1,\"1210\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1249\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1378\":1,\"1428\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":2,\"1478\":1,\"1482\":1,\"1484\":3,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1555\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":1,\"1575\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":3,\"1601\":3,\"1602\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1613\":1,\"1616\":1,\"1617\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":2,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":2,\"1654\":2,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1670\":1,\"1671\":3,\"1672\":1,\"1674\":1,\"1676\":1,\"1718\":1,\"1719\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1785\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1799\":1,\"1806\":1,\"1828\":1,\"1840\":1,\"1842\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1902\":1,\"1906\":2,\"1907\":1,\"1910\":2,\"1912\":2,\"1914\":2,\"1915\":2,\"1918\":2,\"1919\":2,\"1920\":2,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1970\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2003\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2084\":2,\"2089\":2,\"2149\":1,\"2168\":1,\"2170\":1,\"2233\":1,\"2235\":1,\"2237\":1,\"2239\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2266\":1,\"2275\":1,\"2277\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2440\":1,\"2508\":1,\"2510\":1}}],[\"boeddeker\",{\"1\":{\"130\":1,\"1466\":1}}],[\"body\",{\"0\":{\"910\":1,\"1087\":1},\"1\":{\"115\":4,\"124\":1,\"910\":2,\"1058\":3,\"1087\":3,\"1103\":4,\"1105\":3}}],[\"bootphon\",{\"1\":{\"2131\":2}}],[\"bootstrap\",{\"1\":{\"44\":1}}],[\"booktitle\",{\"1\":{\"130\":1}}],[\"booktitle=\",{\"1\":{\"130\":6}}],[\"boosting\",{\"1\":{\"2040\":1}}],[\"boost\",{\"1\":{\"100\":1,\"1936\":1}}],[\"booleans\",{\"1\":{\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1}}],[\"bool=\",{\"1\":{\"1115\":14,\"2180\":2}}],[\"bool\",{\"1\":{\"21\":8,\"56\":2,\"59\":2,\"115\":4,\"121\":1,\"623\":1,\"626\":1,\"627\":2,\"629\":1,\"643\":1,\"645\":1,\"648\":8,\"658\":1,\"677\":2,\"678\":2,\"679\":2,\"681\":1,\"682\":1,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"690\":1,\"691\":5,\"692\":2,\"693\":5,\"697\":5,\"698\":2,\"699\":6,\"700\":2,\"701\":1,\"702\":2,\"708\":1,\"710\":1,\"711\":3,\"712\":4,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":2,\"726\":1,\"729\":1,\"730\":3,\"734\":1,\"735\":1,\"737\":1,\"739\":1,\"740\":3,\"741\":3,\"742\":1,\"747\":1,\"749\":3,\"752\":3,\"753\":1,\"754\":8,\"755\":3,\"756\":3,\"757\":1,\"758\":2,\"760\":2,\"761\":1,\"762\":2,\"763\":3,\"764\":1,\"766\":3,\"768\":1,\"770\":2,\"771\":2,\"772\":1,\"774\":1,\"775\":3,\"776\":3,\"778\":1,\"779\":1,\"782\":1,\"785\":3,\"786\":1,\"793\":1,\"797\":4,\"801\":1,\"802\":2,\"804\":1,\"806\":2,\"807\":1,\"809\":2,\"810\":1,\"813\":1,\"817\":1,\"818\":1,\"819\":1,\"821\":6,\"822\":3,\"824\":1,\"825\":6,\"826\":12,\"827\":1,\"828\":1,\"830\":1,\"831\":2,\"832\":1,\"835\":1,\"838\":1,\"866\":1,\"868\":1,\"887\":1,\"892\":1,\"899\":1,\"901\":1,\"918\":1,\"921\":1,\"932\":2,\"933\":1,\"936\":1,\"943\":2,\"950\":2,\"955\":2,\"956\":1,\"965\":2,\"968\":2,\"972\":2,\"973\":1,\"987\":5,\"997\":2,\"998\":2,\"1003\":1,\"1004\":1,\"1005\":1,\"1013\":1,\"1015\":1,\"1019\":1,\"1028\":3,\"1037\":1,\"1039\":1,\"1046\":1,\"1047\":1,\"1048\":2,\"1049\":1,\"1050\":1,\"1051\":2,\"1052\":5,\"1053\":2,\"1054\":2,\"1055\":2,\"1056\":1,\"1057\":5,\"1058\":1,\"1059\":2,\"1061\":1,\"1062\":1,\"1064\":3,\"1065\":1,\"1066\":1,\"1067\":2,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":3,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":2,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":2,\"1090\":1,\"1093\":2,\"1097\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1115\":14,\"1116\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1132\":1,\"1133\":11,\"1135\":1,\"1137\":1,\"1138\":2,\"1139\":2,\"1140\":5,\"1141\":1,\"1142\":2,\"1145\":4,\"1148\":15,\"1149\":8,\"1150\":6,\"1151\":1,\"1152\":1,\"1153\":2,\"1154\":2,\"1157\":1,\"1158\":5,\"1159\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1167\":4,\"1168\":4,\"1169\":6,\"1170\":1,\"1171\":5,\"1172\":6,\"1173\":2,\"1175\":1,\"1177\":1,\"1178\":2,\"1179\":7,\"1180\":3,\"1181\":3,\"1182\":1,\"1183\":1,\"1185\":1,\"1189\":1,\"1190\":2,\"1191\":1,\"1192\":1,\"1195\":2,\"1196\":4,\"1197\":4,\"1198\":1,\"1200\":2,\"1201\":1,\"1203\":14,\"1204\":4,\"1205\":1,\"1206\":5,\"1207\":2,\"1208\":1,\"1209\":1,\"1211\":1,\"1213\":1,\"1214\":2,\"1215\":2,\"1216\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1220\":2,\"1221\":1,\"1222\":2,\"1223\":1,\"1224\":1,\"1227\":1,\"1228\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1243\":1,\"1244\":4,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":1,\"1257\":4,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1269\":8,\"1270\":2,\"1271\":4,\"1272\":6,\"1273\":8,\"1275\":1,\"1277\":1,\"1279\":1,\"1281\":1,\"1282\":2,\"1283\":1,\"1284\":1,\"1285\":1,\"1289\":1,\"1324\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1352\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":2,\"1374\":1,\"1375\":1,\"1376\":2,\"1377\":3,\"1378\":1,\"1379\":1,\"1390\":1,\"1405\":2,\"1407\":1,\"1424\":1,\"1426\":2,\"1430\":3,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":2,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1454\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1462\":2,\"1463\":3,\"1464\":1,\"1465\":4,\"1467\":1,\"1469\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1484\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1505\":10,\"1507\":1,\"1509\":1,\"1510\":1,\"1511\":1,\"1513\":1,\"1515\":2,\"1516\":11,\"1517\":2,\"1519\":1,\"1521\":1,\"1522\":3,\"1523\":6,\"1524\":6,\"1525\":6,\"1528\":3,\"1529\":2,\"1530\":1,\"1531\":5,\"1532\":1,\"1533\":1,\"1534\":4,\"1535\":1,\"1536\":1,\"1537\":1,\"1538\":1,\"1539\":5,\"1541\":1,\"1542\":1,\"1544\":1,\"1545\":2,\"1546\":1,\"1548\":1,\"1550\":1,\"1551\":2,\"1552\":2,\"1553\":7,\"1554\":4,\"1556\":1,\"1558\":2,\"1559\":1,\"1560\":1,\"1562\":1,\"1563\":1,\"1565\":1,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1570\":5,\"1571\":2,\"1572\":3,\"1574\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1601\":1,\"1602\":3,\"1603\":2,\"1604\":2,\"1606\":1,\"1608\":1,\"1610\":1,\"1611\":12,\"1614\":1,\"1616\":1,\"1617\":1,\"1621\":1,\"1622\":3,\"1625\":1,\"1626\":3,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1639\":2,\"1640\":2,\"1642\":1,\"1643\":4,\"1644\":5,\"1645\":5,\"1647\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"1652\":2,\"1653\":1,\"1654\":6,\"1655\":1,\"1657\":1,\"1658\":5,\"1659\":5,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":4,\"1668\":1,\"1669\":8,\"1670\":2,\"1671\":3,\"1673\":1,\"1675\":1,\"1677\":1,\"1696\":2,\"1697\":2,\"1698\":2,\"1702\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1711\":1,\"1712\":4,\"1714\":1,\"1715\":4,\"1718\":1,\"1719\":1,\"1741\":1,\"1752\":1,\"1759\":2,\"1760\":1,\"1762\":1,\"1764\":1,\"1765\":7,\"1766\":1,\"1767\":1,\"1768\":1,\"1770\":1,\"1771\":7,\"1772\":1,\"1773\":3,\"1775\":1,\"1776\":1,\"1777\":1,\"1778\":13,\"1780\":1,\"1781\":4,\"1783\":1,\"1785\":1,\"1786\":2,\"1787\":7,\"1788\":7,\"1790\":1,\"1791\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1797\":1,\"1798\":8,\"1799\":1,\"1800\":6,\"1801\":3,\"1802\":1,\"1803\":7,\"1804\":20,\"1805\":7,\"1807\":1,\"1808\":1,\"1828\":1,\"1829\":2,\"1830\":1,\"1831\":3,\"1832\":1,\"1833\":3,\"1834\":1,\"1835\":1,\"1836\":3,\"1837\":3,\"1838\":3,\"1839\":7,\"1840\":3,\"1841\":1,\"1842\":1,\"1843\":3,\"1844\":7,\"1845\":1,\"1846\":3,\"1847\":3,\"1848\":8,\"1849\":6,\"1850\":9,\"1851\":31,\"1852\":11,\"1853\":1,\"1854\":1,\"1855\":3,\"1856\":3,\"1857\":6,\"1858\":5,\"1859\":7,\"1860\":1,\"1861\":5,\"1862\":7,\"1863\":5,\"1864\":9,\"1865\":9,\"1866\":5,\"1867\":3,\"1868\":3,\"1869\":1,\"1870\":3,\"1871\":5,\"1872\":3,\"1873\":3,\"1874\":7,\"1875\":1,\"1876\":1,\"1877\":9,\"1878\":17,\"1879\":5,\"1880\":13,\"1891\":1,\"1892\":4,\"1893\":1,\"1894\":1,\"1895\":3,\"1896\":3,\"1897\":1,\"1898\":2,\"1900\":4,\"1903\":1,\"1906\":3,\"1908\":1,\"1910\":2,\"1911\":1,\"1912\":1,\"1913\":1,\"1914\":2,\"1915\":2,\"1917\":1,\"1918\":4,\"1919\":1,\"1920\":3,\"1921\":2,\"1940\":1,\"1949\":2,\"1952\":1,\"1954\":1,\"1955\":1,\"1956\":1,\"1957\":1,\"1958\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"1964\":1,\"1968\":1,\"1970\":6,\"1971\":1,\"1972\":1,\"1975\":5,\"1977\":1,\"1979\":1,\"1980\":1,\"1982\":1,\"1983\":1,\"1984\":5,\"1985\":3,\"1986\":1,\"1987\":3,\"1988\":1,\"1989\":4,\"1990\":1,\"1991\":3,\"1992\":1,\"1993\":1,\"1995\":1,\"1996\":1,\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":7,\"2002\":12,\"2003\":6,\"2004\":6,\"2006\":1,\"2007\":1,\"2008\":2,\"2009\":2,\"2010\":1,\"2011\":1,\"2012\":2,\"2025\":1,\"2026\":6,\"2027\":7,\"2028\":1,\"2029\":3,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2046\":2,\"2048\":1,\"2049\":1,\"2051\":1,\"2053\":1,\"2054\":9,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2063\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2076\":7,\"2077\":1,\"2078\":3,\"2079\":2,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":3,\"2084\":2,\"2086\":11,\"2087\":14,\"2088\":3,\"2089\":2,\"2090\":23,\"2091\":5,\"2095\":23,\"2096\":6,\"2097\":6,\"2098\":6,\"2099\":11,\"2100\":6,\"2101\":6,\"2102\":6,\"2103\":6,\"2104\":6,\"2105\":6,\"2106\":6,\"2107\":6,\"2108\":6,\"2109\":6,\"2110\":6,\"2111\":5,\"2112\":6,\"2113\":6,\"2114\":6,\"2115\":6,\"2116\":6,\"2117\":6,\"2118\":6,\"2120\":1,\"2121\":1,\"2125\":2,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2136\":1,\"2137\":2,\"2142\":2,\"2150\":1,\"2157\":1,\"2161\":1,\"2163\":1,\"2169\":1,\"2170\":3,\"2171\":1,\"2176\":1,\"2178\":3,\"2179\":3,\"2180\":2,\"2181\":1,\"2182\":1,\"2184\":6,\"2186\":13,\"2188\":3,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":2,\"2198\":1,\"2200\":8,\"2201\":1,\"2202\":22,\"2204\":13,\"2210\":1,\"2211\":2,\"2234\":1,\"2235\":1,\"2236\":3,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":4,\"2242\":1,\"2243\":25,\"2244\":27,\"2245\":5,\"2246\":3,\"2247\":1,\"2248\":4,\"2249\":1,\"2250\":3,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":25,\"2256\":5,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":2,\"2261\":1,\"2262\":1,\"2263\":21,\"2264\":24,\"2265\":2,\"2266\":1,\"2267\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":26,\"2280\":5,\"2282\":1,\"2284\":1,\"2286\":1,\"2287\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":2,\"2316\":1,\"2345\":1,\"2352\":1}}],[\"biología\",{\"1\":{\"2457\":1}}],[\"billion\",{\"1\":{\"2154\":1}}],[\"billions\",{\"1\":{\"2154\":1}}],[\"bilinear\",{\"0\":{\"1025\":1},\"1\":{\"1011\":1,\"1025\":3,\"1247\":1}}],[\"bi\",{\"1\":{\"2125\":1}}],[\"bicubic\",{\"1\":{\"1257\":1,\"1919\":1,\"1948\":1}}],[\"bidirectional=true\",{\"1\":{\"1430\":1,\"1460\":1,\"1522\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1581\":1,\"1652\":1,\"1670\":1}}],[\"bidirectional=false\",{\"1\":{\"1241\":1,\"1522\":1,\"1572\":1,\"1598\":1,\"1609\":1,\"1648\":1,\"1650\":1}}],[\"bidirectionality\",{\"1\":{\"1245\":1}}],[\"bidirectional\",{\"1\":{\"808\":1,\"1222\":3,\"1241\":1,\"1282\":3,\"1430\":2,\"1515\":2,\"1516\":2,\"1522\":1,\"1523\":2,\"1528\":2,\"1529\":2,\"1531\":3,\"1532\":2,\"1534\":3,\"1535\":2,\"1537\":2,\"1539\":3,\"1572\":1,\"1581\":3,\"1598\":2,\"1602\":3,\"1626\":2,\"1645\":3,\"1648\":2,\"1650\":2,\"1652\":2,\"1670\":2,\"1671\":3,\"2086\":2,\"2087\":2}}],[\"bidim\",{\"1\":{\"690\":1,\"729\":1,\"1455\":1,\"1524\":1}}],[\"biggan\",{\"1\":{\"1605\":1}}],[\"bigger\",{\"1\":{\"623\":1}}],[\"big\",{\"1\":{\"48\":1,\"56\":1,\"1138\":1,\"1210\":6,\"1211\":2,\"1286\":2,\"1302\":7,\"1303\":7,\"1304\":8,\"1336\":2,\"1337\":7}}],[\"bits=wavfile\",{\"1\":{\"2592\":1}}],[\"bits\",{\"1\":{\"1926\":1,\"1927\":1}}],[\"bit\",{\"1\":{\"48\":1,\"297\":2,\"1013\":1,\"1927\":1,\"2395\":1,\"2531\":1,\"2592\":1,\"2596\":1}}],[\"biz\",{\"1\":{\"45\":1}}],[\"bias=false\",{\"1\":{\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"1503\":1,\"1676\":1}}],[\"bias=none\",{\"1\":{\"717\":1,\"731\":1,\"732\":1,\"747\":1,\"748\":1,\"777\":1,\"784\":1,\"801\":1}}],[\"bias=true\",{\"1\":{\"708\":1,\"723\":1,\"830\":1,\"1199\":1,\"1276\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1508\":1,\"1583\":1,\"1613\":1,\"1690\":1,\"1691\":1,\"1731\":1,\"1732\":1,\"1800\":1}}],[\"biases=true\",{\"1\":{\"1512\":1,\"1596\":1,\"1674\":1}}],[\"biases\",{\"0\":{\"70\":1},\"1\":{\"70\":1,\"1932\":1}}],[\"bias\",{\"0\":{\"919\":1,\"1078\":1,\"1079\":1},\"1\":{\"21\":4,\"115\":2,\"116\":3,\"712\":3,\"731\":2,\"740\":2,\"741\":2,\"747\":2,\"775\":2,\"776\":2,\"784\":2,\"801\":2,\"919\":3,\"1052\":3,\"1064\":1,\"1065\":3,\"1066\":3,\"1078\":3,\"1079\":4,\"1115\":2,\"1199\":1,\"1269\":3,\"1765\":3,\"1778\":3,\"1800\":2,\"1801\":2,\"1803\":3,\"1805\":2,\"1830\":1,\"1831\":2,\"1832\":1,\"1844\":3,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":3,\"1849\":3,\"1850\":3,\"1851\":3,\"1852\":3,\"1856\":3,\"1857\":3,\"1858\":3,\"1861\":3,\"1862\":3,\"1863\":3,\"1864\":3,\"1865\":3,\"1866\":3,\"1867\":3,\"1870\":1,\"1871\":3,\"1872\":3,\"1873\":3,\"1877\":2,\"1880\":3,\"1932\":6,\"1973\":3,\"2259\":3,\"2265\":1,\"2292\":1}}],[\"binomial\",{\"1\":{\"1829\":1}}],[\"bind\",{\"1\":{\"595\":1,\"1972\":1,\"2212\":1}}],[\"binding\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1}}],[\"binarization\",{\"1\":{\"1851\":1,\"1889\":1}}],[\"binaries\",{\"1\":{\"134\":1}}],[\"binary\",{\"0\":{\"1107\":1},\"1\":{\"169\":1,\"181\":1,\"238\":1,\"822\":1,\"1106\":1,\"1107\":2,\"1108\":1,\"2000\":1,\"2394\":2,\"2395\":1,\"2401\":1,\"2530\":2,\"2531\":1,\"2537\":1,\"2592\":1,\"2596\":1}}],[\"bins=0\",{\"1\":{\"1028\":1}}],[\"bins30000000\",{\"1\":{\"110\":3}}],[\"bins\",{\"1\":{\"26\":4,\"73\":2,\"77\":4,\"78\":5,\"79\":1,\"251\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"429\":4,\"1003\":4,\"1028\":2,\"1462\":1,\"1463\":1,\"1529\":1,\"1530\":1,\"1671\":1,\"1766\":1,\"1785\":1,\"1833\":3,\"1941\":4,\"2008\":1,\"2009\":1,\"2012\":2,\"2106\":2,\"2440\":1}}],[\"bin\",{\"0\":{\"1003\":1,\"2662\":1,\"2666\":1},\"1\":{\"18\":1,\"26\":1,\"34\":2,\"35\":1,\"36\":2,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"57\":1,\"58\":1,\"62\":5,\"63\":2,\"64\":2,\"65\":1,\"66\":6,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":2,\"72\":4,\"74\":1,\"75\":1,\"76\":2,\"77\":1,\"78\":1,\"79\":1,\"80\":1,\"81\":1,\"82\":1,\"90\":1,\"92\":1,\"98\":1,\"111\":1,\"144\":2,\"167\":2,\"173\":1,\"175\":1,\"178\":2,\"194\":1,\"196\":2,\"200\":2,\"234\":2,\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"528\":2,\"564\":1,\"650\":1,\"1003\":1,\"1462\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"1810\":1,\"1886\":2,\"1887\":2,\"1888\":2,\"2267\":1,\"2358\":1,\"2364\":1,\"2368\":1,\"2371\":1,\"2394\":1,\"2403\":1,\"2432\":1,\"2455\":1,\"2460\":1,\"2472\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":2,\"2530\":1,\"2539\":1,\"2568\":1,\"2569\":1,\"2579\":1,\"2591\":1,\"2600\":3,\"2605\":1,\"2609\":1,\"2612\":1,\"2617\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2635\":1,\"2648\":1,\"2649\":1,\"2654\":1,\"2658\":1}}],[\"blind\",{\"0\":{\"1680\":1},\"1\":{\"1680\":2,\"1704\":1}}],[\"blue\",{\"1\":{\"750\":1}}],[\"bleu=false\",{\"1\":{\"750\":1}}],[\"bleu\",{\"0\":{\"289\":1},\"1\":{\"255\":1,\"259\":1,\"499\":1,\"750\":8,\"1172\":1,\"1970\":1,\"1984\":1,\"2076\":1,\"2456\":5,\"2457\":3,\"2460\":5,\"2462\":1,\"2520\":1,\"2521\":5}}],[\"blog\",{\"1\":{\"1144\":1,\"1228\":1,\"1345\":1,\"1347\":1}}],[\"blob\",{\"1\":{\"195\":1,\"199\":1,\"202\":1,\"295\":1,\"628\":1,\"817\":1,\"1101\":1,\"1102\":1,\"1180\":1,\"1454\":2,\"1695\":1,\"1717\":1,\"1735\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"1958\":1,\"2131\":1,\"2154\":1,\"2360\":5,\"2372\":1,\"2377\":2,\"2384\":1,\"2387\":1,\"2429\":2,\"2446\":1,\"2458\":5,\"2523\":5,\"2552\":1,\"2554\":1,\"2575\":1,\"2582\":5,\"2585\":1,\"2587\":1}}],[\"blocking=false\",{\"1\":{\"2166\":1}}],[\"blockidx\",{\"1\":{\"1227\":1}}],[\"blockdrop\",{\"1\":{\"115\":1,\"1068\":2,\"1093\":2}}],[\"blockwise\",{\"1\":{\"103\":1,\"104\":2,\"692\":1,\"693\":1,\"2452\":1,\"2586\":1}}],[\"blocked\",{\"1\":{\"21\":2,\"115\":1,\"712\":1,\"1052\":1}}],[\"blocks=3\",{\"1\":{\"1670\":1}}],[\"blocks=7\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"blocks=1\",{\"1\":{\"1543\":1}}],[\"blocks=2\",{\"1\":{\"1543\":2,\"1605\":1,\"1652\":1}}],[\"blocks=6\",{\"1\":{\"747\":1,\"1670\":1}}],[\"blocks\",{\"0\":{\"858\":2,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"886\":1,\"910\":1,\"911\":1,\"934\":1,\"1049\":1,\"1050\":1,\"1052\":1,\"1053\":1,\"1056\":1,\"1065\":1,\"1068\":1,\"1074\":1,\"1087\":1},\"1\":{\"21\":3,\"115\":6,\"116\":7,\"124\":3,\"726\":1,\"749\":4,\"858\":8,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"886\":1,\"910\":5,\"911\":4,\"934\":1,\"1049\":1,\"1050\":1,\"1052\":1,\"1053\":1,\"1056\":1,\"1062\":6,\"1065\":1,\"1066\":3,\"1068\":2,\"1074\":4,\"1075\":4,\"1081\":6,\"1087\":4,\"1103\":1,\"1133\":2,\"1140\":1,\"1142\":1,\"1148\":3,\"1149\":3,\"1150\":3,\"1167\":1,\"1168\":1,\"1169\":1,\"1181\":3,\"1186\":1,\"1196\":1,\"1197\":1,\"1203\":3,\"1204\":1,\"1210\":2,\"1228\":1,\"1271\":1,\"1272\":3,\"1273\":1,\"1345\":1,\"1347\":1,\"1375\":1,\"1379\":3,\"1489\":1,\"1505\":1,\"1531\":2,\"1543\":4,\"1558\":1,\"1624\":1,\"1633\":1,\"1645\":1,\"1652\":2,\"1654\":1,\"1655\":3,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":4,\"1665\":4,\"1669\":1,\"1670\":4,\"1671\":6,\"1719\":3,\"1765\":3,\"1771\":3,\"1787\":3,\"1788\":3,\"1798\":3,\"1800\":3,\"1803\":3,\"1804\":3,\"1805\":1,\"1844\":3,\"1848\":1,\"1851\":3,\"1874\":3,\"1877\":1,\"1878\":3,\"1932\":1,\"2001\":3,\"2004\":3,\"2026\":1,\"2029\":3,\"2054\":3,\"2398\":1,\"2440\":2,\"2534\":1,\"2558\":2,\"2564\":1,\"2584\":2,\"2600\":1}}],[\"block\",{\"0\":{\"124\":1,\"711\":1,\"859\":1,\"860\":1,\"862\":1,\"934\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1104\":1,\"1105\":1,\"1149\":1,\"1150\":1,\"1254\":1,\"1312\":1,\"1313\":1,\"1353\":1,\"1830\":1,\"1831\":1,\"1866\":1,\"1872\":1,\"1873\":1,\"2032\":1,\"2047\":1},\"1\":{\"21\":16,\"104\":8,\"115\":21,\"116\":5,\"124\":9,\"692\":7,\"693\":7,\"711\":3,\"713\":1,\"725\":3,\"726\":3,\"731\":6,\"749\":1,\"754\":2,\"768\":2,\"786\":2,\"826\":2,\"835\":1,\"858\":2,\"859\":6,\"860\":7,\"861\":2,\"862\":6,\"898\":3,\"911\":2,\"934\":10,\"1049\":10,\"1050\":10,\"1053\":1,\"1056\":10,\"1062\":7,\"1066\":2,\"1068\":11,\"1074\":3,\"1075\":2,\"1081\":7,\"1088\":4,\"1089\":4,\"1090\":4,\"1091\":4,\"1092\":4,\"1093\":1,\"1094\":3,\"1103\":4,\"1104\":10,\"1105\":6,\"1133\":1,\"1148\":1,\"1149\":9,\"1150\":10,\"1178\":1,\"1180\":1,\"1181\":1,\"1198\":5,\"1200\":1,\"1203\":1,\"1228\":1,\"1231\":1,\"1254\":2,\"1269\":1,\"1272\":1,\"1312\":1,\"1313\":1,\"1345\":1,\"1347\":1,\"1353\":1,\"1375\":2,\"1379\":1,\"1430\":2,\"1456\":1,\"1458\":1,\"1463\":1,\"1505\":1,\"1506\":1,\"1531\":2,\"1535\":1,\"1537\":1,\"1539\":1,\"1543\":5,\"1545\":1,\"1564\":1,\"1602\":1,\"1655\":1,\"1656\":1,\"1661\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":2,\"1671\":2,\"1768\":2,\"1795\":1,\"1804\":13,\"1812\":1,\"1813\":1,\"1830\":1,\"1831\":1,\"1851\":2,\"1862\":1,\"1864\":2,\"1866\":2,\"1872\":1,\"1873\":1,\"1878\":13,\"1880\":1,\"2001\":1,\"2004\":1,\"2029\":1,\"2032\":3,\"2047\":2,\"2049\":3,\"2054\":1,\"2055\":3,\"2064\":3,\"2090\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2258\":1,\"2264\":2,\"2279\":2,\"2400\":4,\"2405\":1,\"2487\":1,\"2491\":1,\"2497\":1,\"2536\":4,\"2541\":1,\"2600\":2}}],[\"blayers=3\",{\"1\":{\"729\":1}}],[\"blayers\",{\"1\":{\"251\":2,\"756\":1,\"1158\":1,\"1239\":1,\"1524\":1,\"1611\":1,\"1791\":1}}],[\"blas\",{\"1\":{\"134\":1}}],[\"blank=\",{\"1\":{\"2348\":1}}],[\"blank=0\",{\"1\":{\"896\":1,\"1224\":1,\"1348\":1}}],[\"blanks\",{\"1\":{\"245\":2,\"301\":2,\"1138\":1,\"1210\":3,\"1302\":4,\"1303\":4,\"1304\":5,\"1337\":4}}],[\"blank\",{\"0\":{\"1142\":1,\"1143\":1,\"1154\":1,\"1155\":1,\"1186\":1,\"1194\":1,\"1202\":1,\"1210\":1,\"1211\":2,\"1224\":2,\"1225\":1,\"1226\":1,\"1227\":1,\"1286\":2,\"1287\":2,\"1288\":1,\"1293\":2,\"1294\":2,\"1295\":2,\"1296\":2,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1306\":1,\"1307\":1,\"1315\":1,\"1316\":1,\"1318\":1,\"1324\":1,\"1325\":1,\"1331\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":2,\"1337\":1,\"1338\":1,\"1344\":1,\"1346\":1,\"1348\":2,\"1349\":1,\"1350\":1,\"1353\":1,\"1359\":1},\"1\":{\"23\":1,\"175\":2,\"194\":2,\"245\":6,\"249\":2,\"251\":2,\"255\":2,\"259\":2,\"301\":4,\"449\":2,\"698\":3,\"699\":3,\"704\":1,\"705\":3,\"725\":3,\"759\":6,\"806\":3,\"825\":3,\"884\":2,\"896\":1,\"929\":1,\"1057\":4,\"1059\":3,\"1083\":1,\"1099\":3,\"1138\":13,\"1142\":5,\"1143\":1,\"1154\":2,\"1155\":5,\"1171\":3,\"1172\":1,\"1173\":3,\"1186\":5,\"1194\":1,\"1202\":1,\"1206\":1,\"1210\":11,\"1211\":9,\"1224\":4,\"1225\":1,\"1226\":1,\"1227\":1,\"1228\":1,\"1270\":1,\"1286\":8,\"1287\":3,\"1288\":1,\"1293\":2,\"1294\":2,\"1295\":2,\"1296\":2,\"1298\":6,\"1299\":6,\"1300\":1,\"1301\":6,\"1302\":11,\"1303\":11,\"1304\":11,\"1306\":1,\"1307\":1,\"1315\":1,\"1316\":1,\"1318\":1,\"1324\":1,\"1325\":1,\"1331\":1,\"1333\":1,\"1334\":2,\"1335\":1,\"1336\":10,\"1337\":12,\"1338\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":4,\"1349\":4,\"1350\":4,\"1353\":1,\"1359\":1,\"1429\":2,\"1841\":3,\"1892\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":2,\"2373\":1,\"2460\":1,\"2461\":1,\"2555\":1}}],[\"black=lambda\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"blacklist\",{\"1\":{\"45\":1}}],[\"black\",{\"1\":{\"11\":5,\"45\":1,\"1254\":3,\"2500\":3,\"2617\":3,\"2635\":3,\"2642\":1}}],[\"blstmp\",{\"1\":{\"251\":2,\"729\":1,\"730\":1,\"756\":2,\"1158\":2,\"1239\":2,\"1524\":1,\"1525\":1,\"1611\":2,\"1791\":2}}],[\"blstm\",{\"1\":{\"21\":1,\"251\":2,\"808\":1,\"821\":2,\"1462\":1,\"1463\":1,\"1515\":2,\"1516\":1,\"1522\":1,\"1523\":1,\"1528\":2,\"1529\":2,\"1572\":1,\"1626\":2,\"2095\":2,\"2263\":2}}],[\"balanced\",{\"0\":{\"2006\":1,\"2013\":1},\"1\":{\"2006\":1,\"2013\":1}}],[\"baker\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"bak\",{\"1\":{\"1526\":1}}],[\"babble\",{\"1\":{\"1179\":1}}],[\"bayesriskctc\",{\"0\":{\"1136\":1},\"1\":{\"1136\":2}}],[\"bayes\",{\"0\":{\"1136\":1,\"1332\":1},\"1\":{\"1136\":2,\"1137\":1,\"1332\":1}}],[\"bayashi\",{\"1\":{\"206\":2,\"240\":1,\"1860\":1,\"2361\":2,\"2363\":31,\"2450\":1,\"2506\":4,\"2510\":4,\"2512\":16,\"2517\":1,\"2650\":2,\"2653\":33,\"2657\":16}}],[\"ban\",{\"1\":{\"1680\":2}}],[\"banks\",{\"1\":{\"821\":1,\"1785\":1,\"1810\":1,\"1860\":1}}],[\"bank\",{\"1\":{\"701\":6,\"821\":3,\"1198\":2,\"1785\":1,\"1904\":1,\"1916\":1}}],[\"bandreject\",{\"0\":{\"1922\":1},\"1\":{\"1922\":1}}],[\"bandpass\",{\"0\":{\"1921\":1},\"1\":{\"1921\":2}}],[\"bandpassperturbation\",{\"0\":{\"940\":1},\"1\":{\"940\":1}}],[\"bandwidths\",{\"1\":{\"1904\":1}}],[\"bandwidth\",{\"0\":{\"1923\":1},\"1\":{\"1551\":1,\"1553\":1,\"1904\":1,\"1923\":3}}],[\"bandlimit=none\",{\"1\":{\"1247\":1,\"1248\":1}}],[\"bandsplit\",{\"0\":{\"1464\":1},\"1\":{\"1463\":1,\"1464\":3}}],[\"bands\",{\"1\":{\"778\":1,\"1912\":1}}],[\"band\",{\"0\":{\"216\":1},\"1\":{\"216\":1,\"778\":1,\"940\":2,\"1462\":3,\"1463\":3,\"1660\":1,\"1661\":1,\"1662\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1778\":1,\"1793\":1,\"1795\":1,\"1805\":1,\"1810\":1,\"1852\":1,\"1917\":1,\"1922\":1,\"2363\":11,\"2506\":1,\"2512\":4,\"2513\":2,\"2653\":11,\"2657\":4}}],[\"bac009s0724w0121\",{\"1\":{\"2593\":1}}],[\"bachend\",{\"1\":{\"629\":1}}],[\"backbone\",{\"1\":{\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1243\":1,\"1252\":1,\"1253\":2,\"1254\":1,\"1279\":1}}],[\"backpropagation\",{\"1\":{\"745\":2,\"746\":2}}],[\"backprop\",{\"1\":{\"608\":1,\"627\":1}}],[\"backward\",{\"0\":{\"915\":1},\"1\":{\"80\":2,\"87\":2,\"174\":1,\"217\":1,\"263\":2,\"267\":2,\"399\":2,\"464\":2,\"470\":2,\"627\":1,\"681\":3,\"682\":3,\"734\":1,\"745\":4,\"746\":4,\"758\":3,\"759\":3,\"767\":1,\"817\":1,\"828\":1,\"915\":3,\"1086\":2,\"1142\":1,\"1145\":1,\"1187\":7,\"1202\":7,\"1286\":3,\"1287\":3,\"1299\":6,\"1301\":1,\"1303\":6,\"1304\":1,\"1985\":1,\"2002\":3,\"2079\":3,\"2095\":3,\"2201\":2,\"2263\":3,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"backends\",{\"0\":{\"31\":1}}],[\"backend\",{\"0\":{\"593\":1,\"594\":1,\"595\":1,\"596\":1,\"598\":1,\"599\":1,\"602\":1,\"603\":1,\"604\":1,\"605\":1,\"606\":1,\"607\":1,\"609\":1,\"611\":1,\"614\":1,\"615\":1,\"617\":1,\"622\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"635\":1,\"636\":1,\"637\":1,\"639\":1,\"640\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"649\":1,\"650\":1,\"657\":1,\"658\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"701\":1,\"702\":1,\"703\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"766\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"782\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"791\":1,\"792\":1,\"793\":1,\"794\":1,\"799\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"813\":1,\"816\":1,\"817\":1,\"818\":1,\"819\":1,\"821\":1,\"822\":1,\"823\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"830\":1,\"831\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":1,\"867\":1,\"868\":1,\"869\":1,\"870\":1,\"871\":1,\"875\":1,\"877\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"897\":1,\"898\":1,\"899\":1,\"901\":1,\"903\":1,\"905\":1,\"906\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"929\":1,\"930\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":1,\"936\":1,\"937\":1,\"938\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1044\":1,\"1045\":1},\"1\":{\"11\":1,\"15\":1,\"16\":4,\"20\":1,\"24\":2,\"85\":2,\"142\":1,\"173\":1,\"194\":1,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"286\":2,\"429\":2,\"499\":2,\"593\":1,\"594\":1,\"595\":1,\"596\":1,\"598\":1,\"599\":1,\"602\":2,\"603\":2,\"604\":2,\"605\":1,\"606\":2,\"607\":2,\"609\":1,\"611\":1,\"614\":2,\"615\":2,\"617\":1,\"622\":2,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":2,\"635\":1,\"636\":2,\"637\":1,\"639\":1,\"640\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"649\":2,\"650\":4,\"657\":2,\"658\":1,\"665\":3,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"701\":1,\"702\":1,\"703\":1,\"706\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":2,\"728\":2,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"742\":4,\"743\":1,\"745\":1,\"746\":1,\"747\":2,\"748\":1,\"749\":2,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"766\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"782\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"791\":1,\"792\":1,\"793\":2,\"794\":1,\"799\":2,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"813\":1,\"815\":5,\"816\":1,\"817\":1,\"818\":1,\"819\":1,\"821\":1,\"822\":1,\"823\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"830\":1,\"831\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":1,\"867\":2,\"868\":1,\"869\":1,\"870\":2,\"871\":1,\"872\":3,\"873\":3,\"874\":3,\"875\":1,\"877\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"897\":1,\"898\":1,\"899\":1,\"901\":1,\"903\":1,\"905\":1,\"906\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"929\":1,\"930\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":1,\"936\":1,\"937\":2,\"938\":2,\"974\":1,\"975\":1,\"976\":1,\"977\":2,\"978\":2,\"1000\":1,\"1017\":1,\"1041\":1,\"1042\":1,\"1043\":2,\"1044\":2,\"1045\":2,\"1133\":1,\"1149\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1204\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1927\":7,\"1971\":1,\"2001\":1,\"2004\":1,\"2029\":1,\"2131\":1,\"2180\":2,\"2271\":1,\"2372\":1,\"2385\":1,\"2422\":1,\"2429\":1,\"2461\":1,\"2545\":1,\"2554\":1}}],[\"back\",{\"1\":{\"5\":1,\"1011\":1,\"1551\":2,\"1553\":2,\"1643\":1}}],[\"bar=\",{\"1\":{\"2176\":1}}],[\"bark\",{\"1\":{\"1904\":5}}],[\"barkscale\",{\"0\":{\"1904\":1},\"1\":{\"1904\":2}}],[\"bar\",{\"1\":{\"63\":2,\"92\":1,\"144\":1,\"944\":4,\"1245\":1,\"1698\":2,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"badly\",{\"1\":{\"2584\":1}}],[\"badim=320\",{\"1\":{\"729\":1}}],[\"badim\",{\"1\":{\"251\":2,\"756\":1,\"1158\":1,\"1239\":1,\"1524\":1,\"1611\":1,\"1791\":1}}],[\"bad\",{\"1\":{\"62\":1,\"2558\":1}}],[\"batch50m\",{\"1\":{\"2454\":1,\"2455\":2}}],[\"batchify\",{\"1\":{\"1171\":2,\"1206\":2,\"1427\":1,\"1552\":2,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1}}],[\"batch=1\",{\"1\":{\"784\":1}}],[\"batched\",{\"1\":{\"759\":2,\"797\":1,\"1255\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"1829\":2,\"1842\":2,\"1881\":3,\"1889\":2}}],[\"batches=0\",{\"1\":{\"1003\":1,\"1004\":1,\"1028\":1}}],[\"batches\",{\"1\":{\"56\":1,\"74\":1,\"75\":1,\"78\":1,\"612\":1,\"785\":1,\"997\":4,\"998\":3,\"1003\":4,\"1004\":4,\"1005\":1,\"1006\":1,\"1028\":3,\"1171\":1,\"1206\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1895\":1,\"1896\":1,\"1897\":4,\"1899\":1,\"1900\":1,\"1953\":1,\"1955\":1,\"2099\":3,\"2102\":3,\"2106\":2,\"2558\":1}}],[\"batchpartialscorerinterface\",{\"0\":{\"695\":1},\"1\":{\"695\":1,\"706\":1}}],[\"batchfied\",{\"1\":{\"694\":1,\"696\":1,\"734\":1,\"773\":1,\"828\":1,\"1133\":1,\"1190\":1,\"1214\":1,\"1244\":1,\"1273\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"2001\":1}}],[\"batchfy\",{\"0\":{\"1003\":2,\"1004\":2,\"1005\":2,\"1006\":2,\"1028\":1},\"1\":{\"26\":1,\"172\":1,\"691\":1,\"1003\":2,\"1004\":2,\"1005\":2,\"1006\":2,\"1028\":3}}],[\"batchhypothesis\",{\"0\":{\"694\":1},\"1\":{\"691\":9,\"692\":1,\"694\":2,\"797\":8}}],[\"batchbeamsearchonlinesim\",{\"0\":{\"693\":1},\"1\":{\"693\":1}}],[\"batchbeamsearchonline\",{\"0\":{\"692\":1},\"1\":{\"692\":1}}],[\"batchbeamsearch\",{\"0\":{\"691\":1},\"1\":{\"691\":1,\"692\":1,\"693\":1,\"797\":1}}],[\"batchscorerinterface\",{\"0\":{\"696\":1},\"1\":{\"695\":1,\"696\":1,\"733\":1,\"773\":1,\"828\":1,\"1133\":1,\"1190\":1,\"1214\":1,\"1244\":1,\"1951\":1,\"2001\":1}}],[\"batchset\",{\"0\":{\"1028\":1},\"1\":{\"172\":3,\"1000\":1,\"1028\":2}}],[\"batchsampler\",{\"1\":{\"74\":1,\"2010\":1,\"2011\":1,\"2012\":1}}],[\"batchsize=1\",{\"1\":{\"1797\":3}}],[\"batchsize=2\",{\"1\":{\"1028\":1}}],[\"batchsize\",{\"1\":{\"19\":1,\"247\":2,\"249\":2,\"253\":2,\"257\":2,\"261\":2,\"286\":1,\"296\":1,\"727\":1,\"728\":1,\"807\":1}}],[\"batch\",{\"0\":{\"72\":1,\"73\":1,\"95\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"863\":1,\"866\":1,\"1679\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":2},\"1\":{\"21\":2,\"25\":4,\"26\":20,\"33\":3,\"56\":1,\"57\":6,\"60\":4,\"62\":2,\"72\":13,\"73\":13,\"74\":3,\"75\":6,\"76\":9,\"77\":8,\"78\":6,\"79\":9,\"80\":8,\"95\":3,\"110\":3,\"115\":2,\"148\":3,\"172\":4,\"174\":2,\"251\":11,\"255\":11,\"259\":11,\"265\":12,\"269\":12,\"307\":2,\"315\":6,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":10,\"437\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":6,\"599\":1,\"612\":2,\"613\":1,\"617\":3,\"624\":1,\"625\":1,\"676\":7,\"691\":7,\"692\":1,\"693\":1,\"694\":1,\"695\":6,\"696\":7,\"701\":4,\"702\":3,\"705\":1,\"706\":2,\"711\":10,\"712\":3,\"713\":2,\"714\":4,\"715\":4,\"716\":4,\"718\":4,\"719\":4,\"720\":4,\"721\":4,\"722\":4,\"723\":2,\"725\":5,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":5,\"732\":3,\"734\":7,\"735\":6,\"737\":6,\"738\":2,\"740\":5,\"741\":5,\"742\":5,\"747\":4,\"748\":1,\"750\":5,\"754\":14,\"755\":7,\"762\":3,\"763\":3,\"764\":2,\"767\":2,\"770\":2,\"771\":8,\"772\":2,\"773\":5,\"774\":2,\"775\":5,\"776\":5,\"781\":6,\"784\":2,\"785\":20,\"786\":2,\"794\":2,\"797\":3,\"802\":5,\"804\":2,\"806\":5,\"809\":8,\"810\":2,\"812\":2,\"813\":2,\"816\":1,\"817\":2,\"818\":2,\"820\":1,\"821\":16,\"822\":6,\"824\":12,\"826\":14,\"828\":7,\"829\":1,\"836\":1,\"838\":5,\"852\":1,\"863\":1,\"866\":1,\"875\":1,\"899\":1,\"901\":1,\"903\":3,\"905\":1,\"920\":1,\"924\":1,\"936\":1,\"958\":1,\"987\":4,\"997\":3,\"998\":3,\"999\":1,\"1003\":9,\"1004\":9,\"1005\":10,\"1006\":2,\"1009\":1,\"1010\":4,\"1011\":3,\"1025\":3,\"1028\":21,\"1046\":7,\"1048\":3,\"1052\":3,\"1066\":7,\"1073\":5,\"1075\":7,\"1083\":5,\"1106\":3,\"1107\":2,\"1108\":3,\"1113\":5,\"1114\":2,\"1133\":22,\"1140\":4,\"1141\":6,\"1142\":3,\"1145\":4,\"1148\":4,\"1154\":3,\"1160\":2,\"1161\":2,\"1162\":4,\"1163\":2,\"1164\":2,\"1166\":1,\"1169\":4,\"1170\":6,\"1171\":18,\"1172\":6,\"1177\":2,\"1182\":5,\"1186\":1,\"1190\":11,\"1198\":2,\"1203\":4,\"1204\":7,\"1206\":16,\"1209\":18,\"1210\":1,\"1211\":5,\"1214\":17,\"1218\":4,\"1224\":5,\"1243\":1,\"1244\":14,\"1247\":1,\"1248\":1,\"1252\":8,\"1253\":6,\"1254\":6,\"1255\":2,\"1270\":5,\"1273\":20,\"1279\":4,\"1287\":4,\"1298\":1,\"1299\":1,\"1301\":2,\"1302\":1,\"1303\":1,\"1304\":2,\"1334\":1,\"1336\":5,\"1348\":5,\"1352\":5,\"1368\":1,\"1371\":6,\"1372\":1,\"1373\":4,\"1374\":2,\"1375\":4,\"1376\":5,\"1377\":1,\"1379\":1,\"1381\":1,\"1427\":2,\"1428\":1,\"1430\":2,\"1454\":3,\"1463\":4,\"1466\":3,\"1470\":2,\"1471\":2,\"1472\":1,\"1505\":4,\"1510\":2,\"1511\":3,\"1515\":4,\"1516\":4,\"1517\":2,\"1523\":7,\"1524\":1,\"1525\":1,\"1528\":4,\"1529\":1,\"1530\":2,\"1531\":2,\"1532\":1,\"1534\":4,\"1535\":1,\"1539\":4,\"1543\":1,\"1551\":9,\"1552\":27,\"1553\":8,\"1554\":9,\"1558\":5,\"1559\":2,\"1560\":2,\"1563\":2,\"1564\":1,\"1566\":5,\"1567\":5,\"1568\":3,\"1569\":5,\"1571\":5,\"1575\":1,\"1600\":2,\"1602\":3,\"1603\":2,\"1604\":3,\"1609\":1,\"1611\":8,\"1616\":2,\"1617\":2,\"1622\":2,\"1626\":4,\"1638\":1,\"1643\":4,\"1644\":5,\"1645\":4,\"1646\":2,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":4,\"1655\":2,\"1658\":4,\"1659\":4,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":5,\"1668\":5,\"1669\":4,\"1670\":2,\"1671\":5,\"1679\":3,\"1683\":1,\"1693\":2,\"1718\":2,\"1719\":7,\"1755\":2,\"1778\":20,\"1781\":2,\"1804\":30,\"1805\":20,\"1841\":3,\"1850\":1,\"1851\":7,\"1852\":1,\"1879\":7,\"1892\":8,\"1893\":8,\"1896\":1,\"1897\":3,\"1900\":1,\"1910\":3,\"1914\":1,\"1915\":1,\"1917\":2,\"1918\":10,\"1919\":2,\"1940\":1,\"1948\":1,\"1953\":6,\"1954\":2,\"1955\":6,\"1956\":2,\"1957\":7,\"1958\":5,\"1960\":7,\"1963\":3,\"1970\":6,\"1971\":2,\"1975\":10,\"1983\":3,\"1984\":2,\"1986\":5,\"1993\":6,\"2000\":6,\"2001\":18,\"2002\":11,\"2004\":8,\"2006\":2,\"2007\":5,\"2008\":5,\"2009\":5,\"2010\":7,\"2011\":4,\"2012\":14,\"2027\":6,\"2046\":4,\"2049\":2,\"2054\":2,\"2076\":8,\"2078\":11,\"2081\":3,\"2083\":7,\"2084\":10,\"2086\":32,\"2087\":32,\"2088\":4,\"2089\":7,\"2090\":25,\"2091\":11,\"2095\":26,\"2099\":4,\"2102\":3,\"2106\":6,\"2168\":2,\"2170\":2,\"2193\":1,\"2209\":6,\"2243\":12,\"2244\":16,\"2245\":11,\"2255\":16,\"2256\":11,\"2257\":1,\"2259\":4,\"2260\":5,\"2261\":1,\"2263\":11,\"2264\":11,\"2265\":3,\"2267\":1,\"2271\":1,\"2279\":14,\"2280\":11,\"2292\":2,\"2303\":1,\"2305\":1,\"2358\":1,\"2375\":1,\"2440\":2,\"2461\":1,\"2520\":1,\"2558\":4,\"2579\":1,\"2584\":2,\"2644\":3}}],[\"batchnorm1d\",{\"1\":{\"115\":2}}],[\"batchnormalization\",{\"1\":{\"80\":1}}],[\"batchnorm\",{\"0\":{\"1465\":1},\"1\":{\"21\":2,\"1465\":3}}],[\"basis\",{\"1\":{\"275\":1,\"1859\":1,\"2317\":1,\"2330\":1}}],[\"basics\",{\"1\":{\"2401\":1,\"2422\":1,\"2537\":1,\"2545\":1}}],[\"basicblock\",{\"0\":{\"1134\":1},\"1\":{\"1134\":1}}],[\"basic\",{\"0\":{\"1312\":1,\"1313\":1},\"1\":{\"126\":1,\"461\":1,\"1047\":1,\"1312\":1,\"1313\":1,\"1379\":1,\"1506\":1,\"1652\":1,\"1664\":1,\"1665\":1,\"1670\":1,\"1671\":1,\"2397\":1,\"2481\":2,\"2533\":1,\"2618\":1}}],[\"basicnorm\",{\"0\":{\"1047\":1},\"1\":{\"115\":6,\"1047\":3}}],[\"basically\",{\"1\":{\"26\":1,\"235\":1,\"1719\":1}}],[\"basename\",{\"1\":{\"2568\":2}}],[\"basefrequencediscriminator\",{\"0\":{\"1766\":1},\"1\":{\"1766\":1}}],[\"basetransformerdecoder\",{\"0\":{\"1133\":1},\"1\":{\"1133\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1271\":1,\"1273\":1}}],[\"basewriter\",{\"0\":{\"980\":1},\"1\":{\"980\":2,\"983\":1,\"986\":1,\"991\":1,\"994\":1}}],[\"baseevaluator\",{\"0\":{\"979\":1},\"1\":{\"607\":1,\"626\":1,\"975\":1,\"979\":1,\"1042\":1}}],[\"bases\",{\"1\":{\"593\":1,\"594\":1,\"599\":1,\"604\":1,\"605\":1,\"606\":1,\"607\":1,\"609\":1,\"610\":1,\"611\":1,\"612\":1,\"614\":1,\"615\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"695\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"705\":1,\"706\":1,\"707\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"735\":1,\"736\":1,\"738\":1,\"740\":1,\"741\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"758\":1,\"759\":1,\"760\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"781\":1,\"782\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"791\":1,\"792\":1,\"793\":1,\"794\":1,\"795\":1,\"797\":1,\"798\":1,\"799\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":1,\"813\":1,\"814\":1,\"816\":1,\"817\":1,\"818\":1,\"819\":1,\"820\":1,\"821\":1,\"822\":1,\"823\":1,\"824\":1,\"825\":1,\"826\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"936\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"946\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"954\":1,\"955\":1,\"956\":1,\"957\":1,\"958\":1,\"960\":1,\"961\":1,\"962\":1,\"974\":1,\"975\":1,\"976\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"985\":1,\"986\":1,\"987\":1,\"988\":1,\"989\":1,\"990\":1,\"991\":1,\"993\":1,\"994\":1,\"996\":1,\"997\":1,\"998\":1,\"999\":1,\"1000\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1086\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1115\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":1,\"1142\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1154\":1,\"1155\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1176\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1186\":1,\"1187\":1,\"1188\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1200\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1204\":1,\"1205\":1,\"1206\":1,\"1207\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1249\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1382\":1,\"1384\":1,\"1386\":1,\"1388\":1,\"1390\":1,\"1393\":1,\"1394\":1,\"1396\":1,\"1398\":1,\"1399\":1,\"1401\":1,\"1403\":1,\"1405\":1,\"1407\":1,\"1409\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1430\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1452\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1505\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1522\":1,\"1523\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1534\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1555\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1585\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1626\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1637\":1,\"1638\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1718\":1,\"1719\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1774\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1781\":1,\"1782\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1806\":1,\"1808\":1,\"1828\":1,\"1829\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1902\":1,\"1904\":1,\"1905\":1,\"1906\":1,\"1907\":1,\"1909\":1,\"1910\":1,\"1911\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1961\":1,\"1970\":1,\"1971\":1,\"1972\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2095\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1,\"2124\":1,\"2125\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2133\":1,\"2135\":1,\"2136\":1,\"2148\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2171\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2176\":1,\"2177\":1,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":1,\"2184\":1,\"2185\":1,\"2186\":1,\"2187\":1,\"2188\":1,\"2189\":1,\"2191\":1,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2202\":1,\"2203\":1,\"2204\":1,\"2205\":1,\"2233\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2275\":1,\"2277\":1,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2309\":1,\"2312\":1,\"2313\":1,\"2315\":1,\"2316\":1,\"2317\":1,\"2345\":1,\"2346\":1,\"2350\":1}}],[\"base64\",{\"1\":{\"38\":1,\"40\":1,\"41\":1,\"42\":1,\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"base\",{\"0\":{\"1251\":1,\"1253\":1,\"1278\":1,\"1560\":1},\"1\":{\"5\":1,\"7\":1,\"76\":1,\"85\":1,\"134\":1,\"380\":2,\"754\":3,\"820\":3,\"821\":3,\"826\":3,\"979\":1,\"1109\":1,\"1133\":1,\"1251\":1,\"1253\":1,\"1278\":1,\"1437\":1,\"1443\":1,\"1559\":1,\"1560\":2,\"1570\":1,\"1667\":1,\"1718\":1,\"1766\":2,\"1778\":1,\"1804\":6,\"1805\":3,\"1850\":1,\"1852\":1,\"1859\":3,\"1863\":3,\"1864\":3,\"1865\":3,\"1870\":1,\"1877\":3,\"1878\":6,\"1880\":3,\"1901\":1,\"1912\":1,\"1930\":2,\"1932\":2,\"1976\":1,\"1978\":1,\"1989\":1,\"2248\":1,\"2288\":1,\"2431\":3}}],[\"based\",{\"0\":{\"3\":1,\"4\":1,\"404\":1,\"1646\":1,\"2377\":1,\"2378\":1,\"2384\":1,\"2394\":1,\"2428\":1,\"2436\":1,\"2437\":1,\"2530\":1,\"2551\":1,\"2562\":1,\"2563\":1},\"1\":{\"1\":2,\"5\":4,\"16\":1,\"19\":1,\"21\":1,\"23\":1,\"34\":1,\"47\":1,\"62\":1,\"80\":1,\"107\":1,\"119\":1,\"120\":2,\"133\":1,\"628\":1,\"643\":1,\"680\":1,\"683\":1,\"688\":1,\"689\":1,\"691\":1,\"692\":1,\"693\":2,\"697\":1,\"700\":5,\"704\":1,\"705\":2,\"729\":1,\"740\":1,\"741\":1,\"762\":1,\"774\":2,\"775\":1,\"776\":1,\"791\":1,\"797\":1,\"821\":1,\"835\":1,\"950\":1,\"968\":1,\"1048\":3,\"1066\":1,\"1075\":1,\"1132\":1,\"1138\":6,\"1139\":5,\"1155\":1,\"1205\":1,\"1214\":1,\"1215\":1,\"1219\":1,\"1455\":1,\"1523\":1,\"1524\":1,\"1529\":1,\"1568\":1,\"1646\":1,\"1658\":1,\"1703\":1,\"1704\":1,\"1712\":1,\"1715\":1,\"1735\":1,\"1760\":1,\"1773\":1,\"1781\":1,\"1791\":1,\"1800\":1,\"1828\":1,\"1837\":1,\"1860\":1,\"1883\":1,\"1971\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2044\":1,\"2046\":1,\"2049\":1,\"2103\":1,\"2104\":1,\"2142\":1,\"2170\":1,\"2185\":1,\"2203\":1,\"2210\":1,\"2236\":2,\"2237\":1,\"2260\":1,\"2266\":1,\"2277\":1,\"2354\":1,\"2355\":2,\"2372\":4,\"2373\":2,\"2377\":2,\"2384\":1,\"2385\":1,\"2389\":1,\"2408\":1,\"2414\":1,\"2415\":2,\"2416\":1,\"2418\":2,\"2419\":4,\"2420\":1,\"2422\":1,\"2428\":1,\"2429\":1,\"2436\":2,\"2449\":1,\"2457\":1,\"2461\":1,\"2462\":1,\"2465\":1,\"2481\":1,\"2497\":1,\"2503\":1,\"2518\":1,\"2519\":1,\"2525\":1,\"2545\":1,\"2551\":1,\"2554\":1,\"2562\":2,\"2572\":1,\"2586\":1,\"2635\":1}}],[\"bash\",{\"1\":{\"3\":1,\"18\":1,\"85\":1,\"90\":1,\"98\":2,\"136\":4,\"137\":1,\"144\":2,\"197\":1,\"198\":1,\"204\":1,\"2372\":1,\"2385\":1,\"2429\":1,\"2554\":1,\"2568\":4,\"2569\":2}}],[\"begin\",{\"1\":{\"2600\":1}}],[\"beginning\",{\"1\":{\"1688\":1,\"1741\":1,\"1756\":1}}],[\"beginner\",{\"1\":{\"295\":1,\"2401\":1,\"2537\":1}}],[\"behind\",{\"1\":{\"2467\":1}}],[\"behalf\",{\"1\":{\"2121\":1,\"2122\":1}}],[\"behaviour\",{\"1\":{\"2019\":1}}],[\"behavior\",{\"1\":{\"33\":1,\"60\":1,\"72\":1,\"95\":1,\"148\":1}}],[\"bengali\",{\"1\":{\"2584\":1}}],[\"benefits\",{\"1\":{\"2400\":1,\"2536\":1}}],[\"benchmark\",{\"1\":{\"429\":2}}],[\"benchmarks\",{\"1\":{\"16\":1}}],[\"beyond\",{\"1\":{\"1143\":1}}],[\"being\",{\"1\":{\"836\":1,\"1343\":1,\"1352\":1,\"2468\":1}}],[\"bert\",{\"1\":{\"2474\":2,\"2649\":2}}],[\"bereket\",{\"1\":{\"133\":1}}],[\"berrebbi\",{\"1\":{\"130\":1,\"2583\":1}}],[\"below\",{\"1\":{\"107\":1,\"109\":1,\"115\":1,\"628\":1,\"632\":1,\"1169\":1,\"2151\":1,\"2392\":1,\"2425\":1,\"2440\":1,\"2468\":1,\"2492\":1,\"2510\":2,\"2528\":1,\"2548\":1,\"2564\":1,\"2584\":1,\"2628\":1}}],[\"belongs\",{\"1\":{\"1427\":1}}],[\"belong\",{\"1\":{\"79\":1}}],[\"become\",{\"1\":{\"1758\":1,\"1759\":1}}],[\"becomes\",{\"1\":{\"80\":1,\"1222\":1,\"1282\":1}}],[\"because\",{\"1\":{\"40\":1,\"49\":1,\"57\":1,\"69\":1,\"74\":2,\"84\":1,\"102\":1,\"113\":1,\"126\":1,\"239\":1,\"795\":1,\"944\":1,\"952\":1,\"999\":1,\"1015\":1,\"1248\":1,\"1371\":1,\"1406\":1,\"1429\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1622\":1,\"1897\":1,\"2394\":1,\"2411\":1,\"2418\":1,\"2433\":1,\"2441\":1,\"2530\":1,\"2573\":1,\"2584\":1,\"2585\":2,\"2638\":1}}],[\"better\",{\"1\":{\"49\":1,\"1661\":1,\"2022\":1,\"2441\":4,\"2468\":1,\"2510\":1,\"2542\":1,\"2543\":1,\"2564\":1}}],[\"between\",{\"0\":{\"72\":1,\"95\":1},\"1\":{\"24\":1,\"28\":1,\"57\":1,\"112\":1,\"237\":2,\"238\":1,\"595\":1,\"612\":1,\"712\":1,\"737\":1,\"836\":1,\"905\":1,\"997\":1,\"1010\":2,\"1052\":1,\"1141\":1,\"1149\":1,\"1150\":1,\"1242\":1,\"1269\":3,\"1529\":1,\"1560\":1,\"1568\":1,\"1600\":1,\"1603\":1,\"1622\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1688\":1,\"1712\":1,\"1715\":1,\"1719\":1,\"1756\":1,\"1804\":1,\"1895\":1,\"1900\":1,\"1935\":1,\"1940\":1,\"1943\":1,\"2152\":1,\"2168\":1,\"2212\":1,\"2385\":1,\"2387\":2,\"2419\":1,\"2441\":2}}],[\"beta=1\",{\"1\":{\"1233\":1}}],[\"betas=\",{\"1\":{\"174\":1}}],[\"betas\",{\"0\":{\"1299\":1,\"1303\":1},\"1\":{\"62\":1,\"1142\":3,\"1299\":4,\"1301\":2,\"1303\":4,\"1304\":2}}],[\"beta\",{\"1\":{\"23\":1,\"115\":6,\"119\":1,\"249\":2,\"700\":2,\"917\":1,\"1048\":3,\"1067\":4,\"1082\":7,\"1084\":7,\"1096\":9,\"1138\":2,\"1139\":2,\"1142\":1,\"1299\":1,\"1301\":1,\"1303\":1,\"1304\":1,\"1619\":10,\"1778\":1,\"1829\":1,\"1852\":1,\"1860\":4,\"1883\":3,\"2255\":1,\"2260\":3,\"2274\":6}}],[\"best`\",{\"1\":{\"28\":1}}],[\"best\",{\"1\":{\"24\":1,\"26\":2,\"28\":1,\"73\":1,\"108\":3,\"110\":1,\"150\":3,\"174\":1,\"194\":1,\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"240\":1,\"241\":1,\"242\":2,\"285\":2,\"286\":2,\"296\":1,\"501\":1,\"600\":2,\"610\":2,\"633\":2,\"645\":1,\"676\":2,\"691\":2,\"692\":1,\"693\":2,\"697\":3,\"700\":6,\"705\":3,\"781\":2,\"794\":4,\"797\":3,\"812\":2,\"857\":1,\"917\":1,\"952\":1,\"1048\":5,\"1138\":7,\"1139\":6,\"1427\":3,\"1530\":1,\"1563\":1,\"1600\":1,\"1603\":2,\"1622\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"1962\":5,\"2022\":1,\"2186\":1,\"2193\":2,\"2202\":2,\"2204\":1,\"2357\":2,\"2389\":1,\"2408\":1,\"2422\":1,\"2440\":2,\"2449\":1,\"2465\":1,\"2481\":1,\"2492\":1,\"2503\":1,\"2519\":1,\"2525\":1,\"2545\":1,\"2558\":2,\"2578\":2,\"2584\":1,\"2628\":1,\"2639\":1}}],[\"beamformser\",{\"1\":{\"1739\":2}}],[\"beamformed\",{\"1\":{\"1719\":1}}],[\"beamformers\",{\"1\":{\"1739\":1}}],[\"beamformer\",{\"0\":{\"690\":1,\"729\":2,\"854\":1,\"885\":1,\"887\":1,\"1455\":1,\"1524\":2,\"1611\":1,\"1678\":1,\"1680\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1702\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1717\":1,\"1736\":1,\"1739\":2,\"1741\":1,\"1746\":1,\"2489\":1,\"2609\":1,\"2626\":1},\"1\":{\"251\":2,\"690\":1,\"729\":4,\"756\":1,\"854\":1,\"885\":1,\"887\":1,\"1158\":2,\"1239\":1,\"1455\":1,\"1524\":5,\"1611\":3,\"1678\":1,\"1680\":3,\"1695\":1,\"1696\":3,\"1697\":1,\"1698\":3,\"1702\":1,\"1704\":2,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1711\":1,\"1712\":2,\"1713\":2,\"1714\":1,\"1715\":1,\"1717\":1,\"1736\":1,\"1739\":7,\"1741\":1,\"1746\":2,\"1791\":1,\"2490\":4,\"2492\":1,\"2609\":4,\"2626\":4,\"2628\":1}}],[\"beamform\",{\"1\":{\"854\":1,\"885\":1,\"1678\":1,\"1698\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1715\":1}}],[\"beamforming\",{\"0\":{\"854\":1,\"1678\":1},\"1\":{\"854\":1,\"1524\":3,\"1678\":1,\"1695\":1,\"1704\":1,\"1717\":3,\"1719\":1}}],[\"beamsearchtransducerstreaming\",{\"0\":{\"1139\":1},\"1\":{\"1139\":1}}],[\"beamsearchtransducer\",{\"0\":{\"700\":1,\"1048\":1,\"1138\":1},\"1\":{\"700\":1,\"1048\":2,\"1138\":1}}],[\"beamsearchtimesyncstreaming\",{\"0\":{\"699\":1},\"1\":{\"699\":1}}],[\"beamsearchtimesync\",{\"0\":{\"698\":1},\"1\":{\"698\":1}}],[\"beamsearch\",{\"0\":{\"697\":1},\"1\":{\"691\":1,\"697\":2}}],[\"beam\",{\"0\":{\"312\":1,\"319\":1,\"331\":1,\"337\":1,\"389\":1,\"395\":1,\"405\":1,\"413\":1,\"421\":1,\"427\":1,\"447\":1,\"453\":1,\"483\":1,\"489\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"707\":1,\"765\":1,\"797\":1,\"798\":1,\"857\":2,\"1048\":1,\"1060\":1,\"1063\":1,\"1138\":1,\"1139\":1,\"1176\":1,\"1193\":1},\"1\":{\"23\":13,\"98\":1,\"103\":1,\"119\":3,\"150\":7,\"175\":1,\"194\":1,\"249\":2,\"251\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"307\":2,\"315\":2,\"327\":2,\"333\":4,\"384\":2,\"391\":2,\"399\":4,\"408\":2,\"422\":2,\"443\":4,\"449\":2,\"478\":2,\"485\":2,\"676\":3,\"691\":25,\"692\":5,\"693\":16,\"694\":1,\"695\":1,\"697\":24,\"698\":12,\"699\":12,\"700\":11,\"705\":2,\"706\":2,\"707\":1,\"742\":1,\"751\":1,\"765\":1,\"773\":2,\"781\":1,\"785\":1,\"795\":1,\"797\":23,\"798\":1,\"812\":1,\"814\":1,\"815\":2,\"857\":16,\"917\":1,\"1048\":11,\"1060\":2,\"1063\":1,\"1133\":3,\"1138\":11,\"1139\":11,\"1176\":2,\"1193\":1,\"1219\":1,\"2358\":1,\"2455\":2,\"2460\":1,\"2461\":1,\"2520\":2,\"2569\":1,\"2572\":4,\"2579\":1,\"2586\":1,\"2592\":1}}],[\"been\",{\"1\":{\"11\":1,\"107\":1,\"836\":1,\"1427\":1,\"1639\":1,\"2400\":1,\"2411\":1,\"2473\":1,\"2536\":1}}],[\"before=false\",{\"1\":{\"1800\":1}}],[\"before=true\",{\"1\":{\"711\":1,\"749\":1}}],[\"before\",{\"1\":{\"2\":1,\"3\":1,\"26\":1,\"34\":1,\"40\":1,\"47\":1,\"74\":1,\"84\":1,\"85\":1,\"99\":1,\"124\":1,\"127\":1,\"148\":1,\"168\":1,\"180\":1,\"235\":1,\"623\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":2,\"685\":1,\"688\":1,\"711\":2,\"731\":1,\"745\":2,\"749\":2,\"754\":4,\"755\":3,\"758\":1,\"770\":1,\"797\":1,\"803\":1,\"822\":3,\"826\":4,\"1028\":1,\"1115\":2,\"1133\":5,\"1148\":3,\"1149\":3,\"1150\":3,\"1167\":1,\"1168\":1,\"1178\":3,\"1180\":4,\"1181\":2,\"1190\":1,\"1196\":1,\"1197\":1,\"1200\":3,\"1203\":3,\"1204\":2,\"1214\":1,\"1244\":2,\"1248\":1,\"1252\":1,\"1269\":1,\"1271\":1,\"1272\":3,\"1273\":2,\"1470\":1,\"1505\":3,\"1543\":1,\"1551\":1,\"1553\":1,\"1658\":1,\"1659\":1,\"1664\":1,\"1665\":1,\"1669\":3,\"1771\":3,\"1778\":2,\"1787\":3,\"1788\":2,\"1798\":3,\"1800\":1,\"1804\":3,\"1805\":1,\"1850\":2,\"1851\":6,\"1852\":2,\"1856\":1,\"1857\":1,\"1858\":1,\"1867\":1,\"1874\":3,\"1877\":1,\"1878\":3,\"1964\":1,\"2000\":3,\"2001\":3,\"2002\":1,\"2004\":3,\"2026\":1,\"2029\":3,\"2054\":3,\"2078\":1,\"2088\":3,\"2090\":6,\"2091\":3,\"2243\":6,\"2244\":6,\"2245\":3,\"2255\":5,\"2256\":3,\"2264\":6,\"2279\":6,\"2280\":3,\"2401\":1,\"2430\":1,\"2440\":1,\"2468\":1,\"2537\":1,\"2555\":1,\"2558\":3,\"2571\":1,\"2572\":1,\"2585\":1,\"2638\":1}}],[\"be\",{\"1\":{\"1\":2,\"3\":3,\"5\":1,\"6\":1,\"14\":2,\"15\":2,\"17\":1,\"20\":1,\"21\":4,\"22\":4,\"24\":1,\"25\":2,\"27\":1,\"28\":3,\"29\":1,\"37\":1,\"38\":2,\"40\":1,\"45\":1,\"52\":1,\"56\":2,\"57\":2,\"59\":1,\"60\":4,\"65\":1,\"68\":1,\"69\":2,\"72\":2,\"74\":2,\"76\":2,\"79\":2,\"82\":1,\"83\":2,\"84\":2,\"85\":4,\"91\":1,\"98\":1,\"99\":1,\"101\":1,\"102\":3,\"104\":1,\"105\":1,\"107\":3,\"109\":1,\"110\":1,\"112\":3,\"113\":5,\"115\":2,\"117\":2,\"118\":1,\"121\":2,\"122\":2,\"124\":2,\"135\":1,\"142\":1,\"143\":1,\"144\":2,\"148\":1,\"150\":5,\"203\":1,\"235\":1,\"237\":4,\"238\":5,\"239\":2,\"241\":1,\"277\":1,\"564\":1,\"607\":1,\"629\":1,\"634\":1,\"652\":1,\"653\":1,\"655\":1,\"691\":3,\"692\":1,\"693\":4,\"697\":3,\"698\":1,\"699\":1,\"711\":6,\"727\":4,\"728\":4,\"734\":1,\"745\":3,\"746\":3,\"747\":1,\"749\":2,\"752\":1,\"753\":1,\"754\":2,\"756\":1,\"757\":1,\"760\":1,\"761\":1,\"762\":1,\"771\":1,\"772\":1,\"778\":1,\"779\":1,\"785\":2,\"791\":1,\"797\":3,\"799\":1,\"809\":1,\"810\":1,\"820\":1,\"821\":1,\"826\":1,\"831\":1,\"832\":1,\"835\":1,\"836\":1,\"857\":3,\"875\":1,\"899\":1,\"901\":1,\"909\":1,\"927\":1,\"975\":1,\"976\":1,\"1008\":1,\"1011\":2,\"1013\":1,\"1015\":1,\"1019\":1,\"1037\":1,\"1039\":1,\"1042\":1,\"1043\":1,\"1095\":1,\"1109\":1,\"1110\":1,\"1111\":1,\"1112\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":2,\"1134\":1,\"1135\":1,\"1136\":1,\"1137\":1,\"1140\":2,\"1142\":1,\"1143\":2,\"1148\":6,\"1149\":6,\"1150\":6,\"1151\":1,\"1152\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1169\":2,\"1174\":1,\"1175\":1,\"1177\":1,\"1178\":2,\"1179\":1,\"1180\":1,\"1181\":1,\"1184\":1,\"1185\":1,\"1186\":3,\"1187\":11,\"1188\":1,\"1189\":1,\"1198\":3,\"1200\":1,\"1202\":11,\"1203\":6,\"1207\":1,\"1208\":1,\"1210\":3,\"1211\":2,\"1212\":1,\"1213\":1,\"1215\":1,\"1216\":1,\"1220\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":2,\"1228\":1,\"1229\":1,\"1230\":1,\"1231\":1,\"1232\":1,\"1233\":1,\"1234\":1,\"1235\":1,\"1236\":1,\"1237\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1241\":2,\"1243\":2,\"1244\":1,\"1245\":3,\"1246\":1,\"1249\":1,\"1250\":1,\"1252\":3,\"1253\":2,\"1254\":3,\"1257\":1,\"1258\":1,\"1259\":1,\"1260\":1,\"1261\":1,\"1262\":1,\"1263\":1,\"1264\":1,\"1265\":1,\"1266\":1,\"1267\":1,\"1268\":1,\"1269\":3,\"1272\":3,\"1274\":2,\"1275\":1,\"1276\":2,\"1277\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1285\":1,\"1286\":4,\"1287\":4,\"1298\":2,\"1299\":2,\"1301\":1,\"1302\":2,\"1303\":2,\"1304\":1,\"1327\":1,\"1336\":2,\"1337\":2,\"1343\":1,\"1345\":1,\"1347\":1,\"1348\":2,\"1349\":2,\"1350\":2,\"1352\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1371\":2,\"1381\":1,\"1392\":1,\"1406\":1,\"1429\":2,\"1431\":1,\"1432\":2,\"1433\":1,\"1434\":1,\"1435\":1,\"1436\":1,\"1437\":1,\"1438\":1,\"1439\":1,\"1440\":1,\"1441\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1445\":1,\"1446\":1,\"1447\":1,\"1448\":1,\"1449\":1,\"1450\":1,\"1452\":2,\"1453\":1,\"1456\":1,\"1457\":1,\"1458\":1,\"1459\":1,\"1460\":1,\"1461\":1,\"1462\":3,\"1463\":1,\"1464\":3,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":2,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":2,\"1506\":1,\"1507\":1,\"1508\":1,\"1509\":1,\"1510\":1,\"1512\":1,\"1513\":1,\"1517\":1,\"1518\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":2,\"1523\":1,\"1524\":1,\"1532\":1,\"1533\":1,\"1535\":1,\"1536\":1,\"1537\":2,\"1538\":1,\"1540\":1,\"1541\":1,\"1543\":1,\"1544\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1550\":1,\"1551\":5,\"1553\":5,\"1555\":1,\"1556\":1,\"1561\":1,\"1562\":1,\"1564\":2,\"1565\":1,\"1573\":1,\"1574\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1596\":1,\"1597\":1,\"1598\":3,\"1599\":1,\"1600\":1,\"1601\":2,\"1603\":5,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1613\":1,\"1614\":1,\"1616\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":5,\"1624\":1,\"1625\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1630\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1639\":1,\"1641\":1,\"1642\":1,\"1643\":1,\"1646\":1,\"1647\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"1652\":5,\"1653\":1,\"1654\":3,\"1656\":1,\"1657\":1,\"1664\":1,\"1665\":1,\"1669\":2,\"1670\":1,\"1671\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1683\":1,\"1688\":1,\"1693\":2,\"1695\":1,\"1713\":1,\"1735\":3,\"1739\":1,\"1752\":1,\"1755\":2,\"1756\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1765\":1,\"1769\":1,\"1770\":1,\"1773\":1,\"1774\":1,\"1775\":1,\"1778\":3,\"1779\":1,\"1780\":1,\"1782\":1,\"1783\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1792\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":1,\"1800\":1,\"1801\":3,\"1802\":1,\"1803\":1,\"1804\":5,\"1805\":3,\"1806\":1,\"1807\":1,\"1810\":1,\"1837\":1,\"1842\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":2,\"1849\":3,\"1850\":3,\"1851\":4,\"1852\":3,\"1856\":4,\"1857\":1,\"1858\":2,\"1861\":2,\"1862\":1,\"1870\":1,\"1871\":1,\"1877\":3,\"1878\":4,\"1880\":1,\"1881\":1,\"1890\":1,\"1891\":1,\"1897\":2,\"1902\":1,\"1903\":1,\"1905\":7,\"1907\":1,\"1908\":1,\"1912\":1,\"1913\":1,\"1917\":1,\"1925\":2,\"1927\":1,\"1930\":1,\"1932\":2,\"1946\":1,\"1947\":1,\"1951\":2,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1,\"1958\":1,\"1959\":1,\"1962\":1,\"1976\":1,\"1977\":1,\"1978\":1,\"1979\":1,\"1981\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1989\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1994\":1,\"1995\":1,\"1997\":1,\"1998\":1,\"2001\":5,\"2002\":3,\"2004\":5,\"2024\":1,\"2025\":1,\"2029\":3,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2039\":1,\"2040\":1,\"2041\":1,\"2042\":1,\"2043\":1,\"2044\":2,\"2045\":1,\"2046\":2,\"2047\":1,\"2048\":1,\"2050\":2,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":2,\"2055\":1,\"2056\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2064\":1,\"2065\":1,\"2066\":1,\"2067\":1,\"2068\":2,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2079\":2,\"2082\":1,\"2083\":1,\"2086\":4,\"2087\":4,\"2090\":4,\"2095\":4,\"2096\":2,\"2098\":2,\"2099\":4,\"2100\":2,\"2101\":2,\"2102\":3,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2121\":1,\"2122\":1,\"2148\":1,\"2149\":1,\"2150\":1,\"2153\":1,\"2156\":1,\"2168\":1,\"2169\":1,\"2170\":3,\"2197\":2,\"2198\":2,\"2233\":1,\"2234\":1,\"2237\":1,\"2238\":1,\"2240\":1,\"2241\":1,\"2242\":1,\"2243\":5,\"2244\":5,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2255\":5,\"2263\":4,\"2264\":4,\"2266\":1,\"2267\":1,\"2275\":1,\"2276\":1,\"2278\":1,\"2279\":5,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2288\":1,\"2289\":1,\"2290\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2297\":1,\"2298\":1,\"2299\":1,\"2300\":1,\"2347\":1,\"2368\":1,\"2372\":2,\"2384\":4,\"2385\":3,\"2387\":1,\"2389\":1,\"2392\":1,\"2397\":2,\"2398\":2,\"2400\":1,\"2401\":2,\"2403\":1,\"2408\":1,\"2412\":1,\"2415\":1,\"2418\":1,\"2422\":2,\"2425\":1,\"2429\":1,\"2430\":1,\"2431\":2,\"2433\":1,\"2440\":1,\"2449\":1,\"2450\":1,\"2465\":1,\"2468\":2,\"2481\":1,\"2482\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2501\":2,\"2503\":1,\"2525\":2,\"2528\":1,\"2533\":2,\"2534\":2,\"2536\":1,\"2537\":2,\"2539\":1,\"2545\":2,\"2548\":1,\"2554\":1,\"2555\":1,\"2559\":1,\"2564\":2,\"2584\":7,\"2585\":3,\"2600\":2,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2638\":3,\"2641\":2,\"2645\":1}}],[\"bypass\",{\"1\":{\"1552\":1}}],[\"bytesio\",{\"1\":{\"2386\":2}}],[\"bytes\",{\"0\":{\"2165\":1},\"1\":{\"1142\":1,\"1155\":3,\"1398\":6,\"2165\":2}}],[\"byte\",{\"1\":{\"1015\":1,\"1398\":1,\"2638\":1}}],[\"bytetensor\",{\"1\":{\"737\":2,\"899\":1,\"2265\":1}}],[\"byter\",{\"1\":{\"48\":1}}],[\"byan\",{\"1\":{\"110\":3}}],[\"by\",{\"0\":{\"52\":1,\"174\":1,\"903\":1,\"929\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1881\":1,\"2147\":1},\"1\":{\"1\":3,\"5\":1,\"11\":2,\"17\":1,\"19\":1,\"21\":2,\"23\":3,\"25\":1,\"27\":1,\"28\":1,\"29\":1,\"35\":1,\"37\":1,\"45\":1,\"49\":2,\"56\":1,\"57\":3,\"59\":2,\"60\":1,\"64\":1,\"68\":1,\"69\":2,\"72\":1,\"74\":1,\"75\":1,\"77\":1,\"78\":1,\"80\":1,\"82\":2,\"84\":1,\"85\":1,\"91\":1,\"96\":2,\"97\":1,\"99\":1,\"101\":2,\"113\":4,\"115\":4,\"116\":1,\"119\":2,\"120\":1,\"121\":1,\"122\":2,\"133\":1,\"134\":1,\"135\":2,\"136\":1,\"137\":1,\"141\":1,\"142\":1,\"144\":1,\"148\":2,\"150\":1,\"162\":1,\"163\":1,\"169\":1,\"170\":1,\"172\":1,\"181\":1,\"187\":1,\"188\":1,\"201\":1,\"235\":2,\"240\":1,\"295\":1,\"506\":1,\"595\":1,\"607\":1,\"610\":1,\"613\":1,\"619\":3,\"626\":1,\"632\":1,\"641\":1,\"667\":1,\"671\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"691\":2,\"692\":2,\"693\":1,\"697\":6,\"700\":3,\"705\":1,\"708\":1,\"727\":4,\"728\":4,\"729\":1,\"730\":1,\"742\":1,\"745\":4,\"746\":4,\"752\":2,\"756\":2,\"758\":1,\"760\":2,\"768\":1,\"774\":1,\"778\":3,\"782\":1,\"785\":1,\"793\":1,\"797\":1,\"802\":1,\"803\":1,\"804\":1,\"819\":1,\"830\":1,\"831\":2,\"834\":1,\"835\":1,\"837\":1,\"903\":1,\"904\":1,\"917\":2,\"929\":1,\"956\":1,\"973\":1,\"981\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1011\":3,\"1028\":1,\"1046\":1,\"1048\":5,\"1061\":2,\"1067\":1,\"1068\":2,\"1076\":1,\"1082\":1,\"1084\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":2,\"1111\":2,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":2,\"1119\":2,\"1121\":2,\"1123\":2,\"1125\":2,\"1127\":2,\"1130\":2,\"1132\":1,\"1133\":1,\"1134\":2,\"1136\":2,\"1138\":4,\"1139\":4,\"1140\":1,\"1141\":1,\"1143\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":2,\"1153\":1,\"1156\":2,\"1158\":2,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":2,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":2,\"1172\":2,\"1174\":2,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":2,\"1187\":4,\"1188\":2,\"1190\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1200\":1,\"1202\":4,\"1203\":1,\"1204\":1,\"1206\":1,\"1207\":2,\"1209\":1,\"1211\":2,\"1212\":2,\"1214\":1,\"1215\":2,\"1217\":1,\"1218\":1,\"1220\":2,\"1222\":2,\"1224\":2,\"1229\":2,\"1231\":2,\"1233\":2,\"1235\":2,\"1237\":2,\"1239\":2,\"1241\":2,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1249\":2,\"1252\":1,\"1253\":3,\"1254\":2,\"1257\":2,\"1259\":2,\"1261\":2,\"1263\":2,\"1265\":2,\"1267\":2,\"1269\":2,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":2,\"1276\":2,\"1279\":2,\"1280\":2,\"1282\":2,\"1284\":2,\"1286\":2,\"1287\":2,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":1,\"1339\":1,\"1348\":1,\"1360\":2,\"1362\":2,\"1364\":2,\"1366\":2,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1406\":1,\"1427\":1,\"1429\":1,\"1431\":2,\"1432\":1,\"1433\":2,\"1435\":2,\"1437\":2,\"1439\":2,\"1441\":2,\"1443\":2,\"1445\":2,\"1447\":2,\"1449\":2,\"1452\":3,\"1455\":1,\"1456\":2,\"1458\":2,\"1460\":2,\"1462\":1,\"1464\":1,\"1465\":1,\"1466\":2,\"1468\":2,\"1472\":1,\"1473\":1,\"1474\":2,\"1476\":2,\"1478\":1,\"1480\":1,\"1482\":2,\"1484\":1,\"1485\":2,\"1487\":2,\"1489\":2,\"1491\":2,\"1493\":2,\"1495\":2,\"1497\":2,\"1499\":2,\"1501\":2,\"1503\":2,\"1506\":2,\"1508\":2,\"1510\":2,\"1511\":1,\"1512\":2,\"1517\":3,\"1518\":2,\"1520\":2,\"1524\":1,\"1525\":1,\"1530\":1,\"1531\":1,\"1532\":2,\"1535\":2,\"1537\":2,\"1540\":2,\"1542\":1,\"1543\":2,\"1546\":1,\"1547\":2,\"1549\":2,\"1552\":3,\"1553\":2,\"1554\":1,\"1555\":2,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":2,\"1563\":1,\"1564\":2,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":2,\"1575\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":2,\"1583\":2,\"1586\":2,\"1588\":2,\"1590\":2,\"1592\":2,\"1594\":1,\"1595\":1,\"1596\":2,\"1598\":3,\"1601\":1,\"1602\":1,\"1604\":1,\"1605\":3,\"1607\":2,\"1609\":2,\"1611\":1,\"1613\":2,\"1616\":1,\"1617\":1,\"1619\":1,\"1620\":2,\"1624\":2,\"1627\":2,\"1629\":2,\"1631\":2,\"1633\":2,\"1635\":2,\"1639\":1,\"1640\":1,\"1641\":2,\"1643\":2,\"1644\":1,\"1645\":1,\"1646\":2,\"1648\":2,\"1650\":2,\"1652\":3,\"1654\":2,\"1655\":1,\"1656\":2,\"1660\":2,\"1661\":3,\"1662\":3,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1670\":2,\"1671\":1,\"1672\":2,\"1674\":2,\"1676\":2,\"1688\":2,\"1693\":1,\"1715\":1,\"1718\":1,\"1719\":3,\"1735\":1,\"1755\":1,\"1756\":2,\"1760\":1,\"1761\":2,\"1763\":2,\"1767\":1,\"1768\":1,\"1769\":2,\"1774\":2,\"1778\":4,\"1779\":2,\"1782\":2,\"1785\":1,\"1789\":2,\"1791\":2,\"1793\":2,\"1795\":2,\"1797\":1,\"1799\":1,\"1801\":1,\"1805\":4,\"1806\":2,\"1810\":1,\"1828\":1,\"1836\":3,\"1839\":6,\"1840\":1,\"1842\":1,\"1843\":3,\"1850\":4,\"1852\":4,\"1853\":1,\"1854\":1,\"1855\":1,\"1877\":4,\"1881\":1,\"1890\":2,\"1892\":2,\"1893\":2,\"1902\":2,\"1906\":1,\"1907\":2,\"1910\":1,\"1912\":2,\"1914\":1,\"1915\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1941\":3,\"1951\":2,\"1953\":2,\"1955\":2,\"1957\":1,\"1958\":2,\"1960\":1,\"1970\":2,\"1972\":1,\"1975\":2,\"1976\":2,\"1978\":2,\"1980\":1,\"1981\":2,\"1983\":1,\"1984\":3,\"1986\":1,\"1987\":2,\"1988\":1,\"1989\":2,\"1990\":1,\"1991\":2,\"1992\":1,\"1993\":1,\"1994\":2,\"1996\":1,\"1997\":2,\"1999\":1,\"2000\":1,\"2003\":1,\"2010\":1,\"2018\":1,\"2019\":1,\"2024\":2,\"2026\":1,\"2027\":2,\"2029\":1,\"2030\":2,\"2032\":2,\"2034\":2,\"2036\":2,\"2038\":2,\"2040\":3,\"2042\":2,\"2044\":3,\"2046\":1,\"2047\":2,\"2049\":1,\"2050\":2,\"2052\":3,\"2054\":1,\"2055\":2,\"2057\":2,\"2059\":2,\"2061\":2,\"2063\":1,\"2064\":2,\"2066\":2,\"2068\":3,\"2070\":2,\"2072\":2,\"2074\":1,\"2075\":1,\"2076\":2,\"2077\":1,\"2084\":1,\"2089\":1,\"2096\":4,\"2098\":4,\"2099\":7,\"2100\":4,\"2101\":4,\"2102\":5,\"2103\":4,\"2104\":4,\"2105\":4,\"2107\":4,\"2108\":4,\"2109\":4,\"2110\":4,\"2111\":4,\"2112\":4,\"2113\":4,\"2114\":4,\"2115\":4,\"2116\":4,\"2117\":4,\"2118\":4,\"2131\":2,\"2142\":1,\"2147\":1,\"2149\":2,\"2151\":1,\"2168\":2,\"2170\":1,\"2185\":1,\"2201\":1,\"2203\":1,\"2212\":1,\"2233\":2,\"2235\":1,\"2237\":2,\"2239\":1,\"2241\":2,\"2246\":2,\"2247\":1,\"2248\":2,\"2249\":1,\"2250\":2,\"2251\":1,\"2252\":1,\"2259\":1,\"2263\":1,\"2266\":2,\"2275\":2,\"2277\":1,\"2281\":2,\"2283\":2,\"2285\":2,\"2287\":1,\"2288\":2,\"2290\":2,\"2292\":2,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":2,\"2299\":2,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2365\":1,\"2372\":3,\"2373\":4,\"2377\":1,\"2378\":1,\"2385\":2,\"2405\":1,\"2410\":1,\"2415\":1,\"2419\":1,\"2421\":1,\"2429\":4,\"2430\":3,\"2431\":1,\"2433\":1,\"2437\":1,\"2440\":5,\"2452\":1,\"2467\":1,\"2473\":1,\"2501\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2518\":1,\"2541\":1,\"2554\":4,\"2555\":4,\"2559\":1,\"2563\":1,\"2564\":1,\"2568\":1,\"2574\":1,\"2584\":2,\"2585\":1,\"2600\":1,\"2618\":1,\"2640\":1,\"2655\":1,\"2660\":1}}],[\"sjtu\",{\"1\":{\"2618\":2}}],[\"sf\",{\"1\":{\"2456\":2,\"2460\":2,\"2500\":1,\"2510\":2,\"2514\":2,\"2521\":3,\"2659\":2}}],[\"sfi\",{\"1\":{\"1662\":1}}],[\"s>\",{\"1\":{\"2294\":1}}],[\"sk\",{\"1\":{\"2568\":2}}],[\"skcwse\",{\"1\":{\"2059\":1}}],[\"skfwse\",{\"1\":{\"2059\":1}}],[\"skablock\",{\"1\":{\"2064\":1}}],[\"skatdnnprojector\",{\"0\":{\"2066\":1},\"1\":{\"2066\":1}}],[\"skatdnnencoder\",{\"0\":{\"2064\":1},\"1\":{\"2064\":1}}],[\"skattentionmodule\",{\"0\":{\"2063\":1},\"1\":{\"2063\":1}}],[\"ska\",{\"0\":{\"2042\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2074\":1,\"2075\":1},\"1\":{\"2042\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":4,\"2066\":1,\"2074\":1,\"2075\":1}}],[\"skirt\",{\"1\":{\"1921\":3}}],[\"skimseparator\",{\"0\":{\"1654\":1},\"1\":{\"1654\":1}}],[\"skim\",{\"0\":{\"1598\":1,\"1648\":1,\"1652\":2,\"1654\":1},\"1\":{\"1598\":3,\"1648\":3,\"1652\":5,\"1654\":3}}],[\"skipped\",{\"0\":{\"2434\":1,\"2557\":1},\"1\":{\"1713\":1}}],[\"skipping\",{\"1\":{\"1652\":1,\"1654\":1}}],[\"skips\",{\"1\":{\"1697\":1}}],[\"skip=none\",{\"1\":{\"1279\":1}}],[\"skipch\",{\"1\":{\"835\":1}}],[\"skipch=256\",{\"1\":{\"835\":1}}],[\"skip\",{\"0\":{\"2374\":1},\"1\":{\"91\":11,\"96\":4,\"97\":1,\"99\":2,\"110\":2,\"132\":1,\"134\":1,\"299\":1,\"579\":2,\"826\":3,\"835\":1,\"1115\":4,\"1269\":6,\"1458\":1,\"1474\":1,\"1484\":2,\"1522\":9,\"1523\":9,\"1546\":2,\"1551\":1,\"1553\":1,\"1601\":2,\"1605\":3,\"1631\":1,\"1635\":1,\"1659\":3,\"1663\":1,\"1664\":1,\"1665\":3,\"1752\":3,\"1862\":3,\"1878\":1,\"1880\":6,\"2294\":1,\"2374\":1,\"2375\":1,\"2431\":4,\"2434\":1,\"2557\":1,\"2559\":1,\"2585\":2}}],[\"sbdblock\",{\"0\":{\"1795\":1},\"1\":{\"1795\":2}}],[\"sbd\",{\"0\":{\"1793\":1},\"1\":{\"1761\":9,\"1763\":9,\"1793\":3,\"1795\":1,\"1805\":9}}],[\"svoiceseparator\",{\"0\":{\"1645\":1},\"1\":{\"1645\":1}}],[\"svoice\",{\"0\":{\"1540\":1,\"1555\":1,\"1645\":1,\"1735\":1},\"1\":{\"1540\":1,\"1555\":1,\"1645\":2,\"1735\":1}}],[\"svspreprocessor\",{\"0\":{\"2196\":1},\"1\":{\"2196\":2}}],[\"svstask\",{\"0\":{\"2113\":1},\"1\":{\"2113\":2}}],[\"svs\",{\"0\":{\"455\":1,\"1760\":2,\"1761\":1,\"1763\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1774\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1781\":1,\"1782\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1806\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1822\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1827\":1,\"2077\":2,\"2078\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2103\":1,\"2113\":1,\"2690\":1,\"2706\":1},\"1\":{\"397\":1,\"398\":2,\"455\":5,\"1760\":5,\"1761\":1,\"1763\":1,\"1765\":1,\"1766\":1,\"1767\":2,\"1768\":2,\"1769\":1,\"1771\":1,\"1772\":1,\"1773\":2,\"1774\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1781\":1,\"1782\":2,\"1784\":2,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":2,\"1791\":1,\"1793\":2,\"1795\":2,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1805\":2,\"1806\":2,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":2,\"1812\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":2,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":2,\"1821\":1,\"1822\":1,\"1823\":2,\"1824\":2,\"1825\":1,\"1826\":2,\"1827\":2,\"2077\":5,\"2078\":1,\"2081\":1,\"2082\":3,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":3,\"2087\":2,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":2,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2103\":2,\"2113\":2,\"2130\":1,\"2196\":1}}],[\"s4d\",{\"1\":{\"1245\":1,\"1247\":1}}],[\"s4decoder\",{\"0\":{\"1244\":1},\"1\":{\"1244\":2}}],[\"s4\",{\"0\":{\"1217\":1,\"1241\":2,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1297\":1,\"1314\":1,\"1323\":1,\"1339\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1351\":1,\"1357\":1},\"1\":{\"1217\":2,\"1241\":5,\"1244\":3,\"1245\":3,\"1247\":2,\"1248\":2,\"1297\":2,\"1314\":2,\"1323\":3,\"1339\":2,\"1341\":2,\"1342\":1,\"1343\":2,\"1351\":2,\"1357\":2}}],[\"sz=none\",{\"1\":{\"1604\":1}}],[\"sz=\",{\"1\":{\"1604\":1}}],[\"sz\",{\"1\":{\"1071\":2,\"1604\":3}}],[\"sw005320\",{\"1\":{\"2354\":1}}],[\"swap\",{\"1\":{\"1028\":2}}],[\"swich\",{\"1\":{\"819\":1}}],[\"swith\",{\"1\":{\"677\":1,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"758\":1,\"892\":1}}],[\"switch\",{\"1\":{\"34\":1,\"150\":1,\"2170\":1,\"2400\":1,\"2536\":1}}],[\"swish\",{\"0\":{\"819\":2,\"1084\":1},\"1\":{\"115\":6,\"116\":1,\"819\":3,\"1061\":1,\"1066\":1,\"1084\":8,\"1093\":2,\"1096\":3,\"1148\":1,\"1149\":1,\"1169\":1,\"1203\":1,\"1505\":1,\"1518\":1,\"1605\":1,\"1771\":1,\"1778\":1,\"1787\":1,\"1788\":2,\"1798\":1,\"1804\":1,\"1805\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"2003\":1,\"2026\":1,\"2054\":1,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2279\":1}}],[\"squeeze\",{\"1\":{\"2369\":1,\"2372\":2,\"2487\":1,\"2491\":1,\"2497\":2,\"2498\":5,\"2500\":2,\"2501\":1,\"2606\":1,\"2607\":1,\"2610\":1,\"2614\":2,\"2615\":2,\"2616\":5,\"2617\":2,\"2623\":1,\"2624\":1,\"2627\":1,\"2632\":2,\"2633\":2,\"2634\":5,\"2635\":2}}],[\"squaredrelu\",{\"0\":{\"1259\":1},\"1\":{\"1259\":1}}],[\"squared\",{\"0\":{\"1010\":1},\"1\":{\"738\":1,\"1010\":4}}],[\"square\",{\"0\":{\"920\":1},\"1\":{\"702\":1,\"760\":1,\"822\":1,\"920\":1,\"2000\":1,\"2088\":1}}],[\"sqrt\",{\"1\":{\"1618\":1,\"1619\":1}}],[\"sqnorm\",{\"0\":{\"923\":1},\"1\":{\"923\":1}}],[\"sgmse\",{\"1\":{\"1605\":1,\"1619\":1}}],[\"sgd\",{\"0\":{\"666\":1,\"1972\":2},\"1\":{\"662\":1,\"666\":2,\"1972\":7}}],[\"sgdfactory\",{\"0\":{\"662\":1},\"1\":{\"662\":2}}],[\"sge\",{\"1\":{\"141\":1}}],[\"sdw\",{\"0\":{\"1715\":1},\"1\":{\"1715\":2}}],[\"sdes\",{\"0\":{\"1618\":1,\"1619\":1,\"1638\":1,\"1679\":1},\"1\":{\"1618\":2,\"1619\":2,\"1638\":2,\"1679\":2}}],[\"sde\",{\"0\":{\"1638\":1},\"1\":{\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1605\":1,\"1618\":6,\"1619\":6,\"1623\":1,\"1637\":1,\"1638\":9}}],[\"sd=4\",{\"1\":{\"749\":1}}],[\"sd\",{\"1\":{\"629\":1,\"633\":1}}],[\"sdrloss\",{\"0\":{\"1639\":1},\"1\":{\"1639\":1}}],[\"sdr\",{\"1\":{\"528\":2,\"1466\":2,\"1639\":4,\"1640\":3,\"1660\":1,\"1661\":1,\"1662\":1}}],[\"sdk\",{\"1\":{\"45\":1}}],[\"s2tpreprocessor\",{\"0\":{\"2194\":1},\"1\":{\"2194\":2}}],[\"s2ttask\",{\"0\":{\"2110\":1},\"1\":{\"2110\":2}}],[\"s2t\",{\"0\":{\"408\":1,\"416\":1,\"1552\":1,\"1975\":1,\"2101\":1,\"2110\":1,\"2699\":1},\"1\":{\"307\":2,\"343\":2,\"350\":2,\"357\":2,\"397\":2,\"398\":12,\"408\":9,\"416\":9,\"443\":2,\"1551\":1,\"1552\":3,\"1553\":1,\"1975\":3,\"2101\":2,\"2110\":2}}],[\"s2sttask\",{\"0\":{\"2109\":1},\"1\":{\"2109\":2}}],[\"s2sttacotron2loss\",{\"0\":{\"2000\":1},\"1\":{\"2000\":1}}],[\"s2stguidedattentionloss\",{\"0\":{\"1999\":1},\"1\":{\"1999\":1}}],[\"s2stctcloss\",{\"0\":{\"1997\":1},\"1\":{\"1997\":2}}],[\"s2stattentionloss\",{\"0\":{\"1996\":1},\"1\":{\"1996\":1}}],[\"s2st\",{\"0\":{\"399\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2109\":1,\"2516\":1,\"2518\":1,\"2700\":1},\"1\":{\"140\":1,\"161\":1,\"244\":1,\"397\":1,\"398\":6,\"399\":3,\"1976\":2,\"1978\":3,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":3,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":2,\"1994\":1,\"1996\":2,\"1997\":3,\"1999\":2,\"2000\":2,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2109\":2,\"2450\":1,\"2452\":1,\"2517\":1,\"2519\":3,\"2520\":7,\"2521\":1,\"2522\":1,\"2523\":1}}],[\"snt\",{\"1\":{\"2444\":1,\"2445\":1,\"2446\":1}}],[\"sne\",{\"1\":{\"2415\":2,\"2416\":1}}],[\"sndio\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"snyder\",{\"1\":{\"2068\":1,\"2070\":1}}],[\"snr=0\",{\"1\":{\"1646\":1}}],[\"snrloss\",{\"0\":{\"1641\":1},\"1\":{\"1641\":1}}],[\"snr\",{\"1\":{\"1451\":1,\"1454\":1,\"1514\":1,\"1585\":1,\"1640\":3,\"1646\":2,\"2371\":4,\"2612\":4,\"2630\":5}}],[\"snippet\",{\"1\":{\"170\":1}}],[\"snapshots\",{\"1\":{\"499\":3,\"652\":1,\"2431\":1}}],[\"snapshot\",{\"0\":{\"651\":1,\"652\":1,\"656\":1},\"1\":{\"28\":2,\"240\":4,\"241\":3,\"499\":1,\"634\":1,\"651\":4,\"652\":4,\"653\":1,\"654\":4,\"656\":4}}],[\"sst\",{\"0\":{\"2460\":1},\"1\":{\"2452\":1,\"2460\":1}}],[\"ssim\",{\"1\":{\"2259\":4}}],[\"ssimloss\",{\"0\":{\"2259\":1},\"1\":{\"2259\":3}}],[\"sskernelnplr\",{\"0\":{\"1248\":1},\"1\":{\"1248\":2}}],[\"sskerneldiag\",{\"0\":{\"1247\":1},\"1\":{\"1247\":2}}],[\"sskernel\",{\"0\":{\"1245\":1},\"1\":{\"1243\":1,\"1245\":4,\"1248\":1}}],[\"ss\",{\"1\":{\"1242\":1,\"2494\":3}}],[\"ssm=1\",{\"1\":{\"1245\":1}}],[\"ssm=none\",{\"1\":{\"1245\":2}}],[\"ssms\",{\"1\":{\"1245\":2}}],[\"ssm\",{\"0\":{\"1351\":1},\"1\":{\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1241\":1,\"1243\":1,\"1245\":5,\"1246\":1,\"1248\":3,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1297\":1,\"1351\":5}}],[\"ssh\",{\"1\":{\"142\":2,\"143\":1}}],[\"ssls\",{\"1\":{\"2001\":1,\"2004\":1}}],[\"sslrs\",{\"1\":{\"100\":2,\"2552\":1}}],[\"sslr\",{\"1\":{\"100\":1,\"102\":1}}],[\"ssl\",{\"0\":{\"2431\":2,\"2433\":1,\"2439\":1,\"2440\":1},\"1\":{\"70\":1,\"140\":1,\"161\":1,\"244\":1,\"397\":1,\"398\":2,\"1791\":1,\"1805\":9,\"2050\":1,\"2429\":10,\"2431\":7,\"2432\":10,\"2441\":1}}],[\"smi\",{\"1\":{\"148\":1,\"2401\":1,\"2426\":1,\"2537\":1,\"2549\":1}}],[\"smish\",{\"0\":{\"1082\":1},\"1\":{\"115\":4,\"1082\":6,\"1096\":6}}],[\"smp\",{\"1\":{\"144\":1}}],[\"smoothness\",{\"0\":{\"2305\":1},\"1\":{\"2305\":2}}],[\"smoother\",{\"1\":{\"1917\":1}}],[\"smoothed\",{\"1\":{\"113\":1}}],[\"smooth\",{\"1\":{\"113\":2}}],[\"smoothing\",{\"0\":{\"768\":1,\"896\":1},\"1\":{\"22\":4,\"118\":4,\"768\":5,\"825\":4,\"896\":2,\"1057\":4,\"1996\":2,\"2301\":2,\"2440\":1,\"2558\":1}}],[\"smallest\",{\"1\":{\"1618\":1,\"1619\":1}}],[\"smaller\",{\"1\":{\"1464\":1,\"1670\":1,\"1671\":1}}],[\"small\",{\"1\":{\"98\":1,\"235\":1,\"238\":1,\"1214\":1,\"1215\":1,\"1284\":1,\"1639\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"2372\":1,\"2385\":1,\"2387\":1,\"2429\":1,\"2554\":1,\"2558\":1,\"2564\":1,\"2584\":1}}],[\"s3prlpostfrontend\",{\"0\":{\"1791\":1},\"1\":{\"1791\":2}}],[\"s3prlfrontend\",{\"0\":{\"1239\":1},\"1\":{\"1239\":2,\"1791\":1}}],[\"s3prl\",{\"0\":{\"1239\":1,\"1791\":1},\"1\":{\"84\":1,\"101\":2,\"102\":2,\"1239\":2,\"1791\":1,\"2429\":3,\"2431\":5,\"2432\":8,\"2441\":1,\"2552\":4}}],[\"sr=fs\",{\"1\":{\"2456\":1,\"2460\":1,\"2521\":2,\"2522\":1,\"2523\":1}}],[\"sr=rate\",{\"1\":{\"2359\":1,\"2360\":1,\"2458\":1,\"2522\":1,\"2523\":1,\"2580\":1,\"2581\":1,\"2582\":1}}],[\"sr=16000\",{\"1\":{\"1526\":1,\"1560\":1,\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1,\"2600\":2}}],[\"sr\",{\"1\":{\"194\":1,\"1558\":2,\"2359\":1,\"2367\":1,\"2369\":1,\"2372\":1,\"2456\":1,\"2460\":1,\"2470\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2487\":1,\"2491\":1,\"2497\":1,\"2498\":4,\"2521\":1,\"2580\":1,\"2600\":2,\"2604\":1,\"2606\":1,\"2607\":2,\"2610\":1,\"2614\":1,\"2615\":1,\"2616\":6,\"2621\":1,\"2623\":1,\"2624\":2,\"2627\":1,\"2632\":1,\"2633\":1,\"2634\":6,\"2647\":1}}],[\"srcelement\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"srcs=2\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"srcs\",{\"1\":{\"554\":3,\"1660\":2,\"1661\":2,\"1662\":2}}],[\"srcspk\",{\"1\":{\"269\":2}}],[\"srcdir\",{\"1\":{\"287\":1}}],[\"src\",{\"1\":{\"66\":1,\"134\":1,\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"255\":2,\"443\":13,\"554\":2,\"560\":2,\"1167\":1,\"1168\":1,\"1172\":10,\"1196\":1,\"1197\":1,\"1204\":1,\"1271\":1,\"1273\":1,\"1965\":1,\"1970\":12,\"1984\":9,\"1985\":2,\"2001\":1,\"2004\":1,\"2076\":8,\"2152\":2,\"2440\":1,\"2456\":1,\"2460\":1,\"2520\":1,\"2521\":1,\"2558\":1,\"2584\":4}}],[\"srun\",{\"1\":{\"40\":1,\"41\":1}}],[\"s16le\",{\"1\":{\"49\":1,\"52\":2}}],[\"sacrebleu\",{\"1\":{\"2450\":1,\"2456\":1,\"2457\":1,\"2460\":1,\"2517\":1,\"2521\":1}}],[\"sat\",{\"1\":{\"2442\":1}}],[\"satisfy\",{\"1\":{\"1253\":1,\"2099\":1}}],[\"saijo\",{\"1\":{\"1670\":1,\"1671\":1}}],[\"safedumper\",{\"1\":{\"2315\":1}}],[\"safe\",{\"0\":{\"1824\":1,\"2315\":1,\"2342\":2},\"1\":{\"1332\":1,\"1824\":2,\"2315\":1,\"2342\":3}}],[\"sa\",{\"1\":{\"1203\":1,\"1371\":3}}],[\"sabsample\",{\"1\":{\"808\":1}}],[\"says\",{\"1\":{\"201\":1,\"202\":1}}],[\"sanjeev\",{\"1\":{\"130\":1}}],[\"saeki\",{\"1\":{\"130\":1}}],[\"savefig\",{\"0\":{\"916\":1},\"1\":{\"916\":1}}],[\"savefig>\",{\"1\":{\"909\":1}}],[\"savefn\",{\"1\":{\"909\":1}}],[\"savefn=<function\",{\"1\":{\"909\":1}}],[\"savefun=<function\",{\"1\":{\"656\":1}}],[\"save>\",{\"1\":{\"656\":1}}],[\"save\",{\"0\":{\"655\":1},\"1\":{\"98\":1,\"174\":1,\"179\":1,\"251\":6,\"255\":4,\"259\":6,\"263\":4,\"265\":4,\"267\":4,\"269\":4,\"272\":1,\"277\":1,\"429\":2,\"619\":1,\"629\":1,\"655\":3,\"909\":2,\"1187\":2,\"1202\":2,\"1245\":1,\"1407\":1,\"1927\":3,\"2186\":1,\"2193\":1,\"2202\":2,\"2204\":1,\"2393\":1,\"2430\":1,\"2440\":4,\"2529\":1,\"2555\":1,\"2558\":4,\"2599\":1}}],[\"saved\",{\"1\":{\"65\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"655\":1,\"1187\":2,\"1202\":2,\"1286\":1,\"1287\":1,\"2099\":2}}],[\"saving\",{\"1\":{\"69\":1,\"989\":1,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1877\":1}}],[\"samp\",{\"1\":{\"1797\":1}}],[\"sampled\",{\"1\":{\"1905\":1}}],[\"samplers\",{\"0\":{\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2701\":1},\"1\":{\"2005\":2,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2209\":1}}],[\"sampler\",{\"0\":{\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":2,\"2013\":1},\"1\":{\"1646\":4,\"1647\":2,\"1895\":1,\"2005\":3,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":2,\"2013\":1,\"2209\":3}}],[\"samplerate\",{\"1\":{\"109\":1,\"1558\":1}}],[\"samples\",{\"0\":{\"2359\":1},\"1\":{\"60\":1,\"79\":1,\"109\":1,\"760\":1,\"835\":5,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1,\"1071\":9,\"1113\":1,\"1154\":1,\"1171\":1,\"1206\":1,\"1216\":1,\"1301\":1,\"1304\":1,\"1371\":1,\"1390\":1,\"1466\":2,\"1551\":8,\"1552\":1,\"1553\":4,\"1554\":8,\"1558\":1,\"1639\":2,\"1640\":2,\"1660\":3,\"1661\":3,\"1662\":3,\"1719\":3,\"1804\":1,\"1895\":1,\"1896\":2,\"1897\":1,\"1900\":1,\"1918\":1,\"1925\":2,\"1953\":1,\"1955\":1,\"2010\":1,\"2040\":2,\"2046\":1,\"2099\":1,\"2102\":1,\"2301\":2,\"2303\":1,\"2359\":1,\"2365\":1,\"2367\":1,\"2485\":1,\"2498\":4,\"2508\":1,\"2510\":1,\"2515\":1,\"2521\":2,\"2580\":1,\"2604\":1,\"2616\":4,\"2621\":1,\"2634\":4,\"2644\":2,\"2655\":1,\"2660\":1}}],[\"sample\",{\"0\":{\"1950\":1,\"2470\":1},\"1\":{\"58\":11,\"60\":5,\"75\":7,\"79\":1,\"88\":1,\"109\":2,\"240\":13,\"241\":4,\"242\":3,\"648\":1,\"794\":3,\"821\":1,\"822\":1,\"826\":1,\"1115\":2,\"1154\":1,\"1181\":3,\"1198\":1,\"1392\":2,\"1400\":1,\"1462\":1,\"1463\":1,\"1511\":1,\"1551\":1,\"1553\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1644\":1,\"1763\":1,\"1776\":2,\"1785\":5,\"1786\":1,\"1791\":1,\"1797\":2,\"1799\":1,\"1801\":1,\"1804\":1,\"1810\":3,\"1904\":1,\"1916\":1,\"1917\":1,\"1921\":2,\"1922\":2,\"1923\":4,\"1925\":2,\"1926\":2,\"1927\":2,\"1928\":2,\"1929\":2,\"1935\":2,\"1936\":2,\"1938\":2,\"1939\":2,\"1941\":2,\"1942\":1,\"1943\":2,\"1945\":1,\"1946\":2,\"1947\":2,\"1950\":1,\"2040\":2,\"2076\":1,\"2095\":1,\"2184\":1,\"2188\":1,\"2197\":2,\"2200\":1,\"2263\":1,\"2264\":1,\"2267\":1,\"2302\":6,\"2303\":2,\"2305\":2,\"2468\":1,\"2600\":3}}],[\"sampling\",{\"0\":{\"1451\":1,\"1508\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"1688\":1,\"1693\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1755\":1,\"1756\":1},\"1\":{\"47\":1,\"48\":2,\"49\":1,\"275\":1,\"282\":1,\"297\":1,\"747\":1,\"778\":1,\"835\":2,\"1071\":3,\"1220\":1,\"1247\":1,\"1248\":1,\"1255\":1,\"1392\":1,\"1451\":1,\"1462\":2,\"1463\":1,\"1464\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1514\":1,\"1551\":1,\"1553\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1623\":1,\"1637\":1,\"1638\":4,\"1643\":2,\"1644\":2,\"1646\":1,\"1662\":1,\"1688\":1,\"1693\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1755\":1,\"1756\":1,\"1776\":1,\"1778\":3,\"1797\":1,\"1805\":3,\"1812\":1,\"1813\":1,\"1817\":1,\"1822\":1,\"1850\":3,\"1852\":3,\"1859\":1,\"1877\":3,\"1905\":2,\"1912\":1,\"1921\":1,\"1922\":1,\"1923\":2,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1947\":1,\"2197\":1,\"2267\":2,\"2317\":1,\"2330\":1,\"2387\":1,\"2430\":1,\"2468\":1,\"2501\":1,\"2514\":1,\"2522\":1,\"2555\":1,\"2581\":1,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1,\"2659\":1}}],[\"samuele\",{\"1\":{\"130\":1}}],[\"samepad\",{\"0\":{\"1249\":1,\"2297\":1},\"1\":{\"1249\":1,\"2297\":1}}],[\"same\",{\"0\":{\"2429\":1},\"1\":{\"1\":1,\"21\":1,\"28\":2,\"30\":1,\"36\":1,\"46\":1,\"49\":1,\"57\":1,\"62\":1,\"76\":1,\"80\":1,\"85\":2,\"115\":1,\"119\":2,\"135\":1,\"235\":2,\"237\":1,\"238\":1,\"745\":1,\"746\":1,\"764\":1,\"778\":1,\"797\":1,\"899\":1,\"901\":1,\"912\":1,\"926\":1,\"1008\":1,\"1011\":2,\"1028\":1,\"1048\":1,\"1115\":2,\"1132\":1,\"1145\":1,\"1186\":2,\"1210\":1,\"1243\":1,\"1253\":1,\"1369\":1,\"1375\":1,\"1379\":1,\"1392\":4,\"1473\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1576\":1,\"1577\":1,\"1616\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1688\":2,\"1692\":1,\"1719\":2,\"1731\":1,\"1732\":1,\"1756\":2,\"1905\":1,\"1912\":1,\"1917\":1,\"1963\":1,\"2021\":1,\"2099\":1,\"2102\":1,\"2279\":1,\"2385\":2,\"2387\":4,\"2393\":1,\"2394\":1,\"2395\":1,\"2427\":1,\"2430\":1,\"2459\":2,\"2473\":1,\"2529\":1,\"2530\":1,\"2531\":1,\"2543\":1,\"2550\":1,\"2555\":1,\"2564\":1,\"2584\":1,\"2600\":3}}],[\"saon\",{\"1\":{\"23\":2,\"119\":2}}],[\"sythesis\",{\"1\":{\"2196\":1}}],[\"syllabify\",{\"1\":{\"2125\":1}}],[\"syllablescorefeats\",{\"0\":{\"2089\":1},\"1\":{\"2089\":2}}],[\"syllable\",{\"1\":{\"1773\":4,\"2082\":4,\"2090\":2,\"2130\":1,\"2131\":1}}],[\"syl=false\",{\"1\":{\"2122\":1}}],[\"syb\",{\"1\":{\"1773\":12,\"1778\":2,\"1805\":2,\"2082\":14,\"2087\":3,\"2090\":3,\"2095\":3}}],[\"sys\",{\"1\":{\"217\":2,\"224\":2,\"231\":2,\"2368\":1,\"2371\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2568\":5,\"2591\":1,\"2592\":4,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1}}],[\"systems\",{\"1\":{\"141\":2,\"168\":1,\"179\":1,\"2385\":2,\"2447\":1,\"2467\":6,\"2468\":3,\"2473\":2,\"2480\":1,\"2502\":1}}],[\"system\",{\"0\":{\"55\":1,\"57\":1,\"141\":1},\"1\":{\"38\":1,\"47\":1,\"56\":1,\"57\":2,\"59\":2,\"63\":1,\"74\":1,\"85\":2,\"93\":1,\"94\":1,\"130\":1,\"135\":2,\"144\":2,\"235\":1,\"762\":1,\"940\":1,\"1248\":1,\"1639\":1,\"1654\":1,\"1961\":1,\"2090\":1,\"2168\":1,\"2236\":1,\"2360\":1,\"2410\":1,\"2458\":1,\"2467\":2,\"2522\":1,\"2523\":1,\"2581\":1,\"2582\":1}}],[\"sync=true\",{\"1\":{\"2460\":1}}],[\"sync=false\",{\"1\":{\"692\":1}}],[\"sync\",{\"1\":{\"307\":2,\"449\":2,\"692\":2,\"700\":2,\"1048\":2,\"1138\":2,\"1139\":2,\"1143\":1,\"2461\":1}}],[\"synchronous\",{\"1\":{\"23\":3,\"103\":1,\"119\":4,\"692\":1,\"693\":1,\"698\":1,\"699\":1,\"700\":2,\"1048\":2,\"1138\":2,\"1139\":2,\"2452\":1,\"2586\":1}}],[\"synth\",{\"0\":{\"295\":1,\"1817\":1},\"1\":{\"198\":4,\"202\":1,\"205\":1,\"295\":5,\"1817\":1}}],[\"synthesizer\",{\"0\":{\"1980\":2,\"1983\":1,\"1986\":1,\"1994\":1,\"2001\":2,\"2002\":1,\"2003\":1,\"2004\":2},\"1\":{\"1980\":2,\"1983\":1,\"1984\":1,\"1986\":1,\"1994\":1,\"2001\":3,\"2002\":2,\"2003\":3,\"2004\":3}}],[\"synthesized\",{\"1\":{\"198\":1,\"2515\":1,\"2518\":1}}],[\"synthesize\",{\"0\":{\"198\":1},\"1\":{\"195\":1,\"198\":1,\"217\":1,\"224\":1,\"231\":1,\"263\":1,\"1803\":1}}],[\"synthesis`\",{\"1\":{\"2078\":1,\"2081\":1,\"2083\":2,\"2095\":2}}],[\"synthesis\",{\"0\":{\"202\":1,\"218\":1,\"225\":1,\"232\":1,\"242\":1,\"2361\":1,\"2365\":1,\"2508\":1,\"2515\":1,\"2655\":1,\"2660\":1},\"1\":{\"130\":1,\"235\":2,\"681\":1,\"682\":1,\"701\":1,\"758\":1,\"802\":1,\"803\":1,\"804\":1,\"826\":1,\"1773\":1,\"1860\":2,\"2082\":1,\"2086\":1,\"2087\":1,\"2090\":2,\"2095\":1,\"2103\":1,\"2236\":1,\"2257\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2365\":1,\"2415\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2543\":1,\"2655\":1,\"2660\":1}}],[\"syntax\",{\"1\":{\"45\":1,\"144\":1,\"665\":1,\"1012\":1,\"2584\":2}}],[\"symlink\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"syms\",{\"1\":{\"579\":2,\"1955\":1}}],[\"symbol=\",{\"1\":{\"2122\":1,\"2126\":1}}],[\"symbolically\",{\"1\":{\"2432\":1}}],[\"symbolic\",{\"1\":{\"610\":1,\"2639\":1}}],[\"symbols\",{\"1\":{\"461\":4,\"929\":1,\"1210\":1,\"1337\":1,\"1429\":1,\"2120\":3,\"2130\":2,\"2136\":2,\"2137\":2,\"2142\":2,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2349\":3,\"2373\":3,\"2555\":3}}],[\"symbol\",{\"1\":{\"23\":1,\"113\":1,\"119\":1,\"375\":2,\"461\":6,\"491\":2,\"700\":1,\"725\":1,\"726\":1,\"750\":2,\"806\":1,\"825\":2,\"858\":1,\"861\":1,\"863\":1,\"865\":1,\"908\":1,\"1048\":1,\"1057\":3,\"1059\":3,\"1066\":1,\"1073\":1,\"1075\":1,\"1083\":1,\"1099\":2,\"1138\":1,\"1139\":1,\"1173\":2,\"1210\":1,\"1270\":1,\"1841\":1,\"2120\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2135\":1,\"2137\":2,\"2142\":1,\"2178\":3,\"2179\":3,\"2191\":2,\"2194\":6,\"2195\":2,\"2196\":2}}],[\"sym\",{\"1\":{\"23\":2,\"119\":1,\"249\":2,\"251\":4,\"255\":4,\"259\":4,\"408\":4,\"416\":4,\"700\":2,\"750\":4,\"1048\":2,\"1057\":4,\"1059\":4,\"1138\":2,\"1139\":2,\"1171\":4,\"1172\":2,\"1173\":4,\"1206\":3,\"1892\":2,\"1955\":1,\"1970\":2,\"1975\":6,\"1984\":2,\"2027\":2,\"2076\":6}}],[\"symmetry\",{\"1\":{\"1248\":1}}],[\"symmetric\",{\"1\":{\"22\":4,\"933\":1,\"1695\":2,\"1752\":1}}],[\"symm\",{\"1\":{\"22\":2,\"825\":5,\"933\":2}}],[\"slc388\",{\"1\":{\"2387\":1}}],[\"sleep\",{\"1\":{\"2360\":2,\"2458\":2,\"2523\":2,\"2582\":2}}],[\"slelected\",{\"1\":{\"1429\":1}}],[\"slaney\",{\"1\":{\"778\":1,\"1810\":1,\"1912\":1}}],[\"slicing\",{\"1\":{\"2302\":1}}],[\"sliced\",{\"1\":{\"1142\":1,\"1186\":1,\"1210\":2}}],[\"slightly\",{\"1\":{\"1662\":1,\"2441\":1}}],[\"slight\",{\"1\":{\"1466\":1}}],[\"slience\",{\"1\":{\"585\":1}}],[\"slidingwindow\",{\"0\":{\"1255\":1},\"1\":{\"1198\":1,\"1255\":1}}],[\"sliding\",{\"1\":{\"21\":2,\"115\":1,\"1132\":1,\"1198\":2,\"1203\":3,\"1255\":3,\"1971\":1}}],[\"slot\",{\"0\":{\"2475\":1},\"1\":{\"2476\":2}}],[\"slow\",{\"1\":{\"950\":1,\"952\":1,\"956\":1,\"968\":1,\"973\":1,\"1245\":1,\"1274\":1,\"1941\":1}}],[\"slower\",{\"1\":{\"217\":1}}],[\"slope\",{\"1\":{\"115\":2,\"1096\":3,\"1765\":1,\"1778\":3,\"1800\":1,\"1801\":2,\"1803\":1,\"1805\":2,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":1,\"1849\":1,\"1850\":3,\"1851\":1,\"1852\":3,\"1856\":1,\"1857\":1,\"1858\":1,\"1861\":1,\"1866\":1,\"1867\":1,\"1870\":1,\"1871\":1,\"1877\":2}}],[\"slu1\",{\"1\":{\"2646\":1}}],[\"slupreprocessor\",{\"0\":{\"2195\":1},\"1\":{\"2195\":2}}],[\"slutask\",{\"0\":{\"2111\":1},\"1\":{\"2111\":2,\"2474\":1,\"2649\":1}}],[\"slurp\",{\"1\":{\"2472\":6,\"2474\":7,\"2476\":8,\"2648\":6,\"2649\":7}}],[\"slur=true\",{\"1\":{\"1798\":1}}],[\"slur\",{\"1\":{\"1773\":11,\"1778\":6,\"1798\":3,\"1804\":2,\"1805\":6,\"2082\":17,\"2086\":9,\"2087\":9,\"2090\":9,\"2095\":9}}],[\"slurm\",{\"0\":{\"2220\":1,\"2221\":1},\"1\":{\"19\":1,\"40\":1,\"41\":1,\"141\":1,\"142\":3,\"143\":1,\"235\":1,\"429\":1,\"2220\":1,\"2221\":1}}],[\"slu\",{\"0\":{\"157\":1,\"422\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2111\":1,\"2468\":1,\"2469\":1,\"2472\":1,\"2473\":1,\"2475\":1,\"2477\":1,\"2646\":1,\"2703\":1,\"2731\":1},\"1\":{\"130\":1,\"140\":2,\"157\":1,\"161\":1,\"244\":2,\"422\":6,\"2024\":1,\"2026\":1,\"2027\":2,\"2028\":1,\"2029\":1,\"2111\":2,\"2467\":11,\"2468\":8,\"2471\":1,\"2472\":1,\"2473\":4,\"2474\":9,\"2476\":1,\"2478\":1,\"2479\":1,\"2646\":1,\"2648\":1,\"2649\":8}}],[\"slt\",{\"1\":{\"130\":1,\"2064\":1}}],[\"s\",{\"0\":{\"152\":1},\"1\":{\"18\":1,\"45\":1,\"48\":1,\"49\":1,\"60\":1,\"74\":1,\"80\":1,\"84\":1,\"85\":2,\"87\":1,\"98\":2,\"114\":1,\"116\":1,\"134\":1,\"135\":1,\"142\":1,\"143\":1,\"144\":2,\"150\":1,\"161\":1,\"169\":1,\"170\":2,\"174\":2,\"181\":1,\"186\":1,\"192\":1,\"194\":2,\"196\":1,\"197\":1,\"198\":2,\"201\":1,\"202\":2,\"203\":1,\"217\":2,\"234\":1,\"235\":1,\"237\":1,\"240\":3,\"570\":1,\"615\":1,\"629\":1,\"637\":1,\"639\":1,\"642\":1,\"654\":1,\"658\":1,\"700\":1,\"711\":1,\"731\":2,\"732\":2,\"745\":3,\"746\":3,\"749\":1,\"754\":2,\"784\":2,\"826\":2,\"880\":1,\"885\":2,\"897\":1,\"898\":3,\"905\":1,\"952\":1,\"1007\":1,\"1025\":1,\"1057\":3,\"1064\":3,\"1071\":2,\"1106\":1,\"1132\":1,\"1133\":1,\"1138\":1,\"1139\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1203\":1,\"1214\":1,\"1215\":1,\"1241\":1,\"1248\":4,\"1257\":1,\"1272\":1,\"1284\":1,\"1297\":1,\"1452\":1,\"1478\":1,\"1505\":1,\"1598\":1,\"1604\":2,\"1645\":1,\"1655\":2,\"1660\":6,\"1661\":6,\"1662\":6,\"1669\":1,\"1670\":1,\"1671\":1,\"1706\":2,\"1708\":2,\"1719\":2,\"1735\":1,\"1761\":1,\"1763\":1,\"1768\":1,\"1798\":2,\"1805\":1,\"1839\":1,\"1848\":1,\"1851\":2,\"1874\":2,\"1921\":1,\"1922\":1,\"1927\":1,\"1932\":1,\"1936\":1,\"2001\":1,\"2004\":1,\"2018\":1,\"2029\":1,\"2064\":1,\"2086\":1,\"2087\":1,\"2090\":2,\"2142\":1,\"2193\":1,\"2243\":2,\"2244\":2,\"2255\":2,\"2264\":2,\"2274\":2,\"2279\":2,\"2360\":2,\"2372\":1,\"2373\":2,\"2375\":1,\"2377\":1,\"2385\":2,\"2387\":1,\"2392\":1,\"2411\":1,\"2414\":1,\"2417\":1,\"2418\":1,\"2425\":1,\"2426\":1,\"2429\":2,\"2430\":1,\"2432\":1,\"2435\":1,\"2436\":1,\"2444\":1,\"2445\":1,\"2446\":1,\"2458\":2,\"2467\":1,\"2498\":1,\"2499\":1,\"2500\":3,\"2523\":2,\"2528\":1,\"2548\":1,\"2549\":1,\"2553\":1,\"2554\":2,\"2555\":2,\"2559\":1,\"2561\":1,\"2562\":1,\"2568\":2,\"2569\":1,\"2582\":2,\"2600\":3,\"2616\":1,\"2617\":4,\"2634\":1,\"2635\":4}}],[\"sites\",{\"1\":{\"2363\":4,\"2506\":4,\"2653\":4}}],[\"site\",{\"1\":{\"2155\":1,\"2363\":4,\"2431\":1,\"2506\":4,\"2653\":4}}],[\"situations\",{\"1\":{\"638\":1}}],[\"situation\",{\"1\":{\"49\":1,\"2572\":1,\"2585\":1}}],[\"sisnrloss\",{\"0\":{\"1640\":1},\"1\":{\"1640\":1}}],[\"sil\",{\"1\":{\"2294\":1}}],[\"silu\",{\"1\":{\"1517\":1,\"1520\":1}}],[\"silently\",{\"1\":{\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1}}],[\"silence\",{\"0\":{\"297\":1,\"585\":1,\"2210\":1},\"1\":{\"297\":5,\"585\":4,\"1409\":1,\"2194\":1,\"2210\":2}}],[\"si\",{\"1\":{\"1454\":1,\"1640\":6,\"1660\":1,\"1661\":1,\"1662\":1,\"2371\":4,\"2612\":4,\"2630\":5}}],[\"sid2spk\",{\"1\":{\"2514\":3,\"2659\":3}}],[\"siddhu001\",{\"1\":{\"2504\":1}}],[\"siddhana\",{\"1\":{\"2421\":1,\"2463\":1,\"2472\":1,\"2476\":1,\"2480\":1,\"2502\":1,\"2648\":1}}],[\"siddhant\",{\"1\":{\"130\":2,\"2421\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2646\":1}}],[\"siddharth\",{\"1\":{\"130\":1}}],[\"sid1\",{\"1\":{\"2411\":1}}],[\"sid\",{\"1\":{\"1804\":2,\"1851\":2,\"1878\":2,\"2001\":2,\"2002\":2,\"2004\":2,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2279\":2}}],[\"sids=sids\",{\"1\":{\"2515\":1,\"2660\":1}}],[\"sids\",{\"1\":{\"1773\":6,\"1778\":4,\"1804\":5,\"1805\":4,\"1837\":4,\"1850\":2,\"1851\":5,\"1877\":4,\"1878\":5,\"1984\":1,\"1985\":1,\"2001\":3,\"2002\":5,\"2004\":3,\"2082\":6,\"2086\":5,\"2087\":5,\"2090\":5,\"2095\":5,\"2240\":6,\"2243\":5,\"2244\":5,\"2255\":5,\"2263\":5,\"2264\":5,\"2278\":6,\"2279\":5,\"2514\":4,\"2659\":4}}],[\"side\",{\"1\":{\"1478\":1,\"1577\":1,\"1752\":1,\"2301\":1}}],[\"sided\",{\"1\":{\"1241\":1}}],[\"sigpro\",{\"1\":{\"1715\":1}}],[\"sig\",{\"1\":{\"1526\":1}}],[\"sigmoid\",{\"1\":{\"1061\":1,\"1082\":1,\"1084\":1,\"1505\":1,\"1515\":1,\"1524\":1,\"1525\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1595\":1,\"1611\":1,\"1626\":2,\"1654\":1,\"1658\":1,\"1659\":2,\"1669\":1,\"1807\":1,\"1871\":1,\"1873\":1}}],[\"sigma=true\",{\"1\":{\"1605\":1}}],[\"sigma=0\",{\"1\":{\"762\":1,\"763\":1}}],[\"sigma\",{\"1\":{\"632\":5,\"762\":1,\"763\":2,\"821\":2,\"826\":2,\"1171\":1,\"1210\":2,\"1211\":2,\"1286\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1336\":1,\"1337\":2,\"1618\":13,\"1619\":2,\"1999\":1,\"2095\":3,\"2151\":5,\"2263\":3,\"2264\":2,\"2273\":3}}],[\"signature\",{\"1\":{\"1160\":2,\"1161\":2,\"1164\":2,\"1165\":2,\"1177\":2,\"1209\":2,\"1252\":2,\"1253\":2,\"1254\":2}}],[\"signals\",{\"1\":{\"1639\":2,\"1640\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1767\":2,\"1946\":1,\"2467\":1,\"2638\":1}}],[\"signal\",{\"0\":{\"1741\":1},\"1\":{\"130\":2,\"593\":3,\"778\":1,\"869\":2,\"877\":2,\"942\":1,\"1462\":3,\"1464\":3,\"1505\":1,\"1523\":1,\"1528\":1,\"1534\":1,\"1539\":1,\"1551\":3,\"1553\":3,\"1558\":1,\"1560\":5,\"1626\":1,\"1639\":2,\"1640\":2,\"1658\":1,\"1669\":1,\"1688\":1,\"1693\":1,\"1696\":1,\"1698\":1,\"1701\":1,\"1702\":2,\"1710\":4,\"1719\":1,\"1735\":6,\"1736\":1,\"1737\":1,\"1739\":2,\"1741\":5,\"1755\":1,\"1756\":1,\"1758\":1,\"1759\":1,\"1776\":1,\"1785\":1,\"1786\":1,\"1803\":1,\"1808\":4,\"1812\":1,\"1813\":1,\"1814\":1,\"1821\":1,\"1827\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1849\":1,\"1856\":1,\"1858\":1,\"1861\":1,\"1862\":2,\"1880\":1,\"1912\":1,\"1921\":3,\"1922\":3,\"1923\":1,\"1925\":3,\"1926\":1,\"1927\":2,\"1928\":3,\"1929\":3,\"1935\":2,\"1936\":3,\"1938\":3,\"1939\":3,\"1941\":3,\"1943\":2,\"1946\":3,\"1947\":4,\"2207\":1,\"2574\":1,\"2638\":1}}],[\"signed\",{\"1\":{\"875\":1,\"1927\":1}}],[\"significant\",{\"1\":{\"19\":1,\"1928\":1}}],[\"simuleval\",{\"1\":{\"2450\":2,\"2461\":2,\"2517\":2}}],[\"simultaneous\",{\"0\":{\"2460\":1},\"1\":{\"1696\":1,\"1698\":1,\"2451\":1,\"2452\":1}}],[\"simultaneously\",{\"1\":{\"22\":1,\"704\":1,\"705\":1}}],[\"simulation\",{\"1\":{\"1432\":1,\"1436\":2,\"1510\":1,\"1511\":2,\"1643\":1,\"1644\":2}}],[\"simulate\",{\"1\":{\"1233\":1,\"2523\":1,\"2600\":2}}],[\"simulates\",{\"1\":{\"692\":1,\"693\":1}}],[\"sim\",{\"0\":{\"693\":1},\"1\":{\"327\":2,\"449\":2,\"693\":1,\"1115\":2,\"1560\":1,\"2461\":1,\"2592\":6}}],[\"similarity\",{\"1\":{\"1560\":1,\"2259\":1}}],[\"similar\",{\"1\":{\"113\":1,\"115\":1,\"718\":1,\"1025\":1,\"1218\":1,\"2020\":1,\"2355\":1,\"2385\":1,\"2387\":2,\"2398\":1,\"2412\":1,\"2413\":1,\"2416\":1,\"2534\":1,\"2543\":1}}],[\"similarly\",{\"1\":{\"24\":1,\"1905\":1,\"2416\":1,\"2440\":1,\"2564\":1,\"2568\":1}}],[\"simply\",{\"1\":{\"1430\":1,\"1670\":1,\"1895\":1,\"1900\":1,\"2412\":1,\"2433\":1,\"2555\":1}}],[\"simplicity\",{\"1\":{\"173\":1,\"1214\":1,\"2395\":1,\"2531\":1}}],[\"simplified\",{\"1\":{\"115\":2,\"161\":1,\"1076\":3,\"1093\":3}}],[\"simplify\",{\"1\":{\"17\":1,\"99\":1}}],[\"simpleoier\",{\"1\":{\"2429\":2,\"2431\":4}}],[\"simplex\",{\"1\":{\"1529\":1,\"1568\":1}}],[\"simpler\",{\"1\":{\"1245\":1}}],[\"simplest\",{\"1\":{\"235\":1}}],[\"simple\",{\"1\":{\"113\":2,\"116\":1,\"170\":1,\"585\":1,\"1037\":1,\"1065\":1,\"1066\":1,\"1113\":1,\"1251\":1,\"1257\":1,\"1398\":1,\"2309\":1,\"2385\":1,\"2394\":1,\"2397\":1,\"2400\":1,\"2401\":1,\"2411\":1,\"2530\":1,\"2533\":1,\"2536\":1,\"2537\":1}}],[\"size10\",{\"1\":{\"2572\":4}}],[\"sizeddict\",{\"0\":{\"2316\":1},\"1\":{\"2316\":2}}],[\"sized\",{\"0\":{\"2316\":1,\"2324\":1},\"1\":{\"1003\":1,\"2005\":1,\"2316\":2,\"2324\":2}}],[\"size=7\",{\"1\":{\"1800\":1}}],[\"size=64\",{\"1\":{\"1670\":2}}],[\"size=\",{\"1\":{\"1478\":1,\"1480\":1,\"1522\":3,\"1545\":2}}],[\"size=none\",{\"1\":{\"1129\":1,\"1518\":1,\"2042\":1,\"2047\":1}}],[\"size=0\",{\"1\":{\"1028\":1,\"2358\":1,\"2371\":1,\"2494\":1,\"2520\":1,\"2579\":1,\"2612\":1,\"2630\":1}}],[\"size=20\",{\"1\":{\"1652\":1,\"1670\":1,\"2592\":1}}],[\"size=256\",{\"1\":{\"1573\":1,\"1605\":1,\"2028\":1}}],[\"size=2\",{\"1\":{\"835\":1,\"2371\":1,\"2494\":1,\"2612\":1,\"2630\":1}}],[\"size=24\",{\"1\":{\"25\":2,\"1560\":1}}],[\"size=192\",{\"1\":{\"2057\":1}}],[\"size=10\",{\"1\":{\"2358\":1,\"2455\":2,\"2460\":1,\"2520\":1,\"2579\":1}}],[\"size=1024\",{\"1\":{\"1572\":1}}],[\"size=100\",{\"1\":{\"1460\":1}}],[\"size=128\",{\"1\":{\"1430\":1,\"1670\":1}}],[\"size=1\",{\"1\":{\"749\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1,\"1687\":1,\"1689\":1}}],[\"size=16\",{\"1\":{\"692\":1}}],[\"size=320\",{\"1\":{\"1526\":1}}],[\"size=3\",{\"1\":{\"736\":1,\"1512\":1,\"1596\":1,\"1674\":1,\"2520\":1}}],[\"size=40\",{\"1\":{\"692\":1}}],[\"size`\",{\"1\":{\"121\":1}}],[\"sizes=\",{\"1\":{\"1800\":3,\"2042\":1}}],[\"sizes\",{\"1\":{\"102\":1,\"722\":2,\"1065\":1,\"1203\":2,\"1604\":2,\"1761\":1,\"1763\":1,\"1765\":6,\"1778\":4,\"1800\":4,\"1801\":2,\"1803\":6,\"1804\":11,\"1805\":5,\"1844\":6,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":3,\"1849\":3,\"1850\":4,\"1851\":6,\"1852\":4,\"1856\":5,\"1858\":4,\"1870\":4,\"1877\":4,\"1878\":4,\"2070\":1,\"2584\":1}}],[\"size24\",{\"1\":{\"25\":1}}],[\"size\",{\"0\":{\"37\":1,\"72\":1,\"95\":1,\"1324\":1,\"1359\":1,\"2219\":1,\"2324\":1,\"2326\":1},\"1\":{\"21\":8,\"23\":9,\"25\":1,\"26\":3,\"32\":3,\"33\":3,\"36\":2,\"37\":3,\"39\":3,\"56\":1,\"62\":2,\"72\":13,\"73\":8,\"74\":3,\"75\":2,\"76\":4,\"77\":2,\"78\":2,\"79\":2,\"80\":4,\"95\":3,\"98\":1,\"102\":5,\"104\":5,\"115\":29,\"116\":25,\"117\":2,\"119\":2,\"121\":3,\"148\":3,\"150\":6,\"172\":4,\"175\":1,\"194\":1,\"245\":4,\"249\":2,\"251\":4,\"255\":4,\"257\":2,\"259\":4,\"261\":2,\"265\":2,\"269\":2,\"275\":1,\"282\":1,\"301\":4,\"307\":4,\"315\":8,\"321\":2,\"327\":4,\"333\":4,\"339\":2,\"343\":6,\"350\":6,\"357\":2,\"368\":6,\"380\":2,\"384\":4,\"391\":4,\"399\":6,\"408\":4,\"416\":2,\"422\":4,\"429\":12,\"437\":2,\"443\":6,\"449\":4,\"455\":2,\"461\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":4,\"485\":8,\"606\":1,\"612\":1,\"613\":1,\"614\":1,\"679\":1,\"681\":3,\"682\":3,\"683\":1,\"684\":2,\"685\":1,\"688\":1,\"689\":3,\"691\":9,\"693\":18,\"697\":7,\"698\":3,\"699\":3,\"700\":3,\"701\":1,\"708\":1,\"711\":8,\"712\":3,\"713\":3,\"723\":3,\"725\":3,\"734\":1,\"737\":2,\"740\":3,\"741\":3,\"743\":5,\"749\":2,\"754\":2,\"758\":4,\"766\":8,\"771\":4,\"775\":3,\"776\":3,\"784\":1,\"785\":8,\"786\":3,\"792\":1,\"797\":6,\"802\":1,\"806\":3,\"807\":1,\"809\":4,\"817\":1,\"821\":4,\"823\":4,\"824\":3,\"826\":2,\"828\":1,\"835\":2,\"838\":5,\"857\":6,\"864\":1,\"890\":4,\"892\":2,\"917\":1,\"921\":5,\"997\":3,\"998\":3,\"1003\":2,\"1004\":2,\"1005\":7,\"1006\":2,\"1009\":1,\"1010\":3,\"1028\":6,\"1046\":3,\"1047\":1,\"1048\":4,\"1049\":6,\"1050\":3,\"1051\":3,\"1052\":7,\"1053\":8,\"1054\":6,\"1055\":6,\"1056\":6,\"1057\":3,\"1058\":3,\"1060\":1,\"1062\":13,\"1064\":12,\"1065\":15,\"1066\":21,\"1068\":3,\"1069\":8,\"1070\":8,\"1071\":1,\"1072\":1,\"1073\":12,\"1074\":16,\"1075\":18,\"1076\":12,\"1077\":3,\"1079\":10,\"1080\":1,\"1081\":17,\"1083\":9,\"1085\":4,\"1087\":3,\"1092\":3,\"1093\":2,\"1094\":3,\"1095\":4,\"1097\":11,\"1100\":3,\"1101\":9,\"1103\":5,\"1104\":5,\"1105\":5,\"1114\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1132\":5,\"1133\":6,\"1138\":3,\"1139\":3,\"1140\":5,\"1141\":7,\"1142\":3,\"1143\":1,\"1144\":3,\"1145\":2,\"1148\":11,\"1149\":15,\"1150\":15,\"1151\":2,\"1153\":2,\"1154\":3,\"1159\":1,\"1167\":2,\"1168\":2,\"1169\":5,\"1170\":8,\"1171\":3,\"1172\":2,\"1174\":1,\"1178\":5,\"1179\":3,\"1180\":5,\"1181\":5,\"1182\":4,\"1185\":1,\"1186\":3,\"1190\":3,\"1191\":3,\"1192\":3,\"1195\":4,\"1196\":2,\"1197\":2,\"1198\":6,\"1200\":5,\"1201\":4,\"1203\":11,\"1204\":2,\"1206\":3,\"1208\":1,\"1209\":6,\"1210\":3,\"1211\":2,\"1214\":3,\"1215\":1,\"1216\":1,\"1219\":1,\"1220\":3,\"1222\":6,\"1223\":1,\"1224\":2,\"1228\":1,\"1240\":1,\"1243\":1,\"1244\":4,\"1245\":3,\"1248\":2,\"1249\":1,\"1255\":1,\"1269\":4,\"1270\":7,\"1271\":2,\"1272\":8,\"1273\":3,\"1282\":6,\"1283\":1,\"1285\":1,\"1287\":2,\"1298\":3,\"1299\":3,\"1301\":3,\"1302\":3,\"1303\":3,\"1304\":3,\"1324\":1,\"1334\":2,\"1336\":2,\"1345\":1,\"1347\":1,\"1348\":2,\"1351\":1,\"1359\":1,\"1368\":3,\"1369\":1,\"1370\":1,\"1372\":3,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":2,\"1381\":3,\"1427\":2,\"1428\":1,\"1430\":3,\"1432\":1,\"1436\":1,\"1472\":3,\"1473\":1,\"1478\":2,\"1505\":6,\"1510\":2,\"1511\":2,\"1516\":3,\"1518\":1,\"1520\":1,\"1522\":6,\"1523\":9,\"1531\":8,\"1532\":8,\"1534\":3,\"1535\":8,\"1537\":8,\"1539\":3,\"1540\":1,\"1543\":1,\"1545\":4,\"1546\":1,\"1552\":2,\"1555\":1,\"1558\":3,\"1560\":3,\"1572\":4,\"1575\":3,\"1576\":5,\"1577\":6,\"1581\":4,\"1598\":2,\"1602\":5,\"1609\":2,\"1643\":1,\"1644\":1,\"1645\":8,\"1648\":5,\"1650\":5,\"1652\":10,\"1654\":3,\"1655\":2,\"1656\":1,\"1658\":1,\"1659\":1,\"1660\":3,\"1661\":3,\"1662\":3,\"1663\":1,\"1664\":2,\"1665\":2,\"1669\":3,\"1670\":11,\"1671\":11,\"1683\":3,\"1719\":4,\"1735\":4,\"1743\":1,\"1752\":2,\"1761\":1,\"1763\":1,\"1765\":3,\"1769\":1,\"1771\":6,\"1772\":3,\"1775\":1,\"1776\":6,\"1777\":6,\"1778\":10,\"1782\":1,\"1785\":1,\"1786\":1,\"1787\":7,\"1788\":6,\"1792\":1,\"1795\":1,\"1798\":7,\"1799\":3,\"1800\":2,\"1801\":1,\"1803\":3,\"1804\":21,\"1805\":8,\"1806\":1,\"1808\":3,\"1809\":1,\"1810\":3,\"1812\":1,\"1813\":1,\"1815\":1,\"1816\":1,\"1820\":2,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":3,\"1834\":4,\"1835\":3,\"1844\":3,\"1846\":1,\"1847\":1,\"1850\":13,\"1851\":33,\"1852\":15,\"1856\":2,\"1857\":6,\"1858\":1,\"1861\":2,\"1862\":6,\"1863\":3,\"1864\":3,\"1865\":3,\"1866\":3,\"1867\":3,\"1868\":3,\"1871\":3,\"1872\":3,\"1873\":3,\"1874\":7,\"1876\":3,\"1877\":9,\"1878\":25,\"1880\":5,\"1884\":4,\"1885\":4,\"1892\":1,\"1893\":1,\"1896\":1,\"1897\":2,\"1907\":1,\"1917\":3,\"1929\":1,\"1941\":1,\"1947\":1,\"1953\":3,\"1955\":3,\"1957\":3,\"1958\":1,\"1960\":3,\"1961\":1,\"1970\":2,\"1971\":4,\"1975\":1,\"1982\":1,\"1983\":3,\"1984\":3,\"1986\":5,\"1988\":1,\"1990\":1,\"1992\":1,\"1993\":3,\"1996\":1,\"2001\":3,\"2002\":2,\"2003\":2,\"2004\":4,\"2006\":2,\"2007\":2,\"2008\":1,\"2009\":1,\"2010\":2,\"2011\":3,\"2012\":5,\"2018\":2,\"2019\":2,\"2021\":1,\"2023\":2,\"2024\":1,\"2025\":1,\"2026\":5,\"2027\":1,\"2028\":2,\"2029\":8,\"2037\":1,\"2039\":1,\"2040\":2,\"2044\":3,\"2045\":1,\"2049\":7,\"2050\":2,\"2051\":1,\"2052\":2,\"2053\":1,\"2054\":11,\"2055\":4,\"2056\":1,\"2057\":1,\"2058\":1,\"2064\":4,\"2065\":1,\"2066\":2,\"2067\":1,\"2068\":3,\"2069\":1,\"2070\":4,\"2071\":1,\"2072\":2,\"2073\":1,\"2076\":2,\"2078\":1,\"2079\":2,\"2084\":1,\"2086\":2,\"2087\":5,\"2089\":1,\"2090\":7,\"2094\":1,\"2095\":7,\"2099\":1,\"2106\":4,\"2123\":1,\"2128\":1,\"2135\":1,\"2152\":1,\"2180\":2,\"2182\":1,\"2188\":1,\"2219\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2243\":15,\"2244\":27,\"2247\":1,\"2249\":1,\"2251\":1,\"2255\":26,\"2257\":4,\"2258\":2,\"2259\":3,\"2261\":4,\"2263\":7,\"2264\":9,\"2265\":3,\"2267\":3,\"2270\":1,\"2272\":1,\"2273\":3,\"2279\":23,\"2286\":1,\"2293\":1,\"2294\":1,\"2297\":1,\"2303\":3,\"2305\":3,\"2317\":1,\"2324\":3,\"2325\":1,\"2326\":1,\"2349\":3,\"2440\":3,\"2461\":2,\"2521\":1,\"2522\":1,\"2523\":1,\"2558\":2,\"2564\":1,\"2569\":1,\"2584\":3,\"2585\":1,\"2600\":10}}],[\"sinica\",{\"1\":{\"2618\":1}}],[\"sin\",{\"1\":{\"1797\":1}}],[\"sincnet\",{\"1\":{\"1917\":1}}],[\"sincconv\",{\"0\":{\"1917\":1},\"1\":{\"1917\":2}}],[\"sincstride\",{\"1\":{\"1132\":1}}],[\"sinc\",{\"0\":{\"1198\":1,\"1256\":1,\"1904\":1,\"1911\":1,\"1916\":1,\"1917\":1},\"1\":{\"1132\":11,\"1198\":11,\"1255\":1,\"1256\":2,\"1904\":2,\"1911\":2,\"1916\":2,\"1917\":12}}],[\"since\",{\"1\":{\"17\":1,\"235\":1,\"238\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1778\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1805\":1,\"1807\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1891\":1,\"1897\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2375\":1,\"2435\":1,\"2440\":1,\"2559\":1,\"2561\":1}}],[\"sinegen\",{\"0\":{\"1797\":1},\"1\":{\"1797\":3}}],[\"sine\",{\"0\":{\"1797\":1},\"1\":{\"1079\":1,\"1797\":8}}],[\"sinusoidal\",{\"1\":{\"1079\":1}}],[\"sinusoid\",{\"1\":{\"1079\":1}}],[\"sing\",{\"1\":{\"2196\":1}}],[\"singen\",{\"1\":{\"1797\":1}}],[\"singer\",{\"1\":{\"1788\":1}}],[\"singingscorewriter\",{\"0\":{\"1403\":1},\"1\":{\"1403\":1,\"1404\":1}}],[\"singingscorereader\",{\"0\":{\"1401\":1},\"1\":{\"1401\":1}}],[\"singing\",{\"0\":{\"2078\":1,\"2081\":1,\"2083\":1,\"2092\":1,\"2093\":1,\"2095\":3},\"1\":{\"130\":1,\"758\":2,\"1773\":16,\"1778\":9,\"1805\":9,\"2077\":2,\"2078\":4,\"2081\":4,\"2082\":16,\"2083\":5,\"2086\":2,\"2087\":2,\"2090\":3,\"2092\":1,\"2093\":1,\"2095\":9,\"2103\":1,\"2196\":3}}],[\"singlechannel\",{\"1\":{\"2368\":2,\"2486\":2,\"2605\":2,\"2622\":2}}],[\"singlernn\",{\"0\":{\"1650\":1},\"1\":{\"1650\":2}}],[\"single\",{\"0\":{\"34\":1,\"2367\":1,\"2369\":1,\"2484\":1,\"2487\":1,\"2505\":1,\"2604\":1,\"2606\":1,\"2621\":1,\"2623\":1,\"2652\":1},\"1\":{\"19\":2,\"32\":6,\"34\":1,\"47\":1,\"84\":1,\"94\":1,\"148\":1,\"211\":1,\"212\":1,\"229\":1,\"230\":1,\"243\":1,\"597\":1,\"698\":1,\"699\":1,\"728\":1,\"732\":1,\"748\":1,\"759\":1,\"760\":1,\"778\":1,\"785\":1,\"831\":1,\"892\":1,\"1069\":1,\"1130\":1,\"1144\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1351\":1,\"1371\":1,\"1430\":1,\"1476\":1,\"1515\":1,\"1528\":1,\"1543\":1,\"1598\":1,\"1603\":3,\"1611\":1,\"1650\":1,\"1656\":1,\"1710\":1,\"1739\":1,\"1857\":1,\"1905\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1923\":1,\"2044\":1,\"2046\":1,\"2052\":1,\"2068\":1,\"2084\":1,\"2089\":1,\"2131\":1,\"2184\":1,\"2200\":1,\"2369\":1,\"2387\":1,\"2440\":1,\"2481\":1,\"2485\":1,\"2487\":1,\"2508\":1,\"2518\":1,\"2558\":1,\"2559\":1,\"2569\":1,\"2571\":1,\"2606\":1,\"2618\":1,\"2623\":1}}],[\"sint32\",{\"1\":{\"48\":1}}],[\"sint16\",{\"1\":{\"46\":1,\"48\":2}}],[\"scenario\",{\"1\":{\"2472\":1,\"2474\":1,\"2476\":1,\"2543\":1,\"2648\":1,\"2649\":1}}],[\"scenarios\",{\"1\":{\"2384\":1,\"2451\":1,\"2468\":1}}],[\"sctk\",{\"1\":{\"2372\":2}}],[\"sctm\",{\"1\":{\"546\":1}}],[\"scope\",{\"1\":{\"1808\":11,\"2266\":2}}],[\"score2mel\",{\"1\":{\"1778\":4}}],[\"score2wav\",{\"0\":{\"1778\":1},\"1\":{\"1778\":1}}],[\"scoremodel\",{\"0\":{\"1646\":1},\"1\":{\"1646\":1}}],[\"scores\",{\"0\":{\"1427\":2,\"1428\":1},\"1\":{\"691\":7,\"693\":3,\"694\":3,\"696\":1,\"697\":20,\"698\":1,\"699\":1,\"700\":1,\"704\":1,\"707\":2,\"734\":2,\"751\":2,\"765\":3,\"773\":2,\"785\":2,\"797\":6,\"815\":1,\"817\":1,\"828\":2,\"857\":1,\"1048\":1,\"1076\":2,\"1133\":1,\"1137\":1,\"1138\":2,\"1139\":2,\"1176\":2,\"1182\":2,\"1190\":2,\"1209\":2,\"1214\":1,\"1221\":1,\"1244\":2,\"1273\":1,\"1298\":1,\"1299\":1,\"1302\":1,\"1303\":1,\"1427\":7,\"1428\":3,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"2001\":1,\"2310\":1,\"2341\":1}}],[\"scorers\",{\"0\":{\"706\":1,\"773\":1,\"788\":1,\"789\":1,\"790\":1,\"829\":1},\"1\":{\"676\":2,\"691\":14,\"692\":3,\"693\":3,\"697\":20,\"698\":1,\"699\":1,\"706\":2,\"773\":1,\"788\":1,\"789\":1,\"790\":1,\"796\":1,\"797\":5,\"815\":1,\"829\":2,\"857\":4,\"1219\":1}}],[\"scorerinterface\",{\"0\":{\"814\":1},\"1\":{\"650\":1,\"676\":2,\"691\":2,\"693\":2,\"696\":1,\"697\":2,\"698\":3,\"699\":3,\"767\":1,\"783\":1,\"795\":1,\"797\":1,\"814\":1,\"857\":2,\"1117\":1,\"1219\":1}}],[\"scorer\",{\"0\":{\"695\":1,\"696\":1,\"783\":1,\"795\":1,\"814\":1},\"1\":{\"249\":1,\"691\":2,\"693\":2,\"695\":3,\"696\":3,\"697\":2,\"698\":2,\"699\":2,\"705\":1,\"734\":3,\"773\":2,\"783\":1,\"795\":4,\"797\":2,\"814\":3,\"815\":2,\"817\":1,\"828\":2,\"857\":2,\"1133\":1,\"1190\":2,\"1214\":1,\"1221\":1,\"1244\":2,\"1273\":1,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"2001\":1}}],[\"score\",{\"0\":{\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"572\":1,\"704\":1,\"705\":1,\"1388\":1,\"1393\":1,\"1401\":1,\"1403\":1,\"1413\":1,\"1415\":1,\"1646\":1,\"2084\":1,\"2085\":1,\"2089\":1,\"2094\":1,\"2228\":1},\"1\":{\"23\":1,\"115\":2,\"119\":1,\"249\":2,\"290\":1,\"292\":1,\"301\":2,\"572\":2,\"691\":14,\"692\":5,\"693\":2,\"694\":3,\"695\":4,\"696\":2,\"697\":15,\"698\":5,\"699\":5,\"700\":4,\"704\":1,\"705\":1,\"706\":8,\"725\":2,\"731\":1,\"734\":7,\"750\":5,\"751\":1,\"765\":3,\"773\":4,\"785\":2,\"795\":2,\"796\":4,\"797\":6,\"798\":3,\"806\":2,\"815\":5,\"817\":2,\"824\":2,\"825\":2,\"828\":4,\"857\":2,\"1046\":2,\"1048\":4,\"1059\":4,\"1060\":4,\"1063\":3,\"1066\":2,\"1073\":2,\"1075\":2,\"1076\":9,\"1083\":2,\"1093\":3,\"1106\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1133\":6,\"1138\":6,\"1139\":6,\"1142\":1,\"1173\":4,\"1176\":1,\"1186\":2,\"1190\":5,\"1193\":2,\"1204\":1,\"1209\":2,\"1210\":2,\"1214\":5,\"1221\":2,\"1244\":5,\"1270\":2,\"1273\":5,\"1388\":2,\"1393\":2,\"1401\":2,\"1402\":6,\"1403\":2,\"1404\":7,\"1413\":2,\"1415\":2,\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1605\":1,\"1623\":1,\"1637\":1,\"1638\":3,\"1646\":1,\"1647\":1,\"1773\":1,\"1778\":9,\"1804\":6,\"1805\":9,\"1957\":4,\"1958\":2,\"1959\":2,\"1960\":4,\"2001\":4,\"2082\":1,\"2084\":1,\"2085\":1,\"2086\":10,\"2087\":13,\"2089\":1,\"2090\":13,\"2094\":1,\"2095\":12,\"2196\":1,\"2228\":2,\"2373\":1,\"2375\":1,\"2385\":1,\"2430\":1,\"2456\":2,\"2457\":2,\"2460\":2,\"2521\":1,\"2555\":1,\"2559\":1}}],[\"scoring\",{\"0\":{\"363\":1,\"2404\":1,\"2540\":1},\"1\":{\"16\":1,\"98\":2,\"245\":2,\"301\":2,\"363\":2,\"572\":1,\"795\":2,\"814\":1,\"1048\":1,\"2372\":3,\"2375\":1,\"2384\":1,\"2405\":1,\"2418\":1,\"2423\":1,\"2424\":1,\"2429\":1,\"2433\":1,\"2438\":1,\"2541\":1,\"2546\":1,\"2547\":1,\"2554\":1,\"2555\":1,\"2559\":1,\"2560\":1,\"2564\":1,\"2565\":1,\"2569\":1}}],[\"sc=none\",{\"1\":{\"1664\":1,\"1665\":1}}],[\"sc\",{\"1\":{\"1546\":1,\"1664\":1,\"1665\":1,\"2367\":1,\"2368\":4,\"2369\":3,\"2485\":1,\"2486\":4,\"2487\":3,\"2501\":1,\"2604\":1,\"2605\":4,\"2606\":3,\"2607\":1,\"2621\":1,\"2622\":4,\"2623\":3,\"2624\":1}}],[\"scan\",{\"1\":{\"1341\":1,\"1968\":1}}],[\"scaler\",{\"1\":{\"2185\":1,\"2201\":2,\"2203\":1}}],[\"scales=\",{\"1\":{\"1800\":2}}],[\"scales\",{\"1\":{\"1765\":3,\"1778\":4,\"1800\":3,\"1801\":5,\"1803\":3,\"1804\":6,\"1805\":4,\"1821\":1,\"1834\":4,\"1844\":3,\"1845\":1,\"1846\":4,\"1847\":5,\"1848\":3,\"1849\":3,\"1850\":4,\"1851\":3,\"1852\":4,\"1856\":3,\"1857\":5,\"1858\":6,\"1862\":1,\"1870\":1,\"1871\":8,\"1876\":3,\"1877\":4,\"1878\":3}}],[\"scale=8\",{\"1\":{\"2042\":1,\"2047\":1}}],[\"scale=32\",{\"1\":{\"2040\":1}}],[\"scale=false\",{\"1\":{\"1799\":1,\"2368\":1,\"2371\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1}}],[\"scale=true\",{\"1\":{\"1786\":1}}],[\"scale=0\",{\"1\":{\"1458\":1,\"1605\":1,\"1607\":1,\"1631\":1,\"1635\":1,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"scale=15\",{\"1\":{\"2030\":1}}],[\"scale=16\",{\"1\":{\"1605\":1}}],[\"scale=1\",{\"1\":{\"834\":1,\"1314\":2,\"1573\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1731\":1,\"1732\":1}}],[\"scalenorm\",{\"0\":{\"1080\":1},\"1\":{\"1080\":3}}],[\"scaled\",{\"1\":{\"754\":2,\"771\":1,\"785\":1,\"809\":1,\"813\":1,\"826\":2,\"1076\":1,\"1209\":1,\"1669\":3,\"1693\":1,\"1755\":1,\"1778\":1,\"1850\":1,\"1851\":5,\"1852\":1,\"2090\":5,\"2243\":5,\"2244\":5,\"2255\":5,\"2264\":5,\"2279\":5}}],[\"scaledpositionalencoding\",{\"0\":{\"813\":1},\"1\":{\"749\":1,\"813\":1,\"1133\":1,\"1149\":1,\"1150\":1,\"1272\":1,\"1971\":1,\"2001\":1,\"2004\":1,\"2029\":1}}],[\"scale\",{\"0\":{\"1825\":1},\"1\":{\"113\":5,\"242\":1,\"343\":2,\"350\":2,\"368\":2,\"455\":4,\"464\":4,\"470\":4,\"632\":3,\"668\":2,\"669\":2,\"670\":2,\"672\":3,\"770\":1,\"1080\":1,\"1115\":2,\"1130\":2,\"1187\":1,\"1198\":3,\"1207\":1,\"1269\":2,\"1605\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1757\":1,\"1763\":1,\"1778\":10,\"1786\":2,\"1801\":9,\"1804\":8,\"1805\":11,\"1825\":1,\"1846\":3,\"1847\":10,\"1849\":2,\"1850\":4,\"1852\":4,\"1853\":2,\"1854\":2,\"1858\":1,\"1863\":1,\"1868\":3,\"1869\":6,\"1874\":1,\"1877\":10,\"1878\":8,\"1880\":6,\"1904\":2,\"1916\":2,\"1917\":1,\"1929\":3,\"2030\":2,\"2040\":2,\"2049\":3,\"2054\":1,\"2055\":3,\"2064\":4,\"2151\":3,\"2364\":1,\"2372\":1,\"2385\":1,\"2387\":1,\"2429\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2554\":1,\"2654\":1,\"2658\":1}}],[\"scaling=false\",{\"1\":{\"770\":1}}],[\"scaling=1\",{\"1\":{\"681\":1,\"682\":1,\"758\":1}}],[\"scaling=2\",{\"1\":{\"677\":1,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"688\":1}}],[\"scaling\",{\"0\":{\"1757\":1},\"1\":{\"113\":1,\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"688\":2,\"690\":2,\"758\":2,\"762\":1,\"763\":2,\"770\":1,\"1057\":1,\"1142\":1,\"1186\":1,\"1188\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1287\":1,\"1301\":1,\"1304\":1,\"1314\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1455\":2,\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1,\"1757\":1,\"1778\":4,\"1805\":8,\"1850\":5,\"1852\":4,\"1869\":2,\"1877\":5,\"1932\":1,\"1947\":1,\"2032\":1,\"2090\":4}}],[\"scalar=true\",{\"1\":{\"1130\":1,\"1274\":1}}],[\"scalar\",{\"1\":{\"56\":2,\"697\":1,\"734\":3,\"767\":3,\"778\":6,\"817\":3,\"828\":3,\"1130\":3,\"1145\":1,\"1698\":1,\"1704\":1,\"1705\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1773\":1,\"1778\":1,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1912\":5,\"2082\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2170\":1,\"2193\":1,\"2199\":1,\"2240\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2259\":2,\"2263\":1,\"2264\":1,\"2278\":1,\"2279\":1}}],[\"scipy\",{\"0\":{\"1002\":1,\"1027\":1},\"1\":{\"1002\":1,\"1027\":2,\"2598\":2}}],[\"sclite\",{\"0\":{\"290\":1,\"291\":1,\"292\":1},\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"290\":1,\"292\":1,\"570\":1}}],[\"scheme\",{\"1\":{\"2418\":1}}],[\"sched\",{\"1\":{\"2274\":2}}],[\"schedueler\",{\"1\":{\"2260\":1}}],[\"scheduling\",{\"0\":{\"141\":1},\"1\":{\"47\":1,\"85\":1,\"93\":1,\"94\":1,\"141\":2,\"667\":1,\"671\":1}}],[\"schedulerinterface\",{\"0\":{\"672\":1},\"1\":{\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1}}],[\"schedulers\",{\"0\":{\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2702\":1},\"1\":{\"253\":2,\"604\":2,\"667\":1,\"671\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":2,\"2020\":1,\"2021\":2,\"2022\":1,\"2023\":1,\"2185\":1,\"2201\":3,\"2203\":1}}],[\"scheduler\",{\"0\":{\"142\":1,\"667\":1,\"668\":2,\"669\":2,\"670\":2,\"671\":1,\"672\":2,\"673\":2,\"674\":3,\"675\":3,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2274\":1,\"2675\":1},\"1\":{\"15\":1,\"62\":4,\"65\":1,\"85\":2,\"144\":1,\"667\":3,\"668\":2,\"669\":5,\"670\":5,\"671\":3,\"672\":3,\"673\":4,\"674\":6,\"675\":4,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2019\":1,\"2020\":3,\"2021\":4,\"2022\":3,\"2023\":3,\"2186\":1,\"2202\":2,\"2204\":1,\"2255\":1,\"2260\":3,\"2274\":4,\"2357\":1,\"2372\":1,\"2385\":1,\"2429\":1,\"2440\":2,\"2554\":1,\"2558\":3,\"2578\":1,\"2584\":2}}],[\"scheibler\",{\"1\":{\"130\":1}}],[\"screenshot\",{\"1\":{\"2487\":1,\"2491\":1,\"2497\":1,\"2500\":1,\"2501\":1}}],[\"scratch\",{\"0\":{\"204\":1},\"1\":{\"126\":1,\"170\":1,\"2391\":1,\"2423\":1,\"2527\":1,\"2546\":1,\"2584\":2,\"2585\":1}}],[\"scriptmodule\",{\"1\":{\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"708\":1,\"729\":1,\"730\":1,\"752\":1,\"756\":1,\"758\":1,\"760\":1,\"778\":1,\"782\":1,\"793\":1,\"819\":1,\"830\":1,\"831\":1,\"835\":1,\"1046\":1,\"1061\":1,\"1067\":1,\"1082\":1,\"1084\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1140\":1,\"1141\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1174\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1188\":1,\"1190\":1,\"1196\":1,\"1197\":1,\"1200\":1,\"1203\":1,\"1204\":1,\"1206\":1,\"1207\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1244\":1,\"1247\":1,\"1249\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1378\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1524\":1,\"1525\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1552\":1,\"1554\":1,\"1555\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":1,\"1575\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1601\":1,\"1602\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1613\":1,\"1616\":1,\"1617\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1670\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1718\":1,\"1719\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1785\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1799\":1,\"1806\":1,\"1828\":1,\"1840\":1,\"1842\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1902\":1,\"1906\":1,\"1907\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1970\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2003\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2084\":1,\"2089\":1,\"2149\":1,\"2168\":1,\"2170\":1,\"2233\":1,\"2235\":1,\"2237\":1,\"2239\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2266\":1,\"2275\":1,\"2277\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1}}],[\"scripts\",{\"0\":{\"16\":1,\"90\":1,\"2492\":1,\"2628\":1,\"2638\":1,\"2642\":1,\"2643\":1},\"1\":{\"15\":4,\"47\":2,\"51\":1,\"54\":2,\"85\":4,\"90\":1,\"98\":1,\"102\":1,\"127\":1,\"135\":1,\"142\":1,\"144\":1,\"169\":2,\"181\":2,\"235\":1,\"2372\":1,\"2384\":1,\"2385\":2,\"2394\":2,\"2400\":1,\"2405\":1,\"2415\":1,\"2429\":1,\"2440\":3,\"2481\":1,\"2492\":2,\"2530\":2,\"2536\":1,\"2541\":1,\"2554\":1,\"2564\":2,\"2566\":1,\"2573\":1,\"2585\":1,\"2618\":1,\"2628\":2,\"2638\":1,\"2642\":2}}],[\"script\",{\"0\":{\"2568\":1,\"2569\":1,\"2571\":1},\"1\":{\"3\":3,\"4\":1,\"5\":1,\"11\":1,\"15\":2,\"16\":3,\"18\":2,\"19\":1,\"46\":1,\"85\":4,\"90\":1,\"92\":1,\"98\":1,\"99\":1,\"107\":4,\"135\":1,\"143\":1,\"168\":2,\"170\":1,\"179\":1,\"180\":1,\"204\":1,\"235\":5,\"295\":1,\"2368\":1,\"2371\":1,\"2372\":1,\"2373\":1,\"2385\":1,\"2405\":1,\"2429\":2,\"2430\":1,\"2431\":1,\"2486\":1,\"2490\":1,\"2492\":1,\"2494\":1,\"2500\":1,\"2541\":1,\"2554\":1,\"2555\":1,\"2568\":3,\"2569\":1,\"2600\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2628\":1,\"2630\":1,\"2638\":1,\"2640\":1}}],[\"scpfile\",{\"1\":{\"1396\":1,\"1403\":1,\"1407\":2,\"1411\":1,\"1415\":1}}],[\"scp2json\",{\"0\":{\"564\":1,\"574\":1},\"1\":{\"564\":2,\"574\":2}}],[\"scps\",{\"0\":{\"441\":1},\"1\":{\"441\":5,\"564\":13}}],[\"scp\",{\"0\":{\"46\":1,\"568\":1,\"1388\":1,\"1390\":1,\"1393\":1,\"1394\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1405\":1,\"1407\":1,\"1409\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1426\":1,\"2433\":1},\"1\":{\"46\":17,\"47\":5,\"49\":5,\"51\":4,\"52\":1,\"53\":1,\"54\":6,\"57\":2,\"58\":2,\"74\":4,\"75\":4,\"76\":12,\"77\":4,\"78\":4,\"79\":2,\"98\":2,\"169\":1,\"181\":1,\"182\":1,\"183\":1,\"194\":2,\"202\":2,\"237\":2,\"238\":15,\"241\":1,\"276\":5,\"278\":1,\"280\":6,\"281\":5,\"363\":4,\"441\":1,\"525\":1,\"528\":6,\"536\":1,\"564\":2,\"568\":6,\"574\":1,\"995\":2,\"1013\":2,\"1015\":12,\"1030\":4,\"1388\":3,\"1389\":1,\"1390\":2,\"1391\":2,\"1393\":2,\"1394\":3,\"1395\":1,\"1396\":3,\"1397\":1,\"1398\":3,\"1399\":1,\"1401\":2,\"1402\":1,\"1403\":2,\"1404\":1,\"1405\":3,\"1406\":4,\"1407\":3,\"1408\":2,\"1409\":4,\"1410\":1,\"1411\":3,\"1412\":1,\"1413\":3,\"1414\":1,\"1415\":3,\"1416\":1,\"1421\":2,\"1425\":2,\"1426\":2,\"2178\":2,\"2179\":2,\"2181\":1,\"2183\":1,\"2184\":2,\"2190\":1,\"2191\":2,\"2194\":2,\"2195\":2,\"2197\":6,\"2200\":2,\"2373\":2,\"2385\":6,\"2386\":2,\"2387\":4,\"2395\":1,\"2412\":1,\"2430\":6,\"2431\":3,\"2432\":6,\"2461\":1,\"2492\":3,\"2531\":1,\"2555\":6,\"2568\":5,\"2628\":3,\"2638\":6}}],[\"ser\",{\"1\":{\"2457\":1}}],[\"series\",{\"1\":{\"1905\":2,\"2429\":1,\"2552\":1}}],[\"serizel\",{\"1\":{\"1712\":1,\"1715\":1}}],[\"serializable\",{\"1\":{\"2312\":1}}],[\"serialized\",{\"1\":{\"610\":1,\"652\":1}}],[\"serializes\",{\"1\":{\"610\":2,\"612\":1}}],[\"serializer\",{\"1\":{\"610\":1,\"612\":2,\"834\":1,\"981\":1}}],[\"serialize\",{\"1\":{\"610\":1,\"612\":1,\"613\":2,\"652\":1,\"834\":2,\"981\":2}}],[\"serialiterator\",{\"1\":{\"998\":2}}],[\"semantic\",{\"1\":{\"2467\":2,\"2473\":4}}],[\"semodule\",{\"0\":{\"2061\":1},\"1\":{\"2061\":1}}],[\"semi\",{\"1\":{\"1219\":1}}],[\"session\",{\"1\":{\"1409\":1,\"2387\":1,\"2395\":1,\"2531\":1}}],[\"sep2\",{\"1\":{\"2498\":2,\"2616\":2,\"2634\":2}}],[\"sep1\",{\"1\":{\"2498\":2,\"2616\":2,\"2634\":2}}],[\"seperation\",{\"1\":{\"2497\":1}}],[\"seperated\",{\"1\":{\"2497\":1,\"2498\":1,\"2500\":1}}],[\"seperate\",{\"1\":{\"1171\":1,\"1206\":1,\"1552\":1,\"1953\":1,\"1955\":1}}],[\"sep\",{\"1\":{\"2125\":1,\"2372\":1,\"2429\":1,\"2554\":1}}],[\"separable\",{\"1\":{\"1198\":1,\"1656\":1,\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1,\"1835\":1}}],[\"separators\",{\"1\":{\"2643\":1}}],[\"separator\",{\"0\":{\"1377\":2,\"1445\":2,\"1447\":2,\"1449\":2,\"1454\":1,\"1463\":2,\"1505\":2,\"1515\":2,\"1516\":2,\"1523\":2,\"1528\":2,\"1529\":2,\"1534\":2,\"1539\":2,\"1540\":2,\"1555\":2,\"1558\":2,\"1578\":2,\"1579\":2,\"1580\":2,\"1588\":2,\"1590\":2,\"1611\":1,\"1626\":2,\"1645\":2,\"1654\":2,\"1658\":2,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":2,\"1671\":2,\"1719\":2,\"1735\":2},\"1\":{\"1377\":6,\"1445\":2,\"1447\":2,\"1449\":2,\"1454\":1,\"1463\":3,\"1505\":3,\"1515\":3,\"1516\":3,\"1523\":4,\"1528\":3,\"1529\":3,\"1534\":3,\"1539\":3,\"1540\":2,\"1551\":2,\"1553\":3,\"1555\":2,\"1558\":3,\"1578\":2,\"1579\":2,\"1580\":2,\"1588\":2,\"1590\":2,\"1611\":1,\"1626\":3,\"1645\":2,\"1654\":3,\"1658\":3,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":3,\"1671\":2,\"1719\":2,\"1735\":2,\"2131\":3,\"2641\":8,\"2642\":5,\"2643\":1,\"2645\":1}}],[\"separately\",{\"1\":{\"1719\":2}}],[\"separatespeech\",{\"0\":{\"356\":1,\"362\":1,\"374\":1},\"1\":{\"2368\":2,\"2371\":2,\"2486\":2,\"2490\":2,\"2494\":2,\"2605\":2,\"2609\":2,\"2612\":2,\"2622\":2,\"2626\":2,\"2630\":2}}],[\"separate\",{\"0\":{\"2372\":1,\"2495\":1,\"2496\":1,\"2613\":1,\"2614\":1,\"2615\":1,\"2631\":1,\"2632\":1,\"2633\":1},\"1\":{\"74\":1,\"1245\":1,\"1551\":1,\"1553\":1,\"2368\":1,\"2371\":1,\"2372\":2,\"2481\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2497\":1,\"2543\":1,\"2570\":1,\"2573\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2614\":1,\"2615\":1,\"2618\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2632\":1,\"2633\":1}}],[\"separated\",{\"0\":{\"2498\":1,\"2499\":1,\"2616\":1,\"2617\":1,\"2634\":1,\"2635\":1},\"1\":{\"21\":1,\"28\":1,\"29\":1,\"57\":1,\"1558\":1,\"2372\":3,\"2429\":1,\"2481\":1,\"2497\":3,\"2498\":4,\"2500\":4,\"2554\":1,\"2614\":3,\"2615\":3,\"2616\":4,\"2617\":2,\"2618\":1,\"2632\":3,\"2633\":3,\"2634\":4,\"2635\":2}}],[\"separation\",{\"0\":{\"156\":1,\"280\":1,\"528\":1,\"2370\":1,\"2493\":1,\"2611\":1,\"2629\":1,\"2641\":1},\"1\":{\"62\":1,\"130\":1,\"156\":1,\"161\":1,\"280\":3,\"528\":2,\"1132\":1,\"1466\":1,\"1510\":1,\"1511\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"1551\":2,\"1553\":3,\"1568\":1,\"1581\":1,\"1639\":1,\"1643\":1,\"1644\":1,\"1645\":2,\"1660\":2,\"1661\":2,\"1662\":2,\"1670\":1,\"1671\":1,\"2371\":2,\"2481\":1,\"2494\":1,\"2601\":1,\"2612\":2,\"2618\":3,\"2630\":2,\"2635\":1}}],[\"seglstm\",{\"0\":{\"1648\":1},\"1\":{\"1598\":1,\"1648\":2,\"1652\":1,\"1654\":1}}],[\"seg2\",{\"1\":{\"1560\":1}}],[\"seg1\",{\"1\":{\"1560\":1}}],[\"seg\",{\"1\":{\"1560\":1,\"1648\":1,\"1652\":2,\"1654\":2,\"2196\":1}}],[\"segement\",{\"1\":{\"1143\":1}}],[\"segmenter\",{\"0\":{\"2287\":2,\"2295\":2,\"2296\":2},\"1\":{\"2287\":2,\"2294\":2,\"2295\":2,\"2296\":2}}],[\"segmented\",{\"1\":{\"1884\":1,\"1885\":1}}],[\"segmenting\",{\"1\":{\"1560\":2}}],[\"segment=false\",{\"1\":{\"711\":2}}],[\"segment\",{\"0\":{\"816\":1},\"1\":{\"51\":2,\"249\":1,\"343\":4,\"350\":4,\"368\":4,\"816\":1,\"1460\":1,\"1534\":3,\"1539\":3,\"1558\":3,\"1560\":1,\"1645\":3,\"1652\":2,\"1654\":2,\"1670\":5,\"1671\":5,\"1741\":1,\"1743\":1,\"1761\":1,\"1763\":1,\"1778\":3,\"1795\":1,\"1797\":1,\"1804\":4,\"1805\":1,\"1850\":1,\"1851\":4,\"1852\":3,\"1877\":1,\"1878\":4,\"1884\":4,\"1885\":4,\"2184\":2,\"2200\":3,\"2267\":1,\"2287\":2,\"2295\":2,\"2296\":2,\"2368\":2,\"2371\":3,\"2387\":1,\"2486\":2,\"2490\":2,\"2494\":3,\"2605\":2,\"2609\":2,\"2612\":3,\"2622\":2,\"2626\":2,\"2630\":3}}],[\"segments=none\",{\"1\":{\"985\":1}}],[\"segmentstreaminge2e\",{\"0\":{\"816\":1},\"1\":{\"816\":2}}],[\"segments\",{\"0\":{\"1884\":2,\"1885\":2},\"1\":{\"51\":5,\"509\":2,\"512\":2,\"525\":2,\"585\":1,\"1013\":1,\"1409\":2,\"1652\":1,\"1654\":1,\"1670\":2,\"1671\":1,\"1804\":1,\"1851\":1,\"1878\":1,\"1884\":3,\"1885\":3,\"2089\":1,\"2387\":5}}],[\"segmentations\",{\"0\":{\"51\":1}}],[\"segmentation\",{\"0\":{\"305\":1},\"1\":{\"49\":1,\"245\":1,\"1529\":1,\"1568\":1,\"1652\":2,\"1654\":2}}],[\"seki\",{\"1\":{\"705\":1}}],[\"sed\",{\"1\":{\"202\":6,\"240\":1}}],[\"sentiment\",{\"0\":{\"2477\":1},\"1\":{\"2478\":5}}],[\"sent\",{\"1\":{\"727\":1,\"1095\":1,\"1432\":1,\"1510\":1,\"1643\":1}}],[\"sentences\",{\"0\":{\"2347\":1},\"1\":{\"612\":4,\"621\":1,\"2347\":1}}],[\"sentencepiece\",{\"0\":{\"2132\":1,\"2347\":1,\"2349\":2},\"1\":{\"196\":2,\"271\":1,\"272\":1,\"2132\":1,\"2347\":2,\"2349\":5,\"2373\":1,\"2433\":1,\"2555\":1}}],[\"sentencepiecestokenizer\",{\"0\":{\"2132\":1},\"1\":{\"2132\":1}}],[\"sentencepieces\",{\"1\":{\"84\":1}}],[\"sentence\",{\"1\":{\"110\":1,\"218\":1,\"272\":2,\"612\":1,\"750\":1,\"825\":2,\"1059\":4,\"1132\":1,\"1173\":4,\"1956\":1,\"2365\":2,\"2373\":1,\"2441\":1,\"2456\":2,\"2457\":1,\"2460\":2,\"2462\":1,\"2508\":2,\"2510\":2,\"2515\":2,\"2521\":1,\"2555\":1,\"2655\":2,\"2660\":2}}],[\"sends\",{\"1\":{\"2153\":1}}],[\"send\",{\"1\":{\"617\":1,\"629\":1,\"728\":1,\"926\":1}}],[\"sensitive\",{\"1\":{\"150\":1}}],[\"se++\",{\"1\":{\"130\":1}}],[\"se\",{\"0\":{\"156\":1,\"2730\":1},\"1\":{\"130\":1,\"140\":3,\"156\":3,\"161\":1,\"244\":3,\"1604\":1,\"1655\":1,\"1719\":1,\"2046\":1,\"2366\":1,\"2480\":1,\"2490\":2,\"2601\":2,\"2609\":2,\"2618\":3,\"2626\":2,\"2635\":2,\"2641\":1}}],[\"seletected\",{\"1\":{\"209\":1,\"213\":1}}],[\"selective\",{\"1\":{\"2064\":1}}],[\"selecting\",{\"1\":{\"1741\":1}}],[\"selection\",{\"0\":{\"2357\":1,\"2363\":1,\"2371\":1,\"2494\":1,\"2509\":1,\"2512\":1,\"2514\":1,\"2578\":1,\"2612\":1,\"2630\":1,\"2653\":1,\"2657\":1,\"2659\":1},\"1\":{\"23\":1,\"119\":1,\"121\":2,\"700\":1,\"1048\":1,\"1093\":1,\"1115\":6,\"1138\":1,\"1139\":1,\"1180\":2,\"1269\":4,\"2481\":1,\"2514\":3,\"2618\":1,\"2659\":3}}],[\"selector\",{\"0\":{\"942\":1},\"1\":{\"942\":1}}],[\"selects\",{\"1\":{\"62\":1,\"745\":1,\"746\":1}}],[\"selected\",{\"1\":{\"21\":1,\"22\":1,\"24\":1,\"45\":1,\"99\":1,\"150\":1,\"705\":2,\"917\":1,\"940\":1,\"1048\":1,\"1352\":1,\"1375\":1,\"1398\":1,\"2514\":2,\"2659\":2}}],[\"select\",{\"0\":{\"142\":1,\"917\":1,\"918\":1},\"1\":{\"15\":1,\"49\":1,\"106\":1,\"134\":1,\"209\":1,\"213\":1,\"221\":1,\"228\":1,\"429\":2,\"691\":1,\"693\":1,\"697\":1,\"698\":1,\"699\":1,\"705\":2,\"706\":4,\"725\":1,\"797\":1,\"806\":1,\"815\":4,\"824\":1,\"917\":1,\"918\":1,\"942\":1,\"1046\":1,\"1048\":1,\"1066\":2,\"1073\":1,\"1075\":2,\"1083\":1,\"1270\":1,\"1430\":1,\"1505\":1,\"1515\":2,\"1528\":2,\"1529\":2,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":2,\"1558\":1,\"1581\":1,\"1604\":1,\"1626\":2,\"1650\":1,\"1654\":1,\"1658\":1,\"1659\":2,\"1669\":1,\"1670\":1,\"1671\":2,\"1940\":1,\"2357\":1,\"2363\":1,\"2371\":1,\"2514\":3,\"2565\":1,\"2578\":1,\"2612\":1,\"2653\":1,\"2659\":3}}],[\"selfattention\",{\"0\":{\"1081\":1},\"1\":{\"1074\":1,\"1075\":1,\"1081\":5,\"1148\":1,\"1149\":1,\"1203\":1,\"2026\":1,\"2054\":1}}],[\"selfattn\",{\"1\":{\"726\":1,\"1140\":1,\"1148\":1,\"1149\":1,\"1169\":1,\"1203\":1,\"1505\":1,\"1771\":1,\"1778\":1,\"1787\":1,\"1788\":2,\"1798\":1,\"1804\":1,\"1805\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"2003\":1,\"2026\":1,\"2054\":1,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2279\":1,\"2440\":1,\"2564\":1}}],[\"self\",{\"0\":{\"100\":1,\"2574\":1},\"1\":{\"56\":1,\"84\":1,\"100\":1,\"161\":1,\"629\":1,\"691\":15,\"692\":4,\"697\":17,\"711\":3,\"726\":3,\"740\":1,\"741\":1,\"754\":2,\"775\":1,\"776\":1,\"797\":6,\"826\":2,\"827\":3,\"858\":2,\"859\":3,\"886\":6,\"924\":1,\"1049\":4,\"1050\":4,\"1056\":4,\"1133\":3,\"1138\":3,\"1141\":1,\"1160\":2,\"1161\":2,\"1162\":2,\"1163\":2,\"1164\":2,\"1167\":1,\"1168\":1,\"1170\":1,\"1177\":2,\"1182\":1,\"1196\":1,\"1197\":1,\"1203\":3,\"1204\":1,\"1214\":1,\"1243\":1,\"1248\":2,\"1252\":3,\"1253\":4,\"1254\":3,\"1269\":3,\"1271\":1,\"1273\":2,\"1279\":2,\"1427\":1,\"1456\":1,\"1458\":1,\"1464\":2,\"1470\":2,\"1505\":2,\"1524\":1,\"1638\":1,\"1643\":1,\"1660\":1,\"1661\":2,\"1662\":1,\"1665\":2,\"1771\":3,\"1778\":1,\"1787\":3,\"1788\":3,\"1798\":3,\"1804\":4,\"1805\":1,\"1850\":1,\"1851\":6,\"1852\":1,\"1874\":3,\"1877\":1,\"1878\":4,\"2001\":3,\"2003\":1,\"2004\":2,\"2090\":3,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2149\":1,\"2168\":1,\"2170\":1,\"2176\":2,\"2243\":6,\"2244\":6,\"2252\":1,\"2255\":6,\"2264\":2,\"2279\":6,\"2429\":1,\"2440\":1,\"2552\":1,\"2558\":1,\"2574\":3}}],[\"sequeunce\",{\"1\":{\"2083\":1}}],[\"sequentialrnnlm\",{\"0\":{\"817\":1,\"1958\":1},\"1\":{\"815\":1,\"817\":1,\"1958\":2}}],[\"sequential\",{\"1\":{\"60\":1,\"691\":5,\"692\":1,\"697\":6,\"787\":2,\"817\":1,\"1198\":1,\"1875\":1,\"1958\":1,\"2400\":1,\"2536\":1}}],[\"sequencematcher\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"sequencemodel\",{\"0\":{\"1252\":1},\"1\":{\"1252\":2,\"1327\":1}}],[\"sequencemodule\",{\"0\":{\"1253\":1},\"1\":{\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":2,\"1177\":1,\"1209\":2,\"1251\":2,\"1252\":1,\"1253\":4,\"1254\":2,\"1278\":1,\"1279\":2}}],[\"sequenceiterfactory\",{\"0\":{\"1900\":1},\"1\":{\"1900\":1}}],[\"sequenceidentity\",{\"0\":{\"1251\":1},\"1\":{\"1251\":2,\"1253\":1}}],[\"sequenceresidualblock\",{\"0\":{\"1254\":1},\"1\":{\"1254\":2}}],[\"sequence=\",{\"1\":{\"60\":1}}],[\"sequences\",{\"1\":{\"26\":2,\"618\":2,\"621\":1,\"676\":10,\"700\":1,\"701\":2,\"705\":2,\"712\":8,\"725\":11,\"726\":8,\"734\":1,\"735\":1,\"737\":2,\"742\":2,\"754\":1,\"759\":1,\"766\":4,\"767\":1,\"774\":1,\"781\":8,\"802\":1,\"806\":10,\"812\":1,\"817\":1,\"820\":1,\"821\":1,\"822\":1,\"824\":4,\"825\":23,\"826\":1,\"827\":6,\"828\":1,\"852\":1,\"870\":1,\"884\":2,\"905\":1,\"908\":2,\"920\":1,\"924\":1,\"1028\":1,\"1046\":4,\"1047\":2,\"1048\":3,\"1049\":9,\"1050\":9,\"1051\":2,\"1052\":14,\"1053\":5,\"1054\":2,\"1055\":2,\"1056\":9,\"1057\":15,\"1058\":2,\"1059\":10,\"1062\":2,\"1064\":4,\"1065\":2,\"1066\":6,\"1068\":6,\"1069\":2,\"1070\":2,\"1072\":2,\"1073\":7,\"1074\":2,\"1075\":6,\"1077\":3,\"1080\":2,\"1081\":2,\"1083\":4,\"1099\":2,\"1138\":1,\"1139\":1,\"1145\":2,\"1173\":10,\"1270\":10,\"1417\":1,\"1422\":1,\"1881\":1,\"1957\":1,\"1960\":1,\"2000\":1,\"2002\":1,\"2012\":1,\"2078\":2,\"2079\":1,\"2081\":1,\"2083\":2,\"2090\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2259\":2,\"2263\":1,\"2264\":1,\"2265\":2,\"2279\":1}}],[\"sequence\",{\"0\":{\"908\":1,\"1417\":1,\"1826\":1,\"1900\":1},\"1\":{\"23\":1,\"58\":1,\"60\":2,\"75\":1,\"79\":1,\"119\":1,\"172\":1,\"174\":3,\"217\":1,\"224\":1,\"231\":1,\"239\":3,\"274\":1,\"295\":1,\"621\":1,\"676\":2,\"681\":2,\"682\":2,\"691\":4,\"693\":2,\"697\":6,\"700\":7,\"701\":5,\"702\":1,\"704\":1,\"705\":1,\"706\":1,\"712\":3,\"725\":2,\"729\":1,\"730\":1,\"735\":3,\"742\":1,\"749\":1,\"754\":3,\"759\":2,\"768\":1,\"781\":1,\"794\":1,\"797\":2,\"806\":5,\"815\":2,\"820\":4,\"821\":6,\"824\":3,\"826\":6,\"827\":3,\"857\":2,\"863\":1,\"865\":1,\"895\":2,\"905\":1,\"908\":2,\"912\":1,\"922\":1,\"1046\":4,\"1048\":6,\"1049\":1,\"1050\":1,\"1052\":1,\"1056\":1,\"1060\":1,\"1063\":2,\"1065\":2,\"1066\":4,\"1069\":4,\"1071\":6,\"1073\":4,\"1075\":5,\"1076\":2,\"1078\":1,\"1079\":2,\"1083\":4,\"1102\":2,\"1138\":8,\"1139\":7,\"1142\":7,\"1145\":2,\"1155\":2,\"1160\":3,\"1161\":3,\"1162\":2,\"1163\":2,\"1164\":3,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1177\":3,\"1181\":1,\"1186\":5,\"1196\":1,\"1197\":1,\"1209\":1,\"1210\":5,\"1211\":2,\"1222\":1,\"1224\":2,\"1241\":1,\"1242\":1,\"1244\":1,\"1246\":1,\"1252\":6,\"1253\":9,\"1254\":5,\"1257\":3,\"1270\":5,\"1279\":2,\"1287\":2,\"1298\":4,\"1299\":4,\"1301\":5,\"1302\":4,\"1303\":4,\"1304\":5,\"1334\":4,\"1336\":1,\"1337\":3,\"1348\":1,\"1349\":3,\"1350\":3,\"1355\":1,\"1356\":1,\"1381\":1,\"1417\":1,\"1418\":1,\"1423\":1,\"1427\":1,\"1429\":1,\"1430\":2,\"1524\":1,\"1525\":1,\"1572\":1,\"1681\":1,\"1683\":1,\"1744\":1,\"1773\":2,\"1804\":1,\"1808\":6,\"1826\":2,\"1851\":2,\"1878\":1,\"1895\":2,\"1896\":3,\"1900\":3,\"1914\":1,\"1915\":1,\"1940\":1,\"1962\":2,\"2001\":2,\"2002\":9,\"2004\":2,\"2007\":1,\"2012\":1,\"2078\":2,\"2079\":4,\"2081\":3,\"2082\":2,\"2083\":4,\"2086\":3,\"2087\":3,\"2090\":5,\"2095\":7,\"2099\":2,\"2142\":3,\"2177\":1,\"2185\":2,\"2186\":4,\"2188\":1,\"2201\":6,\"2202\":8,\"2203\":2,\"2204\":4,\"2206\":1,\"2208\":1,\"2243\":7,\"2244\":9,\"2255\":9,\"2257\":2,\"2261\":2,\"2263\":9,\"2264\":11,\"2275\":2,\"2279\":6,\"2281\":1,\"2410\":3,\"2414\":1,\"2467\":1}}],[\"seqlength\",{\"1\":{\"1211\":1,\"1224\":1,\"1287\":1,\"1336\":1,\"1348\":1}}],[\"seqlen\",{\"1\":{\"750\":4}}],[\"seq2seq\",{\"1\":{\"60\":1,\"74\":1,\"837\":1,\"2011\":1}}],[\"seqs\",{\"1\":{\"26\":3,\"750\":2,\"1428\":1,\"1429\":4}}],[\"seq\",{\"0\":{\"817\":1,\"1005\":1,\"1958\":1},\"1\":{\"26\":5,\"76\":1,\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"307\":2,\"408\":2,\"797\":1,\"815\":1,\"817\":1,\"1005\":1,\"1048\":2,\"1115\":2,\"1219\":1,\"1427\":4,\"1531\":1,\"1532\":1,\"1535\":1,\"1560\":1,\"1602\":1,\"1648\":1,\"1650\":1,\"1681\":1,\"1744\":1,\"1915\":1,\"1958\":2,\"2024\":1,\"2028\":1}}],[\"seen=none\",{\"1\":{\"2324\":1}}],[\"seen\",{\"1\":{\"201\":1,\"1058\":1,\"2543\":1}}],[\"seems\",{\"1\":{\"734\":2,\"2441\":1}}],[\"seem\",{\"1\":{\"74\":1}}],[\"seed=0\",{\"1\":{\"1901\":1}}],[\"seed=none\",{\"1\":{\"940\":1,\"948\":1,\"952\":1,\"961\":1}}],[\"seed\",{\"0\":{\"2164\":2},\"1\":{\"69\":1,\"82\":2,\"247\":2,\"249\":2,\"251\":2,\"253\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":4,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":4,\"470\":4,\"478\":2,\"485\":2,\"1895\":2,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":2,\"1901\":1,\"2005\":1,\"2099\":2,\"2102\":2,\"2164\":3,\"2186\":1,\"2202\":2,\"2204\":1}}],[\"see\",{\"0\":{\"86\":1},\"1\":{\"26\":1,\"38\":1,\"45\":1,\"47\":1,\"59\":1,\"70\":1,\"71\":1,\"75\":1,\"82\":1,\"85\":1,\"92\":1,\"93\":1,\"94\":1,\"97\":1,\"99\":1,\"107\":1,\"109\":1,\"113\":1,\"117\":1,\"121\":2,\"122\":1,\"133\":1,\"135\":1,\"150\":1,\"166\":1,\"177\":1,\"195\":1,\"197\":2,\"198\":1,\"199\":1,\"203\":1,\"233\":1,\"236\":1,\"240\":2,\"606\":1,\"650\":1,\"705\":1,\"734\":2,\"745\":1,\"746\":1,\"770\":2,\"772\":1,\"804\":1,\"810\":1,\"813\":1,\"817\":2,\"828\":1,\"899\":1,\"901\":1,\"1015\":1,\"1028\":1,\"1031\":1,\"1049\":2,\"1050\":2,\"1052\":2,\"1056\":2,\"1058\":2,\"1068\":2,\"1077\":2,\"1093\":2,\"1101\":1,\"1144\":1,\"1228\":2,\"1243\":1,\"1253\":1,\"1261\":1,\"1371\":1,\"1400\":1,\"1551\":2,\"1552\":1,\"1553\":2,\"1554\":1,\"1564\":1,\"1659\":1,\"1665\":1,\"1785\":1,\"1860\":1,\"1904\":1,\"1917\":3,\"1927\":3,\"1932\":1,\"1958\":1,\"1973\":1,\"2131\":1,\"2199\":1,\"2267\":1,\"2372\":2,\"2378\":1,\"2385\":1,\"2568\":1,\"2569\":1,\"2584\":1,\"2597\":1,\"2638\":1,\"2640\":1,\"2642\":1}}],[\"searchable\",{\"1\":{\"1133\":1,\"1273\":2,\"2452\":1}}],[\"search\",{\"0\":{\"312\":1,\"319\":1,\"331\":1,\"337\":1,\"389\":1,\"395\":1,\"405\":1,\"413\":1,\"421\":1,\"427\":1,\"447\":1,\"453\":1,\"483\":1,\"489\":1,\"691\":1,\"692\":1,\"693\":1,\"694\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"707\":1,\"765\":1,\"797\":1,\"798\":1,\"857\":2,\"1048\":1,\"1060\":1,\"1063\":1,\"1138\":1,\"1139\":1,\"1176\":1,\"1193\":1},\"1\":{\"23\":21,\"97\":1,\"103\":1,\"113\":1,\"119\":12,\"122\":1,\"150\":2,\"249\":1,\"333\":2,\"676\":3,\"691\":13,\"692\":5,\"693\":8,\"694\":1,\"695\":1,\"697\":15,\"698\":4,\"699\":4,\"700\":19,\"705\":1,\"706\":2,\"707\":1,\"742\":1,\"751\":1,\"765\":1,\"773\":2,\"781\":1,\"795\":1,\"796\":1,\"797\":17,\"798\":1,\"812\":1,\"814\":1,\"815\":3,\"857\":8,\"1048\":13,\"1060\":2,\"1063\":2,\"1138\":22,\"1139\":20,\"1176\":2,\"1193\":2,\"2155\":1,\"2414\":1,\"2586\":1}}],[\"secreto\",{\"1\":{\"2457\":1}}],[\"sections\",{\"1\":{\"112\":1,\"114\":1,\"2393\":1,\"2529\":1}}],[\"section\",{\"1\":{\"62\":1,\"107\":1,\"113\":1,\"117\":1,\"121\":2,\"124\":1,\"1522\":1,\"1576\":1,\"1577\":1,\"2394\":1,\"2399\":1,\"2530\":1,\"2535\":1,\"2585\":1}}],[\"secondary\",{\"1\":{\"1069\":1,\"1269\":2}}],[\"seconds=5\",{\"1\":{\"2596\":1}}],[\"seconds=0\",{\"1\":{\"2568\":1}}],[\"seconds\",{\"1\":{\"87\":1,\"203\":2,\"2197\":1,\"2372\":3,\"2373\":2,\"2375\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2393\":1,\"2394\":1,\"2396\":1,\"2427\":1,\"2428\":1,\"2430\":2,\"2433\":1,\"2529\":1,\"2530\":1,\"2532\":1,\"2550\":1,\"2551\":1,\"2555\":2,\"2558\":1,\"2568\":1,\"2584\":2,\"2585\":1,\"2596\":2}}],[\"second\",{\"0\":{\"2649\":1},\"1\":{\"51\":1,\"58\":1,\"75\":2,\"76\":1,\"112\":2,\"115\":1,\"144\":1,\"265\":2,\"269\":2,\"648\":1,\"713\":1,\"987\":2,\"1097\":1,\"1274\":1,\"1276\":1,\"1670\":1,\"1671\":2,\"1849\":1,\"1856\":1,\"1858\":1,\"1905\":1,\"1929\":3,\"1941\":3,\"1947\":3,\"2360\":3,\"2387\":1,\"2431\":1,\"2458\":3,\"2473\":1,\"2474\":7,\"2523\":3,\"2582\":3,\"2649\":7}}],[\"sec\",{\"1\":{\"17\":1,\"110\":2,\"203\":8,\"297\":2,\"429\":2,\"813\":1,\"2360\":3,\"2458\":3,\"2523\":3,\"2582\":3}}],[\"several\",{\"1\":{\"16\":1,\"17\":1,\"21\":1,\"24\":1,\"80\":2,\"92\":1,\"100\":1,\"136\":1,\"168\":1,\"172\":1,\"179\":1,\"235\":1,\"240\":1,\"1741\":1,\"2372\":1,\"2373\":1,\"2384\":1,\"2428\":1,\"2551\":1,\"2555\":1,\"2566\":1,\"2583\":1,\"2584\":1,\"2585\":1}}],[\"set=\",{\"1\":{\"2568\":1}}],[\"set=dump\",{\"1\":{\"2375\":4,\"2558\":4}}],[\"settimeout\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"settings\",{\"1\":{\"109\":1,\"2400\":1,\"2536\":1}}],[\"setting\",{\"1\":{\"23\":1,\"26\":1,\"45\":1,\"99\":1,\"113\":1,\"119\":1,\"135\":1,\"144\":1,\"150\":2,\"295\":1,\"950\":1,\"968\":1,\"1846\":1,\"1847\":1,\"2559\":1}}],[\"setattr\",{\"1\":{\"173\":1}}],[\"sets\",{\"1\":{\"98\":1,\"238\":1,\"239\":1,\"1034\":1,\"1245\":1,\"2373\":6,\"2375\":4,\"2377\":2,\"2385\":1,\"2387\":1,\"2397\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2430\":5,\"2431\":1,\"2432\":3,\"2433\":3,\"2436\":2,\"2440\":1,\"2533\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2555\":7,\"2558\":2,\"2559\":2,\"2562\":2,\"2564\":1,\"2569\":1,\"2584\":4,\"2585\":2}}],[\"sets=\",{\"1\":{\"98\":1}}],[\"setups\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"setup\",{\"0\":{\"196\":1,\"217\":1,\"224\":1,\"231\":1,\"234\":1,\"1882\":1,\"2358\":1,\"2364\":1,\"2384\":1,\"2394\":1,\"2428\":1,\"2455\":1,\"2507\":1,\"2513\":1,\"2517\":1,\"2520\":1,\"2530\":1,\"2551\":1,\"2579\":1,\"2654\":1,\"2658\":1},\"1\":{\"15\":1,\"19\":1,\"70\":1,\"85\":1,\"94\":1,\"135\":10,\"136\":1,\"148\":2,\"167\":6,\"178\":5,\"196\":7,\"200\":3,\"234\":6,\"638\":1,\"1155\":1,\"1243\":1,\"1245\":1,\"1248\":1,\"2354\":1,\"2372\":3,\"2384\":1,\"2385\":1,\"2388\":1,\"2394\":1,\"2409\":1,\"2421\":1,\"2428\":1,\"2429\":1,\"2524\":1,\"2530\":1,\"2544\":1,\"2551\":1,\"2554\":1,\"2566\":1,\"2584\":1,\"2637\":1}}],[\"set\",{\"0\":{\"26\":1,\"598\":1,\"919\":1,\"1032\":1,\"1033\":1,\"1034\":1,\"2164\":2,\"2372\":1,\"2496\":1,\"2614\":1,\"2632\":1},\"1\":{\"1\":2,\"3\":1,\"5\":1,\"19\":3,\"20\":1,\"21\":1,\"26\":1,\"44\":2,\"45\":1,\"66\":2,\"72\":1,\"79\":1,\"85\":1,\"98\":4,\"102\":1,\"113\":2,\"115\":2,\"121\":1,\"122\":1,\"124\":1,\"135\":1,\"142\":2,\"150\":2,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"237\":2,\"238\":2,\"245\":2,\"274\":1,\"301\":2,\"598\":2,\"613\":1,\"693\":8,\"697\":2,\"725\":2,\"727\":1,\"728\":1,\"760\":1,\"778\":1,\"785\":1,\"797\":1,\"806\":2,\"825\":3,\"831\":1,\"835\":1,\"875\":2,\"899\":1,\"901\":1,\"915\":2,\"919\":1,\"922\":2,\"987\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1,\"1032\":1,\"1033\":1,\"1034\":1,\"1046\":2,\"1051\":1,\"1052\":1,\"1054\":1,\"1055\":1,\"1066\":2,\"1073\":2,\"1075\":2,\"1082\":2,\"1083\":2,\"1142\":1,\"1186\":1,\"1198\":1,\"1210\":1,\"1211\":1,\"1219\":1,\"1224\":1,\"1228\":1,\"1241\":1,\"1245\":2,\"1248\":1,\"1270\":2,\"1301\":1,\"1304\":1,\"1337\":3,\"1345\":1,\"1347\":1,\"1349\":3,\"1350\":3,\"1476\":1,\"1551\":1,\"1553\":1,\"1598\":1,\"1639\":1,\"1640\":1,\"1719\":1,\"1765\":1,\"1777\":1,\"1800\":1,\"1803\":1,\"1804\":3,\"1844\":1,\"1848\":2,\"1849\":2,\"1851\":4,\"1857\":1,\"1861\":1,\"1862\":1,\"1871\":1,\"1878\":3,\"1880\":1,\"1901\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1973\":2,\"2001\":3,\"2002\":3,\"2004\":3,\"2030\":1,\"2046\":2,\"2079\":2,\"2084\":1,\"2086\":3,\"2087\":3,\"2089\":1,\"2090\":3,\"2095\":3,\"2099\":1,\"2157\":1,\"2164\":2,\"2193\":1,\"2198\":2,\"2216\":1,\"2217\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2279\":3,\"2373\":13,\"2375\":9,\"2377\":4,\"2385\":2,\"2387\":1,\"2397\":2,\"2398\":2,\"2400\":2,\"2401\":2,\"2403\":3,\"2405\":2,\"2410\":1,\"2411\":1,\"2430\":10,\"2431\":3,\"2432\":6,\"2433\":6,\"2436\":4,\"2439\":1,\"2440\":4,\"2533\":2,\"2534\":2,\"2536\":2,\"2537\":2,\"2539\":3,\"2541\":2,\"2542\":2,\"2543\":1,\"2555\":14,\"2558\":5,\"2559\":5,\"2562\":4,\"2564\":2,\"2568\":6,\"2569\":6,\"2573\":1,\"2584\":8,\"2585\":6}}],[\"sttask\",{\"0\":{\"2112\":1},\"1\":{\"2109\":1,\"2112\":2}}],[\"stil\",{\"1\":{\"2395\":1,\"2531\":1}}],[\"still\",{\"1\":{\"83\":1,\"84\":2,\"109\":1,\"111\":1,\"127\":1,\"1406\":1,\"1928\":1,\"2385\":1,\"2418\":1,\"2433\":1,\"2450\":1,\"2555\":1}}],[\"stiffness=1\",{\"1\":{\"1619\":1}}],[\"stiffness\",{\"1\":{\"1618\":1,\"1619\":3}}],[\"stinterface\",{\"0\":{\"812\":1},\"1\":{\"812\":2}}],[\"stm\",{\"1\":{\"546\":5,\"590\":4}}],[\"stftencoder\",{\"0\":{\"1644\":1},\"1\":{\"1644\":1}}],[\"stftdecoder\",{\"0\":{\"1643\":1},\"1\":{\"1643\":1}}],[\"stft2logmelspectrogram\",{\"0\":{\"954\":1,\"971\":1},\"1\":{\"954\":1,\"971\":1}}],[\"stft\",{\"0\":{\"284\":1,\"512\":1,\"953\":1,\"970\":1,\"1643\":1,\"1644\":1,\"1918\":2},\"1\":{\"242\":1,\"284\":6,\"512\":3,\"648\":2,\"778\":1,\"953\":2,\"970\":2,\"971\":1,\"1158\":2,\"1463\":1,\"1543\":1,\"1551\":3,\"1553\":4,\"1564\":1,\"1604\":3,\"1643\":2,\"1644\":3,\"1655\":3,\"1660\":3,\"1661\":3,\"1662\":3,\"1671\":1,\"1701\":1,\"1702\":1,\"1719\":10,\"1736\":1,\"1737\":1,\"1758\":1,\"1759\":1,\"1785\":6,\"1804\":2,\"1912\":1,\"1918\":6,\"1929\":2,\"1941\":2,\"1947\":2,\"1987\":1,\"1989\":1,\"1991\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2498\":7,\"2600\":6,\"2616\":7,\"2634\":7,\"2641\":2}}],[\"stype\",{\"1\":{\"235\":1}}],[\"styles\",{\"1\":{\"2642\":1}}],[\"style=false\",{\"1\":{\"2315\":1}}],[\"style=none\",{\"1\":{\"2315\":1}}],[\"style=style\",{\"1\":{\"231\":1}}],[\"styletokenlayer\",{\"0\":{\"2262\":1},\"1\":{\"2262\":1}}],[\"styleencoder\",{\"0\":{\"2261\":1},\"1\":{\"2261\":2}}],[\"stylemelgangenerator\",{\"0\":{\"1871\":1},\"1\":{\"1871\":2}}],[\"stylemelgandiscriminator\",{\"0\":{\"1870\":1},\"1\":{\"1870\":2}}],[\"stylegan2\",{\"1\":{\"1508\":1}}],[\"style\",{\"0\":{\"11\":1,\"169\":1,\"181\":1,\"1002\":1,\"1027\":1,\"1870\":2,\"1871\":2,\"1872\":1,\"1873\":1,\"2253\":1,\"2257\":1,\"2261\":1,\"2262\":1},\"1\":{\"11\":1,\"16\":2,\"21\":2,\"84\":1,\"130\":1,\"168\":1,\"179\":1,\"204\":1,\"231\":2,\"237\":2,\"301\":2,\"536\":1,\"981\":1,\"1002\":1,\"1013\":1,\"1015\":1,\"1027\":2,\"1148\":3,\"1149\":1,\"1170\":1,\"1203\":3,\"1252\":1,\"1505\":3,\"1771\":4,\"1778\":1,\"1787\":3,\"1788\":3,\"1798\":3,\"1804\":3,\"1805\":1,\"1850\":1,\"1851\":4,\"1852\":1,\"1870\":3,\"1871\":3,\"1872\":1,\"1873\":1,\"1874\":3,\"1877\":1,\"1878\":3,\"2002\":1,\"2003\":1,\"2026\":1,\"2054\":3,\"2090\":2,\"2095\":2,\"2243\":5,\"2244\":5,\"2253\":1,\"2255\":5,\"2257\":3,\"2261\":8,\"2262\":7,\"2263\":2,\"2264\":2,\"2279\":3,\"2354\":1,\"2363\":4,\"2382\":1,\"2384\":1,\"2388\":1,\"2412\":1,\"2421\":1,\"2430\":2,\"2506\":1,\"2512\":2,\"2524\":1,\"2544\":1,\"2555\":2,\"2638\":1,\"2653\":4,\"2657\":2}}],[\"st1\",{\"1\":{\"201\":1,\"204\":1,\"2461\":1}}],[\"stonmask\",{\"1\":{\"2236\":1}}],[\"stonemask\",{\"1\":{\"2236\":1}}],[\"stochasticdurationpredictor\",{\"0\":{\"1868\":1},\"1\":{\"1868\":2}}],[\"stochasticdepth\",{\"0\":{\"1261\":1},\"1\":{\"1261\":1}}],[\"stochastic\",{\"0\":{\"1352\":1},\"1\":{\"1140\":1,\"1141\":3,\"1148\":1,\"1244\":1,\"1252\":1,\"1254\":1,\"1261\":2,\"1352\":5,\"1868\":2,\"1877\":4,\"1878\":12,\"2054\":1,\"2440\":1,\"2564\":1}}],[\"stoi\",{\"1\":{\"528\":2}}],[\"stored\",{\"1\":{\"1187\":1,\"1202\":1,\"1227\":1,\"2357\":1,\"2492\":1,\"2628\":1}}],[\"store\",{\"1\":{\"286\":1,\"296\":1,\"677\":1,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"758\":1,\"1187\":1,\"1202\":1,\"1670\":1,\"1671\":1,\"2349\":1}}],[\"stores\",{\"1\":{\"185\":1,\"710\":1,\"1186\":1,\"1248\":1,\"1398\":1}}],[\"storage\",{\"1\":{\"198\":1,\"1143\":1,\"1144\":1}}],[\"stopiteration\",{\"1\":{\"607\":1}}],[\"stops\",{\"1\":{\"593\":1,\"834\":1}}],[\"stopping\",{\"1\":{\"91\":1,\"1007\":1,\"2186\":1,\"2193\":1,\"2202\":2,\"2204\":1}}],[\"stopped\",{\"1\":{\"84\":1}}],[\"stop\",{\"0\":{\"91\":1,\"149\":1,\"1007\":1,\"1034\":1},\"1\":{\"91\":1,\"98\":1,\"149\":3,\"150\":1,\"168\":3,\"180\":3,\"186\":1,\"187\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":2,\"241\":2,\"242\":1,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"820\":1,\"821\":2,\"822\":3,\"826\":1,\"1007\":1,\"1034\":2,\"1850\":2,\"1851\":6,\"1852\":2,\"1904\":1,\"1916\":1,\"2000\":2,\"2002\":2,\"2078\":1,\"2079\":2,\"2095\":2,\"2244\":6,\"2255\":6,\"2263\":2,\"2264\":1,\"2279\":6,\"2360\":1,\"2373\":7,\"2375\":4,\"2397\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2412\":1,\"2413\":1,\"2430\":5,\"2433\":3,\"2440\":1,\"2458\":1,\"2523\":1,\"2533\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2555\":7,\"2558\":2,\"2559\":2,\"2564\":1,\"2568\":2,\"2569\":1,\"2582\":1,\"2584\":4,\"2585\":2}}],[\"study\",{\"1\":{\"2584\":1}}],[\"student\",{\"1\":{\"161\":1}}],[\"stuctures\",{\"1\":{\"47\":1}}],[\"std=0\",{\"1\":{\"1797\":1}}],[\"std2\",{\"1\":{\"1069\":2}}],[\"std1\",{\"1\":{\"1069\":2}}],[\"std\",{\"0\":{\"888\":1,\"1819\":1},\"1\":{\"760\":1,\"888\":1,\"941\":1,\"960\":1,\"1065\":2,\"1070\":2,\"1078\":2,\"1079\":2,\"1660\":1,\"1661\":5,\"1662\":5,\"1719\":2,\"1797\":3,\"1819\":1}}],[\"stderr\",{\"1\":{\"143\":1,\"2592\":4}}],[\"stdout=pipe\",{\"1\":{\"203\":1}}],[\"stdout\",{\"1\":{\"143\":1,\"203\":1,\"276\":1}}],[\"st\",{\"0\":{\"204\":1,\"259\":1,\"261\":1,\"443\":1,\"449\":1,\"812\":1,\"874\":2,\"936\":2,\"937\":2,\"938\":2,\"2076\":1,\"2112\":1,\"2452\":1,\"2453\":1,\"2677\":1,\"2705\":1},\"1\":{\"130\":1,\"139\":1,\"140\":1,\"161\":1,\"164\":1,\"165\":1,\"201\":3,\"203\":7,\"204\":1,\"244\":1,\"259\":3,\"261\":2,\"397\":1,\"398\":6,\"399\":13,\"443\":7,\"449\":7,\"750\":1,\"812\":3,\"874\":7,\"889\":1,\"936\":2,\"937\":4,\"938\":4,\"1393\":1,\"1984\":3,\"2076\":6,\"2112\":2,\"2394\":1,\"2447\":2,\"2450\":1,\"2451\":1,\"2452\":5,\"2454\":3,\"2455\":7,\"2456\":4,\"2458\":1,\"2460\":12,\"2461\":10,\"2516\":1,\"2517\":1,\"2521\":6,\"2530\":1}}],[\"steady\",{\"1\":{\"1618\":1,\"1619\":1}}],[\"stereo\",{\"1\":{\"48\":1}}],[\"step3\",{\"1\":{\"2585\":1}}],[\"steplr\",{\"1\":{\"2023\":1}}],[\"stepping\",{\"1\":{\"1253\":1}}],[\"step=none\",{\"1\":{\"792\":1}}],[\"step2\",{\"1\":{\"132\":1}}],[\"step\",{\"0\":{\"134\":1,\"135\":1,\"136\":1,\"243\":1,\"2023\":1,\"2221\":1,\"2637\":1,\"2638\":1,\"2639\":1,\"2640\":1,\"2642\":1,\"2643\":1,\"2644\":1,\"2645\":1},\"1\":{\"23\":5,\"87\":2,\"102\":1,\"113\":1,\"119\":2,\"135\":1,\"174\":1,\"235\":2,\"629\":1,\"667\":1,\"670\":1,\"671\":1,\"681\":1,\"682\":2,\"698\":2,\"699\":2,\"700\":3,\"725\":2,\"727\":1,\"728\":1,\"745\":1,\"746\":1,\"749\":1,\"792\":1,\"799\":1,\"806\":2,\"811\":1,\"824\":2,\"976\":1,\"1043\":1,\"1046\":2,\"1048\":2,\"1057\":1,\"1059\":1,\"1066\":2,\"1069\":2,\"1073\":2,\"1075\":2,\"1083\":2,\"1133\":3,\"1138\":3,\"1139\":3,\"1160\":4,\"1161\":4,\"1162\":3,\"1164\":4,\"1165\":4,\"1177\":4,\"1209\":5,\"1213\":1,\"1214\":2,\"1243\":4,\"1245\":3,\"1246\":1,\"1247\":1,\"1248\":4,\"1252\":4,\"1253\":5,\"1254\":4,\"1270\":2,\"1273\":2,\"1279\":3,\"1281\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1451\":1,\"1514\":1,\"1547\":1,\"1557\":1,\"1585\":1,\"1599\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"1638\":1,\"1735\":5,\"1741\":3,\"1797\":1,\"2001\":2,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":3,\"2021\":7,\"2022\":11,\"2023\":13,\"2199\":2,\"2201\":2,\"2221\":1,\"2258\":3,\"2260\":4,\"2266\":1,\"2267\":2,\"2294\":1,\"2301\":1,\"2302\":3,\"2303\":2,\"2304\":1,\"2305\":2,\"2372\":1,\"2424\":1,\"2429\":1,\"2433\":1,\"2547\":1,\"2552\":1,\"2584\":5,\"2585\":1}}],[\"steps40000\",{\"1\":{\"2357\":1,\"2578\":1}}],[\"steps=1\",{\"1\":{\"1646\":1}}],[\"steps=4000\",{\"1\":{\"834\":1}}],[\"steps\",{\"1\":{\"15\":2,\"23\":2,\"69\":1,\"85\":1,\"119\":1,\"124\":1,\"169\":1,\"181\":1,\"235\":1,\"294\":1,\"700\":1,\"754\":2,\"826\":2,\"921\":1,\"1048\":1,\"1057\":3,\"1101\":1,\"1138\":1,\"1139\":1,\"1180\":1,\"1269\":1,\"1451\":1,\"1514\":1,\"1585\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1646\":3,\"1766\":1,\"1797\":1,\"1941\":6,\"2018\":5,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":2,\"2255\":1,\"2260\":2,\"2264\":2,\"2274\":1,\"2372\":1,\"2384\":1,\"2385\":1,\"2389\":1,\"2394\":1,\"2408\":1,\"2422\":1,\"2429\":1,\"2440\":2,\"2449\":1,\"2465\":1,\"2468\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2530\":1,\"2545\":1,\"2554\":1,\"2558\":2,\"2564\":1,\"2566\":1,\"2584\":1,\"2585\":1,\"2637\":1}}],[\"struggle\",{\"1\":{\"2473\":1}}],[\"structural\",{\"1\":{\"2259\":1}}],[\"structured\",{\"1\":{\"2275\":1}}],[\"structure\",{\"0\":{\"15\":1,\"181\":1},\"1\":{\"85\":1,\"235\":2,\"802\":1,\"1158\":1,\"1198\":1,\"1239\":1,\"1800\":1,\"1905\":1,\"1989\":1,\"1991\":1,\"2248\":1,\"2250\":1,\"2411\":1,\"2467\":1}}],[\"strftime\",{\"1\":{\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1}}],[\"str2triple\",{\"0\":{\"2336\":1},\"1\":{\"2336\":3,\"2337\":1}}],[\"str2pair\",{\"0\":{\"2334\":1},\"1\":{\"2334\":3,\"2335\":1}}],[\"str2bool\",{\"0\":{\"2333\":1},\"1\":{\"1996\":1,\"2000\":2,\"2290\":6,\"2292\":3,\"2294\":6,\"2295\":3,\"2296\":3,\"2301\":2,\"2302\":3,\"2303\":1,\"2304\":1,\"2333\":2}}],[\"stretch\",{\"0\":{\"1947\":1},\"1\":{\"1947\":2}}],[\"stretch2d\",{\"0\":{\"1869\":1},\"1\":{\"1869\":3}}],[\"stream=none\",{\"1\":{\"2342\":1}}],[\"streampositionalencoding\",{\"0\":{\"818\":1},\"1\":{\"818\":1,\"1149\":1,\"1150\":1}}],[\"streamingspeech2text\",{\"1\":{\"2600\":2}}],[\"streamingly\",{\"0\":{\"2596\":1}}],[\"streaming\",{\"0\":{\"103\":1,\"120\":1,\"327\":1,\"357\":1,\"449\":1,\"699\":1,\"707\":1,\"816\":1,\"836\":1,\"1139\":1,\"2586\":1,\"2587\":1},\"1\":{\"103\":1,\"104\":1,\"105\":2,\"106\":2,\"107\":1,\"110\":1,\"111\":1,\"120\":1,\"121\":2,\"122\":1,\"124\":2,\"140\":1,\"155\":2,\"244\":1,\"249\":9,\"286\":1,\"307\":2,\"315\":2,\"327\":2,\"333\":2,\"357\":2,\"422\":2,\"449\":2,\"485\":2,\"692\":3,\"693\":8,\"699\":1,\"706\":2,\"707\":1,\"816\":1,\"818\":1,\"836\":1,\"1048\":3,\"1049\":2,\"1050\":2,\"1051\":1,\"1052\":3,\"1054\":1,\"1055\":1,\"1056\":2,\"1058\":1,\"1068\":2,\"1139\":1,\"1142\":1,\"1186\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1287\":1,\"1301\":1,\"1304\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1432\":4,\"1436\":6,\"1446\":1,\"1510\":4,\"1511\":6,\"1626\":1,\"1643\":5,\"1644\":7,\"1654\":1,\"1658\":1,\"2099\":1,\"2357\":2,\"2460\":4,\"2461\":4,\"2578\":2,\"2586\":2,\"2587\":2,\"2589\":1,\"2590\":1,\"2591\":1,\"2598\":1,\"2600\":23}}],[\"stream\",{\"1\":{\"52\":2,\"1186\":3,\"1210\":3,\"1227\":1,\"1228\":2,\"1344\":1,\"1345\":2,\"1346\":1,\"1347\":2,\"1432\":1,\"1436\":1,\"1510\":1,\"1511\":1,\"1653\":1,\"2315\":1,\"2360\":2,\"2458\":2,\"2523\":2,\"2582\":2,\"2596\":1}}],[\"strm\",{\"1\":{\"1220\":1}}],[\"str=\",{\"1\":{\"1115\":9,\"2180\":2}}],[\"strtobool\",{\"0\":{\"1038\":1},\"1\":{\"1038\":2}}],[\"stronger\",{\"1\":{\"2585\":1}}],[\"strong\",{\"1\":{\"2467\":1,\"2584\":1}}],[\"strongly\",{\"1\":{\"295\":1}}],[\"strornone\",{\"1\":{\"1927\":1,\"1929\":1,\"1941\":1,\"1947\":1}}],[\"strortorch\",{\"1\":{\"749\":1}}],[\"str\",{\"0\":{\"2334\":1,\"2336\":1,\"2338\":1,\"2339\":1},\"1\":{\"113\":1,\"115\":5,\"116\":3,\"117\":1,\"175\":2,\"238\":3,\"564\":1,\"605\":1,\"610\":3,\"614\":1,\"619\":3,\"621\":1,\"623\":2,\"626\":1,\"629\":4,\"633\":4,\"634\":1,\"635\":1,\"641\":3,\"643\":1,\"645\":1,\"647\":5,\"648\":2,\"652\":2,\"653\":1,\"654\":1,\"655\":1,\"665\":4,\"668\":1,\"669\":1,\"672\":4,\"674\":1,\"676\":1,\"691\":13,\"692\":2,\"693\":10,\"694\":4,\"697\":20,\"698\":2,\"699\":2,\"700\":1,\"710\":2,\"725\":7,\"726\":6,\"727\":2,\"728\":2,\"730\":1,\"738\":1,\"742\":2,\"747\":1,\"749\":1,\"751\":1,\"752\":1,\"754\":1,\"756\":2,\"760\":2,\"765\":4,\"797\":6,\"798\":2,\"805\":1,\"806\":3,\"807\":1,\"808\":1,\"820\":1,\"823\":1,\"824\":2,\"825\":1,\"826\":1,\"834\":1,\"835\":1,\"857\":8,\"858\":8,\"859\":5,\"860\":2,\"861\":1,\"862\":4,\"866\":1,\"868\":1,\"872\":2,\"873\":2,\"874\":2,\"886\":3,\"892\":1,\"909\":5,\"910\":2,\"911\":2,\"913\":3,\"918\":1,\"921\":1,\"934\":2,\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"987\":2,\"989\":4,\"1003\":7,\"1004\":7,\"1005\":5,\"1008\":1,\"1012\":1,\"1013\":4,\"1015\":4,\"1023\":1,\"1028\":5,\"1029\":1,\"1046\":3,\"1048\":1,\"1057\":4,\"1058\":3,\"1059\":2,\"1060\":1,\"1063\":2,\"1064\":1,\"1065\":2,\"1066\":7,\"1069\":1,\"1073\":1,\"1075\":2,\"1085\":1,\"1087\":2,\"1088\":2,\"1089\":2,\"1090\":1,\"1091\":2,\"1092\":2,\"1093\":4,\"1094\":1,\"1096\":1,\"1098\":1,\"1103\":2,\"1104\":1,\"1105\":2,\"1110\":1,\"1113\":1,\"1115\":11,\"1116\":1,\"1133\":1,\"1138\":2,\"1139\":2,\"1140\":6,\"1141\":2,\"1145\":3,\"1148\":12,\"1149\":4,\"1150\":2,\"1151\":1,\"1153\":1,\"1158\":2,\"1167\":1,\"1168\":1,\"1169\":7,\"1171\":6,\"1172\":6,\"1173\":2,\"1176\":1,\"1178\":2,\"1179\":5,\"1180\":4,\"1181\":1,\"1190\":3,\"1191\":1,\"1192\":1,\"1193\":2,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":5,\"1200\":1,\"1203\":14,\"1204\":1,\"1206\":5,\"1207\":3,\"1214\":2,\"1215\":2,\"1219\":2,\"1220\":1,\"1222\":1,\"1239\":2,\"1244\":1,\"1257\":1,\"1261\":1,\"1269\":3,\"1270\":3,\"1271\":1,\"1272\":2,\"1273\":1,\"1282\":1,\"1284\":3,\"1289\":1,\"1352\":2,\"1377\":2,\"1382\":1,\"1384\":3,\"1386\":3,\"1394\":1,\"1396\":2,\"1398\":1,\"1399\":1,\"1403\":2,\"1406\":2,\"1407\":5,\"1411\":1,\"1415\":2,\"1417\":2,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":2,\"1427\":1,\"1428\":1,\"1430\":4,\"1438\":1,\"1441\":1,\"1454\":3,\"1470\":1,\"1505\":11,\"1515\":2,\"1516\":2,\"1517\":7,\"1523\":2,\"1524\":3,\"1525\":2,\"1528\":2,\"1529\":2,\"1534\":2,\"1537\":3,\"1539\":4,\"1552\":2,\"1553\":2,\"1558\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":2,\"1571\":1,\"1581\":3,\"1604\":2,\"1611\":6,\"1626\":2,\"1643\":1,\"1644\":1,\"1646\":3,\"1654\":2,\"1658\":4,\"1659\":8,\"1665\":2,\"1667\":1,\"1669\":3,\"1670\":4,\"1671\":6,\"1704\":1,\"1711\":1,\"1713\":1,\"1739\":1,\"1761\":3,\"1763\":4,\"1765\":4,\"1771\":8,\"1773\":4,\"1778\":33,\"1786\":1,\"1787\":8,\"1788\":8,\"1791\":3,\"1798\":8,\"1800\":2,\"1801\":10,\"1803\":4,\"1804\":14,\"1805\":29,\"1810\":2,\"1811\":1,\"1830\":2,\"1831\":2,\"1832\":2,\"1834\":6,\"1836\":2,\"1837\":3,\"1843\":2,\"1844\":4,\"1845\":2,\"1846\":6,\"1847\":5,\"1848\":4,\"1849\":4,\"1850\":21,\"1851\":21,\"1852\":25,\"1856\":8,\"1857\":8,\"1858\":12,\"1859\":2,\"1861\":4,\"1862\":4,\"1866\":4,\"1867\":8,\"1869\":2,\"1870\":2,\"1871\":8,\"1872\":2,\"1873\":4,\"1874\":8,\"1876\":6,\"1877\":21,\"1878\":8,\"1892\":4,\"1893\":2,\"1896\":2,\"1905\":3,\"1906\":1,\"1914\":1,\"1915\":1,\"1917\":3,\"1918\":1,\"1919\":1,\"1923\":1,\"1924\":2,\"1926\":2,\"1927\":1,\"1929\":1,\"1930\":2,\"1932\":4,\"1937\":1,\"1941\":1,\"1944\":1,\"1947\":1,\"1948\":1,\"1955\":4,\"1957\":1,\"1958\":1,\"1960\":1,\"1962\":2,\"1963\":1,\"1964\":4,\"1965\":2,\"1966\":1,\"1967\":6,\"1968\":2,\"1970\":6,\"1975\":8,\"1977\":1,\"1979\":1,\"1984\":10,\"1987\":1,\"1989\":2,\"1991\":1,\"2000\":1,\"2001\":3,\"2002\":7,\"2003\":8,\"2004\":3,\"2006\":1,\"2007\":5,\"2008\":4,\"2009\":4,\"2010\":3,\"2011\":2,\"2012\":6,\"2026\":6,\"2027\":6,\"2028\":1,\"2029\":2,\"2049\":1,\"2054\":12,\"2055\":1,\"2064\":2,\"2076\":10,\"2082\":3,\"2083\":1,\"2084\":2,\"2086\":19,\"2087\":17,\"2089\":2,\"2090\":27,\"2091\":2,\"2095\":19,\"2099\":16,\"2102\":1,\"2106\":2,\"2109\":3,\"2113\":3,\"2115\":3,\"2116\":3,\"2119\":2,\"2120\":6,\"2123\":2,\"2124\":3,\"2125\":2,\"2128\":6,\"2129\":7,\"2130\":7,\"2131\":3,\"2132\":3,\"2133\":1,\"2135\":4,\"2136\":5,\"2137\":11,\"2142\":3,\"2148\":1,\"2152\":2,\"2156\":1,\"2157\":2,\"2170\":2,\"2176\":4,\"2177\":1,\"2178\":21,\"2179\":21,\"2180\":6,\"2181\":5,\"2182\":9,\"2184\":7,\"2185\":3,\"2186\":7,\"2187\":1,\"2188\":1,\"2189\":9,\"2191\":20,\"2193\":24,\"2194\":23,\"2195\":19,\"2196\":16,\"2197\":7,\"2198\":3,\"2199\":5,\"2200\":8,\"2201\":6,\"2202\":14,\"2203\":3,\"2204\":7,\"2208\":3,\"2210\":1,\"2213\":1,\"2214\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1,\"2232\":1,\"2236\":1,\"2240\":3,\"2241\":2,\"2243\":18,\"2244\":18,\"2246\":1,\"2248\":2,\"2250\":1,\"2254\":2,\"2255\":19,\"2259\":2,\"2260\":2,\"2263\":9,\"2264\":12,\"2274\":2,\"2278\":3,\"2279\":17,\"2289\":1,\"2294\":5,\"2301\":1,\"2302\":1,\"2304\":1,\"2305\":1,\"2317\":1,\"2319\":1,\"2325\":1,\"2327\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":4,\"2335\":2,\"2336\":4,\"2337\":1,\"2338\":3,\"2339\":4,\"2343\":8,\"2344\":6,\"2347\":6,\"2349\":6,\"2352\":1,\"2353\":1,\"2358\":1,\"2364\":1,\"2500\":18,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":1,\"2579\":1,\"2617\":18,\"2635\":18,\"2654\":1,\"2658\":1}}],[\"strip\",{\"1\":{\"2386\":1,\"2514\":1,\"2659\":1}}],[\"strip=false\",{\"1\":{\"2131\":1}}],[\"strict\",{\"1\":{\"2201\":1}}],[\"strict=true\",{\"1\":{\"231\":2}}],[\"strings\",{\"1\":{\"727\":2,\"728\":2,\"754\":1,\"760\":1,\"778\":1,\"821\":1,\"826\":1,\"831\":1,\"1476\":1,\"1598\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2084\":1,\"2089\":1,\"2313\":1,\"2500\":2,\"2617\":2,\"2635\":2}}],[\"string\",{\"0\":{\"2155\":1},\"1\":{\"108\":2,\"116\":1,\"143\":1,\"601\":3,\"619\":1,\"621\":1,\"652\":1,\"691\":4,\"692\":2,\"697\":4,\"797\":2,\"1211\":1,\"1224\":1,\"1336\":1,\"1348\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"1532\":1,\"1534\":1,\"1535\":1,\"1539\":1,\"1626\":1,\"1650\":1,\"1671\":1,\"2154\":1,\"2155\":2,\"2357\":5,\"2358\":2,\"2363\":6,\"2455\":1,\"2460\":1,\"2500\":3,\"2512\":2,\"2520\":3,\"2578\":5,\"2579\":2,\"2617\":3,\"2635\":3,\"2653\":6,\"2657\":2}}],[\"stride=64\",{\"1\":{\"1660\":1,\"1661\":1}}],[\"stride=\",{\"1\":{\"1478\":1,\"1480\":1,\"1506\":1,\"1522\":2,\"1545\":1}}],[\"stride=128\",{\"1\":{\"1719\":1}}],[\"stride=1\",{\"1\":{\"1134\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1280\":1,\"1305\":1,\"1311\":1,\"1358\":1,\"1656\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1731\":1,\"1732\":1}}],[\"strides=\",{\"1\":{\"1766\":1,\"1786\":1}}],[\"strides\",{\"1\":{\"722\":2,\"1517\":2,\"1761\":1,\"1763\":2,\"1766\":1,\"1782\":1,\"1786\":2,\"1795\":1,\"1801\":1,\"1805\":1}}],[\"stride\",{\"1\":{\"21\":9,\"115\":3,\"712\":3,\"722\":1,\"1052\":3,\"1097\":1,\"1132\":2,\"1198\":3,\"1269\":1,\"1279\":1,\"1312\":1,\"1313\":1,\"1370\":1,\"1378\":1,\"1510\":1,\"1511\":1,\"1518\":1,\"1520\":1,\"1522\":4,\"1523\":6,\"1545\":2,\"1546\":1,\"1560\":1,\"1576\":3,\"1577\":3,\"1656\":2,\"1660\":2,\"1661\":2,\"1662\":2,\"1663\":1,\"1719\":2,\"1752\":2,\"1766\":1,\"1778\":1,\"1801\":1,\"1805\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1846\":1,\"1847\":1,\"1850\":2,\"1851\":3,\"1852\":2,\"1858\":1,\"1877\":1,\"1917\":2,\"2059\":1,\"2095\":3,\"2243\":3,\"2244\":3,\"2255\":3,\"2257\":3,\"2261\":3,\"2263\":3,\"2264\":3,\"2292\":1}}],[\"straightforward\",{\"1\":{\"2385\":1,\"2412\":1}}],[\"strating\",{\"1\":{\"1015\":1}}],[\"strategies\",{\"1\":{\"700\":1,\"1138\":1,\"1139\":1}}],[\"strategy=\",{\"1\":{\"1136\":2}}],[\"strategy\",{\"1\":{\"74\":1,\"1028\":1,\"1145\":2,\"2186\":1,\"2202\":2,\"2204\":1}}],[\"strange\",{\"1\":{\"74\":1}}],[\"stabilize\",{\"1\":{\"1639\":1}}],[\"stability\",{\"1\":{\"1047\":1,\"1072\":1,\"1080\":1,\"1604\":1}}],[\"stable\",{\"1\":{\"38\":1,\"82\":1,\"1269\":1,\"1640\":1,\"2401\":2,\"2537\":2}}],[\"stamps\",{\"1\":{\"301\":1}}],[\"stackoverflow\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"stacking\",{\"1\":{\"1863\":1}}],[\"stacks\",{\"1\":{\"1377\":1,\"1658\":1,\"1659\":1,\"1804\":3,\"1805\":1,\"1857\":3,\"1862\":3,\"1863\":2,\"1864\":2,\"1865\":3,\"1877\":1,\"1878\":3,\"1880\":3}}],[\"stack\",{\"0\":{\"1744\":1,\"1867\":1},\"1\":{\"116\":1,\"1066\":2,\"1377\":3,\"1390\":1,\"1391\":1,\"1658\":3,\"1659\":3,\"1744\":2,\"1857\":4,\"1867\":2}}],[\"stacked\",{\"1\":{\"60\":1,\"1391\":2,\"1515\":1,\"1522\":1,\"1523\":1,\"1524\":1,\"1528\":1,\"1529\":1,\"1531\":1,\"1532\":1,\"1534\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1626\":1,\"1645\":1,\"1698\":1}}],[\"stands\",{\"1\":{\"2395\":1,\"2531\":1}}],[\"standardize\",{\"1\":{\"2468\":1}}],[\"standardupdater\",{\"1\":{\"604\":1,\"627\":1,\"728\":1,\"976\":1,\"1043\":1}}],[\"standard\",{\"1\":{\"117\":1,\"632\":2,\"762\":1,\"763\":2,\"888\":1,\"1065\":1,\"1069\":2,\"1070\":1,\"1078\":1,\"1079\":1,\"1084\":2,\"1138\":1,\"1141\":1,\"1170\":1,\"1210\":1,\"1211\":1,\"1233\":1,\"1242\":1,\"1253\":1,\"1302\":1,\"1303\":1,\"1336\":1,\"1337\":3,\"1400\":2,\"1688\":1,\"1719\":1,\"1756\":1,\"2125\":1,\"2151\":2,\"2363\":1,\"2506\":1,\"2653\":1}}],[\"standalone\",{\"1\":{\"112\":1,\"119\":1}}],[\"stand\",{\"1\":{\"107\":1}}],[\"stat\",{\"0\":{\"2044\":1,\"2068\":1},\"1\":{\"2044\":1,\"2068\":1}}],[\"statisitcs\",{\"1\":{\"2398\":1,\"2534\":1}}],[\"statisticsif\",{\"1\":{\"506\":1}}],[\"statistics\",{\"1\":{\"235\":1,\"238\":2,\"299\":1,\"962\":1,\"1057\":1,\"1524\":1,\"1739\":4,\"1773\":1,\"1778\":1,\"1800\":1,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1964\":1,\"2082\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2170\":1,\"2240\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2278\":1,\"2279\":1,\"2440\":2}}],[\"statistical\",{\"1\":{\"80\":1}}],[\"staticmethod\",{\"1\":{\"2644\":1}}],[\"static\",{\"1\":{\"606\":1,\"659\":2,\"660\":2,\"661\":2,\"662\":2,\"676\":1,\"697\":2,\"709\":3,\"734\":1,\"742\":4,\"754\":1,\"759\":3,\"767\":1,\"781\":1,\"817\":1,\"820\":1,\"821\":1,\"826\":1,\"828\":1,\"981\":1,\"1079\":1,\"1086\":2,\"1115\":4,\"1180\":2,\"1187\":2,\"1202\":2,\"1269\":4,\"1286\":2,\"1287\":2,\"1371\":1,\"1553\":1,\"1660\":1,\"1661\":1,\"1719\":3,\"1904\":2,\"1916\":2,\"1917\":3,\"2201\":1}}],[\"status\",{\"0\":{\"86\":1,\"88\":1},\"1\":{\"83\":1,\"1225\":2,\"1530\":1,\"1563\":1,\"1600\":1,\"1603\":1,\"1622\":1,\"2558\":2,\"2571\":1}}],[\"stats=stats\",{\"1\":{\"2170\":2}}],[\"stats=true\",{\"1\":{\"1476\":1}}],[\"statspooling\",{\"0\":{\"2068\":1},\"1\":{\"2068\":1}}],[\"stats\",{\"0\":{\"299\":1,\"506\":1,\"1739\":1,\"1964\":2,\"2398\":1,\"2534\":1},\"1\":{\"56\":3,\"102\":1,\"251\":2,\"259\":2,\"299\":3,\"496\":2,\"506\":2,\"619\":1,\"628\":1,\"752\":1,\"760\":2,\"941\":1,\"959\":1,\"1057\":4,\"1072\":1,\"1098\":1,\"1171\":1,\"1172\":1,\"1206\":1,\"1465\":2,\"1477\":1,\"1530\":1,\"1551\":2,\"1553\":3,\"1554\":1,\"1563\":1,\"1600\":1,\"1603\":1,\"1622\":1,\"1739\":2,\"1773\":1,\"1778\":1,\"1800\":4,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1906\":2,\"1964\":3,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":1,\"2168\":3,\"2170\":2,\"2193\":3,\"2199\":1,\"2350\":2,\"2375\":1,\"2400\":1,\"2520\":4,\"2536\":1,\"2558\":1,\"2584\":1}}],[\"statedecoder\",{\"1\":{\"1252\":1,\"1253\":1,\"1254\":1}}],[\"state=none\",{\"1\":{\"1243\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1251\":1,\"1252\":1,\"1253\":2,\"1254\":1,\"1650\":1}}],[\"state=64\",{\"1\":{\"1241\":1}}],[\"statelessdecoder\",{\"0\":{\"1083\":1},\"1\":{\"1083\":2}}],[\"stateless\",{\"0\":{\"1083\":1},\"1\":{\"69\":1,\"116\":2,\"1063\":1,\"1083\":2}}],[\"states=none\",{\"1\":{\"1626\":1,\"1654\":1}}],[\"states\",{\"0\":{\"863\":1,\"866\":1},\"1\":{\"65\":3,\"102\":1,\"240\":2,\"653\":1,\"655\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"683\":1,\"684\":1,\"685\":5,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"691\":9,\"692\":1,\"693\":1,\"694\":3,\"695\":4,\"696\":3,\"697\":9,\"705\":1,\"706\":1,\"707\":1,\"725\":23,\"734\":3,\"758\":1,\"765\":3,\"773\":3,\"791\":1,\"798\":3,\"806\":25,\"824\":23,\"828\":3,\"838\":4,\"863\":5,\"865\":3,\"866\":7,\"891\":2,\"915\":6,\"918\":4,\"1046\":18,\"1065\":2,\"1066\":29,\"1073\":22,\"1074\":2,\"1075\":27,\"1081\":3,\"1083\":22,\"1119\":1,\"1133\":4,\"1140\":2,\"1148\":3,\"1149\":6,\"1150\":6,\"1169\":2,\"1178\":2,\"1179\":2,\"1180\":2,\"1181\":2,\"1190\":3,\"1200\":2,\"1203\":3,\"1214\":3,\"1215\":1,\"1222\":1,\"1244\":3,\"1269\":2,\"1270\":25,\"1272\":3,\"1273\":3,\"1282\":1,\"1598\":1,\"1652\":1,\"1653\":1,\"1654\":1,\"1842\":1,\"1957\":3,\"1958\":3,\"1960\":3,\"1980\":3,\"2001\":4,\"2002\":1,\"2004\":1,\"2029\":2,\"2078\":2,\"2079\":1,\"2083\":3,\"2259\":1,\"2260\":1,\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"state\",{\"0\":{\"635\":1,\"640\":1,\"642\":1,\"643\":1,\"865\":1,\"891\":1,\"913\":1,\"915\":1,\"918\":1,\"1129\":1,\"1130\":1,\"1146\":1,\"1147\":1,\"1156\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1177\":1,\"1183\":1,\"1188\":1,\"1199\":1,\"1209\":1,\"1212\":1,\"1217\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1241\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1259\":1,\"1261\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1297\":1,\"1311\":1,\"1314\":1,\"1317\":1,\"1319\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1351\":1,\"1352\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"2152\":1},\"1\":{\"24\":2,\"65\":4,\"68\":1,\"80\":1,\"85\":1,\"161\":3,\"174\":1,\"194\":1,\"605\":9,\"609\":1,\"610\":1,\"611\":1,\"612\":1,\"613\":1,\"635\":8,\"637\":4,\"640\":8,\"642\":7,\"643\":3,\"658\":6,\"676\":1,\"677\":5,\"678\":5,\"679\":5,\"681\":5,\"682\":5,\"684\":5,\"685\":5,\"686\":5,\"687\":5,\"688\":5,\"689\":5,\"690\":1,\"691\":4,\"692\":2,\"696\":4,\"697\":4,\"704\":3,\"705\":11,\"706\":28,\"707\":2,\"708\":1,\"710\":13,\"725\":9,\"729\":1,\"730\":1,\"734\":12,\"751\":2,\"752\":1,\"756\":1,\"758\":5,\"760\":1,\"766\":4,\"773\":4,\"778\":1,\"781\":1,\"782\":1,\"792\":5,\"793\":1,\"796\":4,\"797\":2,\"806\":9,\"807\":3,\"812\":1,\"815\":19,\"817\":7,\"819\":1,\"824\":9,\"828\":5,\"829\":6,\"830\":1,\"831\":1,\"835\":1,\"863\":1,\"865\":3,\"891\":2,\"913\":3,\"915\":1,\"918\":5,\"1046\":8,\"1060\":2,\"1061\":1,\"1062\":5,\"1063\":8,\"1064\":4,\"1065\":3,\"1066\":7,\"1067\":1,\"1069\":10,\"1073\":7,\"1074\":2,\"1075\":3,\"1081\":6,\"1082\":1,\"1083\":6,\"1084\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1129\":1,\"1130\":3,\"1133\":4,\"1134\":1,\"1136\":1,\"1137\":1,\"1140\":1,\"1141\":1,\"1145\":3,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1156\":2,\"1158\":1,\"1160\":9,\"1161\":8,\"1162\":10,\"1163\":7,\"1164\":9,\"1165\":4,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1174\":1,\"1176\":2,\"1177\":9,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":2,\"1184\":1,\"1188\":3,\"1190\":6,\"1193\":4,\"1196\":1,\"1197\":1,\"1199\":1,\"1200\":1,\"1203\":1,\"1204\":1,\"1206\":1,\"1207\":1,\"1209\":4,\"1211\":1,\"1212\":2,\"1214\":3,\"1215\":1,\"1217\":3,\"1218\":1,\"1220\":1,\"1221\":8,\"1222\":1,\"1224\":1,\"1229\":1,\"1231\":1,\"1233\":3,\"1235\":2,\"1237\":2,\"1239\":1,\"1241\":4,\"1243\":5,\"1244\":8,\"1245\":7,\"1246\":6,\"1247\":11,\"1248\":13,\"1249\":1,\"1251\":1,\"1252\":15,\"1253\":19,\"1254\":15,\"1257\":1,\"1259\":2,\"1261\":2,\"1263\":1,\"1265\":2,\"1267\":2,\"1269\":1,\"1270\":10,\"1271\":1,\"1272\":1,\"1273\":5,\"1274\":2,\"1276\":2,\"1278\":2,\"1279\":10,\"1280\":3,\"1281\":1,\"1282\":1,\"1284\":1,\"1297\":2,\"1311\":2,\"1314\":2,\"1317\":1,\"1319\":2,\"1322\":1,\"1323\":3,\"1327\":2,\"1328\":2,\"1329\":2,\"1339\":2,\"1340\":1,\"1341\":2,\"1342\":1,\"1343\":2,\"1351\":3,\"1352\":1,\"1355\":2,\"1356\":2,\"1357\":2,\"1358\":2,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1378\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":3,\"1452\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1514\":3,\"1515\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1523\":1,\"1524\":1,\"1525\":1,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":2,\"1532\":2,\"1534\":1,\"1535\":2,\"1537\":2,\"1539\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1552\":1,\"1554\":1,\"1555\":1,\"1557\":3,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":1,\"1575\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":2,\"1583\":1,\"1585\":3,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":3,\"1599\":1,\"1601\":1,\"1602\":2,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1612\":3,\"1613\":1,\"1615\":3,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1623\":3,\"1624\":1,\"1626\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1637\":3,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":2,\"1646\":1,\"1648\":2,\"1650\":2,\"1652\":3,\"1654\":3,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1718\":1,\"1719\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1781\":6,\"1782\":1,\"1785\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1799\":1,\"1806\":1,\"1828\":1,\"1840\":1,\"1842\":3,\"1853\":1,\"1854\":1,\"1855\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1902\":1,\"1906\":1,\"1907\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":6,\"1958\":2,\"1959\":6,\"1960\":6,\"1970\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2001\":2,\"2003\":1,\"2014\":3,\"2015\":3,\"2016\":3,\"2017\":3,\"2022\":3,\"2024\":1,\"2026\":1,\"2027\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2084\":1,\"2089\":1,\"2149\":1,\"2152\":7,\"2157\":1,\"2168\":1,\"2170\":1,\"2193\":3,\"2199\":1,\"2233\":1,\"2235\":1,\"2237\":1,\"2239\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2266\":1,\"2275\":1,\"2277\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2452\":1}}],[\"start=none\",{\"1\":{\"2315\":1}}],[\"start=false\",{\"1\":{\"1491\":1,\"1627\":1}}],[\"starts\",{\"1\":{\"239\":1,\"997\":1,\"998\":1}}],[\"starting\",{\"1\":{\"66\":2,\"91\":1,\"996\":1,\"1255\":1,\"1808\":1}}],[\"start\",{\"0\":{\"91\":1,\"149\":1,\"598\":1},\"1\":{\"28\":1,\"51\":1,\"56\":1,\"65\":1,\"69\":2,\"108\":3,\"109\":1,\"110\":1,\"149\":3,\"218\":2,\"225\":2,\"232\":2,\"239\":1,\"501\":1,\"598\":2,\"613\":1,\"691\":1,\"693\":1,\"697\":1,\"699\":1,\"794\":3,\"797\":1,\"818\":1,\"857\":1,\"870\":1,\"981\":1,\"997\":1,\"998\":1,\"1269\":1,\"1398\":3,\"1423\":3,\"1426\":1,\"1804\":1,\"1808\":5,\"1851\":1,\"1878\":1,\"1884\":1,\"1885\":3,\"1904\":1,\"1916\":1,\"2193\":1,\"2199\":3,\"2266\":2,\"2360\":1,\"2365\":2,\"2373\":2,\"2387\":2,\"2401\":2,\"2410\":1,\"2430\":1,\"2440\":1,\"2458\":1,\"2508\":2,\"2510\":2,\"2515\":2,\"2523\":1,\"2537\":2,\"2555\":2,\"2564\":1,\"2571\":1,\"2582\":1,\"2600\":3,\"2655\":2,\"2660\":2}}],[\"started\",{\"1\":{\"5\":1,\"87\":1}}],[\"stage=100\",{\"1\":{\"2568\":1}}],[\"stage=1\",{\"1\":{\"2568\":1}}],[\"stage2\",{\"0\":{\"2397\":1,\"2533\":1}}],[\"stage1\",{\"0\":{\"2397\":1,\"2533\":1}}],[\"stages\",{\"1\":{\"85\":1,\"91\":6,\"93\":1,\"99\":1,\"149\":2,\"168\":1,\"179\":1,\"235\":1,\"1468\":1,\"1485\":1,\"1489\":1,\"1624\":1,\"2372\":3,\"2374\":3,\"2397\":1,\"2429\":2,\"2434\":3,\"2440\":1,\"2441\":1,\"2533\":1,\"2554\":2,\"2557\":3,\"2564\":1}}],[\"stage3\",{\"1\":{\"46\":1}}],[\"stage\",{\"0\":{\"91\":2,\"149\":2,\"180\":1,\"186\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"2431\":1,\"2433\":1},\"1\":{\"19\":1,\"91\":4,\"98\":2,\"99\":3,\"105\":1,\"107\":1,\"110\":2,\"111\":1,\"137\":1,\"149\":7,\"162\":2,\"163\":2,\"164\":1,\"168\":2,\"180\":2,\"182\":1,\"185\":1,\"186\":2,\"187\":2,\"190\":1,\"235\":10,\"236\":3,\"237\":4,\"238\":3,\"239\":3,\"240\":5,\"241\":5,\"242\":3,\"1181\":1,\"1244\":2,\"1252\":2,\"1254\":1,\"1551\":2,\"1553\":2,\"1603\":1,\"2020\":1,\"2372\":2,\"2373\":20,\"2375\":12,\"2377\":2,\"2378\":2,\"2397\":2,\"2398\":4,\"2400\":2,\"2401\":3,\"2403\":2,\"2405\":2,\"2412\":2,\"2413\":2,\"2419\":1,\"2429\":3,\"2430\":14,\"2433\":8,\"2436\":2,\"2437\":2,\"2440\":3,\"2441\":2,\"2533\":2,\"2534\":2,\"2536\":2,\"2537\":3,\"2539\":2,\"2541\":2,\"2554\":3,\"2555\":20,\"2558\":6,\"2559\":6,\"2562\":2,\"2563\":2,\"2564\":2,\"2568\":4,\"2569\":2,\"2571\":2,\"2583\":1,\"2584\":14,\"2585\":4,\"2638\":1}}],[\"suffer\",{\"1\":{\"2467\":1}}],[\"sufficient\",{\"1\":{\"1639\":1,\"2387\":1,\"2418\":1}}],[\"suffixes\",{\"1\":{\"1551\":1,\"1553\":1}}],[\"suffix\",{\"1\":{\"610\":2,\"909\":2,\"1441\":1,\"1659\":5,\"1962\":3}}],[\"suffix=\",{\"1\":{\"610\":1,\"909\":1}}],[\"sus\",{\"1\":{\"2457\":1}}],[\"su\",{\"1\":{\"2259\":1}}],[\"sun\",{\"1\":{\"142\":1}}],[\"sudo\",{\"1\":{\"132\":5,\"134\":4,\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2372\":1}}],[\"sujay\",{\"1\":{\"130\":1}}],[\"subplot\",{\"1\":{\"2498\":6,\"2616\":6,\"2634\":6}}],[\"subprocess\",{\"1\":{\"203\":1}}],[\"subreporter\",{\"0\":{\"2199\":1},\"1\":{\"2185\":2,\"2193\":1,\"2198\":2,\"2199\":2,\"2201\":3,\"2203\":2}}],[\"subramanian\",{\"1\":{\"130\":1}}],[\"subcenter\",{\"0\":{\"2040\":2},\"1\":{\"2040\":3}}],[\"subclasses\",{\"1\":{\"752\":1,\"756\":1,\"760\":1,\"778\":1,\"831\":1,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1134\":1,\"1136\":1,\"1151\":1,\"1156\":1,\"1158\":1,\"1174\":1,\"1184\":1,\"1187\":2,\"1188\":1,\"1202\":2,\"1207\":1,\"1212\":1,\"1215\":1,\"1220\":1,\"1222\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1245\":1,\"1249\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":1,\"1518\":1,\"1520\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1543\":1,\"1547\":1,\"1549\":1,\"1555\":1,\"1561\":1,\"1564\":1,\"1573\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1596\":1,\"1598\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1613\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1641\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1656\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1761\":1,\"1763\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1801\":1,\"1806\":1,\"1890\":1,\"1902\":1,\"1907\":1,\"1912\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1976\":1,\"1978\":1,\"1981\":1,\"1984\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1994\":1,\"1997\":1,\"2024\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2050\":1,\"2052\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2149\":1,\"2168\":1,\"2233\":1,\"2237\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2275\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1}}],[\"subbands\",{\"1\":{\"1462\":2,\"1464\":3,\"1594\":1,\"1778\":1,\"1852\":1,\"1860\":8}}],[\"subwriter\",{\"1\":{\"1383\":3}}],[\"subwordlm\",{\"1\":{\"611\":2}}],[\"subword\",{\"1\":{\"609\":1,\"611\":1,\"620\":1}}],[\"submodules\",{\"0\":{\"1937\":1},\"1\":{\"1937\":2,\"2156\":1}}],[\"submodule\",{\"1\":{\"1655\":4,\"1719\":4}}],[\"submodel\",{\"0\":{\"1263\":1},\"1\":{\"1263\":1}}],[\"submission\",{\"1\":{\"1604\":1,\"1655\":1,\"1719\":1,\"2560\":1}}],[\"submit\",{\"1\":{\"141\":1,\"144\":1,\"2387\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":2,\"2487\":1,\"2491\":1,\"2497\":1,\"2500\":1,\"2501\":1,\"2503\":1,\"2525\":1,\"2545\":1,\"2560\":1}}],[\"submitting\",{\"1\":{\"40\":1,\"93\":1}}],[\"subtype\",{\"1\":{\"1407\":2,\"1426\":1}}],[\"subtracted\",{\"1\":{\"1639\":1,\"1640\":1}}],[\"subtract\",{\"0\":{\"922\":1},\"1\":{\"922\":1,\"1228\":1,\"1345\":1,\"1347\":1}}],[\"subtask\",{\"1\":{\"399\":13}}],[\"sub\",{\"0\":{\"398\":1},\"1\":{\"712\":6,\"728\":1,\"1052\":5,\"1053\":2,\"1071\":2,\"1095\":2,\"1115\":2,\"1383\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1793\":1,\"1795\":1,\"2040\":2,\"2193\":3,\"2444\":1,\"2445\":1,\"2446\":1,\"2468\":1,\"2472\":7,\"2476\":12,\"2648\":7}}],[\"substraction\",{\"0\":{\"1332\":1},\"1\":{\"1332\":1}}],[\"substantial\",{\"1\":{\"1274\":1}}],[\"subsequent\",{\"0\":{\"921\":1},\"1\":{\"921\":3,\"1101\":1,\"1735\":1}}],[\"subset\",{\"1\":{\"110\":1,\"922\":4,\"2396\":1,\"2411\":1,\"2518\":1,\"2532\":1,\"2568\":2,\"2638\":2}}],[\"subsampled\",{\"1\":{\"714\":2,\"715\":2,\"716\":2,\"717\":1,\"718\":2,\"719\":2,\"720\":2,\"721\":2,\"722\":2,\"777\":1}}],[\"subsample\",{\"0\":{\"889\":1},\"1\":{\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"777\":1,\"808\":2,\"878\":3,\"889\":1,\"933\":2,\"1222\":1,\"2295\":1,\"2296\":1,\"2600\":2}}],[\"subsampling\",{\"0\":{\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"777\":1,\"823\":1,\"864\":1},\"1\":{\"115\":3,\"122\":1,\"245\":2,\"624\":3,\"625\":3,\"676\":2,\"709\":2,\"714\":2,\"715\":2,\"716\":2,\"717\":2,\"718\":3,\"719\":2,\"720\":2,\"721\":2,\"722\":2,\"724\":3,\"742\":3,\"777\":2,\"799\":1,\"823\":4,\"858\":2,\"861\":2,\"864\":2,\"889\":2,\"909\":3,\"933\":1,\"936\":3,\"1053\":3,\"1058\":2,\"1071\":1,\"1085\":3,\"1095\":3,\"1097\":3,\"1655\":1,\"1719\":1,\"2439\":1}}],[\"successive\",{\"1\":{\"1804\":1}}],[\"success\",{\"1\":{\"1225\":1}}],[\"successful\",{\"1\":{\"1186\":1,\"1210\":1,\"2432\":1,\"2508\":1}}],[\"successfully\",{\"1\":{\"106\":1,\"137\":1,\"201\":1,\"202\":1,\"2372\":1,\"2474\":1,\"2543\":1,\"2553\":1,\"2568\":1,\"2649\":1}}],[\"sucessfully\",{\"1\":{\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1}}],[\"such\",{\"0\":{\"126\":1,\"2584\":1},\"1\":{\"3\":1,\"20\":1,\"21\":1,\"22\":2,\"49\":1,\"57\":2,\"64\":1,\"84\":1,\"85\":1,\"564\":1,\"613\":1,\"1339\":1,\"1342\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1758\":1,\"1759\":1,\"2385\":2,\"2393\":1,\"2419\":1,\"2420\":1,\"2427\":1,\"2430\":2,\"2452\":1,\"2529\":1,\"2550\":1,\"2555\":2,\"2558\":1,\"2564\":1,\"2574\":1,\"2644\":1}}],[\"sure\",{\"1\":{\"106\":1,\"2514\":1,\"2553\":1,\"2584\":1,\"2591\":1,\"2638\":1,\"2642\":1,\"2659\":1}}],[\"supplied\",{\"1\":{\"113\":1}}],[\"supplement\",{\"1\":{\"112\":1}}],[\"supposed\",{\"1\":{\"60\":1}}],[\"supporting\",{\"1\":{\"280\":1,\"2309\":3}}],[\"supported\",{\"0\":{\"49\":1,\"52\":1,\"133\":1},\"1\":{\"24\":2,\"31\":9,\"35\":1,\"49\":3,\"58\":1,\"74\":1,\"84\":1,\"102\":1,\"111\":1,\"112\":1,\"148\":1,\"286\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1337\":1,\"1462\":1,\"1543\":1,\"1655\":2,\"1656\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1719\":2,\"1927\":2,\"2363\":1,\"2388\":1,\"2415\":1,\"2421\":1,\"2452\":1,\"2524\":1,\"2544\":1,\"2597\":1,\"2653\":1}}],[\"support\",{\"1\":{\"6\":1,\"7\":1,\"11\":1,\"22\":1,\"30\":1,\"31\":2,\"45\":1,\"60\":1,\"84\":4,\"93\":1,\"113\":1,\"118\":2,\"120\":1,\"124\":1,\"133\":1,\"141\":1,\"148\":1,\"235\":1,\"240\":1,\"612\":1,\"1245\":1,\"1340\":1,\"1419\":1,\"1603\":1,\"1905\":1,\"1933\":1,\"2125\":1,\"2400\":1,\"2429\":1,\"2452\":1,\"2468\":1,\"2481\":1,\"2536\":1,\"2552\":1,\"2618\":1}}],[\"supports\",{\"1\":{\"5\":1,\"19\":1,\"20\":1,\"27\":1,\"35\":1,\"100\":1,\"103\":1,\"112\":1,\"148\":1,\"597\":1,\"650\":1,\"745\":1,\"746\":1,\"1406\":1,\"1688\":1,\"1756\":1,\"1927\":1,\"2431\":1,\"2452\":1,\"2506\":1,\"2558\":3,\"2574\":1}}],[\"supervivencia\",{\"1\":{\"2457\":1}}],[\"supervised\",{\"0\":{\"100\":1,\"2574\":1},\"1\":{\"84\":1,\"100\":1,\"161\":1,\"2429\":1,\"2552\":1,\"2574\":3}}],[\"superficial\",{\"1\":{\"2433\":1}}],[\"super\",{\"1\":{\"83\":1}}],[\"summed\",{\"1\":{\"1551\":1,\"1553\":1,\"1603\":1}}],[\"summarize\",{\"1\":{\"1773\":1,\"1778\":1,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"2082\":1,\"2170\":1,\"2240\":1,\"2278\":1}}],[\"summarized\",{\"1\":{\"197\":1,\"198\":1}}],[\"summarization\",{\"1\":{\"678\":1}}],[\"summarywriter\",{\"1\":{\"996\":1}}],[\"summary\",{\"0\":{\"2154\":1,\"2159\":2,\"2165\":1},\"1\":{\"607\":1,\"2154\":1,\"2159\":2,\"2165\":2,\"2185\":1,\"2193\":1,\"2199\":1,\"2201\":2,\"2203\":1}}],[\"sums\",{\"1\":{\"1143\":1}}],[\"sum\",{\"0\":{\"923\":1,\"1333\":1,\"2163\":1},\"1\":{\"77\":1,\"78\":1,\"102\":1,\"299\":1,\"691\":1,\"697\":1,\"707\":2,\"760\":2,\"797\":1,\"923\":2,\"1211\":1,\"1224\":1,\"1333\":1,\"1334\":2,\"1336\":1,\"1341\":1,\"1348\":1,\"1558\":1,\"1604\":2,\"1605\":1,\"1858\":1,\"2163\":1,\"2301\":1,\"2302\":1,\"2500\":1,\"2617\":1,\"2635\":1}}],[\"suitable\",{\"1\":{\"15\":1,\"74\":1,\"85\":1,\"135\":1,\"150\":1}}],[\"suggesting\",{\"1\":{\"2473\":1}}],[\"suggest\",{\"1\":{\"14\":1,\"2564\":1}}],[\"soviet\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"social\",{\"1\":{\"2467\":1}}],[\"socket\",{\"1\":{\"44\":4,\"45\":4}}],[\"solution\",{\"1\":{\"2270\":1,\"2272\":1}}],[\"solving\",{\"1\":{\"1639\":1}}],[\"solves\",{\"1\":{\"1695\":1}}],[\"solver\",{\"0\":{\"1530\":1,\"1600\":1,\"1603\":1,\"1622\":1},\"1\":{\"1218\":1,\"1524\":1,\"1525\":1,\"1530\":3,\"1563\":1,\"1600\":4,\"1603\":4,\"1611\":1,\"1622\":3}}],[\"solve\",{\"0\":{\"1035\":1,\"1742\":1},\"1\":{\"1035\":1,\"1639\":1,\"1742\":3,\"2585\":1}}],[\"sop\",{\"1\":{\"1975\":1}}],[\"soplin\",{\"1\":{\"130\":1}}],[\"sons\",{\"1\":{\"1705\":1}}],[\"song\",{\"1\":{\"1605\":1}}],[\"sot\",{\"1\":{\"491\":2,\"2128\":1,\"2129\":1,\"2137\":1}}],[\"sota\",{\"1\":{\"21\":1}}],[\"sos=0\",{\"1\":{\"612\":1}}],[\"sos\",{\"0\":{\"852\":2},\"1\":{\"255\":2,\"259\":2,\"691\":2,\"693\":2,\"697\":2,\"698\":3,\"699\":3,\"797\":1,\"852\":4,\"857\":2,\"870\":2,\"1057\":1,\"1171\":1,\"1219\":1,\"1220\":1,\"1955\":1,\"1975\":1,\"2076\":1,\"2294\":1,\"2348\":1,\"2373\":1,\"2555\":1}}],[\"soft\",{\"1\":{\"700\":1,\"1048\":1,\"1138\":1,\"1139\":1,\"1528\":2}}],[\"softmax\",{\"0\":{\"1106\":1,\"1108\":1},\"1\":{\"249\":2,\"295\":3,\"605\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"688\":1,\"700\":3,\"703\":2,\"731\":1,\"758\":1,\"759\":1,\"1065\":2,\"1106\":1,\"1108\":1,\"1133\":1,\"1145\":6,\"1186\":2,\"1190\":1,\"1204\":1,\"1214\":1,\"1244\":1,\"1269\":1,\"1273\":1,\"1871\":2,\"1873\":2,\"2030\":1,\"2294\":1,\"2394\":2,\"2530\":2,\"2543\":4}}],[\"softplus\",{\"1\":{\"115\":3,\"1067\":8,\"1096\":5}}],[\"soy\",{\"1\":{\"201\":1}}],[\"soxi\",{\"1\":{\"203\":1}}],[\"sox\",{\"1\":{\"132\":3,\"196\":1,\"200\":1,\"568\":1,\"952\":5,\"1927\":3,\"2372\":1,\"2385\":1}}],[\"sort\",{\"1\":{\"265\":1,\"269\":1,\"429\":2,\"700\":2,\"987\":3,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":4,\"1048\":2,\"1138\":2,\"1139\":2,\"1553\":2,\"2007\":2,\"2008\":2,\"2009\":2,\"2010\":4,\"2012\":4,\"2193\":4,\"2315\":1,\"2568\":1,\"2638\":1}}],[\"sortagrad\",{\"1\":{\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"981\":1}}],[\"sorting\",{\"1\":{\"74\":1,\"75\":1,\"2011\":1,\"2638\":2}}],[\"sortedbatchsampler\",{\"0\":{\"2010\":1},\"1\":{\"2010\":1}}],[\"sorteddata\",{\"1\":{\"1004\":1}}],[\"sorted\",{\"0\":{\"2010\":1},\"1\":{\"73\":1,\"75\":2,\"429\":2,\"691\":1,\"697\":1,\"700\":1,\"745\":1,\"746\":1,\"797\":1,\"933\":1,\"1003\":2,\"1004\":1,\"1005\":2,\"1048\":1,\"1138\":1,\"1139\":1,\"2010\":2,\"2012\":2}}],[\"souden\",{\"1\":{\"885\":1,\"1524\":1,\"1611\":1,\"1706\":1,\"1707\":1}}],[\"soundscpwriter\",{\"0\":{\"1407\":1},\"1\":{\"1407\":2,\"1408\":2}}],[\"soundscpreader\",{\"0\":{\"1405\":1},\"1\":{\"1391\":1,\"1402\":1,\"1405\":2,\"1406\":3}}],[\"sounds\",{\"1\":{\"1390\":1}}],[\"soundwriter\",{\"0\":{\"994\":1},\"1\":{\"994\":2,\"995\":1}}],[\"soundreader\",{\"0\":{\"993\":1},\"1\":{\"993\":2}}],[\"soundhdf5writer\",{\"0\":{\"991\":1},\"1\":{\"991\":2,\"992\":1}}],[\"soundhdf5reader\",{\"0\":{\"990\":1},\"1\":{\"990\":2}}],[\"soundhdf5file\",{\"0\":{\"989\":1},\"1\":{\"989\":3}}],[\"sound\",{\"0\":{\"1390\":1,\"1405\":1,\"1407\":1,\"1426\":1,\"2225\":1,\"2229\":1,\"2231\":1},\"1\":{\"57\":3,\"247\":2,\"496\":2,\"506\":2,\"522\":4,\"525\":2,\"533\":2,\"989\":1,\"1390\":1,\"1405\":2,\"1407\":2,\"1426\":2,\"1803\":1,\"1928\":1,\"2183\":1,\"2190\":1,\"2225\":1,\"2229\":2,\"2231\":1}}],[\"soundfile\",{\"0\":{\"52\":1,\"1426\":1},\"1\":{\"49\":4,\"1426\":2,\"2359\":2,\"2367\":2,\"2368\":1,\"2371\":1,\"2372\":2,\"2384\":1,\"2386\":2,\"2387\":2,\"2456\":1,\"2460\":1,\"2470\":2,\"2476\":2,\"2478\":2,\"2485\":2,\"2486\":1,\"2490\":1,\"2494\":1,\"2497\":2,\"2501\":2,\"2514\":1,\"2521\":1,\"2522\":2,\"2580\":2,\"2581\":2,\"2604\":2,\"2605\":1,\"2607\":2,\"2609\":1,\"2612\":1,\"2614\":2,\"2615\":2,\"2621\":2,\"2622\":1,\"2624\":2,\"2626\":1,\"2630\":1,\"2632\":2,\"2633\":2,\"2647\":2,\"2659\":1}}],[\"soundifile\",{\"1\":{\"49\":1}}],[\"sources\",{\"1\":{\"280\":1,\"691\":2,\"697\":3,\"1655\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"1739\":2,\"2501\":1,\"2523\":1}}],[\"source\",{\"0\":{\"280\":1,\"528\":1,\"1102\":1},\"1\":{\"5\":3,\"30\":1,\"48\":1,\"57\":1,\"74\":1,\"85\":1,\"130\":3,\"187\":1,\"235\":2,\"280\":3,\"294\":1,\"528\":2,\"560\":1,\"676\":5,\"731\":6,\"781\":5,\"784\":1,\"785\":1,\"794\":3,\"806\":2,\"826\":1,\"936\":3,\"1011\":1,\"1036\":1,\"1046\":1,\"1049\":3,\"1050\":3,\"1051\":1,\"1052\":3,\"1054\":1,\"1055\":1,\"1056\":3,\"1066\":2,\"1068\":2,\"1073\":2,\"1075\":2,\"1076\":2,\"1083\":1,\"1101\":1,\"1102\":2,\"1116\":9,\"1270\":2,\"1300\":1,\"1306\":1,\"1454\":1,\"1540\":1,\"1566\":1,\"1611\":1,\"1714\":1,\"1719\":2,\"1986\":1,\"2005\":1,\"2181\":2,\"2264\":2,\"2294\":1,\"2363\":1,\"2383\":1,\"2393\":1,\"2427\":1,\"2452\":2,\"2456\":1,\"2460\":1,\"2461\":1,\"2468\":2,\"2506\":1,\"2518\":1,\"2521\":1,\"2529\":1,\"2550\":1,\"2568\":1,\"2653\":1}}],[\"soon\",{\"1\":{\"43\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"so\",{\"1\":{\"15\":1,\"45\":1,\"57\":2,\"59\":1,\"60\":2,\"62\":1,\"83\":1,\"85\":1,\"109\":1,\"126\":1,\"132\":1,\"134\":1,\"136\":1,\"144\":1,\"148\":1,\"149\":1,\"204\":1,\"240\":1,\"295\":1,\"607\":1,\"608\":1,\"875\":1,\"1015\":1,\"1198\":2,\"1244\":1,\"1252\":1,\"1254\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1693\":2,\"1755\":2,\"1766\":1,\"1810\":1,\"1861\":1,\"1956\":1,\"2011\":1,\"2355\":1,\"2372\":1,\"2384\":1,\"2389\":1,\"2394\":1,\"2395\":1,\"2408\":1,\"2410\":1,\"2412\":1,\"2414\":1,\"2422\":1,\"2424\":1,\"2432\":1,\"2449\":1,\"2450\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2530\":1,\"2531\":1,\"2545\":1,\"2547\":1,\"2564\":1,\"2568\":1,\"2573\":1,\"2574\":1,\"2583\":1,\"2640\":1}}],[\"someki\",{\"1\":{\"2597\":2}}],[\"some2\",{\"1\":{\"1969\":1}}],[\"some1\",{\"1\":{\"1969\":1}}],[\"somehow\",{\"1\":{\"1906\":1}}],[\"sometype\",{\"1\":{\"59\":5}}],[\"sometimes\",{\"1\":{\"25\":1,\"69\":1,\"102\":1,\"1251\":1,\"1639\":1}}],[\"somewhere\",{\"1\":{\"47\":1,\"2158\":5}}],[\"some\",{\"1\":{\"3\":1,\"5\":1,\"21\":2,\"32\":1,\"34\":1,\"38\":1,\"47\":4,\"49\":2,\"57\":4,\"58\":7,\"64\":1,\"80\":1,\"82\":2,\"84\":1,\"85\":3,\"91\":2,\"100\":1,\"102\":1,\"112\":3,\"115\":3,\"119\":1,\"126\":1,\"127\":4,\"132\":2,\"135\":1,\"136\":2,\"141\":1,\"142\":1,\"149\":2,\"150\":1,\"226\":1,\"238\":1,\"595\":1,\"730\":1,\"958\":1,\"987\":2,\"1015\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1383\":3,\"1389\":4,\"1391\":4,\"1395\":4,\"1397\":4,\"1402\":4,\"1404\":4,\"1406\":8,\"1414\":4,\"1416\":4,\"1421\":4,\"1425\":12,\"1525\":1,\"1560\":2,\"1639\":1,\"1739\":1,\"1905\":1,\"1973\":1,\"2090\":1,\"2142\":1,\"2212\":1,\"2309\":1,\"2384\":2,\"2385\":1,\"2387\":1,\"2389\":3,\"2394\":1,\"2408\":2,\"2419\":2,\"2422\":3,\"2430\":1,\"2433\":1,\"2447\":1,\"2448\":1,\"2449\":2,\"2450\":1,\"2451\":1,\"2461\":1,\"2464\":1,\"2465\":2,\"2467\":2,\"2480\":1,\"2481\":2,\"2499\":1,\"2500\":1,\"2502\":1,\"2503\":2,\"2508\":1,\"2523\":1,\"2525\":4,\"2530\":1,\"2542\":1,\"2545\":3,\"2555\":1,\"2564\":1,\"2572\":1,\"2584\":2,\"2585\":1,\"2593\":1,\"2617\":2,\"2635\":2,\"2638\":1,\"2645\":1}}],[\"spgispeech\",{\"1\":{\"2357\":2,\"2578\":2}}],[\"spriet\",{\"1\":{\"1715\":1}}],[\"spring2023\",{\"0\":{\"161\":1,\"2725\":1}}],[\"spring\",{\"0\":{\"2380\":1,\"2388\":1,\"2406\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1},\"1\":{\"152\":1,\"2380\":1,\"2388\":1,\"2421\":1,\"2524\":1,\"2544\":1}}],[\"spoofing\",{\"1\":{\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1113\":2,\"2543\":2}}],[\"spoken\",{\"0\":{\"157\":1,\"2463\":1,\"2467\":1},\"1\":{\"130\":2,\"150\":1,\"157\":1,\"161\":1,\"2384\":1,\"2452\":2,\"2463\":1,\"2464\":1,\"2467\":4,\"2504\":1,\"2646\":1}}],[\"spline\",{\"0\":{\"1026\":1,\"1887\":1,\"1888\":1},\"1\":{\"1026\":1,\"1031\":1,\"1887\":1,\"1888\":1}}],[\"splitted\",{\"1\":{\"2046\":1}}],[\"splitting\",{\"1\":{\"172\":1,\"1015\":1,\"1652\":1,\"1654\":1}}],[\"splitjson\",{\"0\":{\"576\":1},\"1\":{\"576\":2}}],[\"splits\",{\"1\":{\"441\":2,\"1436\":1,\"1511\":1,\"1644\":1}}],[\"split\",{\"0\":{\"441\":1,\"1743\":1,\"2147\":1},\"1\":{\"28\":1,\"80\":1,\"148\":2,\"174\":1,\"175\":1,\"194\":1,\"203\":1,\"217\":2,\"224\":2,\"231\":1,\"238\":4,\"441\":3,\"576\":1,\"1462\":3,\"1463\":3,\"1539\":1,\"1743\":2,\"2063\":1,\"2074\":1,\"2075\":1,\"2131\":1,\"2147\":1,\"2360\":1,\"2386\":1,\"2430\":1,\"2458\":1,\"2472\":5,\"2474\":5,\"2476\":15,\"2478\":1,\"2514\":2,\"2523\":1,\"2555\":1,\"2582\":1,\"2600\":1,\"2648\":5,\"2649\":5,\"2659\":2}}],[\"spsd\",{\"1\":{\"885\":2,\"1706\":2,\"1708\":1,\"1712\":2,\"1715\":2}}],[\"spc\",{\"1\":{\"702\":2,\"821\":2,\"2325\":2}}],[\"spcs=none\",{\"1\":{\"821\":1}}],[\"spcs\",{\"1\":{\"702\":2,\"821\":1}}],[\"spm\",{\"0\":{\"271\":1,\"272\":1,\"2664\":1},\"1\":{\"271\":1,\"272\":1}}],[\"spatio\",{\"1\":{\"1696\":1,\"1697\":1,\"1702\":1}}],[\"spatially\",{\"1\":{\"1715\":1}}],[\"spatialdropout\",{\"0\":{\"1256\":1},\"1\":{\"1256\":2}}],[\"spatial\",{\"1\":{\"1054\":1,\"1153\":1,\"1256\":2,\"1670\":2,\"1671\":2}}],[\"sparse\",{\"0\":{\"1036\":1},\"1\":{\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"1036\":1}}],[\"spans\",{\"1\":{\"1269\":3}}],[\"span\",{\"1\":{\"1269\":1}}],[\"spandh\",{\"1\":{\"940\":1}}],[\"spanish\",{\"0\":{\"201\":1},\"1\":{\"201\":3,\"296\":1,\"461\":1,\"2357\":2,\"2518\":1,\"2523\":1,\"2578\":2}}],[\"spacing\",{\"1\":{\"712\":1,\"1052\":1}}],[\"space=false\",{\"1\":{\"2122\":2,\"2126\":1}}],[\"space\",{\"0\":{\"2147\":1},\"1\":{\"117\":2,\"175\":1,\"194\":1,\"217\":1,\"224\":1,\"251\":2,\"255\":2,\"259\":2,\"461\":6,\"579\":2,\"750\":3,\"766\":3,\"825\":1,\"1057\":3,\"1059\":3,\"1064\":3,\"1114\":1,\"1115\":4,\"1142\":4,\"1143\":1,\"1155\":3,\"1171\":1,\"1172\":1,\"1173\":3,\"1206\":1,\"1245\":1,\"1248\":1,\"1269\":7,\"1374\":1,\"1376\":1,\"1406\":1,\"1892\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":2,\"2120\":1,\"2121\":1,\"2122\":1,\"2126\":2,\"2130\":1,\"2137\":1,\"2147\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2280\":1,\"2507\":2,\"2510\":4,\"2513\":2,\"2543\":1,\"2585\":1}}],[\"spaces\",{\"0\":{\"1129\":1,\"1130\":1,\"1146\":1,\"1147\":1,\"1156\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1177\":1,\"1183\":1,\"1188\":1,\"1199\":1,\"1209\":1,\"1212\":1,\"1217\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1241\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1259\":1,\"1261\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1297\":1,\"1311\":1,\"1314\":1,\"1317\":1,\"1319\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1351\":1,\"1352\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1},\"1\":{\"3\":1,\"245\":2,\"301\":2,\"1129\":1,\"1130\":2,\"1156\":1,\"1160\":2,\"1161\":1,\"1162\":2,\"1163\":2,\"1164\":2,\"1165\":1,\"1166\":1,\"1177\":2,\"1183\":1,\"1188\":2,\"1199\":1,\"1209\":1,\"1212\":1,\"1217\":2,\"1233\":2,\"1235\":1,\"1237\":1,\"1241\":2,\"1245\":2,\"1247\":2,\"1248\":2,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1259\":1,\"1261\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1279\":2,\"1280\":2,\"1297\":2,\"1311\":2,\"1314\":2,\"1317\":1,\"1319\":2,\"1322\":1,\"1323\":3,\"1327\":2,\"1328\":2,\"1329\":2,\"1339\":2,\"1340\":1,\"1341\":2,\"1342\":1,\"1343\":2,\"1351\":2,\"1352\":1,\"1355\":2,\"1356\":2,\"1357\":2,\"1358\":2}}],[\"sp\",{\"1\":{\"110\":3,\"1605\":1,\"2357\":5,\"2387\":1,\"2454\":1,\"2455\":2,\"2460\":3,\"2461\":2,\"2578\":5,\"2589\":1,\"2590\":1,\"2600\":2}}],[\"spktrainer\",{\"0\":{\"2198\":1},\"1\":{\"2114\":1,\"2198\":2}}],[\"spks\",{\"1\":{\"1568\":1,\"1778\":1,\"1804\":2,\"1805\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"1877\":1,\"1878\":2,\"2001\":2,\"2002\":2,\"2003\":1,\"2004\":2,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2279\":2,\"2514\":3,\"2659\":3}}],[\"spk=1\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"spk=0\",{\"1\":{\"1524\":1}}],[\"spk=2\",{\"1\":{\"1460\":1}}],[\"spkn\",{\"1\":{\"1375\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1534\":1,\"1539\":1,\"1558\":1,\"1611\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1669\":1,\"1671\":1}}],[\"spk2sid\",{\"1\":{\"2514\":3,\"2659\":3}}],[\"spk2enroll\",{\"1\":{\"2200\":1}}],[\"spk2\",{\"1\":{\"1375\":1,\"1400\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1534\":1,\"1539\":1,\"1552\":2,\"1558\":1,\"1611\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1669\":1,\"1671\":1,\"2372\":1,\"2497\":1,\"2498\":2,\"2614\":1,\"2615\":1,\"2616\":2,\"2632\":1,\"2633\":1,\"2634\":2}}],[\"spk2utt=none\",{\"1\":{\"941\":1}}],[\"spk2utt\",{\"1\":{\"237\":3,\"496\":2,\"506\":2,\"2197\":3,\"2373\":1,\"2385\":3,\"2387\":1,\"2395\":1,\"2412\":1,\"2430\":1,\"2492\":2,\"2531\":1,\"2555\":1,\"2568\":2,\"2628\":2,\"2638\":1}}],[\"spk1\",{\"1\":{\"1375\":1,\"1400\":2,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1534\":1,\"1539\":1,\"1552\":2,\"1558\":1,\"1611\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1669\":1,\"1671\":1,\"2372\":1,\"2497\":1,\"2498\":2,\"2614\":1,\"2615\":1,\"2616\":2,\"2632\":1,\"2633\":1,\"2634\":2}}],[\"spkr\",{\"1\":{\"629\":1}}],[\"spkrs=2\",{\"1\":{\"749\":1}}],[\"spkrs\",{\"1\":{\"249\":1,\"251\":1,\"530\":2,\"546\":2,\"551\":2,\"557\":2,\"794\":3}}],[\"spk\",{\"0\":{\"429\":1,\"437\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2114\":1,\"2198\":1,\"2704\":1},\"1\":{\"79\":1,\"140\":1,\"161\":1,\"244\":1,\"343\":2,\"397\":1,\"398\":2,\"429\":12,\"437\":2,\"735\":1,\"754\":5,\"821\":5,\"826\":5,\"1113\":1,\"1132\":1,\"1363\":1,\"1366\":1,\"1367\":1,\"1371\":5,\"1374\":2,\"1375\":7,\"1376\":3,\"1377\":1,\"1400\":1,\"1446\":1,\"1454\":3,\"1463\":3,\"1505\":3,\"1515\":3,\"1516\":4,\"1523\":3,\"1524\":3,\"1528\":3,\"1529\":3,\"1530\":1,\"1531\":2,\"1534\":3,\"1539\":3,\"1551\":1,\"1552\":3,\"1553\":4,\"1554\":1,\"1558\":3,\"1563\":1,\"1600\":1,\"1603\":1,\"1611\":2,\"1622\":2,\"1626\":3,\"1645\":3,\"1654\":3,\"1655\":1,\"1658\":3,\"1659\":5,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":3,\"1671\":3,\"1705\":2,\"1719\":3,\"1778\":4,\"1804\":4,\"1805\":3,\"1850\":3,\"1851\":6,\"1852\":2,\"1877\":3,\"1878\":4,\"2001\":5,\"2002\":6,\"2003\":2,\"2004\":5,\"2030\":2,\"2032\":2,\"2034\":2,\"2036\":2,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":5,\"2047\":2,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2086\":6,\"2087\":6,\"2090\":6,\"2095\":6,\"2114\":2,\"2184\":1,\"2198\":2,\"2200\":2,\"2243\":6,\"2244\":6,\"2255\":6,\"2263\":6,\"2264\":6,\"2279\":6,\"2412\":5,\"2492\":1,\"2512\":3,\"2513\":2,\"2514\":10,\"2628\":1,\"2638\":1,\"2642\":1,\"2657\":3,\"2659\":10}}],[\"spkpreprocessor\",{\"0\":{\"2197\":1},\"1\":{\"79\":1,\"2197\":2}}],[\"sph\",{\"1\":{\"53\":2,\"1927\":1,\"2385\":1}}],[\"sph2pipe\",{\"1\":{\"53\":3,\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2384\":1,\"2385\":2,\"2386\":1,\"2568\":4}}],[\"sphere\",{\"0\":{\"53\":1}}],[\"sphinx\",{\"1\":{\"10\":1,\"12\":1,\"13\":1,\"528\":2,\"564\":1}}],[\"spicify\",{\"1\":{\"47\":1}}],[\"speak\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":2,\"2582\":1}}],[\"speakertask\",{\"0\":{\"2114\":1},\"1\":{\"2114\":2}}],[\"speakerbeam\",{\"0\":{\"1659\":1},\"1\":{\"1659\":3}}],[\"speakers\",{\"1\":{\"237\":2,\"633\":1,\"794\":1,\"1375\":2,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1529\":1,\"1531\":1,\"1534\":1,\"1539\":1,\"1551\":2,\"1553\":2,\"1558\":1,\"1622\":1,\"1626\":1,\"1645\":2,\"1654\":1,\"1655\":1,\"1658\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1669\":1,\"1671\":1,\"1719\":1,\"1804\":1,\"1851\":1,\"1878\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2030\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2200\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2385\":1}}],[\"speaker\",{\"0\":{\"2406\":1,\"2410\":1,\"2415\":1,\"2416\":1,\"2417\":1,\"2505\":1,\"2511\":1,\"2514\":1,\"2594\":1,\"2652\":1,\"2656\":1,\"2659\":1},\"1\":{\"150\":1,\"161\":2,\"237\":3,\"243\":2,\"265\":2,\"269\":2,\"343\":1,\"429\":1,\"437\":1,\"491\":2,\"506\":1,\"735\":1,\"754\":5,\"821\":5,\"826\":5,\"987\":2,\"1114\":1,\"1371\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1400\":3,\"1419\":1,\"1515\":1,\"1516\":1,\"1524\":1,\"1528\":1,\"1551\":4,\"1553\":3,\"1554\":3,\"1659\":5,\"1660\":1,\"1661\":1,\"1662\":1,\"1665\":1,\"1773\":6,\"1778\":4,\"1804\":5,\"1805\":4,\"1837\":4,\"1850\":2,\"1851\":6,\"1877\":4,\"1878\":5,\"2001\":4,\"2002\":6,\"2004\":4,\"2030\":1,\"2040\":1,\"2044\":1,\"2046\":9,\"2049\":1,\"2054\":1,\"2055\":1,\"2064\":1,\"2068\":1,\"2070\":1,\"2082\":6,\"2086\":6,\"2087\":6,\"2090\":6,\"2095\":6,\"2128\":1,\"2129\":1,\"2179\":1,\"2197\":1,\"2198\":1,\"2200\":1,\"2240\":6,\"2243\":6,\"2244\":6,\"2255\":6,\"2263\":6,\"2264\":6,\"2278\":6,\"2279\":6,\"2373\":3,\"2385\":4,\"2406\":1,\"2407\":1,\"2409\":1,\"2410\":4,\"2411\":1,\"2412\":2,\"2414\":3,\"2415\":7,\"2416\":1,\"2417\":1,\"2418\":2,\"2419\":4,\"2420\":1,\"2430\":3,\"2508\":1,\"2512\":6,\"2513\":2,\"2514\":8,\"2515\":3,\"2518\":1,\"2555\":3,\"2568\":2,\"2657\":3,\"2659\":6}}],[\"spemb\",{\"1\":{\"754\":1,\"821\":1,\"826\":1}}],[\"spemb=none\",{\"1\":{\"754\":1,\"821\":1,\"826\":1}}],[\"spembs=spembs\",{\"1\":{\"2515\":1,\"2660\":1}}],[\"spembs=none\",{\"1\":{\"735\":1,\"754\":2,\"821\":2,\"826\":2}}],[\"spembs\",{\"1\":{\"735\":1,\"754\":2,\"821\":2,\"826\":2,\"1773\":6,\"1778\":4,\"1804\":5,\"1805\":4,\"1837\":4,\"1850\":2,\"1851\":5,\"1877\":4,\"1878\":5,\"1984\":1,\"1985\":1,\"2001\":3,\"2002\":5,\"2004\":3,\"2082\":6,\"2086\":5,\"2087\":5,\"2090\":5,\"2095\":5,\"2240\":6,\"2243\":5,\"2244\":5,\"2255\":5,\"2263\":5,\"2264\":5,\"2278\":6,\"2279\":5,\"2514\":3,\"2659\":3}}],[\"specs\",{\"1\":{\"2498\":1,\"2616\":1,\"2634\":1}}],[\"specgram\",{\"1\":{\"1785\":2}}],[\"spectogram\",{\"1\":{\"2260\":1,\"2487\":1}}],[\"spectogramdenoiser\",{\"0\":{\"2260\":1},\"1\":{\"2260\":1}}],[\"specturm\",{\"1\":{\"1646\":2}}],[\"spectra\",{\"1\":{\"1515\":1}}],[\"spectral\",{\"0\":{\"887\":1,\"1711\":1},\"1\":{\"887\":2,\"1198\":1,\"1523\":1,\"1711\":2,\"1767\":1,\"1768\":1,\"1778\":2,\"1782\":1,\"1793\":1,\"1795\":1,\"1801\":3,\"1805\":2,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":5,\"1849\":7,\"1850\":2,\"1852\":2,\"1877\":2,\"1917\":2,\"2290\":1}}],[\"spectrogram2waveform\",{\"0\":{\"2317\":1},\"1\":{\"2317\":1}}],[\"spectrograms\",{\"1\":{\"821\":1,\"1781\":1}}],[\"spectrogram\",{\"0\":{\"404\":1,\"648\":1,\"945\":1,\"947\":1,\"951\":2,\"953\":1,\"954\":1,\"966\":1,\"967\":1,\"969\":2,\"970\":1,\"971\":1,\"1987\":1,\"1991\":1,\"2246\":1,\"2250\":1},\"1\":{\"235\":3,\"238\":2,\"239\":1,\"241\":1,\"242\":1,\"648\":3,\"701\":2,\"702\":1,\"802\":4,\"803\":3,\"804\":1,\"821\":1,\"945\":2,\"947\":1,\"950\":1,\"951\":4,\"953\":2,\"954\":1,\"956\":2,\"966\":2,\"967\":1,\"968\":1,\"969\":4,\"970\":2,\"971\":1,\"973\":2,\"1127\":1,\"1207\":1,\"1216\":2,\"1285\":1,\"1517\":1,\"1781\":1,\"1785\":3,\"1805\":1,\"1850\":1,\"1859\":4,\"1869\":2,\"1877\":1,\"1982\":1,\"1987\":2,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"2002\":1,\"2078\":2,\"2081\":2,\"2083\":2,\"2095\":1,\"2246\":2,\"2250\":1,\"2257\":1,\"2261\":1,\"2263\":2,\"2317\":1,\"2325\":2,\"2330\":2,\"2481\":1,\"2491\":1,\"2497\":1,\"2498\":8,\"2501\":1,\"2508\":1,\"2515\":1,\"2616\":7,\"2634\":7}}],[\"spectrums\",{\"0\":{\"2498\":1,\"2616\":1,\"2634\":1}}],[\"spectrum\",{\"1\":{\"100\":1,\"1463\":1,\"1510\":1,\"1516\":2,\"1522\":1,\"1551\":1,\"1553\":1,\"1643\":2,\"1644\":1,\"1671\":1,\"1859\":1,\"2090\":1}}],[\"spec=none\",{\"1\":{\"1570\":1}}],[\"specaug\",{\"0\":{\"1037\":1,\"1127\":2,\"1257\":3},\"1\":{\"1037\":2,\"1057\":2,\"1113\":1,\"1127\":5,\"1171\":1,\"1172\":1,\"1198\":1,\"1206\":1,\"1215\":2,\"1257\":7,\"1371\":1,\"1892\":1,\"1893\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2046\":1,\"2076\":1,\"2430\":1,\"2555\":1,\"2558\":1,\"2564\":1}}],[\"specaugment\",{\"0\":{\"950\":1},\"1\":{\"950\":2,\"1037\":2,\"1057\":1,\"1257\":1}}],[\"spec\",{\"0\":{\"943\":1,\"950\":1,\"955\":1,\"956\":1,\"965\":1,\"968\":2,\"972\":1,\"973\":1,\"1001\":1,\"1009\":1,\"1010\":1,\"1011\":1,\"1018\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1025\":1,\"1026\":1,\"1031\":1,\"1035\":1,\"1036\":1,\"1037\":1,\"1039\":1,\"1040\":1},\"1\":{\"648\":2,\"943\":3,\"950\":3,\"955\":4,\"956\":3,\"965\":3,\"968\":5,\"972\":5,\"973\":3,\"1001\":1,\"1009\":1,\"1010\":1,\"1011\":2,\"1018\":1,\"1019\":4,\"1021\":1,\"1022\":1,\"1025\":1,\"1026\":1,\"1031\":2,\"1035\":1,\"1036\":1,\"1037\":5,\"1039\":4,\"1040\":4,\"1158\":1,\"1517\":3,\"1570\":2,\"1606\":1,\"1643\":5,\"1644\":5,\"1785\":1,\"1859\":2,\"1914\":3,\"1915\":4,\"1940\":4,\"1987\":1,\"1989\":1,\"1991\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2498\":6,\"2616\":8,\"2634\":8}}],[\"special\",{\"1\":{\"74\":1,\"231\":1,\"1248\":1,\"1356\":1,\"2373\":1,\"2387\":1,\"2555\":1}}],[\"specifies\",{\"1\":{\"25\":1,\"52\":1,\"135\":1,\"1211\":1,\"1224\":1,\"1336\":1,\"1348\":1,\"2375\":1,\"2559\":3}}],[\"specified\",{\"0\":{\"91\":2,\"96\":1,\"149\":1},\"1\":{\"1\":1,\"3\":2,\"5\":1,\"21\":2,\"28\":4,\"29\":1,\"30\":1,\"37\":1,\"46\":1,\"57\":1,\"68\":1,\"91\":1,\"109\":1,\"144\":1,\"149\":3,\"503\":1,\"544\":1,\"637\":1,\"639\":1,\"642\":3,\"646\":1,\"658\":2,\"725\":1,\"745\":5,\"746\":5,\"806\":1,\"824\":1,\"889\":1,\"1011\":2,\"1025\":1,\"1046\":1,\"1073\":1,\"1083\":1,\"1095\":1,\"1103\":1,\"1244\":1,\"1252\":1,\"1270\":1,\"1430\":1,\"1553\":1,\"1603\":1,\"1670\":2,\"1693\":1,\"1739\":1,\"1755\":1,\"1834\":1,\"1876\":1,\"1915\":1,\"1926\":1,\"1940\":1,\"2148\":1,\"2373\":1,\"2430\":1,\"2431\":1,\"2440\":2,\"2555\":1,\"2558\":1}}],[\"specifically\",{\"1\":{\"1011\":1,\"1210\":1,\"1661\":1,\"1662\":1,\"2403\":1,\"2410\":1,\"2440\":1,\"2441\":1,\"2518\":1,\"2539\":1}}],[\"specification\",{\"1\":{\"19\":1,\"21\":1,\"645\":1}}],[\"specific\",{\"1\":{\"22\":1,\"23\":1,\"25\":1,\"48\":1,\"59\":2,\"99\":1,\"102\":1,\"119\":1,\"136\":1,\"144\":2,\"169\":1,\"181\":1,\"235\":1,\"754\":1,\"820\":1,\"821\":1,\"826\":1,\"1180\":1,\"1269\":1,\"1454\":1,\"1560\":2,\"1905\":1,\"1932\":1,\"2090\":1,\"2099\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2393\":1,\"2427\":1,\"2529\":2,\"2550\":2,\"2568\":1,\"2584\":2}}],[\"specifying\",{\"1\":{\"21\":1,\"85\":2,\"150\":1,\"240\":1,\"608\":1,\"2131\":1}}],[\"specify\",{\"1\":{\"19\":1,\"26\":1,\"28\":1,\"45\":1,\"47\":1,\"57\":1,\"71\":1,\"77\":1,\"78\":1,\"91\":1,\"99\":1,\"102\":2,\"106\":1,\"135\":3,\"240\":1,\"241\":1,\"276\":1,\"279\":1,\"281\":1,\"283\":1,\"284\":1,\"295\":2,\"987\":1,\"1015\":1,\"2372\":1,\"2373\":1,\"2429\":1,\"2430\":1,\"2552\":1,\"2555\":1,\"2567\":1,\"2584\":1,\"2600\":2}}],[\"speecrecognition\",{\"0\":{\"163\":1}}],[\"speech=data\",{\"1\":{\"2596\":2}}],[\"speech=speech\",{\"1\":{\"2515\":1,\"2592\":1,\"2660\":1}}],[\"speech=none\",{\"1\":{\"1190\":1,\"1273\":1}}],[\"speech2speech\",{\"1\":{\"2520\":3,\"2521\":1,\"2522\":1,\"2523\":1}}],[\"speech2understand\",{\"1\":{\"2474\":2,\"2649\":2}}],[\"speech2textstreaming\",{\"1\":{\"2460\":4,\"2591\":1,\"2592\":1}}],[\"speech2text\",{\"1\":{\"2358\":3,\"2359\":1,\"2360\":1,\"2455\":3,\"2456\":1,\"2459\":1,\"2472\":4,\"2474\":3,\"2476\":4,\"2478\":4,\"2500\":6,\"2520\":1,\"2579\":3,\"2580\":1,\"2581\":1,\"2582\":1,\"2592\":4,\"2596\":2,\"2600\":10,\"2617\":6,\"2635\":6,\"2648\":4,\"2649\":3}}],[\"speechbrain\",{\"0\":{\"2416\":1},\"1\":{\"2409\":1,\"2415\":1,\"2416\":5,\"2417\":2}}],[\"speeches\",{\"1\":{\"1714\":1}}],[\"speechdata\",{\"1\":{\"182\":1}}],[\"speechrecognition\",{\"0\":{\"162\":1}}],[\"speechprocessing\",{\"0\":{\"161\":1}}],[\"speech\",{\"0\":{\"155\":1,\"156\":1,\"158\":1,\"166\":1,\"175\":1,\"176\":1,\"183\":1,\"192\":1,\"193\":1,\"197\":1,\"198\":1,\"199\":1,\"201\":1,\"202\":1,\"233\":1,\"348\":1,\"2356\":1,\"2361\":1,\"2366\":1,\"2369\":1,\"2370\":1,\"2447\":1,\"2451\":1,\"2453\":1,\"2460\":1,\"2480\":1,\"2483\":1,\"2487\":1,\"2491\":1,\"2492\":1,\"2493\":1,\"2495\":1,\"2498\":1,\"2499\":1,\"2502\":1,\"2518\":2,\"2594\":1,\"2601\":1,\"2603\":1,\"2606\":1,\"2610\":1,\"2611\":1,\"2613\":1,\"2616\":1,\"2617\":1,\"2618\":1,\"2620\":1,\"2623\":1,\"2627\":1,\"2628\":1,\"2629\":1,\"2631\":1,\"2634\":1,\"2635\":1,\"2641\":1,\"2723\":1,\"2725\":1,\"2726\":1},\"1\":{\"57\":4,\"59\":1,\"60\":5,\"98\":1,\"108\":3,\"109\":1,\"110\":1,\"130\":9,\"152\":1,\"156\":1,\"161\":5,\"164\":5,\"169\":2,\"170\":1,\"171\":3,\"175\":2,\"181\":2,\"185\":2,\"193\":2,\"194\":1,\"195\":1,\"197\":1,\"198\":3,\"202\":1,\"203\":1,\"217\":2,\"235\":3,\"243\":1,\"245\":1,\"247\":2,\"249\":2,\"251\":1,\"257\":2,\"259\":1,\"261\":2,\"263\":1,\"265\":1,\"267\":1,\"295\":4,\"501\":1,\"528\":1,\"636\":1,\"681\":1,\"682\":1,\"691\":5,\"692\":2,\"693\":1,\"697\":6,\"701\":1,\"704\":1,\"705\":2,\"729\":1,\"736\":1,\"754\":1,\"762\":1,\"774\":2,\"786\":1,\"797\":3,\"826\":3,\"857\":1,\"880\":1,\"940\":1,\"1037\":1,\"1057\":19,\"1071\":5,\"1113\":9,\"1132\":1,\"1171\":10,\"1190\":1,\"1198\":2,\"1206\":4,\"1214\":1,\"1215\":1,\"1239\":1,\"1257\":1,\"1273\":9,\"1284\":1,\"1371\":11,\"1462\":3,\"1463\":3,\"1466\":1,\"1510\":1,\"1511\":2,\"1523\":1,\"1524\":4,\"1528\":1,\"1529\":1,\"1551\":16,\"1552\":19,\"1553\":19,\"1554\":14,\"1568\":1,\"1581\":1,\"1604\":1,\"1611\":2,\"1617\":1,\"1643\":1,\"1644\":3,\"1645\":1,\"1655\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":2,\"1671\":2,\"1696\":1,\"1697\":1,\"1698\":3,\"1704\":4,\"1706\":1,\"1707\":3,\"1708\":1,\"1712\":10,\"1713\":4,\"1715\":12,\"1719\":1,\"1739\":8,\"1798\":1,\"1804\":1,\"1805\":1,\"1837\":13,\"1850\":9,\"1852\":8,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":10,\"1878\":2,\"1892\":10,\"1893\":10,\"1923\":2,\"1975\":10,\"1980\":2,\"1984\":14,\"1985\":4,\"2001\":4,\"2002\":4,\"2003\":4,\"2004\":4,\"2027\":10,\"2032\":1,\"2046\":7,\"2076\":10,\"2104\":1,\"2178\":3,\"2179\":3,\"2181\":4,\"2184\":7,\"2191\":3,\"2194\":6,\"2195\":3,\"2200\":6,\"2235\":2,\"2236\":1,\"2240\":16,\"2243\":2,\"2244\":2,\"2255\":1,\"2257\":3,\"2261\":3,\"2262\":1,\"2263\":1,\"2264\":2,\"2265\":1,\"2277\":2,\"2278\":28,\"2279\":2,\"2294\":8,\"2354\":6,\"2355\":1,\"2359\":5,\"2360\":5,\"2363\":2,\"2367\":1,\"2368\":2,\"2369\":3,\"2371\":4,\"2372\":5,\"2373\":2,\"2380\":3,\"2385\":8,\"2387\":2,\"2388\":7,\"2395\":2,\"2399\":1,\"2400\":2,\"2411\":1,\"2415\":1,\"2417\":1,\"2421\":5,\"2429\":2,\"2430\":2,\"2441\":1,\"2447\":1,\"2448\":1,\"2451\":8,\"2452\":4,\"2456\":4,\"2458\":4,\"2460\":4,\"2467\":3,\"2468\":4,\"2473\":1,\"2480\":1,\"2481\":5,\"2485\":1,\"2486\":2,\"2487\":4,\"2490\":2,\"2491\":3,\"2494\":3,\"2497\":4,\"2498\":1,\"2499\":1,\"2500\":7,\"2501\":8,\"2502\":1,\"2506\":2,\"2514\":11,\"2515\":1,\"2516\":1,\"2518\":6,\"2521\":10,\"2522\":10,\"2523\":10,\"2524\":6,\"2531\":2,\"2535\":1,\"2536\":2,\"2543\":3,\"2544\":5,\"2554\":2,\"2555\":2,\"2574\":1,\"2580\":5,\"2581\":5,\"2582\":5,\"2583\":1,\"2586\":1,\"2592\":5,\"2600\":2,\"2601\":1,\"2604\":1,\"2605\":2,\"2606\":3,\"2607\":5,\"2609\":2,\"2610\":2,\"2612\":4,\"2614\":2,\"2615\":2,\"2617\":4,\"2618\":7,\"2621\":1,\"2622\":2,\"2623\":3,\"2624\":5,\"2626\":2,\"2627\":2,\"2630\":4,\"2632\":2,\"2633\":2,\"2635\":5,\"2638\":2,\"2653\":2,\"2659\":11}}],[\"speedup\",{\"1\":{\"1274\":1}}],[\"speedperturbation\",{\"0\":{\"952\":1},\"1\":{\"952\":2}}],[\"speeds\",{\"1\":{\"294\":1,\"1661\":1}}],[\"speed\",{\"0\":{\"294\":1,\"1946\":1},\"1\":{\"19\":1,\"23\":1,\"31\":1,\"119\":1,\"148\":1,\"294\":4,\"464\":2,\"470\":2,\"774\":1,\"952\":6,\"1245\":2,\"1639\":1,\"1778\":1,\"1804\":1,\"1805\":1,\"1877\":1,\"1878\":1,\"1905\":2,\"1946\":5,\"1947\":4,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2279\":1,\"2364\":1,\"2373\":3,\"2430\":4,\"2507\":1,\"2510\":2,\"2513\":1,\"2555\":4,\"2564\":1,\"2654\":1,\"2658\":1}}],[\"sh`\",{\"1\":{\"2385\":1}}],[\"shfl\",{\"1\":{\"1143\":1}}],[\"shef\",{\"1\":{\"940\":1}}],[\"shellcheck\",{\"1\":{\"2638\":1}}],[\"shell\",{\"0\":{\"90\":1},\"1\":{\"18\":1,\"46\":1,\"47\":1,\"90\":1,\"99\":1,\"142\":2,\"143\":1,\"2373\":1,\"2430\":1,\"2555\":1,\"2568\":1,\"2569\":1,\"2638\":1}}],[\"shuld\",{\"1\":{\"1274\":1}}],[\"shuffling\",{\"1\":{\"988\":2,\"997\":2,\"998\":2,\"2099\":1,\"2102\":1}}],[\"shufflingenabler\",{\"0\":{\"988\":1},\"1\":{\"988\":2}}],[\"shuffled\",{\"1\":{\"612\":1}}],[\"shuffle\",{\"0\":{\"1006\":1},\"1\":{\"265\":1,\"269\":1,\"613\":1,\"981\":2,\"997\":3,\"998\":3,\"1006\":1,\"1028\":1,\"1894\":1,\"1895\":2,\"1896\":1,\"1897\":1,\"1898\":2,\"1900\":3,\"2345\":1}}],[\"shuffle=true\",{\"1\":{\"174\":1,\"612\":1,\"997\":1,\"998\":1}}],[\"shuai\",{\"1\":{\"130\":1}}],[\"shun\",{\"1\":{\"130\":2}}],[\"shijing2014\",{\"1\":{\"2618\":1}}],[\"shih\",{\"1\":{\"1214\":1}}],[\"shi22d\",{\"1\":{\"130\":1}}],[\"shi2022muskits\",{\"1\":{\"130\":1}}],[\"shinnosuketakamichi\",{\"1\":{\"2363\":4,\"2506\":4,\"2653\":4}}],[\"shinnosuke\",{\"1\":{\"130\":1}}],[\"shinjiw\",{\"1\":{\"2618\":1}}],[\"shinji\",{\"1\":{\"130\":8,\"198\":1,\"2354\":1,\"2357\":4,\"2578\":4,\"2618\":1}}],[\"shi\",{\"1\":{\"130\":5,\"2356\":1,\"2380\":1,\"2388\":1,\"2406\":1,\"2447\":1,\"2516\":1,\"2524\":1,\"2576\":1,\"2618\":1}}],[\"shigekikarita\",{\"1\":{\"200\":1}}],[\"shigeki\",{\"1\":{\"130\":2,\"166\":1,\"176\":1,\"199\":1}}],[\"shift=none\",{\"1\":{\"648\":1,\"2498\":3,\"2616\":3,\"2634\":3}}],[\"shiftms\",{\"1\":{\"562\":2}}],[\"shifting\",{\"1\":{\"115\":1,\"1061\":1,\"1096\":1,\"1941\":2}}],[\"shifts\",{\"1\":{\"109\":1,\"2584\":2}}],[\"shift\",{\"0\":{\"1941\":1},\"1\":{\"14\":1,\"102\":1,\"108\":5,\"109\":5,\"110\":1,\"115\":1,\"247\":2,\"275\":3,\"282\":3,\"295\":2,\"297\":3,\"501\":2,\"509\":2,\"512\":2,\"519\":2,\"541\":2,\"585\":2,\"648\":2,\"771\":1,\"809\":1,\"834\":4,\"945\":1,\"947\":1,\"951\":1,\"953\":1,\"966\":1,\"967\":1,\"969\":1,\"970\":1,\"1061\":2,\"1076\":1,\"1096\":2,\"1808\":7,\"1896\":1,\"1941\":4,\"2082\":1,\"2188\":1,\"2210\":1,\"2211\":1,\"2266\":1,\"2317\":3,\"2325\":3}}],[\"shifted\",{\"1\":{\"14\":1,\"612\":1,\"1808\":1}}],[\"shall\",{\"1\":{\"1198\":1}}],[\"shannon\",{\"1\":{\"825\":1}}],[\"shanmugam\",{\"1\":{\"130\":1}}],[\"shape`\",{\"1\":{\"1011\":1,\"1133\":2,\"1214\":1,\"1273\":1,\"2001\":1}}],[\"shape=none\",{\"1\":{\"989\":1}}],[\"shape=\",{\"1\":{\"987\":2,\"1472\":1,\"1575\":1,\"1683\":1}}],[\"shape=false\",{\"1\":{\"982\":1,\"985\":1,\"990\":1,\"993\":1}}],[\"shapes\",{\"1\":{\"691\":1,\"697\":1,\"797\":1,\"1144\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1228\":1,\"1253\":2,\"1254\":1,\"1279\":1,\"1345\":1,\"1347\":1,\"1576\":1,\"1577\":1}}],[\"shape2\",{\"1\":{\"78\":2}}],[\"shape\",{\"0\":{\"281\":1,\"533\":1,\"1687\":1,\"1689\":1,\"2432\":1},\"1\":{\"73\":1,\"74\":6,\"75\":6,\"76\":8,\"77\":4,\"78\":9,\"79\":4,\"173\":2,\"174\":1,\"238\":2,\"239\":3,\"276\":1,\"281\":4,\"429\":2,\"533\":3,\"564\":9,\"629\":1,\"658\":1,\"691\":4,\"692\":1,\"695\":1,\"696\":1,\"697\":3,\"706\":2,\"708\":2,\"734\":1,\"744\":2,\"746\":1,\"764\":1,\"773\":1,\"793\":2,\"796\":1,\"797\":2,\"799\":1,\"815\":1,\"828\":1,\"830\":2,\"835\":5,\"899\":1,\"901\":1,\"1011\":2,\"1013\":3,\"1019\":1,\"1025\":4,\"1037\":1,\"1039\":1,\"1040\":1,\"1047\":2,\"1072\":2,\"1080\":2,\"1133\":1,\"1142\":6,\"1155\":2,\"1160\":1,\"1161\":1,\"1162\":2,\"1163\":1,\"1164\":1,\"1177\":1,\"1186\":6,\"1190\":2,\"1210\":3,\"1214\":1,\"1221\":1,\"1228\":3,\"1243\":2,\"1244\":2,\"1247\":1,\"1248\":3,\"1252\":2,\"1253\":3,\"1254\":1,\"1256\":3,\"1273\":1,\"1274\":1,\"1276\":1,\"1279\":2,\"1298\":5,\"1299\":5,\"1301\":7,\"1302\":5,\"1303\":5,\"1304\":7,\"1334\":2,\"1337\":3,\"1345\":3,\"1347\":3,\"1349\":3,\"1350\":3,\"1384\":2,\"1385\":4,\"1386\":2,\"1387\":4,\"1391\":1,\"1408\":1,\"1427\":2,\"1462\":2,\"1464\":2,\"1487\":1,\"1491\":1,\"1516\":1,\"1517\":2,\"1531\":2,\"1532\":1,\"1535\":1,\"1559\":2,\"1560\":4,\"1577\":1,\"1592\":1,\"1594\":3,\"1602\":3,\"1618\":3,\"1619\":3,\"1627\":1,\"1638\":3,\"1648\":1,\"1650\":1,\"1652\":1,\"1655\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1687\":1,\"1688\":5,\"1689\":1,\"1693\":5,\"1701\":4,\"1702\":2,\"1710\":2,\"1716\":1,\"1717\":1,\"1718\":2,\"1719\":5,\"1735\":3,\"1736\":1,\"1737\":1,\"1755\":5,\"1756\":5,\"1758\":1,\"1759\":1,\"1766\":1,\"1767\":4,\"1768\":3,\"1787\":3,\"1808\":15,\"1957\":1,\"1958\":1,\"1960\":1,\"1964\":1,\"2001\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":2,\"2012\":2,\"2106\":2,\"2210\":2,\"2267\":2,\"2271\":1,\"2367\":1,\"2432\":3,\"2485\":1,\"2585\":1,\"2604\":1,\"2616\":2,\"2621\":1,\"2634\":2}}],[\"sharma\",{\"1\":{\"130\":1}}],[\"sharing\",{\"0\":{\"99\":1},\"1\":{\"99\":1,\"115\":1,\"2388\":1,\"2524\":1}}],[\"sharded\",{\"0\":{\"35\":1},\"1\":{\"35\":2,\"429\":2,\"2186\":1,\"2202\":2,\"2204\":1}}],[\"shared\",{\"1\":{\"38\":1,\"40\":1,\"115\":1,\"116\":2,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"708\":1,\"729\":1,\"730\":1,\"752\":1,\"756\":1,\"758\":1,\"760\":1,\"778\":1,\"782\":1,\"793\":1,\"819\":1,\"830\":1,\"831\":1,\"835\":1,\"997\":2,\"1046\":1,\"1061\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1082\":1,\"1084\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1140\":1,\"1141\":1,\"1144\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1174\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1188\":1,\"1190\":1,\"1196\":1,\"1197\":1,\"1200\":1,\"1203\":1,\"1204\":1,\"1206\":1,\"1207\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1244\":1,\"1247\":1,\"1249\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1378\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1524\":1,\"1525\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1552\":1,\"1554\":1,\"1555\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":1,\"1575\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1601\":1,\"1602\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":2,\"1613\":1,\"1616\":1,\"1617\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1670\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1718\":1,\"1719\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1785\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1799\":1,\"1806\":1,\"1828\":1,\"1840\":1,\"1842\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1902\":1,\"1906\":1,\"1907\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1970\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2003\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2084\":1,\"2089\":1,\"2149\":1,\"2168\":1,\"2170\":1,\"2233\":1,\"2235\":1,\"2237\":1,\"2239\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2266\":1,\"2275\":1,\"2277\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2316\":1,\"2564\":1}}],[\"share\",{\"1\":{\"23\":1,\"28\":1,\"99\":1,\"119\":1,\"124\":1,\"277\":1,\"997\":1,\"1115\":2,\"1172\":2,\"1452\":1,\"1554\":1,\"1951\":1,\"1970\":2,\"2565\":1}}],[\"shot\",{\"1\":{\"1216\":1}}],[\"shoud\",{\"1\":{\"692\":1,\"693\":1,\"1652\":1}}],[\"shouldn\",{\"1\":{\"1245\":1,\"2430\":1,\"2555\":1}}],[\"should\",{\"1\":{\"3\":3,\"11\":1,\"20\":1,\"21\":1,\"24\":1,\"56\":1,\"57\":1,\"72\":1,\"84\":1,\"85\":1,\"101\":1,\"105\":1,\"112\":1,\"113\":2,\"121\":1,\"122\":1,\"124\":2,\"128\":1,\"142\":1,\"148\":1,\"203\":1,\"613\":2,\"752\":1,\"753\":1,\"754\":1,\"756\":1,\"757\":1,\"760\":2,\"761\":1,\"778\":2,\"779\":1,\"785\":1,\"820\":1,\"821\":1,\"826\":1,\"831\":2,\"832\":1,\"836\":1,\"909\":1,\"927\":1,\"1095\":1,\"1109\":1,\"1110\":1,\"1111\":1,\"1112\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1130\":1,\"1131\":1,\"1134\":1,\"1135\":1,\"1136\":1,\"1137\":1,\"1151\":1,\"1152\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1160\":3,\"1161\":3,\"1162\":1,\"1163\":1,\"1164\":3,\"1165\":2,\"1174\":1,\"1175\":1,\"1177\":3,\"1184\":1,\"1185\":1,\"1187\":4,\"1188\":1,\"1189\":1,\"1202\":4,\"1207\":1,\"1208\":1,\"1209\":2,\"1210\":1,\"1212\":1,\"1213\":1,\"1215\":1,\"1216\":1,\"1220\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1229\":1,\"1230\":1,\"1231\":1,\"1232\":1,\"1233\":1,\"1234\":1,\"1235\":1,\"1236\":1,\"1237\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1243\":1,\"1245\":2,\"1246\":1,\"1249\":1,\"1250\":1,\"1252\":4,\"1253\":5,\"1254\":4,\"1257\":1,\"1258\":1,\"1259\":1,\"1260\":1,\"1261\":1,\"1262\":1,\"1263\":1,\"1264\":1,\"1265\":1,\"1266\":1,\"1267\":1,\"1268\":1,\"1274\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1285\":1,\"1286\":2,\"1287\":2,\"1337\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1431\":1,\"1432\":2,\"1433\":1,\"1434\":1,\"1435\":1,\"1436\":1,\"1437\":1,\"1438\":1,\"1439\":1,\"1440\":1,\"1441\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1445\":1,\"1446\":1,\"1447\":1,\"1448\":1,\"1449\":1,\"1450\":1,\"1452\":1,\"1453\":1,\"1456\":1,\"1457\":1,\"1458\":1,\"1459\":1,\"1460\":1,\"1461\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1474\":1,\"1475\":1,\"1476\":2,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1506\":1,\"1507\":1,\"1508\":1,\"1509\":1,\"1510\":1,\"1512\":1,\"1513\":1,\"1518\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1523\":1,\"1531\":1,\"1532\":2,\"1533\":1,\"1535\":2,\"1536\":1,\"1537\":1,\"1538\":1,\"1540\":1,\"1541\":1,\"1543\":1,\"1544\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1550\":1,\"1551\":1,\"1553\":1,\"1555\":1,\"1556\":1,\"1561\":1,\"1562\":1,\"1564\":1,\"1565\":1,\"1573\":1,\"1574\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1596\":1,\"1597\":1,\"1598\":2,\"1599\":1,\"1602\":1,\"1603\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1613\":1,\"1614\":1,\"1616\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1624\":1,\"1625\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1630\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1641\":1,\"1642\":1,\"1643\":1,\"1646\":1,\"1647\":1,\"1648\":2,\"1649\":1,\"1650\":2,\"1651\":1,\"1652\":1,\"1653\":1,\"1656\":1,\"1657\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1769\":1,\"1770\":1,\"1774\":1,\"1775\":1,\"1779\":1,\"1780\":1,\"1782\":1,\"1783\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1792\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":1,\"1801\":1,\"1802\":1,\"1806\":1,\"1807\":1,\"1890\":1,\"1891\":1,\"1902\":1,\"1903\":1,\"1906\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1912\":2,\"1913\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1946\":1,\"1947\":1,\"1951\":2,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1956\":1,\"1958\":1,\"1959\":1,\"1976\":1,\"1977\":1,\"1978\":1,\"1979\":1,\"1981\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1989\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1994\":1,\"1995\":1,\"1997\":1,\"1998\":1,\"2024\":1,\"2025\":1,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2039\":1,\"2040\":1,\"2041\":1,\"2042\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2047\":1,\"2048\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2055\":1,\"2056\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2064\":1,\"2065\":1,\"2066\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2083\":1,\"2084\":1,\"2089\":1,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2149\":1,\"2150\":1,\"2168\":1,\"2169\":1,\"2233\":1,\"2234\":1,\"2237\":1,\"2238\":1,\"2241\":1,\"2242\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2266\":1,\"2267\":1,\"2275\":1,\"2276\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2288\":1,\"2289\":1,\"2290\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2297\":1,\"2298\":1,\"2299\":1,\"2300\":1,\"2398\":1,\"2401\":1,\"2403\":1,\"2412\":1,\"2534\":1,\"2537\":1,\"2539\":1,\"2543\":1,\"2564\":1,\"2570\":1,\"2584\":1,\"2585\":1,\"2600\":2,\"2638\":2}}],[\"shorter\",{\"1\":{\"1255\":1}}],[\"shortest\",{\"1\":{\"1003\":3,\"1004\":3,\"1005\":3,\"1006\":1,\"1028\":3}}],[\"shortcut=false\",{\"1\":{\"1633\":1,\"1635\":1}}],[\"shortcut\",{\"1\":{\"1012\":1}}],[\"short\",{\"0\":{\"864\":1,\"1095\":1},\"1\":{\"121\":4,\"711\":2,\"823\":2,\"864\":2,\"1085\":1,\"1093\":4,\"1095\":2,\"1896\":1,\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2197\":3,\"2200\":1,\"2373\":2,\"2385\":1,\"2433\":2,\"2555\":2}}],[\"showcase\",{\"1\":{\"2387\":1}}],[\"shown\",{\"1\":{\"68\":1,\"70\":1,\"1639\":1,\"2099\":1,\"2355\":1,\"2357\":1,\"2371\":1,\"2394\":1,\"2440\":1,\"2468\":1,\"2473\":1,\"2530\":1,\"2564\":1,\"2578\":1,\"2612\":1}}],[\"shows\",{\"1\":{\"62\":1,\"166\":1,\"177\":1,\"2362\":1,\"2504\":1,\"2576\":1,\"2600\":1,\"2651\":1}}],[\"show\",{\"0\":{\"62\":1,\"87\":1,\"88\":1,\"293\":1,\"2498\":1,\"2616\":1,\"2634\":1},\"1\":{\"58\":1,\"62\":5,\"69\":1,\"90\":1,\"108\":1,\"238\":3,\"343\":2,\"350\":2,\"368\":2,\"2357\":1,\"2359\":1,\"2360\":1,\"2368\":1,\"2371\":2,\"2380\":1,\"2386\":1,\"2389\":1,\"2394\":1,\"2405\":1,\"2406\":1,\"2408\":1,\"2422\":1,\"2429\":1,\"2440\":3,\"2441\":1,\"2447\":1,\"2449\":1,\"2451\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2463\":1,\"2465\":1,\"2467\":1,\"2480\":1,\"2481\":1,\"2486\":1,\"2490\":1,\"2494\":2,\"2498\":2,\"2500\":1,\"2502\":1,\"2503\":1,\"2518\":1,\"2521\":2,\"2522\":2,\"2523\":2,\"2525\":1,\"2530\":1,\"2541\":1,\"2545\":1,\"2552\":1,\"2558\":1,\"2564\":1,\"2578\":1,\"2580\":1,\"2581\":1,\"2582\":1,\"2605\":1,\"2609\":1,\"2612\":2,\"2616\":1,\"2622\":1,\"2626\":1,\"2630\":2,\"2634\":1}}],[\"sh\",{\"0\":{\"18\":1,\"273\":1,\"274\":1,\"275\":1,\"276\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"285\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"2640\":1},\"1\":{\"1\":3,\"2\":2,\"3\":5,\"4\":1,\"5\":5,\"11\":2,\"12\":1,\"15\":3,\"16\":2,\"17\":1,\"18\":5,\"19\":7,\"25\":6,\"46\":5,\"47\":1,\"49\":2,\"51\":1,\"54\":2,\"85\":22,\"90\":5,\"91\":6,\"92\":5,\"93\":2,\"94\":2,\"96\":3,\"97\":1,\"98\":1,\"99\":3,\"101\":2,\"102\":2,\"105\":2,\"110\":1,\"111\":1,\"113\":3,\"126\":1,\"127\":2,\"128\":1,\"134\":2,\"135\":12,\"136\":7,\"137\":2,\"142\":2,\"148\":1,\"149\":2,\"150\":1,\"167\":3,\"168\":3,\"171\":1,\"178\":2,\"179\":2,\"180\":1,\"186\":1,\"187\":2,\"190\":1,\"196\":2,\"197\":4,\"198\":4,\"200\":1,\"201\":1,\"202\":1,\"204\":2,\"205\":2,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":2,\"223\":2,\"229\":2,\"230\":2,\"234\":2,\"235\":4,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":2,\"241\":2,\"242\":1,\"274\":2,\"275\":2,\"276\":2,\"277\":2,\"278\":1,\"279\":2,\"280\":3,\"281\":2,\"282\":2,\"283\":2,\"284\":2,\"285\":1,\"286\":5,\"287\":1,\"288\":1,\"290\":1,\"292\":1,\"294\":3,\"295\":5,\"296\":4,\"297\":2,\"298\":2,\"1551\":1,\"1553\":1,\"2372\":13,\"2373\":5,\"2375\":4,\"2377\":2,\"2378\":2,\"2384\":2,\"2385\":8,\"2387\":3,\"2394\":9,\"2397\":1,\"2398\":2,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":2,\"2409\":6,\"2412\":1,\"2413\":1,\"2415\":2,\"2416\":2,\"2417\":2,\"2418\":2,\"2419\":1,\"2428\":1,\"2429\":20,\"2430\":5,\"2431\":2,\"2432\":4,\"2433\":3,\"2436\":2,\"2437\":2,\"2440\":6,\"2492\":1,\"2530\":9,\"2533\":1,\"2534\":2,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":2,\"2551\":1,\"2552\":7,\"2554\":11,\"2555\":7,\"2558\":2,\"2559\":2,\"2562\":2,\"2563\":2,\"2564\":2,\"2566\":5,\"2567\":3,\"2568\":12,\"2569\":4,\"2571\":1,\"2573\":2,\"2584\":7,\"2585\":3,\"2587\":1,\"2628\":1,\"2637\":1,\"2638\":7,\"2640\":4,\"2642\":1}}],[\"rb\",{\"1\":{\"2386\":1,\"2592\":1}}],[\"r9y9\",{\"1\":{\"2142\":1}}],[\"rhythm\",{\"1\":{\"2090\":1}}],[\"rho\",{\"1\":{\"64\":1}}],[\"rho=0\",{\"1\":{\"64\":1}}],[\"rng=<module\",{\"1\":{\"1950\":1}}],[\"rnnattractor\",{\"0\":{\"1376\":1},\"1\":{\"1376\":1}}],[\"rnnencoder\",{\"0\":{\"1222\":1},\"1\":{\"1222\":3}}],[\"rnntnumba\",{\"0\":{\"1287\":1},\"1\":{\"1286\":1,\"1287\":2}}],[\"rnntstatus\",{\"0\":{\"1225\":1},\"1\":{\"1225\":1}}],[\"rnntlossnumba\",{\"0\":{\"1224\":1},\"1\":{\"1224\":2}}],[\"rnnt\",{\"0\":{\"1142\":2,\"1143\":1,\"1154\":2,\"1155\":2,\"1186\":2,\"1194\":1,\"1202\":2,\"1210\":2,\"1211\":2,\"1224\":2,\"1225\":1,\"1226\":1,\"1227\":1,\"1286\":2,\"1287\":2,\"1288\":2,\"1293\":2,\"1294\":2,\"1295\":2,\"1296\":2,\"1298\":2,\"1299\":2,\"1300\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1306\":2,\"1307\":2,\"1315\":1,\"1316\":2,\"1318\":2,\"1324\":2,\"1325\":2,\"1331\":2,\"1333\":2,\"1334\":2,\"1335\":2,\"1336\":3,\"1337\":3,\"1338\":2,\"1344\":1,\"1346\":1,\"1348\":3,\"1349\":3,\"1350\":3,\"1353\":1,\"1359\":1},\"1\":{\"1142\":4,\"1143\":1,\"1154\":3,\"1155\":3,\"1186\":5,\"1194\":1,\"1202\":2,\"1210\":4,\"1211\":3,\"1224\":3,\"1225\":3,\"1226\":1,\"1227\":1,\"1286\":2,\"1287\":2,\"1288\":2,\"1293\":2,\"1294\":2,\"1295\":2,\"1296\":2,\"1298\":5,\"1299\":5,\"1300\":2,\"1301\":5,\"1302\":5,\"1303\":5,\"1304\":5,\"1306\":2,\"1307\":2,\"1315\":1,\"1316\":2,\"1318\":2,\"1324\":2,\"1325\":2,\"1331\":2,\"1333\":2,\"1334\":3,\"1335\":2,\"1336\":3,\"1337\":4,\"1338\":2,\"1344\":1,\"1346\":1,\"1348\":3,\"1349\":4,\"1350\":4,\"1353\":1,\"1359\":1}}],[\"rnnseparator\",{\"0\":{\"1626\":1},\"1\":{\"1626\":1}}],[\"rnns\",{\"1\":{\"837\":1}}],[\"rnnp\",{\"0\":{\"808\":1},\"1\":{\"808\":1}}],[\"rnndecoder\",{\"0\":{\"806\":1,\"1073\":1,\"1220\":1},\"1\":{\"700\":1,\"806\":1,\"1073\":2,\"1220\":2}}],[\"rnnlm=none\",{\"1\":{\"676\":2,\"742\":1,\"781\":2,\"812\":2,\"816\":1,\"836\":1}}],[\"rnnlm\",{\"0\":{\"614\":1,\"807\":1},\"1\":{\"31\":1,\"249\":8,\"251\":4,\"255\":4,\"259\":4,\"285\":2,\"286\":1,\"605\":2,\"606\":1,\"614\":3,\"650\":1,\"676\":2,\"710\":2,\"733\":1,\"742\":2,\"781\":2,\"807\":2,\"812\":2,\"817\":1,\"1063\":1,\"1958\":1}}],[\"rnn\",{\"0\":{\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"758\":1,\"791\":1,\"805\":2,\"806\":1,\"808\":1,\"817\":1,\"833\":1,\"840\":2,\"841\":2,\"842\":2,\"850\":1,\"851\":1,\"855\":1,\"856\":1,\"870\":1,\"878\":1,\"892\":1,\"915\":2,\"1073\":1,\"1220\":1,\"1222\":1,\"1282\":1,\"1289\":1,\"1376\":1,\"1626\":1,\"1958\":1,\"2086\":2,\"2087\":2,\"2088\":2},\"1\":{\"21\":4,\"24\":1,\"30\":4,\"112\":1,\"113\":1,\"116\":10,\"174\":1,\"286\":4,\"614\":1,\"635\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":2,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"758\":1,\"791\":1,\"805\":5,\"806\":5,\"807\":1,\"808\":4,\"815\":2,\"817\":1,\"833\":1,\"840\":3,\"841\":3,\"842\":3,\"850\":2,\"851\":2,\"855\":1,\"856\":1,\"870\":1,\"878\":1,\"889\":4,\"892\":1,\"915\":2,\"1063\":1,\"1073\":7,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1177\":2,\"1209\":1,\"1220\":3,\"1221\":1,\"1222\":3,\"1252\":2,\"1253\":2,\"1254\":2,\"1270\":6,\"1279\":1,\"1282\":2,\"1289\":1,\"1336\":1,\"1348\":1,\"1376\":1,\"1430\":5,\"1462\":4,\"1463\":3,\"1515\":4,\"1516\":5,\"1528\":4,\"1529\":4,\"1531\":3,\"1532\":6,\"1534\":6,\"1535\":6,\"1537\":6,\"1539\":6,\"1581\":5,\"1602\":1,\"1626\":6,\"1645\":2,\"1650\":5,\"1652\":1,\"1670\":5,\"1671\":4,\"1958\":3,\"2003\":2,\"2086\":5,\"2087\":3,\"2088\":2,\"2371\":1,\"2377\":3,\"2436\":3,\"2558\":1,\"2562\":3,\"2612\":1,\"2630\":1,\"2641\":1,\"2642\":1}}],[\"rwd\",{\"1\":{\"1870\":1}}],[\"rwkvdecoder\",{\"0\":{\"1075\":1},\"1\":{\"1075\":3}}],[\"rwkv\",{\"0\":{\"1062\":1,\"1074\":2,\"1075\":1,\"1081\":1,\"1086\":1,\"1100\":1},\"1\":{\"116\":11,\"1062\":1,\"1074\":6,\"1075\":3,\"1081\":1,\"1086\":1,\"1100\":1}}],[\"r1\",{\"1\":{\"1712\":1,\"1735\":1}}],[\"r^\",{\"1\":{\"1698\":1}}],[\"rcublock\",{\"0\":{\"1624\":1},\"1\":{\"1624\":1}}],[\"r+\",{\"1\":{\"989\":1}}],[\"rir\",{\"1\":{\"2178\":2,\"2179\":2,\"2184\":2,\"2191\":2,\"2194\":2,\"2195\":2,\"2197\":6,\"2200\":2}}],[\"rirconvolve\",{\"0\":{\"949\":1},\"1\":{\"949\":2}}],[\"risk\",{\"0\":{\"1136\":1,\"1332\":1},\"1\":{\"1136\":4,\"1137\":3,\"1145\":2,\"1332\":1}}],[\"right=1\",{\"1\":{\"1887\":1}}],[\"right=true\",{\"1\":{\"648\":1}}],[\"right\",{\"1\":{\"648\":1,\"1143\":1,\"1392\":2,\"1659\":1,\"1752\":1}}],[\"rttmreader\",{\"0\":{\"1399\":1},\"1\":{\"1399\":2,\"1400\":1}}],[\"rttm\",{\"0\":{\"1399\":1,\"1419\":2},\"1\":{\"1399\":3,\"1400\":4,\"1419\":5}}],[\"rtype\",{\"1\":{\"605\":2,\"617\":1,\"621\":1,\"710\":3,\"749\":2,\"750\":2,\"794\":4,\"890\":1,\"905\":2}}],[\"rtf^h\",{\"1\":{\"1707\":1}}],[\"rtf\",{\"0\":{\"501\":1,\"1698\":1,\"1705\":1,\"1707\":1,\"1713\":1,\"1714\":1},\"1\":{\"107\":2,\"108\":2,\"110\":4,\"111\":1,\"203\":3,\"218\":3,\"225\":3,\"232\":3,\"501\":3,\"1524\":4,\"1611\":1,\"1698\":2,\"1705\":8,\"1707\":4,\"1713\":14,\"1714\":3,\"2365\":3,\"2508\":3,\"2510\":3,\"2515\":3,\"2655\":3,\"2660\":3}}],[\"rxfilename\",{\"1\":{\"496\":1}}],[\"rspecifier\",{\"1\":{\"496\":2,\"506\":1,\"509\":1,\"512\":1,\"519\":1,\"522\":1,\"525\":1,\"533\":1,\"541\":1,\"585\":1,\"982\":1,\"985\":1,\"990\":1,\"993\":1,\"1013\":3}}],[\"rfft\",{\"1\":{\"2270\":1}}],[\"rf^\",{\"1\":{\"1696\":2,\"1698\":1}}],[\"rf\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"1696\":2,\"1697\":2}}],[\"rm\",{\"1\":{\"135\":1,\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"294\":1,\"2429\":1,\"2432\":1,\"2440\":1}}],[\"rmsnorm\",{\"0\":{\"1072\":1},\"1\":{\"1072\":3,\"1093\":1,\"1098\":1}}],[\"rms\",{\"1\":{\"115\":6,\"1072\":2,\"1098\":1}}],[\"ryuichi\",{\"1\":{\"130\":2}}],[\"rkwv\",{\"1\":{\"116\":1}}],[\"r\",{\"0\":{\"1226\":1},\"1\":{\"110\":1,\"171\":2,\"175\":2,\"185\":1,\"193\":1,\"194\":2,\"203\":1,\"286\":1,\"296\":1,\"745\":7,\"746\":7,\"885\":1,\"1031\":4,\"1143\":1,\"1144\":2,\"1187\":3,\"1202\":3,\"1226\":1,\"1227\":2,\"1228\":2,\"1248\":2,\"1286\":3,\"1287\":3,\"1297\":1,\"1351\":2,\"1379\":2,\"1462\":1,\"1463\":1,\"1529\":1,\"1568\":1,\"1594\":1,\"1664\":2,\"1665\":2,\"1698\":1,\"1704\":1,\"1712\":1,\"1715\":1,\"1961\":2,\"2386\":1,\"2429\":1,\"2432\":1,\"2440\":1,\"2592\":2}}],[\"rubric\",{\"1\":{\"1355\":1,\"1905\":1,\"2154\":1}}],[\"russian\",{\"1\":{\"461\":1}}],[\"ruled\",{\"1\":{\"1773\":9,\"2082\":10}}],[\"rule\",{\"1\":{\"231\":1,\"834\":1}}],[\"rules\",{\"0\":{\"45\":1},\"1\":{\"1452\":1}}],[\"runing\",{\"1\":{\"2584\":1}}],[\"run=\",{\"1\":{\"144\":1}}],[\"runs\",{\"1\":{\"110\":1,\"111\":1,\"141\":1,\"607\":1}}],[\"runtime\",{\"1\":{\"5\":2,\"7\":1,\"203\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"running\",{\"1\":{\"3\":1,\"44\":1,\"136\":1,\"235\":1,\"628\":1,\"691\":8,\"697\":8,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"797\":8,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1465\":2,\"1467\":1,\"1469\":1,\"1475\":1,\"1476\":1,\"1477\":2,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1964\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2468\":1,\"2508\":1,\"2510\":1,\"2572\":1}}],[\"run\",{\"0\":{\"18\":1,\"143\":1,\"235\":1,\"2640\":1},\"1\":{\"1\":4,\"2\":2,\"3\":5,\"4\":1,\"5\":2,\"15\":1,\"16\":2,\"17\":1,\"18\":3,\"19\":7,\"25\":6,\"47\":1,\"49\":2,\"54\":1,\"69\":1,\"85\":11,\"90\":3,\"91\":6,\"92\":5,\"93\":2,\"94\":2,\"96\":3,\"97\":1,\"98\":1,\"99\":2,\"102\":1,\"105\":2,\"107\":2,\"110\":1,\"113\":1,\"142\":2,\"143\":1,\"144\":1,\"148\":1,\"149\":2,\"150\":1,\"166\":1,\"168\":3,\"171\":1,\"179\":2,\"180\":1,\"186\":1,\"187\":1,\"190\":1,\"197\":1,\"198\":1,\"203\":2,\"204\":2,\"209\":1,\"213\":1,\"233\":1,\"234\":1,\"235\":6,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":2,\"241\":2,\"242\":1,\"275\":2,\"276\":2,\"279\":2,\"280\":2,\"281\":2,\"282\":2,\"283\":2,\"284\":2,\"294\":2,\"297\":1,\"429\":2,\"836\":3,\"976\":1,\"1043\":1,\"1778\":1,\"1804\":1,\"1805\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1877\":1,\"1878\":1,\"2099\":1,\"2102\":1,\"2186\":1,\"2201\":1,\"2202\":2,\"2204\":1,\"2357\":5,\"2363\":3,\"2368\":1,\"2371\":1,\"2372\":5,\"2377\":1,\"2378\":2,\"2385\":2,\"2394\":1,\"2398\":1,\"2412\":1,\"2413\":1,\"2417\":2,\"2418\":2,\"2419\":1,\"2423\":1,\"2429\":6,\"2433\":1,\"2436\":1,\"2437\":2,\"2438\":1,\"2440\":1,\"2441\":1,\"2471\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2486\":1,\"2490\":2,\"2492\":1,\"2497\":1,\"2506\":1,\"2508\":1,\"2510\":3,\"2512\":1,\"2515\":1,\"2530\":1,\"2534\":1,\"2546\":1,\"2554\":6,\"2562\":1,\"2563\":2,\"2564\":2,\"2569\":3,\"2571\":1,\"2578\":5,\"2584\":4,\"2585\":1,\"2587\":1,\"2598\":2,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2628\":1,\"2630\":1,\"2637\":1,\"2638\":1,\"2640\":5,\"2653\":3,\"2657\":1}}],[\"round\",{\"0\":{\"2013\":1},\"1\":{\"2013\":1}}],[\"routines\",{\"1\":{\"2156\":1}}],[\"routine\",{\"1\":{\"607\":1,\"626\":1,\"627\":1,\"727\":2,\"728\":1}}],[\"route\",{\"1\":{\"44\":1}}],[\"rop\",{\"1\":{\"1143\":1}}],[\"row\",{\"1\":{\"1025\":1,\"1352\":2,\"2359\":6,\"2456\":7,\"2460\":7,\"2521\":10,\"2580\":6}}],[\"rows\",{\"1\":{\"1010\":3,\"1144\":1,\"1227\":1,\"1228\":2,\"1344\":1,\"1345\":2,\"1346\":1,\"1347\":2,\"1352\":2}}],[\"roshan\",{\"1\":{\"130\":1}}],[\"robustness\",{\"1\":{\"2584\":1}}],[\"robust\",{\"1\":{\"130\":1,\"235\":1,\"736\":1,\"754\":1,\"774\":1,\"786\":1,\"2068\":1,\"2070\":1,\"2243\":1}}],[\"robin\",{\"1\":{\"130\":1}}],[\"rotaryrelativepositionbias\",{\"0\":{\"1079\":1},\"1\":{\"1079\":3}}],[\"rotary\",{\"1\":{\"116\":1,\"1079\":5}}],[\"root=$\",{\"1\":{\"135\":1}}],[\"root=\",{\"1\":{\"134\":1,\"136\":1}}],[\"root\",{\"1\":{\"1\":2,\"4\":3,\"10\":1,\"12\":1,\"128\":1,\"134\":4,\"135\":15,\"136\":7,\"137\":1,\"171\":3,\"175\":3,\"185\":2,\"193\":2,\"194\":4,\"2568\":4}}],[\"ravanelli\",{\"1\":{\"1917\":1}}],[\"ragged\",{\"1\":{\"1136\":1}}],[\"rather\",{\"1\":{\"691\":1,\"693\":1,\"697\":1,\"797\":1,\"1138\":1,\"1618\":1,\"1619\":1,\"2387\":1}}],[\"rational\",{\"0\":{\"1886\":1,\"1887\":1,\"1888\":1},\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"ratio\",{\"1\":{\"150\":1,\"691\":4,\"692\":2,\"693\":5,\"697\":6,\"698\":3,\"699\":3,\"797\":5,\"801\":1,\"821\":2,\"826\":2,\"857\":5,\"1254\":1,\"1257\":1,\"1430\":1,\"1515\":1,\"1528\":1,\"1529\":1,\"1532\":1,\"1534\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1581\":1,\"1598\":1,\"1626\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1670\":1,\"1671\":1,\"1778\":1,\"1804\":1,\"1852\":1,\"1860\":4,\"1878\":1,\"1883\":3,\"1896\":1,\"1915\":2,\"2002\":2,\"2079\":2,\"2095\":2,\"2263\":2,\"2264\":2}}],[\"rates=false\",{\"1\":{\"2174\":1,\"2222\":1,\"2225\":1,\"2229\":1,\"2231\":1}}],[\"rates\",{\"1\":{\"263\":2,\"267\":2,\"429\":2,\"1245\":1,\"1392\":1,\"1551\":1,\"1553\":1,\"2106\":2,\"2182\":1}}],[\"rate=wavfile\",{\"1\":{\"2592\":1}}],[\"rate=text2speech\",{\"1\":{\"2365\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2655\":1,\"2660\":1}}],[\"rate=rate\",{\"1\":{\"2359\":1,\"2360\":2,\"2386\":1,\"2458\":2,\"2522\":1,\"2523\":2,\"2580\":1,\"2581\":1,\"2582\":2,\"2596\":1}}],[\"rate=none\",{\"1\":{\"1245\":1}}],[\"rate=1\",{\"1\":{\"1243\":1,\"1247\":1,\"1248\":1}}],[\"rate=16000\",{\"1\":{\"201\":1,\"2596\":1}}],[\"rate=0\",{\"1\":{\"729\":1,\"736\":1,\"747\":3,\"749\":3,\"756\":1,\"770\":1,\"787\":1,\"802\":1,\"803\":1,\"807\":2,\"837\":1,\"914\":1,\"2078\":2,\"2081\":1,\"2083\":1,\"2253\":1}}],[\"rate=fs\",{\"1\":{\"218\":1,\"225\":1,\"232\":1,\"2456\":1,\"2460\":1,\"2501\":2,\"2521\":2,\"2522\":1,\"2523\":1}}],[\"rate=sr\",{\"1\":{\"194\":1,\"2367\":1,\"2369\":2,\"2372\":3,\"2470\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2487\":2,\"2491\":2,\"2497\":3,\"2604\":1,\"2606\":2,\"2607\":2,\"2610\":2,\"2614\":3,\"2615\":3,\"2621\":1,\"2623\":2,\"2624\":2,\"2627\":2,\"2632\":3,\"2633\":3,\"2647\":1}}],[\"rate\",{\"1\":{\"21\":18,\"22\":6,\"49\":1,\"92\":1,\"102\":1,\"109\":1,\"115\":23,\"116\":16,\"118\":3,\"251\":4,\"275\":1,\"282\":1,\"630\":1,\"631\":1,\"672\":1,\"703\":3,\"711\":3,\"712\":3,\"713\":3,\"714\":3,\"715\":3,\"716\":3,\"717\":2,\"718\":3,\"719\":3,\"720\":3,\"721\":3,\"722\":3,\"725\":3,\"726\":6,\"730\":1,\"731\":1,\"732\":1,\"737\":2,\"740\":3,\"741\":3,\"747\":1,\"748\":1,\"749\":6,\"754\":15,\"756\":1,\"768\":1,\"770\":2,\"771\":3,\"772\":3,\"775\":3,\"776\":3,\"777\":2,\"778\":1,\"784\":3,\"785\":3,\"786\":3,\"787\":1,\"792\":3,\"800\":1,\"801\":2,\"802\":2,\"805\":1,\"806\":6,\"808\":1,\"809\":3,\"810\":3,\"813\":3,\"818\":3,\"821\":4,\"825\":9,\"826\":21,\"827\":3,\"834\":2,\"838\":1,\"858\":6,\"911\":6,\"914\":1,\"933\":1,\"989\":1,\"1049\":3,\"1050\":3,\"1052\":3,\"1054\":3,\"1056\":3,\"1057\":6,\"1065\":9,\"1066\":15,\"1068\":2,\"1070\":3,\"1071\":3,\"1073\":6,\"1074\":6,\"1075\":9,\"1076\":3,\"1077\":3,\"1083\":3,\"1093\":5,\"1115\":4,\"1132\":1,\"1133\":6,\"1140\":5,\"1141\":6,\"1145\":3,\"1148\":11,\"1149\":9,\"1150\":9,\"1151\":1,\"1153\":1,\"1158\":2,\"1167\":4,\"1168\":4,\"1169\":4,\"1170\":2,\"1180\":5,\"1181\":13,\"1182\":1,\"1195\":1,\"1196\":4,\"1197\":4,\"1198\":1,\"1200\":3,\"1203\":9,\"1204\":4,\"1209\":2,\"1214\":1,\"1215\":1,\"1217\":1,\"1239\":2,\"1244\":1,\"1247\":2,\"1248\":2,\"1255\":1,\"1270\":2,\"1271\":5,\"1272\":10,\"1273\":4,\"1391\":1,\"1406\":2,\"1462\":2,\"1463\":1,\"1464\":1,\"1505\":9,\"1510\":1,\"1511\":1,\"1524\":1,\"1525\":1,\"1531\":1,\"1551\":1,\"1553\":1,\"1558\":1,\"1602\":1,\"1611\":2,\"1616\":1,\"1617\":1,\"1643\":2,\"1644\":2,\"1669\":9,\"1763\":1,\"1769\":1,\"1771\":9,\"1772\":3,\"1776\":6,\"1777\":3,\"1778\":11,\"1785\":5,\"1786\":1,\"1787\":9,\"1788\":9,\"1791\":3,\"1797\":4,\"1798\":9,\"1799\":1,\"1801\":1,\"1804\":16,\"1805\":8,\"1806\":1,\"1808\":3,\"1810\":3,\"1812\":1,\"1813\":1,\"1817\":1,\"1822\":1,\"1835\":3,\"1850\":10,\"1851\":29,\"1852\":11,\"1859\":1,\"1862\":3,\"1863\":3,\"1864\":3,\"1865\":3,\"1868\":3,\"1874\":9,\"1877\":9,\"1878\":18,\"1880\":3,\"1904\":1,\"1912\":1,\"1916\":1,\"1917\":1,\"1921\":3,\"1922\":3,\"1923\":4,\"1925\":3,\"1926\":1,\"1927\":2,\"1928\":3,\"1929\":3,\"1932\":2,\"1934\":1,\"1935\":3,\"1936\":3,\"1938\":3,\"1939\":3,\"1941\":3,\"1942\":1,\"1943\":3,\"1945\":1,\"1946\":3,\"1947\":3,\"1958\":1,\"1960\":3,\"1971\":3,\"1993\":1,\"2001\":9,\"2002\":6,\"2003\":2,\"2004\":9,\"2018\":4,\"2026\":3,\"2029\":9,\"2054\":11,\"2076\":1,\"2078\":4,\"2081\":1,\"2083\":1,\"2086\":12,\"2087\":15,\"2090\":24,\"2095\":6,\"2184\":1,\"2188\":1,\"2197\":3,\"2200\":1,\"2239\":1,\"2243\":24,\"2244\":32,\"2255\":29,\"2260\":3,\"2262\":3,\"2263\":6,\"2264\":31,\"2265\":3,\"2267\":3,\"2279\":32,\"2295\":1,\"2296\":1,\"2304\":1,\"2359\":1,\"2360\":1,\"2375\":3,\"2386\":2,\"2387\":1,\"2403\":1,\"2418\":1,\"2430\":2,\"2440\":9,\"2456\":1,\"2458\":1,\"2460\":1,\"2501\":3,\"2514\":1,\"2521\":1,\"2522\":3,\"2523\":1,\"2539\":1,\"2542\":1,\"2555\":2,\"2558\":7,\"2559\":3,\"2564\":7,\"2580\":1,\"2581\":3,\"2582\":1,\"2584\":3,\"2596\":2,\"2607\":3,\"2615\":3,\"2624\":3,\"2633\":3,\"2659\":1}}],[\"ram\",{\"1\":{\"144\":1}}],[\"rawnet3projector\",{\"0\":{\"2057\":1},\"1\":{\"2057\":1}}],[\"rawnet3encoder\",{\"0\":{\"2055\":1},\"1\":{\"2055\":1}}],[\"rawnet3\",{\"0\":{\"2055\":1,\"2057\":1},\"1\":{\"2055\":2,\"2057\":1}}],[\"rawnet2\",{\"1\":{\"2032\":1}}],[\"rawnet\",{\"0\":{\"2032\":1},\"1\":{\"2032\":2,\"2055\":1,\"2409\":1,\"2415\":2,\"2416\":1,\"2418\":1,\"2419\":2}}],[\"rawsampler\",{\"0\":{\"1899\":1},\"1\":{\"1899\":1}}],[\"rawwav\",{\"1\":{\"1365\":1,\"1440\":1,\"1454\":1}}],[\"raw\",{\"0\":{\"183\":1,\"184\":1},\"1\":{\"84\":1,\"98\":2,\"110\":3,\"169\":1,\"181\":1,\"238\":8,\"579\":1,\"1198\":2,\"1255\":2,\"1454\":1,\"1551\":2,\"1554\":2,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1980\":2,\"2055\":2,\"2077\":2,\"2235\":2,\"2267\":2,\"2277\":2,\"2357\":9,\"2368\":2,\"2371\":4,\"2373\":4,\"2375\":10,\"2385\":1,\"2430\":2,\"2431\":3,\"2432\":7,\"2433\":2,\"2454\":1,\"2455\":2,\"2460\":3,\"2461\":2,\"2472\":3,\"2474\":2,\"2476\":3,\"2478\":2,\"2486\":2,\"2490\":4,\"2492\":2,\"2494\":4,\"2500\":4,\"2507\":2,\"2510\":4,\"2513\":2,\"2519\":1,\"2520\":4,\"2555\":5,\"2558\":7,\"2559\":6,\"2560\":1,\"2564\":3,\"2572\":3,\"2578\":9,\"2584\":1,\"2585\":2,\"2589\":1,\"2590\":1,\"2599\":1,\"2600\":4,\"2605\":2,\"2609\":4,\"2612\":4,\"2617\":1,\"2622\":2,\"2626\":4,\"2628\":2,\"2630\":5,\"2635\":1,\"2648\":3,\"2649\":2}}],[\"raising\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"raise\",{\"1\":{\"1008\":1,\"2585\":1}}],[\"raised\",{\"1\":{\"823\":1,\"1085\":1}}],[\"raises\",{\"1\":{\"59\":1,\"607\":2,\"1011\":1,\"1025\":1}}],[\"rainy\",{\"1\":{\"58\":1}}],[\"ran\",{\"1\":{\"85\":1}}],[\"ranges\",{\"1\":{\"1761\":1,\"1763\":1,\"1805\":1,\"1986\":1}}],[\"range\",{\"1\":{\"69\":1,\"113\":1,\"115\":2,\"174\":3,\"295\":2,\"869\":2,\"877\":2,\"940\":1,\"959\":1,\"1064\":3,\"1096\":2,\"1257\":3,\"1713\":1,\"1808\":2,\"1905\":1,\"1914\":1,\"1915\":1,\"1928\":1,\"1940\":3,\"2099\":1,\"2102\":1,\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2197\":4,\"2200\":1,\"2266\":1,\"2267\":2,\"2398\":1,\"2500\":1,\"2534\":1,\"2592\":1,\"2596\":1,\"2600\":1,\"2617\":1,\"2635\":1}}],[\"randint\",{\"1\":{\"989\":1,\"2514\":2,\"2659\":2}}],[\"randn\",{\"1\":{\"959\":1,\"2149\":1,\"2210\":1,\"2514\":1,\"2659\":1}}],[\"rand\",{\"0\":{\"1384\":1,\"1386\":1,\"2226\":1},\"1\":{\"38\":1,\"40\":1,\"41\":1,\"42\":1,\"1384\":1,\"1386\":1,\"2188\":1,\"2226\":2}}],[\"randomsegmenter\",{\"0\":{\"2296\":1},\"1\":{\"2296\":1}}],[\"randomtextreader\",{\"0\":{\"1398\":1},\"1\":{\"1398\":2}}],[\"randomly\",{\"1\":{\"612\":1,\"837\":1,\"940\":2,\"1352\":4,\"1398\":1,\"1905\":1,\"1940\":1,\"2514\":2,\"2659\":2}}],[\"randomness\",{\"1\":{\"82\":1}}],[\"random\",{\"0\":{\"1884\":2,\"1885\":1,\"2164\":2,\"2296\":1},\"1\":{\"38\":1,\"69\":1,\"80\":1,\"82\":2,\"905\":1,\"942\":1,\"950\":5,\"956\":2,\"959\":1,\"968\":5,\"973\":2,\"989\":1,\"1015\":1,\"1314\":3,\"1354\":1,\"1398\":1,\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"1778\":1,\"1851\":1,\"1852\":1,\"1870\":1,\"1884\":3,\"1885\":1,\"1895\":1,\"1900\":1,\"1901\":1,\"1923\":1,\"1929\":1,\"1950\":2,\"2099\":1,\"2102\":1,\"2164\":2,\"2210\":1,\"2296\":1,\"2514\":4,\"2659\":4}}],[\"rank1\",{\"0\":{\"1712\":1},\"1\":{\"1712\":1}}],[\"rank=1\",{\"1\":{\"1245\":1,\"1314\":1,\"1339\":1,\"1342\":1}}],[\"rank0\",{\"1\":{\"38\":3}}],[\"rank\",{\"0\":{\"37\":1,\"1342\":1,\"1343\":1,\"2213\":1,\"2216\":1,\"2218\":1},\"1\":{\"36\":2,\"37\":3,\"39\":3,\"429\":4,\"644\":1,\"1245\":3,\"1248\":5,\"1339\":1,\"1342\":2,\"1343\":3,\"1351\":1,\"1712\":7,\"1715\":6,\"1735\":1,\"1930\":1,\"1932\":3,\"1934\":1,\"2180\":4,\"2213\":1,\"2216\":5,\"2217\":3,\"2218\":2}}],[\"revamp\",{\"1\":{\"2452\":1}}],[\"revisit\",{\"1\":{\"2435\":1,\"2561\":1}}],[\"reverb\",{\"1\":{\"1551\":1,\"1553\":1}}],[\"reverberant\",{\"1\":{\"1466\":1,\"2184\":1,\"2200\":1}}],[\"reversibleinstancenorm1doutput\",{\"0\":{\"1237\":1},\"1\":{\"1237\":1}}],[\"reversibleinstancenorm1dinput\",{\"0\":{\"1235\":1},\"1\":{\"1235\":1}}],[\"reversediffusionpredictor\",{\"0\":{\"1637\":1},\"1\":{\"1637\":1}}],[\"reversed\",{\"1\":{\"629\":1}}],[\"reverse=false\",{\"1\":{\"629\":1,\"799\":1,\"941\":1}}],[\"reverse\",{\"0\":{\"1740\":1,\"1945\":1},\"1\":{\"496\":2,\"629\":2,\"1003\":1,\"1004\":1,\"1005\":1,\"1028\":1,\"1638\":4,\"1646\":3,\"1740\":2,\"1945\":2}}],[\"revert\",{\"1\":{\"115\":1,\"691\":1,\"1067\":1,\"1096\":1,\"1551\":1,\"1553\":1}}],[\"reusibility\",{\"1\":{\"2400\":1,\"2536\":1}}],[\"reuse\",{\"1\":{\"38\":1,\"49\":1,\"2373\":1}}],[\"rei\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"reim\",{\"0\":{\"1750\":1},\"1\":{\"1750\":1}}],[\"reordering\",{\"1\":{\"2184\":1,\"2200\":1}}],[\"reject\",{\"1\":{\"1922\":1}}],[\"red\",{\"1\":{\"2500\":2,\"2617\":2,\"2635\":2}}],[\"red=lambda\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"redirect\",{\"1\":{\"769\":1}}],[\"reduction\",{\"1\":{\"738\":2,\"754\":2,\"821\":2,\"826\":2,\"885\":1,\"1143\":3,\"1144\":2,\"1211\":3,\"1224\":3,\"1226\":1,\"1227\":1,\"1228\":2,\"1286\":1,\"1287\":1,\"1336\":3,\"1344\":2,\"1345\":1,\"1346\":2,\"1347\":1,\"1348\":3,\"1604\":1,\"1706\":1,\"1707\":1,\"1711\":1,\"1712\":3,\"1715\":4,\"1778\":1,\"1850\":1,\"1851\":3,\"1852\":1,\"2002\":3,\"2059\":1,\"2078\":3,\"2083\":2,\"2086\":3,\"2087\":3,\"2090\":3,\"2095\":3,\"2236\":1,\"2241\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2259\":3,\"2263\":3,\"2264\":3,\"2279\":3,\"2301\":1,\"2302\":1,\"2304\":1,\"2305\":1}}],[\"reduction=8\",{\"1\":{\"2074\":1,\"2075\":1}}],[\"reduction=4\",{\"1\":{\"2063\":1}}],[\"reduction=\",{\"1\":{\"738\":1,\"759\":1,\"1211\":1,\"1224\":1,\"1336\":1,\"1348\":1,\"1604\":1,\"1711\":1}}],[\"reducing\",{\"1\":{\"82\":1,\"608\":1}}],[\"reduce=true\",{\"1\":{\"1890\":1}}],[\"reducehelper\",{\"0\":{\"1227\":1},\"1\":{\"1227\":1}}],[\"reduced\",{\"1\":{\"1143\":1,\"1298\":1,\"1299\":1,\"1302\":1,\"1303\":1,\"2439\":1}}],[\"reducelronplateau\",{\"0\":{\"2022\":1},\"1\":{\"62\":3,\"2022\":2}}],[\"reduce\",{\"0\":{\"287\":1,\"1143\":1,\"1194\":1,\"1226\":1,\"1227\":1,\"1344\":2,\"1346\":2},\"1\":{\"49\":1,\"102\":3,\"287\":1,\"606\":1,\"785\":1,\"1013\":1,\"1143\":1,\"1144\":1,\"1145\":3,\"1194\":1,\"1226\":1,\"1227\":1,\"1228\":2,\"1243\":1,\"1344\":2,\"1345\":2,\"1346\":2,\"1347\":2,\"1655\":2,\"1710\":1,\"1717\":1,\"1719\":2,\"2439\":1}}],[\"reduces\",{\"1\":{\"26\":1,\"1227\":1,\"1482\":1}}],[\"rename\",{\"0\":{\"913\":1},\"1\":{\"913\":1}}],[\"renamed\",{\"1\":{\"24\":1,\"635\":1}}],[\"renduchintala\",{\"1\":{\"130\":1}}],[\"reworked\",{\"1\":{\"107\":1,\"119\":1}}],[\"reagan\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"rearrange\",{\"1\":{\"1522\":2,\"1523\":3,\"1572\":2}}],[\"rearrange=false\",{\"1\":{\"1522\":1,\"1572\":1}}],[\"reaches\",{\"1\":{\"834\":1}}],[\"real+imag\",{\"1\":{\"1478\":2,\"1480\":2}}],[\"real=false\",{\"1\":{\"1314\":1}}],[\"really\",{\"1\":{\"595\":1,\"2212\":1,\"2573\":1}}],[\"realtime\",{\"0\":{\"2516\":1,\"2576\":1,\"2650\":1},\"1\":{\"139\":1,\"140\":2,\"155\":3,\"158\":2,\"161\":1,\"164\":1,\"165\":1,\"199\":1,\"202\":1,\"206\":1,\"244\":2,\"295\":1,\"2361\":1,\"2516\":1,\"2576\":1,\"2650\":1}}],[\"real\",{\"0\":{\"107\":1,\"206\":1,\"2369\":1,\"2487\":1,\"2491\":1,\"2586\":1,\"2606\":1,\"2610\":1,\"2623\":1,\"2627\":1},\"1\":{\"107\":2,\"108\":1,\"203\":3,\"501\":1,\"928\":2,\"1108\":1,\"1247\":1,\"1248\":2,\"1314\":1,\"1432\":1,\"1436\":1,\"1452\":2,\"1510\":1,\"1511\":1,\"1516\":3,\"1522\":2,\"1523\":1,\"1643\":1,\"1644\":1,\"1671\":1,\"1695\":2,\"1733\":1,\"1746\":1,\"1767\":2,\"1836\":1,\"2216\":1,\"2217\":1,\"2236\":1,\"2301\":1,\"2302\":3,\"2367\":3,\"2369\":1,\"2395\":1,\"2430\":1,\"2468\":1,\"2485\":3,\"2487\":1,\"2491\":1,\"2492\":5,\"2523\":1,\"2531\":1,\"2555\":1,\"2604\":3,\"2606\":1,\"2610\":1,\"2621\":3,\"2623\":1,\"2627\":1,\"2628\":5}}],[\"readframes\",{\"1\":{\"2592\":1}}],[\"readhelper\",{\"1\":{\"2432\":1}}],[\"reads\",{\"1\":{\"2432\":1}}],[\"readasdataurl\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"readable\",{\"0\":{\"2154\":1},\"1\":{\"2154\":8}}],[\"reading\",{\"1\":{\"641\":1,\"1015\":1}}],[\"ready\",{\"1\":{\"217\":1,\"224\":1,\"231\":1,\"235\":1,\"239\":1,\"2429\":1,\"2571\":1}}],[\"readlines\",{\"1\":{\"217\":1,\"224\":1,\"231\":1,\"2514\":1,\"2659\":1}}],[\"readmes\",{\"1\":{\"1454\":1}}],[\"readme\",{\"1\":{\"165\":1,\"244\":1,\"2354\":1,\"2372\":1,\"2384\":1,\"2429\":1,\"2554\":1,\"2568\":1}}],[\"reader=kaldiio\",{\"1\":{\"2432\":1}}],[\"readers\",{\"0\":{\"982\":1,\"985\":1,\"990\":1,\"993\":1,\"1013\":1},\"1\":{\"982\":2,\"985\":2,\"990\":2,\"993\":2,\"1013\":1}}],[\"reader\",{\"0\":{\"1013\":1},\"1\":{\"120\":1,\"1013\":1,\"1014\":2,\"1388\":1,\"1389\":2,\"1390\":1,\"1391\":2,\"1394\":1,\"1395\":2,\"1398\":3,\"1399\":1,\"1400\":2,\"1401\":1,\"1402\":2,\"1405\":1,\"1406\":4,\"1409\":1,\"1410\":2,\"1413\":1,\"1414\":2,\"2360\":3,\"2432\":1,\"2458\":3,\"2523\":3,\"2582\":3}}],[\"read\",{\"0\":{\"621\":1,\"1398\":1,\"1417\":1,\"1420\":2,\"1422\":2,\"1424\":2,\"1426\":1,\"2386\":1},\"1\":{\"99\":1,\"127\":1,\"619\":1,\"621\":3,\"1013\":1,\"1014\":2,\"1143\":1,\"1398\":2,\"1417\":2,\"1419\":1,\"1420\":3,\"1421\":1,\"1422\":5,\"1424\":3,\"1425\":1,\"1426\":2,\"2359\":2,\"2367\":1,\"2372\":1,\"2386\":3,\"2424\":1,\"2432\":1,\"2456\":2,\"2460\":2,\"2470\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2497\":1,\"2501\":1,\"2514\":1,\"2518\":1,\"2521\":2,\"2522\":1,\"2547\":1,\"2564\":1,\"2580\":2,\"2581\":1,\"2596\":1,\"2604\":1,\"2607\":1,\"2614\":1,\"2615\":1,\"2621\":1,\"2624\":1,\"2632\":1,\"2633\":1,\"2647\":1,\"2659\":1}}],[\"reasons\",{\"1\":{\"2419\":1}}],[\"reasonable\",{\"1\":{\"235\":1,\"2420\":1}}],[\"reason\",{\"1\":{\"44\":3,\"149\":1,\"1552\":1,\"1897\":1,\"2414\":1,\"2501\":1,\"2584\":1,\"2585\":1}}],[\"ret\",{\"1\":{\"1116\":1,\"1746\":1,\"1921\":1,\"1922\":1,\"1923\":1,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1947\":1}}],[\"retrained\",{\"1\":{\"2460\":1}}],[\"retrieved\",{\"1\":{\"1187\":1,\"1202\":1}}],[\"retrieve\",{\"1\":{\"698\":1,\"699\":1,\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1}}],[\"retrying\",{\"1\":{\"44\":1}}],[\"retval\",{\"1\":{\"59\":5,\"2097\":2}}],[\"return=true\",{\"1\":{\"742\":1}}],[\"returns\",{\"1\":{\"78\":1,\"600\":1,\"601\":3,\"607\":2,\"608\":1,\"612\":1,\"617\":1,\"618\":1,\"619\":1,\"629\":2,\"630\":1,\"631\":1,\"633\":1,\"635\":1,\"637\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"645\":1,\"646\":1,\"647\":1,\"651\":1,\"652\":2,\"656\":1,\"658\":1,\"661\":1,\"665\":1,\"672\":2,\"674\":1,\"676\":8,\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"690\":1,\"691\":7,\"692\":3,\"693\":2,\"695\":1,\"696\":2,\"697\":10,\"698\":1,\"699\":1,\"700\":7,\"701\":2,\"702\":1,\"703\":1,\"704\":1,\"706\":6,\"708\":1,\"711\":2,\"712\":3,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":6,\"726\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"734\":4,\"735\":1,\"737\":2,\"738\":1,\"740\":1,\"741\":1,\"742\":3,\"745\":4,\"746\":4,\"747\":1,\"749\":2,\"750\":2,\"754\":4,\"755\":1,\"758\":2,\"759\":3,\"762\":1,\"763\":1,\"764\":1,\"766\":1,\"767\":2,\"768\":1,\"770\":1,\"771\":2,\"772\":1,\"773\":2,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"781\":5,\"782\":1,\"784\":1,\"785\":3,\"786\":1,\"793\":1,\"796\":1,\"797\":6,\"799\":1,\"802\":1,\"804\":1,\"806\":7,\"809\":2,\"810\":1,\"812\":2,\"813\":1,\"815\":4,\"817\":3,\"818\":1,\"820\":3,\"821\":4,\"822\":1,\"824\":5,\"825\":7,\"826\":4,\"827\":1,\"828\":3,\"829\":2,\"830\":1,\"835\":2,\"838\":1,\"852\":2,\"855\":2,\"856\":1,\"857\":1,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"865\":1,\"866\":1,\"867\":1,\"869\":1,\"870\":1,\"872\":1,\"873\":1,\"874\":1,\"875\":1,\"877\":1,\"880\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"889\":1,\"890\":1,\"891\":1,\"892\":1,\"895\":1,\"896\":1,\"898\":1,\"899\":1,\"901\":1,\"903\":1,\"906\":1,\"908\":1,\"910\":1,\"911\":1,\"912\":1,\"914\":1,\"915\":1,\"917\":1,\"918\":1,\"922\":1,\"923\":1,\"925\":1,\"926\":1,\"927\":1,\"933\":1,\"934\":1,\"935\":1,\"956\":1,\"973\":1,\"1001\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1010\":1,\"1011\":1,\"1012\":1,\"1016\":1,\"1025\":1,\"1028\":1,\"1031\":1,\"1046\":6,\"1047\":1,\"1048\":8,\"1049\":2,\"1050\":2,\"1051\":1,\"1052\":4,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":2,\"1057\":3,\"1058\":2,\"1059\":3,\"1062\":1,\"1064\":1,\"1065\":2,\"1066\":8,\"1068\":2,\"1069\":5,\"1070\":1,\"1071\":4,\"1072\":1,\"1073\":7,\"1074\":1,\"1075\":7,\"1076\":6,\"1077\":1,\"1078\":1,\"1079\":3,\"1080\":1,\"1081\":2,\"1083\":6,\"1086\":2,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1116\":2,\"1132\":1,\"1133\":4,\"1138\":8,\"1139\":7,\"1140\":1,\"1141\":1,\"1142\":2,\"1145\":3,\"1148\":1,\"1149\":3,\"1150\":3,\"1153\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1169\":1,\"1170\":1,\"1173\":3,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":2,\"1186\":1,\"1190\":3,\"1198\":1,\"1200\":1,\"1203\":1,\"1204\":1,\"1209\":4,\"1210\":1,\"1214\":3,\"1221\":2,\"1243\":2,\"1244\":3,\"1246\":1,\"1247\":1,\"1248\":2,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1269\":1,\"1270\":7,\"1272\":1,\"1273\":3,\"1279\":1,\"1334\":1,\"1352\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1372\":1,\"1373\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1392\":1,\"1427\":1,\"1430\":1,\"1432\":1,\"1436\":1,\"1451\":1,\"1452\":1,\"1454\":1,\"1455\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1466\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1505\":1,\"1510\":1,\"1511\":2,\"1514\":1,\"1515\":1,\"1516\":2,\"1517\":1,\"1522\":1,\"1523\":1,\"1524\":3,\"1525\":2,\"1528\":1,\"1529\":1,\"1530\":1,\"1531\":3,\"1534\":1,\"1539\":1,\"1545\":1,\"1546\":1,\"1552\":1,\"1553\":1,\"1557\":1,\"1558\":1,\"1563\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1571\":1,\"1572\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1585\":1,\"1594\":1,\"1595\":1,\"1600\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1611\":1,\"1612\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1622\":1,\"1623\":1,\"1626\":1,\"1637\":1,\"1638\":3,\"1639\":1,\"1640\":1,\"1643\":1,\"1644\":3,\"1645\":1,\"1646\":1,\"1654\":1,\"1655\":1,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1680\":1,\"1688\":1,\"1693\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1717\":1,\"1719\":3,\"1735\":1,\"1736\":1,\"1739\":1,\"1741\":1,\"1746\":1,\"1752\":1,\"1755\":1,\"1756\":1,\"1758\":1,\"1759\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1771\":1,\"1772\":1,\"1773\":3,\"1776\":1,\"1777\":1,\"1778\":2,\"1781\":3,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1798\":1,\"1800\":2,\"1803\":1,\"1804\":2,\"1805\":2,\"1808\":3,\"1810\":1,\"1811\":1,\"1829\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":2,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":2,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":2,\"1851\":2,\"1852\":2,\"1853\":1,\"1855\":1,\"1856\":1,\"1857\":2,\"1858\":1,\"1859\":1,\"1860\":2,\"1861\":1,\"1862\":2,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":2,\"1872\":1,\"1873\":1,\"1874\":1,\"1876\":1,\"1877\":2,\"1878\":2,\"1879\":1,\"1880\":1,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1889\":1,\"1904\":1,\"1910\":1,\"1916\":1,\"1917\":1,\"1918\":2,\"1921\":1,\"1922\":1,\"1923\":1,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1947\":1,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"1963\":1,\"1971\":1,\"1983\":1,\"1986\":1,\"1993\":1,\"1999\":1,\"2000\":1,\"2001\":3,\"2002\":2,\"2004\":1,\"2029\":1,\"2049\":1,\"2054\":1,\"2078\":1,\"2079\":1,\"2081\":1,\"2082\":3,\"2083\":2,\"2084\":2,\"2086\":2,\"2087\":2,\"2088\":1,\"2089\":1,\"2090\":2,\"2091\":1,\"2095\":2,\"2097\":5,\"2142\":1,\"2154\":1,\"2155\":2,\"2170\":1,\"2193\":2,\"2199\":1,\"2239\":1,\"2240\":3,\"2243\":2,\"2244\":2,\"2245\":1,\"2252\":1,\"2254\":1,\"2255\":2,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":2,\"2260\":4,\"2261\":1,\"2262\":1,\"2263\":2,\"2264\":2,\"2265\":1,\"2267\":3,\"2268\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2274\":1,\"2278\":3,\"2279\":2,\"2280\":1,\"2325\":1,\"2330\":1,\"2644\":1}}],[\"return\",{\"1\":{\"56\":2,\"59\":2,\"60\":6,\"174\":1,\"175\":1,\"217\":1,\"224\":1,\"231\":1,\"593\":1,\"594\":2,\"605\":2,\"608\":1,\"618\":1,\"619\":1,\"621\":1,\"629\":3,\"633\":1,\"635\":1,\"637\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"645\":1,\"646\":1,\"658\":1,\"672\":2,\"674\":1,\"676\":8,\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"690\":1,\"691\":10,\"692\":3,\"693\":5,\"695\":1,\"696\":1,\"697\":13,\"700\":7,\"701\":2,\"702\":1,\"703\":1,\"705\":2,\"706\":3,\"708\":1,\"710\":3,\"711\":2,\"712\":3,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":6,\"726\":1,\"729\":1,\"730\":1,\"731\":3,\"732\":1,\"734\":5,\"735\":1,\"737\":2,\"738\":1,\"740\":1,\"741\":1,\"742\":5,\"743\":1,\"745\":3,\"746\":3,\"747\":1,\"754\":6,\"755\":1,\"758\":2,\"759\":3,\"762\":1,\"763\":1,\"764\":1,\"766\":1,\"767\":3,\"768\":1,\"770\":1,\"771\":2,\"772\":1,\"773\":2,\"774\":1,\"775\":1,\"776\":1,\"781\":5,\"782\":1,\"784\":1,\"785\":3,\"786\":1,\"792\":2,\"793\":1,\"794\":4,\"796\":1,\"797\":8,\"799\":2,\"802\":1,\"804\":1,\"806\":5,\"809\":2,\"810\":1,\"812\":2,\"813\":1,\"815\":3,\"817\":3,\"818\":1,\"819\":1,\"820\":4,\"821\":5,\"822\":1,\"824\":5,\"825\":3,\"826\":6,\"827\":1,\"828\":4,\"830\":1,\"835\":2,\"838\":1,\"852\":2,\"855\":1,\"856\":1,\"857\":1,\"858\":1,\"863\":1,\"865\":1,\"866\":1,\"867\":1,\"869\":1,\"870\":2,\"872\":1,\"873\":1,\"874\":1,\"875\":2,\"877\":1,\"878\":2,\"883\":1,\"884\":1,\"885\":2,\"886\":1,\"887\":1,\"889\":1,\"890\":1,\"891\":1,\"898\":1,\"899\":1,\"901\":1,\"903\":1,\"905\":2,\"906\":1,\"908\":1,\"911\":1,\"912\":1,\"914\":1,\"915\":1,\"917\":2,\"918\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"933\":2,\"934\":1,\"935\":1,\"944\":1,\"982\":1,\"985\":1,\"990\":1,\"993\":1,\"1003\":1,\"1004\":1,\"1013\":4,\"1016\":1,\"1025\":1,\"1046\":2,\"1048\":8,\"1049\":2,\"1050\":2,\"1051\":1,\"1052\":4,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":2,\"1057\":3,\"1058\":2,\"1059\":1,\"1062\":1,\"1064\":1,\"1065\":2,\"1066\":5,\"1068\":2,\"1069\":3,\"1070\":1,\"1071\":3,\"1072\":1,\"1073\":5,\"1074\":1,\"1075\":4,\"1076\":3,\"1077\":1,\"1078\":1,\"1079\":2,\"1081\":2,\"1083\":3,\"1086\":2,\"1096\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1132\":2,\"1133\":14,\"1138\":8,\"1139\":7,\"1140\":1,\"1141\":1,\"1145\":3,\"1148\":4,\"1153\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1173\":1,\"1177\":1,\"1182\":1,\"1187\":1,\"1190\":3,\"1195\":1,\"1198\":1,\"1199\":1,\"1202\":1,\"1203\":4,\"1204\":1,\"1206\":1,\"1208\":1,\"1209\":3,\"1214\":3,\"1221\":1,\"1244\":3,\"1252\":3,\"1253\":3,\"1254\":3,\"1255\":2,\"1270\":5,\"1272\":3,\"1273\":7,\"1279\":1,\"1286\":1,\"1287\":1,\"1332\":1,\"1339\":1,\"1342\":1,\"1352\":1,\"1356\":1,\"1368\":1,\"1370\":1,\"1372\":1,\"1373\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1379\":1,\"1392\":1,\"1424\":1,\"1426\":1,\"1427\":1,\"1430\":1,\"1432\":1,\"1436\":1,\"1451\":1,\"1454\":1,\"1455\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1466\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1505\":1,\"1510\":1,\"1511\":2,\"1514\":1,\"1515\":1,\"1516\":2,\"1522\":1,\"1523\":1,\"1524\":3,\"1525\":2,\"1528\":1,\"1529\":1,\"1530\":1,\"1534\":1,\"1539\":1,\"1545\":1,\"1546\":1,\"1552\":2,\"1553\":1,\"1557\":1,\"1558\":1,\"1563\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1571\":1,\"1572\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1585\":1,\"1594\":1,\"1595\":1,\"1600\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1611\":1,\"1612\":1,\"1615\":1,\"1616\":1,\"1622\":1,\"1623\":1,\"1626\":1,\"1637\":1,\"1639\":1,\"1640\":1,\"1643\":1,\"1644\":2,\"1645\":1,\"1646\":1,\"1654\":1,\"1655\":1,\"1658\":1,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1664\":1,\"1665\":1,\"1666\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1680\":1,\"1695\":1,\"1696\":2,\"1697\":2,\"1698\":2,\"1702\":1,\"1703\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1711\":1,\"1712\":2,\"1713\":1,\"1715\":2,\"1717\":1,\"1719\":4,\"1736\":1,\"1739\":1,\"1741\":1,\"1746\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1771\":1,\"1772\":1,\"1773\":6,\"1776\":1,\"1777\":1,\"1778\":4,\"1781\":3,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1798\":1,\"1800\":2,\"1803\":1,\"1804\":2,\"1805\":4,\"1808\":3,\"1810\":1,\"1811\":1,\"1828\":1,\"1829\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":4,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":2,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":4,\"1851\":2,\"1852\":4,\"1853\":1,\"1855\":1,\"1856\":1,\"1857\":2,\"1858\":1,\"1859\":1,\"1860\":2,\"1861\":1,\"1862\":3,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":2,\"1872\":1,\"1873\":1,\"1874\":1,\"1876\":1,\"1877\":4,\"1878\":2,\"1879\":1,\"1880\":2,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1889\":1,\"1904\":1,\"1910\":1,\"1916\":1,\"1917\":1,\"1918\":2,\"1921\":1,\"1922\":1,\"1923\":1,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1947\":1,\"1953\":1,\"1955\":1,\"1957\":2,\"1958\":1,\"1960\":2,\"1963\":2,\"1968\":1,\"1971\":2,\"1980\":4,\"1984\":1,\"1985\":1,\"1986\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1993\":1,\"1999\":1,\"2000\":1,\"2001\":5,\"2002\":1,\"2004\":2,\"2049\":1,\"2054\":1,\"2076\":1,\"2077\":4,\"2078\":1,\"2079\":1,\"2081\":1,\"2082\":6,\"2083\":2,\"2084\":2,\"2086\":2,\"2087\":2,\"2088\":1,\"2089\":1,\"2090\":2,\"2091\":1,\"2095\":2,\"2096\":1,\"2097\":3,\"2098\":1,\"2099\":3,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2142\":1,\"2154\":1,\"2155\":2,\"2168\":1,\"2170\":10,\"2193\":1,\"2235\":4,\"2239\":1,\"2240\":6,\"2243\":2,\"2244\":2,\"2245\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2252\":1,\"2254\":1,\"2255\":2,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":2,\"2260\":4,\"2261\":1,\"2262\":1,\"2263\":2,\"2264\":2,\"2265\":1,\"2267\":3,\"2268\":1,\"2270\":1,\"2272\":1,\"2273\":1,\"2274\":1,\"2277\":4,\"2278\":6,\"2279\":2,\"2280\":1,\"2358\":1,\"2472\":1,\"2476\":1,\"2500\":1,\"2520\":1,\"2579\":1,\"2617\":1,\"2635\":1,\"2644\":1,\"2648\":1}}],[\"returned\",{\"1\":{\"44\":1,\"1187\":2,\"1202\":2,\"1286\":2,\"1287\":2,\"1298\":1,\"1299\":1,\"1302\":1,\"1303\":1,\"1371\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1598\":1,\"1600\":1,\"1603\":1,\"1622\":1,\"1652\":1,\"1654\":1,\"1739\":2,\"2153\":1}}],[\"refineblock\",{\"0\":{\"1627\":1},\"1\":{\"1627\":1}}],[\"refines\",{\"1\":{\"802\":1}}],[\"reflectionpad1d\",{\"1\":{\"1856\":1,\"1857\":1,\"1858\":1,\"1867\":1,\"1870\":1}}],[\"reflect\",{\"1\":{\"953\":1,\"967\":1,\"970\":1}}],[\"ref2\",{\"1\":{\"280\":1,\"528\":1,\"1551\":2,\"1554\":2}}],[\"ref1\",{\"1\":{\"280\":1,\"1551\":2,\"1554\":2}}],[\"reffiles\",{\"1\":{\"280\":1,\"528\":2}}],[\"refs\",{\"1\":{\"207\":1,\"546\":3,\"551\":3,\"554\":3,\"557\":3,\"2457\":1}}],[\"ref\",{\"1\":{\"47\":3,\"251\":2,\"350\":2,\"357\":2,\"363\":4,\"368\":2,\"528\":5,\"549\":1,\"572\":2,\"729\":1,\"756\":1,\"1109\":1,\"1158\":1,\"1218\":7,\"1239\":1,\"1430\":2,\"1437\":1,\"1443\":1,\"1463\":2,\"1466\":2,\"1470\":2,\"1471\":2,\"1515\":1,\"1523\":2,\"1524\":1,\"1530\":2,\"1551\":1,\"1553\":4,\"1554\":3,\"1560\":2,\"1563\":2,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1570\":1,\"1571\":2,\"1600\":2,\"1603\":2,\"1611\":1,\"1622\":3,\"1639\":2,\"1640\":2,\"1641\":1,\"1646\":1,\"1660\":1,\"1666\":2,\"1668\":2,\"1670\":2,\"1671\":3,\"1713\":1,\"1714\":1,\"1733\":1,\"1791\":1,\"1799\":1,\"2019\":1,\"2181\":3,\"2184\":7,\"2200\":7,\"2260\":3,\"2262\":5,\"2368\":1,\"2371\":1,\"2461\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":21,\"2605\":1,\"2609\":1,\"2612\":1,\"2617\":21,\"2622\":1,\"2626\":1,\"2630\":1,\"2635\":21,\"2640\":2,\"2644\":2}}],[\"refused\",{\"1\":{\"44\":2}}],[\"referece\",{\"1\":{\"2257\":1,\"2261\":1}}],[\"referenceencoder\",{\"0\":{\"2257\":1},\"1\":{\"2257\":1}}],[\"references\",{\"1\":{\"1084\":1,\"1144\":1,\"1228\":1,\"1345\":1,\"1347\":1,\"1462\":1,\"1528\":1,\"1529\":1,\"1568\":1,\"2380\":1,\"2388\":1,\"2406\":1,\"2419\":1,\"2421\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":1,\"2544\":1,\"2638\":2,\"2640\":1}}],[\"reference\",{\"1\":{\"280\":1,\"678\":1,\"681\":1,\"682\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"700\":1,\"750\":2,\"758\":1,\"885\":3,\"899\":1,\"900\":2,\"901\":1,\"902\":2,\"1037\":1,\"1047\":1,\"1049\":1,\"1056\":1,\"1061\":1,\"1067\":1,\"1072\":1,\"1076\":1,\"1080\":1,\"1082\":1,\"1101\":1,\"1102\":1,\"1138\":1,\"1139\":1,\"1257\":1,\"1430\":1,\"1455\":1,\"1463\":2,\"1466\":1,\"1470\":1,\"1471\":1,\"1515\":2,\"1522\":1,\"1523\":2,\"1560\":1,\"1566\":1,\"1572\":1,\"1576\":1,\"1577\":1,\"1581\":1,\"1603\":2,\"1604\":1,\"1639\":2,\"1640\":1,\"1645\":1,\"1655\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1670\":2,\"1671\":3,\"1695\":1,\"1696\":4,\"1697\":3,\"1698\":3,\"1704\":3,\"1705\":3,\"1706\":3,\"1707\":3,\"1708\":2,\"1712\":3,\"1713\":4,\"1715\":3,\"1719\":3,\"2032\":1,\"2040\":1,\"2152\":1,\"2257\":9,\"2260\":1,\"2261\":5,\"2262\":2,\"2359\":1,\"2390\":1,\"2456\":2,\"2460\":2,\"2500\":1,\"2514\":5,\"2521\":2,\"2526\":1,\"2580\":1,\"2617\":1,\"2635\":1,\"2638\":1,\"2640\":1,\"2642\":1,\"2659\":5}}],[\"refers\",{\"1\":{\"1454\":2,\"2412\":1,\"2467\":1}}],[\"referred\",{\"1\":{\"58\":1,\"75\":1,\"76\":1,\"77\":1,\"78\":1,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"2168\":1,\"2170\":1}}],[\"refer\",{\"1\":{\"24\":1,\"33\":1,\"95\":1,\"103\":1,\"115\":1,\"120\":1,\"124\":2,\"148\":1,\"1142\":1,\"1180\":1,\"1186\":1,\"1210\":1,\"1211\":2,\"1224\":1,\"1269\":1,\"1286\":2,\"1287\":1,\"1301\":1,\"1304\":1,\"1336\":1,\"1337\":2,\"1349\":1,\"1350\":1,\"1371\":1,\"1432\":1,\"1436\":1,\"1510\":1,\"1511\":1,\"1643\":1,\"1644\":1,\"1976\":1,\"2384\":1,\"2394\":1,\"2401\":1,\"2411\":1,\"2429\":1,\"2437\":1,\"2530\":1,\"2537\":1,\"2554\":1,\"2558\":1,\"2563\":1}}],[\"regresar\",{\"1\":{\"2457\":1}}],[\"regressive\",{\"1\":{\"735\":1,\"754\":3,\"803\":1,\"2080\":1,\"2243\":2}}],[\"reg\",{\"0\":{\"1746\":1},\"1\":{\"1746\":4}}],[\"regulates\",{\"1\":{\"1781\":1}}],[\"regulator\",{\"0\":{\"774\":1,\"1781\":1,\"1842\":1},\"1\":{\"774\":5,\"1781\":4,\"1842\":1}}],[\"regularized\",{\"1\":{\"1746\":1,\"2252\":1}}],[\"regularizing\",{\"1\":{\"837\":1}}],[\"regularization\",{\"1\":{\"24\":1,\"113\":2,\"825\":1,\"1026\":1,\"1035\":1,\"1036\":1,\"1142\":2,\"1186\":2,\"1210\":2,\"1211\":2,\"1224\":2,\"1287\":2,\"1301\":2,\"1304\":2,\"1337\":2,\"1349\":2,\"1350\":2,\"1746\":2}}],[\"regular\",{\"1\":{\"24\":1,\"113\":2,\"1482\":1,\"1529\":1,\"1568\":1,\"1576\":1,\"1577\":1}}],[\"registry\",{\"1\":{\"1319\":1,\"1327\":3}}],[\"registering\",{\"1\":{\"1217\":1}}],[\"register\",{\"0\":{\"675\":1},\"1\":{\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"675\":2,\"1217\":2,\"1800\":2,\"2193\":1,\"2199\":1}}],[\"registered\",{\"1\":{\"626\":1,\"727\":2,\"728\":2,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1012\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1327\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1}}],[\"region\",{\"1\":{\"115\":2,\"1096\":2}}],[\"regard\",{\"1\":{\"629\":1,\"2385\":1}}],[\"regarding\",{\"1\":{\"48\":1}}],[\"regardless\",{\"1\":{\"33\":1,\"46\":1,\"72\":1,\"95\":1,\"148\":1}}],[\"relevant\",{\"1\":{\"1243\":1}}],[\"release\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"200\":1,\"234\":1}}],[\"released\",{\"1\":{\"84\":1}}],[\"reload\",{\"1\":{\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1240\":1,\"1269\":1,\"1792\":1,\"1957\":1,\"2401\":1,\"2537\":1}}],[\"relpositionalencoding\",{\"0\":{\"810\":1,\"1077\":1},\"1\":{\"810\":1,\"1077\":1}}],[\"relpositionmultiheadedattention\",{\"0\":{\"809\":1,\"1076\":1},\"1\":{\"711\":1,\"771\":1,\"809\":2,\"1076\":2}}],[\"relativepositionalencoding\",{\"1\":{\"1077\":1}}],[\"relativepositionbias\",{\"0\":{\"1078\":1},\"1\":{\"116\":1,\"1065\":1,\"1066\":1,\"1078\":3}}],[\"relative\",{\"1\":{\"116\":1,\"124\":1,\"706\":1,\"771\":2,\"772\":1,\"809\":2,\"810\":1,\"815\":1,\"935\":1,\"1065\":1,\"1066\":1,\"1076\":1,\"1077\":1,\"1078\":3,\"1079\":3,\"1148\":2,\"1203\":2,\"1255\":1,\"1713\":1,\"1714\":1,\"1798\":1,\"1851\":2,\"1874\":1,\"1878\":1,\"2054\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2279\":2}}],[\"relationship\",{\"0\":{\"95\":1},\"1\":{\"2387\":1}}],[\"relation\",{\"0\":{\"72\":1}}],[\"related\",{\"0\":{\"303\":1,\"304\":1,\"305\":1,\"309\":1,\"310\":1,\"311\":1,\"312\":1,\"313\":1,\"314\":1,\"317\":1,\"318\":1,\"319\":1,\"320\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"329\":1,\"330\":1,\"331\":1,\"332\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"341\":1,\"342\":1,\"345\":1,\"346\":1,\"347\":1,\"348\":1,\"349\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"365\":1,\"366\":1,\"367\":1,\"370\":1,\"371\":1,\"372\":1,\"373\":1,\"374\":1,\"382\":1,\"383\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"406\":1,\"407\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"418\":1,\"419\":1,\"420\":1,\"421\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"439\":1,\"440\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"451\":1,\"452\":1,\"453\":1,\"454\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"463\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"472\":1,\"473\":1,\"474\":1,\"475\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"484\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"2643\":1},\"1\":{\"20\":1,\"57\":1,\"83\":1,\"85\":1,\"112\":1,\"117\":1,\"124\":1,\"134\":1,\"594\":1,\"1210\":2,\"1454\":2,\"2001\":1,\"2002\":1,\"2004\":1,\"2046\":1,\"2086\":1,\"2087\":1,\"2374\":1,\"2393\":1,\"2401\":2,\"2427\":1,\"2434\":1,\"2529\":1,\"2537\":2,\"2550\":1,\"2557\":1,\"2638\":1}}],[\"rel\",{\"0\":{\"935\":1},\"1\":{\"62\":1,\"116\":2,\"771\":2,\"809\":2,\"935\":1,\"1065\":2,\"1066\":2,\"1076\":2,\"1140\":3,\"1148\":4,\"1149\":1,\"1169\":3,\"1203\":2,\"1505\":2,\"1771\":2,\"1778\":3,\"1787\":2,\"1788\":4,\"1798\":2,\"1804\":2,\"1805\":2,\"1850\":3,\"1851\":4,\"1852\":3,\"1874\":2,\"1877\":2,\"1878\":2,\"2003\":3,\"2022\":1,\"2026\":3,\"2054\":4,\"2090\":3,\"2243\":4,\"2244\":4,\"2255\":4,\"2279\":4,\"2440\":3,\"2564\":3}}],[\"relu\",{\"1\":{\"21\":4,\"115\":2,\"712\":3,\"725\":1,\"726\":2,\"858\":2,\"1051\":1,\"1052\":3,\"1065\":1,\"1069\":1,\"1070\":1,\"1115\":2,\"1134\":2,\"1229\":1,\"1231\":2,\"1375\":1,\"1430\":1,\"1470\":1,\"1505\":2,\"1515\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1528\":1,\"1529\":1,\"1534\":2,\"1537\":1,\"1539\":4,\"1543\":1,\"1581\":1,\"1626\":1,\"1654\":2,\"1655\":1,\"1656\":1,\"1658\":2,\"1659\":3,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":2,\"1670\":1,\"1671\":1,\"1719\":1}}],[\"rely\",{\"1\":{\"18\":1,\"112\":1,\"113\":1}}],[\"re\",{\"1\":{\"14\":1,\"760\":1,\"778\":1,\"831\":1,\"1243\":1,\"1476\":1,\"1598\":1,\"1750\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2084\":1,\"2089\":1,\"2368\":1,\"2486\":1,\"2490\":1,\"2584\":1,\"2585\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"remark\",{\"1\":{\"1198\":1}}],[\"remaining\",{\"1\":{\"1849\":1}}],[\"remain\",{\"1\":{\"6\":1}}],[\"removed\",{\"1\":{\"52\":1,\"868\":1,\"933\":1,\"1652\":1,\"1654\":1,\"2347\":1,\"2373\":1,\"2433\":1,\"2555\":1,\"2567\":1}}],[\"remove\",{\"0\":{\"288\":1,\"1429\":1,\"1822\":1,\"2331\":1,\"2332\":1},\"1\":{\"11\":1,\"19\":1,\"217\":1,\"224\":1,\"231\":1,\"288\":1,\"461\":2,\"922\":1,\"1429\":3,\"1765\":2,\"1800\":2,\"1803\":2,\"1807\":1,\"1822\":1,\"1844\":2,\"1849\":4,\"1857\":2,\"1858\":2,\"1861\":2,\"1862\":2,\"1871\":2,\"1880\":2,\"2120\":1,\"2130\":1,\"2136\":1,\"2137\":1,\"2295\":1,\"2296\":1,\"2331\":2,\"2332\":2,\"2347\":2,\"2373\":1,\"2433\":1,\"2555\":1}}],[\"remember\",{\"1\":{\"2\":1,\"3\":1,\"99\":1,\"2398\":1,\"2534\":1,\"2572\":1,\"2638\":2,\"2642\":1}}],[\"recreating\",{\"1\":{\"2430\":1,\"2555\":1}}],[\"recall\",{\"1\":{\"2410\":1}}],[\"recap\",{\"0\":{\"193\":1}}],[\"recién\",{\"1\":{\"2457\":1}}],[\"recitation\",{\"1\":{\"2380\":1,\"2388\":1,\"2421\":1,\"2524\":1,\"2544\":1}}],[\"recipe\",{\"0\":{\"176\":1,\"233\":1,\"235\":1,\"2379\":1,\"2636\":1,\"2637\":1},\"1\":{\"2\":1,\"3\":1,\"15\":3,\"16\":1,\"17\":2,\"31\":1,\"46\":1,\"48\":3,\"85\":2,\"96\":2,\"139\":2,\"140\":1,\"162\":1,\"164\":3,\"165\":2,\"169\":1,\"181\":1,\"182\":1,\"197\":1,\"198\":1,\"201\":1,\"233\":2,\"234\":1,\"235\":10,\"243\":3,\"244\":1,\"274\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2372\":4,\"2373\":1,\"2374\":1,\"2377\":1,\"2384\":2,\"2385\":5,\"2387\":4,\"2391\":1,\"2394\":3,\"2398\":1,\"2412\":1,\"2423\":1,\"2429\":4,\"2430\":1,\"2434\":1,\"2436\":1,\"2527\":1,\"2530\":3,\"2534\":1,\"2546\":1,\"2554\":4,\"2555\":1,\"2557\":1,\"2562\":1,\"2564\":1,\"2569\":1,\"2583\":1,\"2584\":3,\"2585\":1,\"2618\":2,\"2637\":3,\"2638\":2,\"2640\":1}}],[\"recipes\",{\"0\":{\"2\":1,\"85\":1},\"1\":{\"2\":1,\"3\":1,\"17\":1,\"48\":2,\"75\":1,\"84\":1,\"85\":5,\"132\":1,\"135\":1,\"136\":1,\"141\":1,\"162\":1,\"163\":1,\"164\":1,\"204\":1,\"235\":1,\"237\":1,\"2354\":1,\"2372\":3,\"2384\":5,\"2385\":1,\"2388\":1,\"2421\":1,\"2423\":1,\"2429\":3,\"2436\":1,\"2452\":1,\"2524\":1,\"2544\":1,\"2546\":1,\"2554\":3,\"2562\":1,\"2574\":1,\"2635\":1,\"2640\":1}}],[\"recursive\",{\"0\":{\"1965\":1,\"2161\":2,\"2162\":2,\"2163\":2},\"1\":{\"1356\":1,\"1961\":1,\"1965\":1,\"2161\":2,\"2162\":2,\"2163\":2}}],[\"recursive=false\",{\"1\":{\"1356\":1}}],[\"recursive=true\",{\"1\":{\"1355\":1,\"2514\":2,\"2659\":2}}],[\"recursively\",{\"1\":{\"1143\":1,\"2153\":1,\"2166\":1,\"2324\":1}}],[\"recurrently\",{\"1\":{\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1177\":1,\"1209\":1,\"1252\":1,\"1253\":2,\"1254\":1}}],[\"recurrent\",{\"1\":{\"685\":1,\"838\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1164\":1,\"1165\":1,\"1177\":1,\"1209\":1,\"1222\":1,\"1243\":1,\"1248\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1282\":1,\"1522\":1,\"1523\":1,\"1572\":1,\"2400\":1,\"2536\":1}}],[\"rec=8\",{\"1\":{\"749\":1}}],[\"rec\",{\"1\":{\"286\":1,\"296\":1}}],[\"recession\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"receptive=\",{\"1\":{\"2074\":1,\"2075\":1}}],[\"receptive\",{\"1\":{\"1862\":2,\"1880\":2}}],[\"receptance\",{\"1\":{\"1074\":1}}],[\"recently\",{\"1\":{\"2400\":1,\"2536\":1,\"2543\":1}}],[\"recent\",{\"1\":{\"45\":3,\"2462\":1,\"2473\":1}}],[\"receives\",{\"1\":{\"795\":1,\"1244\":1,\"1252\":1,\"1254\":1}}],[\"received\",{\"1\":{\"60\":1}}],[\"receive\",{\"1\":{\"6\":1}}],[\"reconstructed\",{\"1\":{\"2325\":1}}],[\"reconstruction\",{\"1\":{\"1735\":1,\"1860\":1}}],[\"reconstructs\",{\"1\":{\"1735\":1}}],[\"reconfigure\",{\"1\":{\"1643\":1,\"1644\":1}}],[\"recover\",{\"1\":{\"1522\":1}}],[\"recommmend\",{\"1\":{\"952\":1}}],[\"recommendations\",{\"1\":{\"1245\":2}}],[\"recommended\",{\"1\":{\"45\":1,\"79\":1,\"102\":1,\"1211\":1,\"1241\":1,\"1245\":1,\"1286\":1,\"1336\":1,\"1522\":1,\"2639\":1}}],[\"recommend\",{\"1\":{\"25\":1,\"38\":1,\"40\":1,\"48\":1,\"295\":1,\"2372\":1,\"2384\":1,\"2394\":1,\"2428\":1,\"2530\":1,\"2551\":1}}],[\"recombined\",{\"1\":{\"912\":1,\"1048\":1}}],[\"recombine\",{\"0\":{\"912\":1},\"1\":{\"912\":2,\"1048\":2}}],[\"recompute\",{\"1\":{\"699\":3}}],[\"recorder\",{\"1\":{\"2360\":5,\"2458\":5,\"2523\":5,\"2582\":5}}],[\"recorded\",{\"0\":{\"2359\":1},\"1\":{\"2387\":1,\"2501\":1,\"2522\":1,\"2581\":1,\"2586\":1,\"2600\":1}}],[\"recordings\",{\"0\":{\"2360\":1,\"2456\":1,\"2458\":1,\"2521\":1,\"2522\":1,\"2523\":1,\"2580\":1,\"2581\":1,\"2582\":1,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1},\"1\":{\"2481\":1,\"2501\":1,\"2522\":1,\"2581\":1,\"2618\":1}}],[\"recording\",{\"1\":{\"1400\":1,\"2523\":1}}],[\"record\",{\"1\":{\"51\":3,\"286\":1,\"296\":1,\"2360\":7,\"2371\":1,\"2458\":7,\"2523\":7,\"2582\":7,\"2596\":3,\"2612\":1,\"2617\":1,\"2630\":1,\"2635\":1}}],[\"recoding\",{\"0\":{\"51\":1}}],[\"recognizing\",{\"1\":{\"2467\":1}}],[\"recognizes\",{\"1\":{\"2467\":1,\"2474\":1,\"2649\":1}}],[\"recognized\",{\"1\":{\"549\":1}}],[\"recognize\",{\"0\":{\"175\":1,\"192\":1,\"197\":1,\"2359\":1,\"2360\":1,\"2580\":1,\"2581\":1,\"2582\":1,\"2593\":1,\"2594\":1,\"2596\":1},\"1\":{\"175\":2,\"194\":2,\"195\":1,\"197\":1,\"676\":3,\"692\":2,\"731\":1,\"742\":1,\"812\":1,\"816\":1,\"836\":1,\"2360\":1,\"2581\":1,\"2582\":1,\"2586\":1,\"2592\":1,\"2593\":1}}],[\"recognition\",{\"0\":{\"155\":1,\"166\":1,\"176\":1,\"2356\":1,\"2406\":1,\"2410\":1,\"2417\":1,\"2723\":1,\"2726\":1},\"1\":{\"16\":1,\"19\":1,\"130\":1,\"148\":1,\"150\":1,\"161\":1,\"164\":2,\"170\":1,\"171\":1,\"197\":1,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"601\":4,\"645\":1,\"647\":1,\"704\":1,\"705\":2,\"729\":1,\"731\":1,\"742\":2,\"812\":1,\"880\":1,\"940\":1,\"1037\":1,\"1198\":2,\"1257\":1,\"1524\":1,\"1712\":1,\"1715\":1,\"2030\":2,\"2040\":1,\"2046\":1,\"2055\":1,\"2068\":1,\"2070\":1,\"2198\":1,\"2354\":2,\"2372\":2,\"2373\":1,\"2380\":2,\"2385\":2,\"2388\":4,\"2406\":1,\"2407\":1,\"2409\":1,\"2410\":2,\"2411\":1,\"2415\":1,\"2417\":1,\"2418\":2,\"2419\":1,\"2421\":2,\"2429\":2,\"2430\":1,\"2467\":1,\"2499\":1,\"2521\":1,\"2524\":3,\"2544\":2,\"2554\":2,\"2555\":1,\"2583\":1,\"2593\":1,\"2617\":1,\"2635\":1}}],[\"recog\",{\"0\":{\"249\":1,\"286\":1,\"649\":1,\"650\":2},\"1\":{\"19\":1,\"24\":1,\"150\":1,\"175\":1,\"194\":1,\"197\":4,\"247\":2,\"249\":4,\"286\":7,\"649\":2,\"650\":6,\"676\":4,\"742\":2,\"816\":2,\"836\":2}}],[\"repititons\",{\"1\":{\"1870\":1}}],[\"rep\",{\"1\":{\"1655\":2,\"1719\":1}}],[\"replacing\",{\"1\":{\"1269\":1}}],[\"replacement\",{\"0\":{\"1950\":1},\"1\":{\"1950\":1,\"2398\":1,\"2534\":1}}],[\"replaced\",{\"1\":{\"1429\":1}}],[\"replaces\",{\"1\":{\"713\":1}}],[\"replace\",{\"0\":{\"1944\":1},\"1\":{\"3\":1,\"62\":1,\"92\":1,\"100\":1,\"175\":3,\"194\":3,\"217\":1,\"224\":1,\"231\":6,\"245\":2,\"255\":2,\"259\":2,\"301\":2,\"786\":1,\"905\":1,\"913\":1,\"943\":1,\"950\":1,\"955\":1,\"965\":2,\"968\":2,\"972\":2,\"1019\":2,\"1037\":2,\"1039\":2,\"1220\":1,\"1257\":1,\"1400\":1,\"1712\":1,\"1715\":1,\"1914\":1,\"1915\":1,\"1940\":1,\"1944\":2,\"2398\":2,\"2472\":2,\"2476\":2,\"2500\":1,\"2534\":2,\"2617\":1,\"2635\":1,\"2648\":2}}],[\"replicated\",{\"1\":{\"774\":1}}],[\"repetitions\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"repetition\",{\"1\":{\"327\":2,\"449\":2,\"692\":1,\"2461\":1,\"2592\":1}}],[\"repeats=4\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"repeats\",{\"1\":{\"1379\":1,\"1655\":1,\"1664\":1,\"1665\":1,\"1719\":1,\"1870\":3}}],[\"repeat=1\",{\"1\":{\"1244\":1,\"1252\":1}}],[\"repeat=3\",{\"1\":{\"835\":1}}],[\"repeat=true\",{\"1\":{\"612\":1,\"997\":1,\"998\":1}}],[\"repeating\",{\"1\":{\"774\":1}}],[\"repeatable\",{\"1\":{\"25\":1}}],[\"repeat\",{\"0\":{\"787\":1,\"914\":2},\"1\":{\"21\":3,\"76\":1,\"725\":2,\"726\":2,\"787\":2,\"835\":2,\"858\":2,\"904\":1,\"914\":4,\"997\":2,\"998\":2,\"1244\":1,\"1252\":1,\"1379\":1,\"1664\":1,\"1665\":1,\"1804\":2,\"1805\":1,\"1811\":1,\"1863\":1}}],[\"repeated\",{\"0\":{\"1429\":1},\"1\":{\"21\":1,\"57\":1,\"725\":1,\"726\":1,\"858\":1,\"914\":1,\"1244\":1,\"1252\":1,\"1429\":2}}],[\"repositories\",{\"1\":{\"2585\":1}}],[\"repository\",{\"0\":{\"2585\":1},\"1\":{\"5\":1,\"99\":1,\"1605\":1,\"2380\":1,\"2388\":1,\"2406\":1,\"2421\":1,\"2446\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":1,\"2544\":1,\"2575\":1,\"2583\":1,\"2584\":1,\"2585\":1}}],[\"repo\",{\"1\":{\"99\":4,\"112\":1,\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2254\":1,\"2361\":1,\"2447\":1,\"2480\":1,\"2650\":1}}],[\"reportedvalue\",{\"0\":{\"2192\":1},\"1\":{\"2175\":1,\"2192\":2,\"2205\":1,\"2206\":1}}],[\"reported\",{\"0\":{\"2230\":1},\"1\":{\"168\":1,\"179\":1,\"607\":1,\"608\":1,\"2230\":2}}],[\"reporter=none\",{\"1\":{\"996\":2}}],[\"reporter\",{\"0\":{\"615\":1,\"811\":1,\"2175\":1,\"2192\":1,\"2193\":2,\"2199\":1,\"2205\":1,\"2206\":1,\"2230\":1,\"2232\":1},\"1\":{\"65\":1,\"615\":2,\"628\":1,\"629\":1,\"754\":2,\"811\":2,\"820\":2,\"821\":2,\"826\":2,\"996\":1,\"1962\":4,\"2175\":2,\"2185\":2,\"2192\":2,\"2193\":12,\"2198\":2,\"2199\":4,\"2201\":5,\"2203\":2,\"2205\":2,\"2206\":2,\"2230\":2,\"2232\":2}}],[\"reports\",{\"1\":{\"107\":1,\"754\":1,\"820\":1,\"821\":1,\"826\":1}}],[\"report\",{\"1\":{\"84\":1,\"251\":4,\"253\":2,\"255\":3,\"259\":5,\"265\":2,\"269\":2,\"608\":1,\"615\":2,\"750\":3,\"754\":1,\"811\":2,\"820\":1,\"821\":1,\"826\":1,\"1057\":6,\"1059\":4,\"1171\":2,\"1172\":1,\"1173\":4,\"1206\":2,\"1892\":2,\"1970\":1,\"1975\":2,\"1984\":3,\"2027\":2,\"2076\":3,\"2441\":1}}],[\"reporting\",{\"1\":{\"34\":1,\"113\":2,\"1057\":1}}],[\"repr\",{\"1\":{\"760\":1,\"778\":1,\"831\":1,\"1476\":1,\"1598\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2084\":1,\"2089\":1}}],[\"represented\",{\"1\":{\"1186\":1,\"1248\":1,\"1301\":1,\"1304\":1,\"1427\":1}}],[\"representing\",{\"1\":{\"1186\":2,\"1187\":1,\"1202\":1,\"1210\":2,\"1286\":1,\"1287\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1427\":1,\"1451\":2,\"1514\":2,\"1557\":2,\"1585\":2,\"1612\":2,\"1615\":2,\"1623\":2,\"1637\":2,\"1638\":1,\"2414\":1}}],[\"represents\",{\"1\":{\"238\":1,\"239\":1,\"1142\":2,\"1186\":3,\"1194\":1,\"1210\":3,\"1226\":1,\"1228\":2,\"1298\":5,\"1299\":5,\"1301\":5,\"1302\":5,\"1303\":5,\"1304\":5,\"1334\":4,\"1345\":2,\"1347\":2,\"2197\":1,\"2395\":1,\"2531\":1}}],[\"represent\",{\"1\":{\"60\":1,\"745\":1,\"746\":1,\"929\":1,\"1400\":1}}],[\"representations\",{\"0\":{\"100\":1},\"1\":{\"84\":1,\"100\":1,\"1269\":1,\"1551\":2,\"1553\":2,\"2399\":1,\"2401\":1,\"2467\":1,\"2535\":1,\"2537\":1,\"2543\":1,\"2574\":2}}],[\"representation\",{\"0\":{\"2415\":1},\"1\":{\"22\":1,\"102\":1,\"161\":1,\"760\":1,\"778\":1,\"831\":1,\"875\":1,\"1239\":1,\"1248\":1,\"1284\":1,\"1476\":1,\"1598\":1,\"1719\":1,\"1735\":1,\"1765\":1,\"1777\":1,\"1798\":3,\"1800\":1,\"1803\":1,\"1804\":2,\"1844\":1,\"1851\":1,\"1853\":1,\"1863\":1,\"1874\":1,\"1878\":2,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2049\":1,\"2050\":1,\"2055\":1,\"2064\":1,\"2070\":1,\"2084\":1,\"2089\":1,\"2401\":1,\"2415\":1,\"2537\":1}}],[\"reproduces\",{\"1\":{\"2099\":1,\"2102\":1}}],[\"reproducing\",{\"1\":{\"130\":1,\"168\":1,\"179\":1}}],[\"reproducible\",{\"1\":{\"82\":1,\"130\":1,\"2584\":1}}],[\"reproducibility\",{\"0\":{\"82\":1},\"1\":{\"1257\":1,\"1895\":1,\"1900\":1,\"2529\":1,\"2550\":1}}],[\"reproduction\",{\"1\":{\"21\":1}}],[\"reproducability\",{\"1\":{\"5\":1}}],[\"res2net\",{\"1\":{\"2049\":1,\"2055\":1,\"2064\":1}}],[\"resblocks\",{\"1\":{\"1804\":2,\"1878\":2}}],[\"resblock\",{\"0\":{\"2059\":1},\"1\":{\"1605\":1,\"1635\":1,\"1765\":4,\"1778\":2,\"1800\":4,\"1803\":4,\"1804\":4,\"1805\":2,\"1844\":4,\"1850\":2,\"1851\":4,\"1852\":2,\"1877\":2,\"1878\":4,\"2059\":1,\"2064\":1}}],[\"resamp\",{\"1\":{\"1605\":1}}],[\"resampling\",{\"1\":{\"952\":1,\"1923\":1}}],[\"resampler\",{\"1\":{\"1791\":1}}],[\"resample=none\",{\"1\":{\"1629\":1}}],[\"resample=1\",{\"1\":{\"1501\":1}}],[\"resample\",{\"0\":{\"1823\":1},\"1\":{\"952\":1,\"1508\":1,\"1791\":1,\"1823\":2}}],[\"reshapes\",{\"1\":{\"1561\":1}}],[\"reshape\",{\"1\":{\"1182\":1}}],[\"reshaped\",{\"1\":{\"1142\":1,\"1186\":1,\"1210\":2}}],[\"reshuffles\",{\"1\":{\"997\":1,\"998\":1}}],[\"resnetblockddpmpp\",{\"0\":{\"1635\":1},\"1\":{\"1635\":1}}],[\"resnetblockddpm\",{\"0\":{\"1633\":1},\"1\":{\"1633\":1}}],[\"resnetblockbigganpp\",{\"0\":{\"1631\":1},\"1\":{\"1631\":1}}],[\"resnet=none\",{\"1\":{\"1263\":1}}],[\"resnets\",{\"1\":{\"1252\":1}}],[\"resnet\",{\"0\":{\"1231\":1},\"1\":{\"1115\":4,\"1231\":1,\"1633\":1}}],[\"res\",{\"0\":{\"1872\":1,\"1873\":1},\"1\":{\"952\":1,\"1546\":1,\"1605\":1,\"1872\":1,\"1873\":1,\"1923\":2}}],[\"resize\",{\"1\":{\"950\":1,\"968\":2,\"1252\":1,\"2275\":1}}],[\"residual=false\",{\"1\":{\"2083\":1}}],[\"residual=none\",{\"1\":{\"1244\":1,\"1252\":1,\"1254\":1}}],[\"residualaffinecouplinglayer\",{\"0\":{\"1865\":1},\"1\":{\"1865\":2}}],[\"residualaffinecouplingblock\",{\"0\":{\"1864\":1},\"1\":{\"1864\":2}}],[\"residualblock\",{\"0\":{\"1629\":1,\"1866\":1,\"2258\":1},\"1\":{\"1629\":1,\"1866\":2,\"2258\":2}}],[\"residualstack\",{\"0\":{\"1867\":1},\"1\":{\"1867\":2}}],[\"residuals\",{\"1\":{\"1252\":1,\"1254\":1}}],[\"residual\",{\"0\":{\"1130\":1,\"1156\":1,\"1183\":1,\"1188\":1,\"1233\":2,\"1830\":1,\"1831\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1},\"1\":{\"251\":2,\"255\":2,\"259\":2,\"835\":1,\"1130\":4,\"1156\":3,\"1183\":2,\"1188\":3,\"1220\":1,\"1233\":7,\"1244\":3,\"1252\":4,\"1254\":3,\"1352\":2,\"1594\":1,\"1765\":3,\"1800\":3,\"1803\":3,\"1830\":1,\"1831\":1,\"1844\":3,\"1848\":1,\"1851\":3,\"1857\":2,\"1862\":4,\"1864\":3,\"1865\":2,\"1866\":2,\"1867\":2,\"1880\":9,\"2002\":1,\"2032\":1,\"2083\":1,\"2090\":1,\"2095\":1,\"2258\":1,\"2263\":1,\"2292\":1}}],[\"respect\",{\"1\":{\"1301\":1,\"1304\":1}}],[\"respective\",{\"1\":{\"22\":1,\"118\":1,\"648\":2}}],[\"respectively\",{\"0\":{\"39\":1},\"1\":{\"1\":1,\"21\":1,\"22\":1,\"25\":1,\"26\":2,\"37\":1,\"59\":1,\"74\":1,\"118\":1,\"728\":1,\"1767\":2,\"1856\":1,\"2154\":1,\"2357\":1,\"2578\":1,\"2638\":1,\"2641\":2,\"2642\":1}}],[\"response\",{\"0\":{\"1809\":1},\"1\":{\"885\":1,\"1696\":1,\"1698\":1,\"1706\":1,\"1707\":1,\"1809\":1,\"1883\":1}}],[\"resolve\",{\"0\":{\"2227\":1},\"1\":{\"2227\":1,\"2360\":6,\"2458\":6,\"2523\":6,\"2582\":6,\"2585\":1}}],[\"resolved\",{\"1\":{\"1154\":1}}],[\"resolutions=\",{\"1\":{\"1605\":1}}],[\"resolution\",{\"1\":{\"689\":1,\"1604\":1,\"2194\":1,\"2275\":1}}],[\"resource\",{\"1\":{\"144\":1,\"2574\":1,\"2584\":1,\"2585\":1}}],[\"resources\",{\"1\":{\"44\":2,\"2373\":1,\"2401\":2,\"2423\":1,\"2430\":1,\"2431\":1,\"2537\":2,\"2546\":1,\"2555\":1,\"2572\":1}}],[\"rescore\",{\"0\":{\"1427\":1,\"1428\":1,\"1429\":1},\"1\":{\"1427\":1,\"1428\":2,\"1429\":1}}],[\"rescoring\",{\"1\":{\"315\":2,\"485\":2}}],[\"resch\",{\"1\":{\"835\":1}}],[\"resch=512\",{\"1\":{\"835\":1}}],[\"rescale=true\",{\"1\":{\"1605\":1,\"1631\":1}}],[\"rescale=false\",{\"1\":{\"1458\":1,\"1635\":1}}],[\"rescale\",{\"1\":{\"116\":2,\"1075\":3}}],[\"rest=true\",{\"1\":{\"1388\":1}}],[\"rest\",{\"1\":{\"760\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1253\":2,\"1254\":1,\"1279\":1,\"1728\":1,\"2099\":1,\"2102\":1}}],[\"restore\",{\"0\":{\"651\":1},\"1\":{\"242\":1,\"651\":3}}],[\"restart\",{\"0\":{\"2018\":1},\"1\":{\"240\":1,\"2018\":2}}],[\"restriction\",{\"1\":{\"2153\":1}}],[\"restricts\",{\"1\":{\"2099\":1,\"2102\":1}}],[\"restricted\",{\"1\":{\"113\":1}}],[\"restrict\",{\"1\":{\"69\":1,\"1895\":1,\"1900\":1}}],[\"reserve\",{\"1\":{\"1652\":1,\"1654\":1}}],[\"reserved\",{\"1\":{\"1005\":1,\"1028\":1,\"1057\":1,\"1530\":1,\"1563\":1,\"2201\":1}}],[\"resencoder\",{\"0\":{\"1229\":1},\"1\":{\"1229\":1}}],[\"research\",{\"1\":{\"130\":1,\"199\":1,\"202\":1,\"295\":1,\"1005\":1,\"1028\":1,\"2363\":1,\"2506\":1,\"2543\":1,\"2573\":1,\"2653\":1}}],[\"reset\",{\"0\":{\"915\":1},\"1\":{\"124\":1,\"677\":2,\"678\":2,\"679\":2,\"680\":2,\"681\":2,\"682\":1,\"683\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"692\":2,\"698\":2,\"699\":2,\"758\":2,\"762\":3,\"763\":5,\"770\":1,\"791\":2,\"792\":1,\"810\":1,\"813\":2,\"818\":1,\"915\":1,\"1048\":2,\"1049\":2,\"1050\":2,\"1052\":2,\"1056\":2,\"1058\":2,\"1062\":2,\"1065\":2,\"1068\":2,\"1069\":2,\"1070\":2,\"1071\":2,\"1077\":1,\"1078\":2,\"1079\":2,\"1081\":2,\"1139\":1,\"1368\":1,\"1372\":1,\"1472\":1,\"1477\":2,\"1575\":1,\"1765\":2,\"1800\":2,\"1803\":2,\"1830\":2,\"1832\":2,\"1844\":2,\"1857\":2,\"1858\":2,\"1870\":2,\"1871\":2,\"2199\":1}}],[\"resuming\",{\"1\":{\"1895\":1,\"1900\":1,\"2099\":1,\"2102\":1}}],[\"resumed\",{\"1\":{\"65\":1,\"69\":2}}],[\"resume\",{\"0\":{\"65\":1,\"654\":1},\"1\":{\"65\":1,\"240\":3,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"654\":3,\"2186\":1,\"2201\":1,\"2202\":2,\"2204\":1}}],[\"resulted\",{\"1\":{\"808\":1,\"2193\":1}}],[\"resulting\",{\"1\":{\"754\":1,\"1735\":1,\"2243\":1}}],[\"result2json\",{\"0\":{\"570\":1},\"1\":{\"570\":2}}],[\"result\",{\"0\":{\"191\":1,\"293\":1},\"1\":{\"68\":1,\"69\":1,\"175\":3,\"194\":3,\"201\":1,\"203\":2,\"249\":2,\"257\":2,\"261\":2,\"343\":2,\"570\":1,\"607\":1,\"608\":1,\"616\":2,\"710\":1,\"742\":1,\"794\":1,\"920\":1,\"921\":1,\"924\":1,\"956\":1,\"973\":1,\"1186\":1,\"1227\":2,\"1370\":1,\"1429\":1,\"1810\":1,\"2360\":1,\"2375\":1,\"2405\":1,\"2440\":7,\"2458\":1,\"2523\":1,\"2541\":1,\"2559\":1,\"2564\":6,\"2582\":1,\"2600\":2}}],[\"results\",{\"0\":{\"600\":1,\"633\":1,\"2572\":1},\"1\":{\"3\":1,\"28\":3,\"83\":1,\"99\":1,\"102\":1,\"110\":1,\"168\":1,\"179\":1,\"192\":1,\"194\":1,\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"240\":13,\"285\":1,\"530\":2,\"600\":4,\"633\":4,\"676\":2,\"692\":1,\"693\":1,\"697\":1,\"781\":2,\"797\":1,\"812\":2,\"836\":1,\"857\":1,\"1032\":1,\"1033\":1,\"1171\":1,\"1206\":1,\"1552\":1,\"1953\":1,\"1955\":1,\"2418\":1,\"2419\":1,\"2441\":2,\"2456\":1,\"2457\":1,\"2459\":2,\"2460\":1,\"2560\":2,\"2572\":2,\"2592\":7,\"2596\":6}}],[\"request\",{\"1\":{\"144\":1,\"2446\":1,\"2575\":1}}],[\"requested\",{\"1\":{\"1\":1,\"2155\":1}}],[\"requiring\",{\"1\":{\"84\":1,\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1}}],[\"require\",{\"1\":{\"74\":1,\"286\":1,\"296\":1,\"754\":1,\"1618\":1,\"1619\":1,\"1778\":2,\"1805\":2,\"1850\":2,\"1852\":2,\"1877\":2,\"1980\":2,\"1985\":1,\"2077\":2,\"2235\":2,\"2243\":1,\"2277\":2,\"2573\":1}}],[\"requirements\",{\"0\":{\"132\":1,\"133\":1},\"1\":{\"134\":1,\"1655\":1,\"1719\":1,\"2096\":2,\"2098\":2,\"2099\":3,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2372\":1,\"2394\":1,\"2530\":1}}],[\"requirement\",{\"1\":{\"73\":1,\"2099\":1}}],[\"requires\",{\"1\":{\"49\":1,\"59\":1,\"75\":1,\"76\":1,\"77\":1,\"84\":1,\"150\":1,\"692\":1,\"693\":1}}],[\"required=false\",{\"1\":{\"2313\":1}}],[\"required\",{\"1\":{\"1\":1,\"21\":1,\"22\":1,\"52\":1,\"53\":1,\"56\":1,\"59\":3,\"75\":1,\"132\":1,\"148\":1,\"695\":1,\"696\":1,\"745\":1,\"746\":1,\"796\":1,\"815\":1,\"828\":1,\"981\":1,\"999\":1,\"1015\":1,\"1142\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1186\":1,\"1190\":2,\"1210\":2,\"1221\":1,\"1244\":1,\"1253\":3,\"1254\":1,\"1279\":1,\"1406\":1,\"1558\":1,\"1778\":2,\"1805\":2,\"1850\":2,\"1852\":2,\"1877\":2,\"1972\":1,\"1980\":2,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"2011\":1,\"2077\":2,\"2096\":4,\"2097\":4,\"2098\":4,\"2099\":5,\"2100\":4,\"2101\":4,\"2102\":4,\"2103\":4,\"2104\":4,\"2105\":4,\"2107\":4,\"2108\":4,\"2109\":4,\"2110\":4,\"2111\":4,\"2112\":4,\"2113\":4,\"2114\":4,\"2115\":4,\"2116\":4,\"2117\":4,\"2118\":4,\"2235\":2,\"2247\":1,\"2249\":1,\"2251\":1,\"2277\":2,\"2372\":1,\"2429\":1,\"2552\":1,\"2568\":1,\"2598\":1}}],[\"wyz\",{\"1\":{\"2618\":1}}],[\"wc\",{\"1\":{\"2568\":1}}],[\"wv5r\",{\"1\":{\"2506\":1}}],[\"wf7zdhbaigs6zn8g9g\",{\"1\":{\"2494\":1}}],[\"wb+\",{\"1\":{\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"w=true\",{\"1\":{\"2078\":1}}],[\"w=5\",{\"1\":{\"1037\":1,\"1040\":1}}],[\"wn\",{\"0\":{\"1806\":1},\"1\":{\"1806\":2}}],[\"wnormalization\",{\"1\":{\"1611\":1}}],[\"wnonlinear\",{\"1\":{\"1611\":1}}],[\"wnet\",{\"1\":{\"1611\":1}}],[\"wnv\",{\"1\":{\"202\":1}}],[\"wmpdr\",{\"1\":{\"1524\":2,\"1739\":1}}],[\"w3\",{\"1\":{\"612\":2}}],[\"w2v=false\",{\"1\":{\"1178\":1}}],[\"w2v\",{\"0\":{\"1310\":1},\"1\":{\"1178\":4,\"1310\":1}}],[\"w2\",{\"1\":{\"612\":2}}],[\"w1\",{\"1\":{\"612\":2}}],[\"wxfilename\",{\"1\":{\"506\":2}}],[\"wdropout\",{\"1\":{\"251\":2,\"756\":1,\"1158\":1,\"1239\":1,\"1611\":1,\"1791\":1}}],[\"wpd\",{\"0\":{\"1696\":1,\"1697\":1,\"1698\":1,\"1736\":1},\"1\":{\"1524\":2,\"1696\":3,\"1697\":3,\"1698\":3,\"1736\":2,\"1739\":3,\"1741\":1}}],[\"wprojs\",{\"1\":{\"251\":2,\"730\":1,\"756\":1,\"1158\":1,\"1239\":1,\"1525\":1,\"1611\":1,\"1791\":1}}],[\"wpe\",{\"0\":{\"730\":2,\"962\":2,\"1525\":2,\"1701\":1,\"1703\":1,\"1710\":1,\"1737\":1,\"1758\":2,\"1759\":2},\"1\":{\"251\":8,\"730\":2,\"756\":2,\"962\":4,\"1158\":3,\"1239\":2,\"1525\":6,\"1611\":5,\"1701\":2,\"1703\":1,\"1710\":2,\"1737\":1,\"1758\":5,\"1759\":5,\"1791\":2}}],[\"wtype\",{\"1\":{\"251\":1,\"730\":1,\"756\":1,\"1158\":1,\"1239\":1,\"1525\":1,\"1791\":1}}],[\"wget\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1,\"2431\":1,\"2432\":1,\"2585\":2}}],[\"wunits\",{\"1\":{\"251\":2,\"730\":1,\"756\":1,\"1158\":1,\"1239\":1,\"1525\":1,\"1611\":1,\"1791\":1}}],[\"wu\",{\"1\":{\"130\":3}}],[\"wkvlinearattention\",{\"0\":{\"1086\":1},\"1\":{\"1086\":4}}],[\"wkv\",{\"0\":{\"1100\":1},\"1\":{\"116\":1,\"1074\":1,\"1075\":1,\"1081\":3,\"1100\":2}}],[\"wshare\",{\"1\":{\"740\":2,\"741\":2,\"775\":2,\"776\":2,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1}}],[\"wspecifier\",{\"0\":{\"1029\":1},\"1\":{\"247\":2,\"496\":1,\"506\":2,\"509\":1,\"512\":1,\"522\":1,\"525\":1,\"585\":1,\"983\":1,\"986\":1,\"991\":1,\"994\":1,\"1015\":2,\"1029\":4,\"1030\":1}}],[\"ws4rxvn4ntx\",{\"1\":{\"211\":1}}],[\"wsl\",{\"1\":{\"133\":1}}],[\"ws\",{\"1\":{\"88\":1,\"240\":3,\"629\":1,\"762\":2,\"763\":2,\"799\":1,\"856\":2,\"1524\":1,\"1680\":3,\"1999\":1,\"2002\":1,\"2239\":2}}],[\"wsj\",{\"1\":{\"3\":1,\"16\":1,\"85\":2,\"98\":1,\"2372\":3,\"2497\":3,\"2498\":4,\"2500\":2,\"2614\":3,\"2616\":4,\"2617\":4,\"2632\":3,\"2634\":4,\"2635\":4}}],[\"wsj1\",{\"1\":{\"3\":1,\"85\":1}}],[\"wsj0\",{\"0\":{\"2372\":1,\"2496\":1,\"2614\":1,\"2632\":1},\"1\":{\"3\":1,\"85\":1,\"2371\":4,\"2494\":1,\"2612\":4,\"2630\":5,\"2638\":1,\"2640\":1}}],[\"wrd\",{\"1\":{\"2444\":1,\"2445\":1,\"2446\":1}}],[\"wrt\",{\"1\":{\"1142\":1,\"1186\":1,\"1210\":1}}],[\"wrong\",{\"1\":{\"1011\":1}}],[\"wrap=none\",{\"1\":{\"1327\":1}}],[\"wrap\",{\"1\":{\"1278\":1,\"1327\":3,\"1356\":1}}],[\"wraps\",{\"1\":{\"743\":1}}],[\"wrapped\",{\"1\":{\"2018\":1,\"2148\":1}}],[\"wrappers\",{\"0\":{\"1443\":1,\"1530\":1,\"1563\":1,\"1600\":1,\"1603\":1,\"1622\":1},\"1\":{\"1443\":1,\"1530\":1,\"1551\":2,\"1553\":3,\"1554\":1,\"1563\":1,\"1600\":1,\"1603\":2,\"1622\":2}}],[\"wrapper\",{\"0\":{\"1443\":1},\"1\":{\"92\":1,\"605\":1,\"606\":1,\"706\":1,\"710\":1,\"792\":1,\"811\":1,\"829\":1,\"1218\":2,\"1245\":1,\"1254\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1443\":2,\"1551\":1,\"1553\":1,\"2125\":1,\"2131\":1,\"2254\":1}}],[\"wrapping\",{\"1\":{\"46\":1,\"2148\":1}}],[\"writable\",{\"0\":{\"644\":1},\"1\":{\"644\":1}}],[\"writes\",{\"1\":{\"1227\":1}}],[\"writer\",{\"0\":{\"1015\":1,\"1023\":1,\"1382\":1},\"1\":{\"1015\":3,\"1023\":1,\"1024\":1,\"1382\":2,\"1383\":2,\"1396\":1,\"1397\":3,\"1403\":1,\"1404\":3,\"1407\":1,\"1408\":5,\"1411\":1,\"1412\":3,\"1415\":1,\"1416\":3,\"2185\":1,\"2193\":1,\"2199\":1,\"2201\":2,\"2203\":1}}],[\"writers\",{\"0\":{\"980\":1,\"983\":1,\"986\":1,\"991\":1,\"994\":1,\"1015\":1,\"1023\":1,\"1029\":1},\"1\":{\"980\":2,\"983\":2,\"986\":2,\"991\":2,\"994\":2,\"1015\":1,\"1023\":1,\"1029\":2}}],[\"write\",{\"0\":{\"463\":1,\"2638\":1},\"1\":{\"124\":2,\"276\":1,\"279\":2,\"294\":2,\"461\":2,\"496\":2,\"509\":2,\"512\":2,\"522\":2,\"525\":2,\"983\":1,\"986\":1,\"991\":1,\"994\":1,\"1015\":5,\"1023\":1,\"1383\":1,\"1964\":1,\"2348\":1,\"2360\":2,\"2458\":2,\"2521\":1,\"2523\":2,\"2568\":2,\"2582\":2,\"2592\":3,\"2640\":1}}],[\"written\",{\"1\":{\"11\":1,\"25\":1,\"85\":1,\"240\":1}}],[\"writing\",{\"1\":{\"5\":1,\"115\":1}}],[\"ww\",{\"1\":{\"45\":2}}],[\"www\",{\"1\":{\"45\":1,\"130\":2,\"1082\":1,\"1528\":1,\"1529\":1,\"1568\":1,\"1695\":1,\"2032\":2,\"2363\":1,\"2506\":1,\"2512\":2,\"2653\":1,\"2657\":2}}],[\"wlayers\",{\"1\":{\"251\":2,\"730\":1,\"756\":1,\"1158\":1,\"1239\":1,\"1525\":1,\"1611\":1,\"1791\":1}}],[\"wlan\",{\"1\":{\"45\":1}}],[\"wl\",{\"1\":{\"45\":2}}],[\"w\",{\"1\":{\"22\":1,\"118\":1,\"150\":1,\"629\":3,\"685\":1,\"692\":1,\"705\":1,\"743\":2,\"744\":4,\"745\":7,\"746\":8,\"821\":1,\"875\":5,\"876\":4,\"950\":1,\"968\":1,\"1001\":3,\"1037\":1,\"1040\":1,\"1057\":1,\"1059\":1,\"1116\":1,\"1141\":4,\"1170\":4,\"1179\":1,\"1187\":3,\"1202\":3,\"1248\":3,\"1286\":3,\"1287\":3,\"1339\":3,\"1379\":2,\"1576\":2,\"1577\":2,\"1604\":1,\"1655\":1,\"1664\":2,\"1665\":2,\"1670\":1,\"1671\":1,\"1687\":1,\"1688\":6,\"1689\":1,\"1693\":5,\"1717\":1,\"1719\":1,\"1755\":5,\"1756\":6,\"1868\":2,\"1877\":1,\"2002\":4,\"2078\":1,\"2095\":4,\"2099\":1,\"2143\":1,\"2263\":4,\"2264\":1,\"2266\":2,\"2267\":2,\"2568\":3,\"2584\":1}}],[\"won\",{\"1\":{\"1429\":1,\"2384\":1}}],[\"wonder\",{\"1\":{\"96\":1}}],[\"wo\",{\"0\":{\"292\":1,\"557\":1},\"1\":{\"292\":1,\"557\":2}}],[\"worth\",{\"1\":{\"1243\":1}}],[\"wordtokenizer\",{\"0\":{\"2136\":1},\"1\":{\"2136\":2}}],[\"wordlm\",{\"1\":{\"609\":1,\"611\":1,\"866\":2,\"918\":2}}],[\"word\",{\"0\":{\"2136\":1},\"1\":{\"249\":6,\"276\":1,\"298\":1,\"307\":4,\"315\":4,\"327\":4,\"384\":5,\"391\":4,\"408\":5,\"422\":4,\"443\":8,\"449\":4,\"461\":1,\"478\":4,\"485\":7,\"609\":1,\"611\":1,\"620\":3,\"743\":1,\"817\":1,\"866\":1,\"875\":3,\"918\":1,\"1057\":1,\"1427\":7,\"1428\":1,\"1429\":2,\"1958\":1,\"2125\":1,\"2131\":1,\"2136\":2,\"2375\":2,\"2414\":1,\"2472\":10,\"2474\":2,\"2476\":16,\"2478\":2,\"2559\":2,\"2569\":1,\"2572\":3,\"2599\":1,\"2600\":2,\"2648\":10,\"2649\":2}}],[\"word100\",{\"1\":{\"191\":1}}],[\"words\",{\"1\":{\"74\":1,\"538\":1,\"875\":1,\"2168\":1,\"2467\":1,\"2569\":1}}],[\"worry\",{\"1\":{\"69\":1}}],[\"worldsize\",{\"1\":{\"644\":1}}],[\"world\",{\"0\":{\"37\":1,\"2219\":1},\"1\":{\"32\":3,\"36\":2,\"37\":3,\"39\":3,\"58\":1,\"429\":2,\"2134\":2,\"2180\":2,\"2219\":1,\"2236\":1,\"2468\":1}}],[\"workaround\",{\"1\":{\"2121\":1,\"2122\":1}}],[\"working\",{\"1\":{\"1142\":5,\"1155\":3,\"1186\":1,\"1210\":2,\"2387\":2}}],[\"worker\",{\"0\":{\"1901\":1},\"1\":{\"594\":5,\"1901\":3,\"2099\":1}}],[\"workererror\",{\"0\":{\"594\":1},\"1\":{\"594\":1}}],[\"workers\",{\"1\":{\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"1895\":1,\"1896\":1,\"1900\":1,\"2099\":1,\"2440\":2,\"2558\":2}}],[\"workspace\",{\"0\":{\"1324\":1},\"1\":{\"1142\":2,\"1155\":2,\"1186\":2,\"1210\":4,\"1324\":1}}],[\"works\",{\"1\":{\"189\":1,\"836\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":2,\"2462\":1}}],[\"workshop\",{\"1\":{\"130\":1,\"940\":1}}],[\"work\",{\"1\":{\"1\":1,\"2\":1,\"85\":3,\"944\":1,\"999\":1,\"1245\":1,\"1926\":1,\"2450\":1,\"2473\":1,\"2570\":1}}],[\"would\",{\"1\":{\"17\":1,\"49\":1,\"79\":1,\"629\":1,\"742\":1,\"799\":1,\"1409\":1,\"1810\":1,\"2046\":1,\"2372\":1,\"2375\":2,\"2388\":1,\"2394\":1,\"2400\":1,\"2468\":1,\"2501\":1,\"2524\":1,\"2530\":1,\"2536\":1,\"2559\":1,\"2635\":2,\"2641\":1,\"2645\":1}}],[\"week\",{\"1\":{\"2585\":1}}],[\"western\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"we1zhirfzhtu\",{\"1\":{\"2460\":1}}],[\"wednesday\",{\"1\":{\"2422\":1,\"2545\":1}}],[\"weng\",{\"1\":{\"1462\":1,\"1463\":1}}],[\"wenet\",{\"1\":{\"120\":1}}],[\"well\",{\"1\":{\"204\":1,\"999\":1,\"1142\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1719\":1,\"2372\":1,\"2570\":1}}],[\"wer\",{\"0\":{\"2444\":1},\"1\":{\"113\":2,\"251\":1,\"259\":1,\"811\":1,\"825\":2,\"1057\":3,\"1059\":7,\"1171\":1,\"1173\":7,\"1206\":1,\"1892\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":1,\"2375\":2,\"2440\":1,\"2441\":1,\"2559\":2,\"2564\":1,\"2572\":1}}],[\"were\",{\"1\":{\"3\":1,\"119\":2,\"836\":1,\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1}}],[\"web\",{\"0\":{\"1527\":1},\"1\":{\"85\":1,\"198\":1,\"363\":1,\"1523\":1,\"1527\":2,\"2040\":1,\"2523\":1}}],[\"website\",{\"1\":{\"48\":1}}],[\"weight0\",{\"1\":{\"2572\":4}}],[\"weighting\",{\"1\":{\"1701\":2,\"1702\":1}}],[\"weight=weight\",{\"1\":{\"2170\":2}}],[\"weight=1\",{\"1\":{\"1530\":1,\"1563\":1,\"1603\":1,\"1622\":1}}],[\"weight=20\",{\"1\":{\"822\":1}}],[\"weight=0\",{\"1\":{\"611\":1,\"1026\":1,\"1036\":1,\"1604\":1,\"2358\":1,\"2378\":1,\"2437\":1,\"2455\":2,\"2460\":1,\"2520\":1,\"2563\":1,\"2579\":1,\"2592\":2}}],[\"weightedaverage\",{\"0\":{\"2205\":1},\"1\":{\"2205\":2}}],[\"weighted\",{\"0\":{\"1950\":1},\"1\":{\"102\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"691\":3,\"697\":3,\"754\":2,\"755\":3,\"758\":1,\"785\":1,\"797\":3,\"822\":3,\"825\":1,\"826\":2,\"1074\":1,\"1076\":1,\"1081\":1,\"1086\":1,\"1209\":1,\"1603\":1,\"1696\":1,\"1698\":1,\"1701\":1,\"1715\":2,\"1778\":1,\"1850\":1,\"1851\":3,\"1852\":1,\"1879\":3,\"1950\":1,\"2000\":1,\"2003\":1,\"2086\":3,\"2087\":3,\"2088\":3,\"2090\":3,\"2091\":3,\"2095\":3,\"2243\":3,\"2244\":3,\"2245\":3,\"2255\":3,\"2256\":3,\"2263\":3,\"2264\":3,\"2279\":3,\"2280\":3}}],[\"weights=none\",{\"1\":{\"1603\":1}}],[\"weights=false\",{\"1\":{\"807\":1}}],[\"weights\",{\"0\":{\"70\":1},\"1\":{\"70\":1,\"240\":2,\"249\":2,\"629\":5,\"642\":1,\"646\":2,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"691\":3,\"693\":3,\"697\":3,\"698\":1,\"699\":1,\"742\":1,\"754\":2,\"758\":1,\"762\":1,\"763\":1,\"778\":1,\"781\":1,\"797\":2,\"799\":3,\"820\":4,\"821\":3,\"826\":3,\"856\":3,\"857\":3,\"1001\":2,\"1057\":1,\"1065\":3,\"1115\":2,\"1182\":1,\"1219\":1,\"1229\":1,\"1233\":1,\"1284\":1,\"1452\":1,\"1551\":3,\"1553\":4,\"1603\":3,\"1810\":1,\"1890\":3,\"1892\":1,\"1950\":1,\"1958\":1,\"1973\":1,\"2002\":1,\"2078\":1,\"2079\":1,\"2095\":1,\"2156\":1,\"2263\":1,\"2573\":1}}],[\"weight\",{\"0\":{\"1716\":1},\"1\":{\"22\":10,\"56\":2,\"62\":1,\"113\":1,\"118\":6,\"150\":10,\"173\":1,\"175\":1,\"194\":1,\"217\":1,\"224\":1,\"231\":1,\"249\":6,\"251\":8,\"255\":6,\"259\":10,\"265\":2,\"269\":2,\"307\":6,\"315\":4,\"327\":4,\"333\":2,\"384\":2,\"391\":6,\"408\":6,\"422\":6,\"443\":12,\"449\":4,\"478\":4,\"485\":4,\"629\":3,\"678\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":2,\"689\":2,\"691\":1,\"693\":1,\"697\":1,\"698\":2,\"699\":2,\"700\":3,\"731\":1,\"743\":1,\"747\":1,\"754\":1,\"758\":1,\"784\":1,\"797\":1,\"801\":1,\"821\":3,\"822\":2,\"825\":15,\"826\":3,\"857\":1,\"1035\":1,\"1048\":3,\"1057\":10,\"1069\":3,\"1106\":1,\"1107\":1,\"1108\":1,\"1138\":3,\"1139\":3,\"1140\":1,\"1141\":3,\"1162\":1,\"1163\":1,\"1171\":3,\"1172\":3,\"1175\":1,\"1179\":1,\"1180\":1,\"1199\":1,\"1206\":3,\"1217\":1,\"1279\":1,\"1337\":1,\"1371\":2,\"1444\":1,\"1551\":1,\"1553\":1,\"1600\":3,\"1603\":2,\"1604\":2,\"1622\":2,\"1688\":1,\"1712\":3,\"1715\":3,\"1716\":4,\"1756\":1,\"1765\":7,\"1773\":2,\"1778\":5,\"1800\":7,\"1801\":1,\"1803\":7,\"1804\":10,\"1805\":7,\"1807\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1837\":2,\"1844\":7,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":5,\"1849\":7,\"1850\":5,\"1851\":3,\"1852\":5,\"1857\":7,\"1858\":7,\"1861\":6,\"1862\":7,\"1863\":3,\"1864\":3,\"1865\":3,\"1870\":5,\"1871\":7,\"1877\":8,\"1878\":11,\"1880\":7,\"1890\":6,\"1892\":3,\"1905\":2,\"1955\":1,\"1970\":1,\"1972\":1,\"1973\":9,\"1974\":1,\"1975\":3,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":2,\"2002\":1,\"2027\":3,\"2063\":1,\"2074\":1,\"2075\":1,\"2076\":3,\"2078\":1,\"2082\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":5,\"2161\":1,\"2163\":1,\"2168\":3,\"2170\":3,\"2199\":1,\"2205\":2,\"2230\":1,\"2239\":2,\"2240\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":5,\"2264\":5,\"2278\":1,\"2279\":1,\"2290\":1,\"2292\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2440\":4,\"2461\":1,\"2558\":3,\"2564\":1,\"2569\":1,\"2584\":2}}],[\"we\",{\"1\":{\"10\":1,\"14\":1,\"16\":1,\"17\":2,\"18\":1,\"22\":1,\"25\":2,\"26\":1,\"27\":1,\"28\":1,\"29\":1,\"30\":1,\"35\":1,\"40\":1,\"47\":1,\"48\":1,\"49\":1,\"56\":1,\"57\":2,\"60\":1,\"62\":1,\"63\":1,\"64\":1,\"73\":1,\"74\":2,\"82\":1,\"83\":2,\"84\":4,\"85\":1,\"93\":1,\"112\":1,\"113\":4,\"115\":2,\"116\":1,\"117\":1,\"118\":2,\"120\":2,\"124\":1,\"133\":2,\"134\":1,\"135\":1,\"143\":1,\"148\":1,\"168\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"179\":1,\"181\":1,\"198\":1,\"201\":1,\"204\":2,\"221\":1,\"235\":3,\"237\":2,\"240\":1,\"241\":1,\"242\":3,\"295\":1,\"770\":1,\"797\":1,\"1001\":1,\"1011\":1,\"1015\":1,\"1138\":1,\"1337\":1,\"1484\":2,\"1601\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":3,\"1798\":1,\"1874\":1,\"1878\":1,\"1897\":1,\"1905\":1,\"1951\":1,\"1956\":1,\"2046\":1,\"2244\":1,\"2355\":2,\"2357\":1,\"2363\":1,\"2371\":1,\"2372\":6,\"2373\":7,\"2374\":1,\"2375\":4,\"2380\":1,\"2383\":1,\"2384\":8,\"2385\":5,\"2386\":1,\"2387\":9,\"2388\":1,\"2389\":2,\"2392\":1,\"2393\":2,\"2394\":4,\"2395\":4,\"2396\":1,\"2397\":1,\"2398\":3,\"2399\":2,\"2400\":6,\"2401\":5,\"2403\":3,\"2405\":2,\"2406\":1,\"2408\":2,\"2409\":1,\"2410\":4,\"2411\":1,\"2412\":3,\"2413\":1,\"2414\":2,\"2415\":3,\"2416\":2,\"2418\":1,\"2419\":3,\"2420\":3,\"2422\":2,\"2424\":1,\"2425\":1,\"2427\":1,\"2428\":1,\"2429\":4,\"2430\":6,\"2431\":5,\"2432\":2,\"2433\":5,\"2434\":1,\"2440\":1,\"2447\":1,\"2449\":2,\"2450\":4,\"2451\":1,\"2452\":1,\"2457\":1,\"2462\":1,\"2463\":1,\"2465\":2,\"2466\":1,\"2467\":2,\"2468\":1,\"2473\":1,\"2480\":1,\"2481\":3,\"2482\":2,\"2494\":1,\"2502\":1,\"2503\":2,\"2506\":1,\"2512\":2,\"2514\":1,\"2518\":1,\"2524\":1,\"2525\":2,\"2528\":1,\"2529\":3,\"2530\":4,\"2531\":4,\"2532\":1,\"2533\":1,\"2534\":3,\"2535\":2,\"2536\":6,\"2537\":5,\"2539\":3,\"2541\":2,\"2542\":1,\"2543\":1,\"2545\":2,\"2547\":1,\"2548\":1,\"2550\":2,\"2551\":1,\"2552\":3,\"2554\":1,\"2555\":9,\"2557\":1,\"2558\":4,\"2559\":3,\"2564\":2,\"2565\":2,\"2566\":1,\"2568\":3,\"2569\":3,\"2570\":2,\"2571\":3,\"2572\":2,\"2573\":1,\"2578\":1,\"2583\":2,\"2584\":8,\"2585\":2,\"2593\":1,\"2598\":1,\"2600\":1,\"2612\":1,\"2618\":1,\"2630\":1,\"2637\":1,\"2653\":1,\"2657\":2,\"2659\":1}}],[\"watch\",{\"1\":{\"996\":1}}],[\"watanabe\",{\"1\":{\"130\":8,\"198\":1,\"704\":1,\"705\":1,\"880\":1,\"1604\":1,\"1655\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1670\":1,\"1671\":1,\"1719\":1,\"2354\":1,\"2357\":4,\"2578\":4,\"2618\":1}}],[\"watanabe2018espnet\",{\"1\":{\"130\":1}}],[\"waiting\",{\"1\":{\"196\":1,\"200\":1,\"234\":1}}],[\"wait\",{\"1\":{\"143\":1,\"1926\":1}}],[\"wavform\",{\"1\":{\"2498\":4,\"2616\":3,\"2634\":3}}],[\"wavfile=\",{\"1\":{\"2593\":2}}],[\"wavfile\",{\"1\":{\"237\":1,\"2592\":4,\"2593\":1}}],[\"wavlm\",{\"1\":{\"2432\":1,\"2440\":2,\"2441\":7,\"2574\":1}}],[\"wavpath\",{\"1\":{\"2386\":2}}],[\"wav=true\",{\"1\":{\"2368\":1,\"2371\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1}}],[\"wavs\",{\"1\":{\"1426\":1,\"1643\":1,\"1918\":1,\"2492\":2,\"2628\":2}}],[\"wavscp\",{\"1\":{\"98\":1,\"2386\":2}}],[\"wavscp=dump\",{\"1\":{\"98\":1}}],[\"wavdir\",{\"1\":{\"562\":2}}],[\"wav2vec2encoderlayerstablelayernorm\",{\"1\":{\"1933\":1}}],[\"wav2vec2\",{\"0\":{\"1178\":1,\"1310\":1},\"1\":{\"102\":1,\"1178\":5,\"1180\":1,\"1310\":1,\"2377\":2,\"2429\":1,\"2552\":1}}],[\"wav2vec\",{\"1\":{\"101\":1,\"2294\":1,\"2574\":1}}],[\"waveshow\",{\"1\":{\"2386\":1,\"2521\":2,\"2522\":2,\"2523\":2}}],[\"waves\",{\"1\":{\"2372\":3,\"2497\":3,\"2498\":4,\"2500\":2,\"2614\":3,\"2615\":3,\"2616\":4,\"2617\":2,\"2632\":3,\"2633\":3,\"2634\":4,\"2635\":2}}],[\"waveplot\",{\"1\":{\"2359\":1,\"2360\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2580\":1,\"2581\":1,\"2582\":1}}],[\"wavefrom\",{\"1\":{\"1797\":1}}],[\"waveforms\",{\"1\":{\"242\":1,\"1454\":3,\"1551\":2,\"1553\":2,\"1905\":1}}],[\"waveform\",{\"0\":{\"242\":1},\"1\":{\"235\":1,\"242\":1,\"295\":1,\"785\":1,\"835\":4,\"1551\":2,\"1553\":2,\"1616\":1,\"1773\":3,\"1778\":3,\"1804\":2,\"1805\":3,\"1810\":1,\"1837\":2,\"1850\":3,\"1851\":2,\"1852\":3,\"1859\":3,\"1877\":3,\"1878\":2,\"1905\":4,\"1921\":2,\"1922\":2,\"1923\":2,\"1925\":2,\"1926\":1,\"1927\":1,\"1928\":2,\"1929\":3,\"1935\":3,\"1936\":2,\"1938\":2,\"1939\":2,\"1941\":3,\"1942\":1,\"1943\":3,\"1945\":1,\"1946\":2,\"1947\":2,\"2055\":2,\"2082\":3,\"2240\":3,\"2254\":2,\"2278\":3,\"2317\":1,\"2325\":2,\"2481\":1,\"2487\":1,\"2491\":1,\"2497\":1,\"2501\":1,\"2508\":1,\"2515\":1}}],[\"wavenet\",{\"0\":{\"708\":1,\"793\":1,\"830\":1,\"835\":2,\"869\":1,\"877\":1,\"893\":1,\"1830\":1,\"1831\":1,\"1880\":3},\"1\":{\"235\":1,\"282\":2,\"295\":16,\"541\":1,\"708\":1,\"793\":2,\"802\":1,\"803\":1,\"804\":1,\"830\":1,\"835\":4,\"869\":1,\"877\":1,\"893\":1,\"1830\":1,\"1831\":1,\"1863\":3,\"1864\":6,\"1865\":6,\"1880\":8,\"2263\":1}}],[\"wavegan==0\",{\"1\":{\"2362\":1,\"2504\":2,\"2651\":1}}],[\"wavegan\",{\"0\":{\"214\":1,\"1832\":1,\"1834\":1,\"1861\":2,\"1862\":2,\"1869\":1,\"1876\":1,\"2254\":1},\"1\":{\"207\":1,\"214\":4,\"217\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"229\":1,\"230\":1,\"231\":1,\"295\":9,\"1832\":1,\"1834\":1,\"1861\":3,\"1862\":3,\"1869\":1,\"1876\":1,\"2254\":2,\"2363\":17,\"2506\":3,\"2510\":3,\"2512\":10,\"2653\":17,\"2657\":10}}],[\"wave\",{\"1\":{\"84\":2,\"1778\":1,\"1805\":1,\"1811\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"2369\":2,\"2386\":3,\"2397\":1,\"2432\":1,\"2433\":1,\"2487\":2,\"2491\":2,\"2501\":2,\"2533\":1,\"2591\":1,\"2592\":1,\"2606\":2,\"2607\":2,\"2610\":2,\"2623\":2,\"2624\":2,\"2627\":2}}],[\"wav\",{\"0\":{\"46\":1,\"53\":1,\"273\":1,\"282\":1,\"286\":1,\"295\":1,\"296\":1,\"519\":1,\"541\":1,\"568\":1,\"1002\":1,\"1027\":1},\"1\":{\"46\":17,\"47\":7,\"48\":1,\"49\":8,\"51\":6,\"52\":3,\"53\":3,\"54\":10,\"57\":2,\"58\":1,\"98\":2,\"182\":1,\"183\":1,\"194\":3,\"197\":4,\"198\":6,\"201\":3,\"202\":3,\"203\":3,\"205\":2,\"237\":2,\"242\":3,\"275\":1,\"282\":3,\"286\":12,\"295\":5,\"296\":10,\"343\":2,\"350\":2,\"368\":2,\"509\":1,\"512\":1,\"519\":3,\"525\":1,\"541\":3,\"568\":4,\"989\":1,\"991\":1,\"994\":1,\"1002\":1,\"1015\":1,\"1027\":2,\"1179\":1,\"1383\":3,\"1390\":1,\"1391\":11,\"1405\":1,\"1406\":15,\"1407\":2,\"1408\":5,\"1410\":1,\"1421\":6,\"1425\":14,\"1616\":1,\"1773\":6,\"1778\":3,\"1804\":1,\"1805\":3,\"1811\":1,\"1834\":2,\"1837\":2,\"1850\":3,\"1851\":1,\"1852\":3,\"1862\":4,\"1876\":1,\"1877\":3,\"1878\":1,\"1927\":2,\"2082\":6,\"2183\":1,\"2190\":1,\"2240\":3,\"2254\":1,\"2278\":3,\"2360\":4,\"2365\":4,\"2367\":2,\"2372\":2,\"2373\":2,\"2385\":10,\"2386\":4,\"2387\":7,\"2395\":1,\"2412\":1,\"2430\":7,\"2458\":4,\"2461\":1,\"2470\":2,\"2476\":2,\"2478\":2,\"2485\":2,\"2492\":3,\"2497\":2,\"2508\":4,\"2510\":4,\"2514\":1,\"2515\":4,\"2521\":8,\"2522\":5,\"2523\":9,\"2531\":1,\"2555\":7,\"2568\":8,\"2582\":4,\"2593\":2,\"2600\":14,\"2604\":2,\"2614\":2,\"2621\":2,\"2628\":3,\"2632\":2,\"2638\":2,\"2647\":3,\"2655\":4,\"2659\":1,\"2660\":4}}],[\"warsitz\",{\"1\":{\"1704\":1}}],[\"warming\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"warmupsteplr\",{\"0\":{\"2023\":1},\"1\":{\"2023\":3}}],[\"warmupreducelronplateau\",{\"0\":{\"2022\":1},\"1\":{\"2022\":3}}],[\"warmuplr\",{\"0\":{\"2021\":1},\"1\":{\"2019\":1,\"2020\":1,\"2021\":4,\"2022\":2,\"2023\":2,\"2440\":1,\"2558\":1}}],[\"warmup\",{\"0\":{\"2018\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1},\"1\":{\"668\":2,\"670\":3,\"754\":2,\"792\":1,\"826\":2,\"834\":1,\"888\":1,\"1057\":3,\"2018\":5,\"2019\":1,\"2020\":4,\"2021\":6,\"2022\":7,\"2023\":7,\"2264\":2,\"2440\":1,\"2558\":1,\"2564\":1,\"2584\":1}}],[\"warm\",{\"1\":{\"113\":1,\"2395\":1,\"2531\":1}}],[\"warns\",{\"1\":{\"1007\":1}}],[\"warning\",{\"1\":{\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"745\":1,\"746\":1,\"944\":1,\"952\":1,\"1255\":1,\"1257\":1,\"1926\":1,\"2153\":1,\"2204\":2}}],[\"warn\",{\"1\":{\"44\":3,\"231\":1}}],[\"warp=80\",{\"1\":{\"968\":1,\"973\":1}}],[\"warped\",{\"1\":{\"956\":1,\"973\":1}}],[\"warping\",{\"1\":{\"950\":1,\"968\":1,\"1011\":1,\"1040\":1,\"1919\":1,\"1948\":1}}],[\"warp\",{\"0\":{\"973\":1,\"1011\":1,\"1036\":1,\"1040\":1,\"1359\":1,\"1919\":1,\"1948\":2},\"1\":{\"8\":2,\"20\":1,\"112\":1,\"113\":1,\"127\":1,\"136\":2,\"167\":5,\"178\":5,\"196\":4,\"200\":1,\"234\":4,\"950\":3,\"956\":4,\"968\":3,\"973\":6,\"1011\":4,\"1036\":1,\"1037\":1,\"1040\":3,\"1143\":3,\"1144\":3,\"1227\":1,\"1228\":3,\"1257\":4,\"1337\":2,\"1344\":1,\"1345\":3,\"1346\":1,\"1347\":3,\"1349\":2,\"1350\":2,\"1359\":1,\"1919\":3,\"1948\":5}}],[\"ways\",{\"1\":{\"38\":1,\"62\":1,\"80\":1,\"92\":1,\"1137\":1,\"2372\":1,\"2424\":1,\"2547\":1,\"2573\":1}}],[\"way\",{\"1\":{\"25\":1,\"64\":1,\"85\":1,\"169\":1,\"181\":1,\"1951\":1,\"2413\":1,\"2452\":1,\"2585\":1,\"2599\":1}}],[\"wang\",{\"1\":{\"130\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1670\":1,\"1671\":1,\"1712\":1,\"1715\":1}}],[\"wangyou\",{\"1\":{\"130\":2,\"1622\":1,\"2366\":1,\"2601\":1,\"2618\":2}}],[\"wandb\",{\"0\":{\"2232\":1},\"1\":{\"70\":5,\"429\":12,\"2186\":2,\"2193\":1,\"2199\":1,\"2202\":4,\"2204\":2,\"2232\":2}}],[\"wan\",{\"1\":{\"45\":1}}],[\"wanna\",{\"1\":{\"11\":1}}],[\"wants\",{\"1\":{\"1252\":1,\"1253\":1,\"1254\":1}}],[\"want\",{\"1\":{\"1\":1,\"2\":1,\"17\":1,\"19\":2,\"46\":1,\"47\":1,\"115\":1,\"133\":1,\"148\":1,\"149\":1,\"198\":1,\"235\":1,\"295\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1484\":2,\"1601\":2,\"1905\":1,\"2393\":1,\"2400\":1,\"2401\":1,\"2429\":1,\"2433\":1,\"2437\":1,\"2529\":1,\"2536\":1,\"2537\":1,\"2552\":1,\"2563\":1,\"2584\":5,\"2644\":1}}],[\"waspaa2021\",{\"1\":{\"156\":1}}],[\"waspaa\",{\"1\":{\"140\":1,\"156\":1,\"244\":1}}],[\"was\",{\"1\":{\"3\":1,\"21\":1,\"85\":1,\"825\":1,\"940\":1,\"1245\":1,\"1719\":1,\"2421\":1,\"2501\":1}}],[\"whl\",{\"1\":{\"2482\":1}}],[\"who\",{\"1\":{\"2401\":1,\"2537\":1}}],[\"whose\",{\"1\":{\"607\":1,\"770\":1,\"875\":1,\"1016\":1,\"1695\":1}}],[\"whole\",{\"1\":{\"69\":2,\"1180\":1,\"1398\":1,\"1409\":1,\"1454\":1,\"1543\":1,\"1551\":1,\"1553\":1,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"2099\":1,\"2102\":1,\"2394\":1,\"2530\":1}}],[\"what\",{\"0\":{\"152\":1},\"1\":{\"613\":1,\"754\":1,\"820\":1,\"821\":1,\"826\":1,\"1243\":1,\"2355\":1,\"2441\":1,\"2462\":1,\"2501\":1,\"2569\":1}}],[\"whatever\",{\"1\":{\"102\":1,\"2440\":1}}],[\"why\",{\"0\":{\"48\":1,\"2584\":1},\"1\":{\"84\":1,\"952\":1,\"1972\":1,\"2419\":1,\"2420\":1,\"2441\":1,\"2510\":1,\"2584\":1}}],[\"whisperfrontend\",{\"0\":{\"1284\":1},\"1\":{\"1284\":1}}],[\"whisper\",{\"0\":{\"98\":1,\"491\":1,\"1174\":1,\"1214\":1,\"1215\":1,\"1284\":1,\"2128\":1,\"2129\":1},\"1\":{\"98\":7,\"130\":1,\"461\":2,\"491\":9,\"697\":1,\"1174\":1,\"1214\":4,\"1215\":4,\"1216\":2,\"1284\":4,\"1285\":1,\"2128\":1,\"2129\":1,\"2137\":2,\"2178\":2,\"2179\":2,\"2191\":2}}],[\"while\",{\"1\":{\"23\":1,\"76\":1,\"80\":1,\"102\":1,\"112\":1,\"119\":1,\"150\":1,\"196\":1,\"200\":1,\"234\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1143\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1175\":1,\"1177\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1279\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1409\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1551\":2,\"1553\":2,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1643\":1,\"1644\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1932\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2090\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2358\":1,\"2385\":1,\"2452\":1,\"2499\":1,\"2520\":1,\"2579\":1,\"2617\":1,\"2635\":1}}],[\"white=lambda\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"white\",{\"1\":{\"11\":1,\"45\":2}}],[\"which\",{\"0\":{\"39\":1},\"1\":{\"1\":1,\"46\":1,\"47\":1,\"58\":1,\"60\":3,\"76\":1,\"79\":1,\"85\":2,\"102\":1,\"109\":1,\"128\":1,\"143\":1,\"150\":2,\"236\":1,\"238\":1,\"239\":1,\"295\":1,\"593\":1,\"652\":1,\"669\":1,\"698\":1,\"699\":1,\"704\":1,\"705\":1,\"713\":1,\"727\":1,\"745\":4,\"746\":4,\"754\":2,\"762\":1,\"764\":1,\"785\":1,\"786\":1,\"802\":2,\"803\":2,\"821\":2,\"826\":2,\"1003\":1,\"1004\":1,\"1143\":1,\"1144\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1169\":1,\"1177\":1,\"1186\":1,\"1210\":1,\"1227\":1,\"1243\":1,\"1245\":2,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1327\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1375\":1,\"1377\":2,\"1398\":1,\"1427\":2,\"1484\":1,\"1601\":1,\"1618\":1,\"1619\":1,\"1664\":1,\"1665\":1,\"1679\":1,\"1688\":1,\"1693\":1,\"1755\":1,\"1756\":1,\"1785\":1,\"1798\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1858\":1,\"1864\":1,\"1874\":1,\"1878\":1,\"1905\":1,\"1917\":1,\"1946\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2046\":1,\"2078\":1,\"2081\":1,\"2083\":2,\"2090\":1,\"2095\":1,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2168\":1,\"2170\":1,\"2193\":1,\"2243\":1,\"2263\":1,\"2264\":1,\"2372\":1,\"2373\":1,\"2387\":4,\"2389\":1,\"2392\":1,\"2398\":1,\"2400\":1,\"2414\":2,\"2418\":1,\"2422\":1,\"2425\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2433\":1,\"2441\":2,\"2450\":1,\"2467\":1,\"2482\":1,\"2510\":1,\"2525\":1,\"2528\":1,\"2534\":1,\"2536\":1,\"2545\":1,\"2548\":1,\"2554\":1,\"2555\":2,\"2564\":1,\"2565\":1,\"2568\":1,\"2573\":1,\"2584\":1,\"2599\":1,\"2638\":2}}],[\"whether\",{\"1\":{\"21\":8,\"82\":1,\"104\":2,\"115\":4,\"116\":1,\"121\":1,\"137\":1,\"643\":1,\"648\":2,\"658\":1,\"691\":1,\"693\":1,\"697\":1,\"700\":1,\"702\":1,\"711\":2,\"712\":3,\"725\":1,\"749\":2,\"754\":8,\"755\":2,\"762\":1,\"763\":2,\"766\":2,\"770\":1,\"771\":1,\"797\":1,\"802\":1,\"806\":1,\"809\":1,\"821\":6,\"822\":2,\"824\":1,\"825\":1,\"826\":11,\"866\":1,\"895\":1,\"918\":1,\"933\":2,\"997\":2,\"998\":2,\"1025\":1,\"1048\":1,\"1051\":1,\"1052\":4,\"1053\":1,\"1054\":1,\"1055\":1,\"1057\":4,\"1059\":2,\"1067\":1,\"1071\":3,\"1075\":1,\"1084\":1,\"1093\":2,\"1095\":1,\"1097\":1,\"1133\":5,\"1145\":1,\"1148\":7,\"1149\":5,\"1150\":5,\"1173\":2,\"1178\":1,\"1180\":1,\"1181\":2,\"1187\":1,\"1200\":1,\"1202\":1,\"1203\":7,\"1228\":1,\"1269\":4,\"1270\":1,\"1272\":3,\"1286\":1,\"1287\":1,\"1345\":1,\"1347\":1,\"1430\":2,\"1462\":1,\"1463\":1,\"1505\":5,\"1515\":1,\"1516\":4,\"1522\":2,\"1523\":3,\"1528\":2,\"1529\":1,\"1531\":2,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":2,\"1551\":6,\"1553\":6,\"1558\":1,\"1572\":2,\"1598\":2,\"1602\":1,\"1604\":1,\"1626\":2,\"1645\":2,\"1648\":1,\"1650\":1,\"1652\":3,\"1654\":3,\"1658\":2,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":3,\"1670\":1,\"1671\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":2,\"1715\":2,\"1719\":1,\"1741\":1,\"1752\":1,\"1765\":3,\"1771\":3,\"1773\":1,\"1778\":8,\"1781\":3,\"1786\":1,\"1787\":3,\"1798\":4,\"1800\":5,\"1801\":2,\"1803\":3,\"1804\":10,\"1805\":5,\"1829\":1,\"1833\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":3,\"1840\":1,\"1843\":1,\"1844\":3,\"1846\":1,\"1847\":1,\"1848\":4,\"1849\":3,\"1850\":6,\"1851\":18,\"1852\":7,\"1855\":1,\"1856\":1,\"1857\":2,\"1858\":2,\"1859\":3,\"1861\":1,\"1862\":3,\"1863\":2,\"1864\":4,\"1865\":4,\"1866\":2,\"1867\":1,\"1868\":1,\"1870\":1,\"1871\":2,\"1872\":1,\"1873\":1,\"1874\":3,\"1877\":6,\"1878\":8,\"1879\":2,\"1880\":6,\"1980\":2,\"1985\":1,\"2001\":3,\"2002\":5,\"2004\":3,\"2012\":1,\"2029\":2,\"2040\":1,\"2054\":5,\"2077\":2,\"2078\":3,\"2079\":2,\"2086\":3,\"2087\":4,\"2088\":2,\"2090\":9,\"2091\":2,\"2095\":11,\"2142\":1,\"2170\":1,\"2197\":1,\"2235\":2,\"2243\":14,\"2244\":16,\"2245\":2,\"2255\":16,\"2256\":2,\"2263\":10,\"2264\":12,\"2277\":2,\"2279\":15,\"2280\":2,\"2302\":2,\"2303\":1,\"2305\":1,\"2372\":1,\"2440\":1,\"2501\":1,\"2558\":1}}],[\"when\",{\"1\":{\"5\":2,\"13\":1,\"17\":3,\"26\":1,\"44\":1,\"48\":1,\"49\":1,\"75\":1,\"85\":1,\"91\":1,\"96\":1,\"113\":2,\"132\":1,\"136\":1,\"148\":1,\"276\":1,\"281\":1,\"610\":1,\"623\":1,\"638\":1,\"710\":2,\"734\":1,\"758\":1,\"785\":2,\"795\":1,\"812\":1,\"823\":1,\"989\":1,\"999\":1,\"1057\":1,\"1085\":1,\"1142\":1,\"1171\":1,\"1186\":1,\"1206\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1257\":1,\"1301\":1,\"1304\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1390\":1,\"1407\":1,\"1427\":1,\"1484\":2,\"1522\":1,\"1545\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1601\":2,\"1604\":1,\"1639\":3,\"1640\":1,\"1652\":1,\"1654\":1,\"1659\":1,\"1660\":2,\"1661\":3,\"1662\":2,\"1712\":2,\"1715\":1,\"1719\":1,\"1739\":2,\"1797\":1,\"1804\":2,\"1878\":2,\"1895\":1,\"1900\":1,\"1905\":1,\"1953\":1,\"1955\":1,\"2046\":1,\"2050\":1,\"2099\":3,\"2102\":1,\"2387\":1,\"2510\":1}}],[\"where2\",{\"1\":{\"47\":1}}],[\"where\",{\"1\":{\"3\":2,\"22\":1,\"38\":1,\"47\":1,\"57\":1,\"75\":1,\"76\":1,\"78\":1,\"118\":1,\"121\":1,\"128\":1,\"612\":1,\"652\":1,\"714\":2,\"715\":2,\"716\":2,\"718\":2,\"719\":2,\"720\":2,\"721\":2,\"722\":2,\"830\":1,\"917\":1,\"1010\":1,\"1011\":1,\"1015\":1,\"1048\":1,\"1061\":1,\"1082\":1,\"1084\":1,\"1132\":1,\"1248\":1,\"1274\":1,\"1276\":1,\"1337\":1,\"1340\":1,\"1349\":1,\"1350\":1,\"1381\":1,\"1383\":3,\"1452\":1,\"1517\":2,\"1639\":1,\"1683\":1,\"1695\":1,\"1735\":1,\"1766\":1,\"1834\":1,\"2155\":1,\"2385\":1,\"2387\":1,\"2395\":1,\"2412\":1,\"2423\":1,\"2450\":1,\"2467\":1,\"2468\":1,\"2473\":1,\"2482\":1,\"2492\":1,\"2531\":1,\"2546\":1,\"2585\":1,\"2628\":1}}],[\"wiener\",{\"1\":{\"1708\":1,\"1712\":3,\"1715\":4,\"1719\":1}}],[\"wiesner\",{\"1\":{\"130\":1}}],[\"wiley\",{\"1\":{\"1705\":1}}],[\"william\",{\"1\":{\"130\":1}}],[\"will\",{\"1\":{\"1\":1,\"2\":1,\"3\":1,\"4\":1,\"6\":2,\"13\":1,\"17\":1,\"21\":2,\"23\":2,\"24\":1,\"40\":1,\"52\":1,\"68\":1,\"69\":1,\"72\":1,\"79\":1,\"98\":1,\"102\":1,\"107\":1,\"119\":2,\"122\":1,\"143\":1,\"144\":2,\"148\":1,\"150\":1,\"167\":1,\"171\":1,\"178\":1,\"203\":1,\"235\":1,\"237\":1,\"239\":2,\"691\":3,\"693\":3,\"697\":3,\"711\":2,\"727\":2,\"728\":2,\"745\":1,\"746\":1,\"749\":2,\"754\":2,\"797\":3,\"820\":2,\"821\":2,\"826\":2,\"857\":3,\"899\":1,\"901\":1,\"1019\":1,\"1037\":1,\"1039\":1,\"1133\":2,\"1142\":2,\"1143\":1,\"1148\":3,\"1149\":2,\"1150\":2,\"1186\":5,\"1187\":2,\"1198\":1,\"1202\":2,\"1203\":3,\"1210\":6,\"1211\":3,\"1224\":3,\"1228\":1,\"1241\":1,\"1269\":1,\"1272\":2,\"1286\":2,\"1287\":2,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":2,\"1337\":3,\"1345\":1,\"1347\":1,\"1348\":2,\"1349\":3,\"1350\":3,\"1371\":2,\"1381\":1,\"1392\":1,\"1462\":2,\"1463\":1,\"1464\":1,\"1478\":1,\"1505\":2,\"1524\":1,\"1551\":3,\"1553\":3,\"1598\":2,\"1600\":1,\"1603\":4,\"1622\":4,\"1652\":4,\"1654\":4,\"1664\":1,\"1665\":1,\"1669\":2,\"1683\":1,\"1693\":1,\"1695\":1,\"1755\":1,\"1765\":1,\"1778\":2,\"1800\":1,\"1803\":1,\"1804\":4,\"1805\":2,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":2,\"1849\":3,\"1850\":2,\"1851\":4,\"1852\":2,\"1856\":4,\"1857\":1,\"1858\":2,\"1861\":2,\"1862\":1,\"1870\":1,\"1871\":1,\"1877\":2,\"1878\":4,\"1880\":1,\"1905\":1,\"2001\":5,\"2002\":3,\"2004\":5,\"2029\":2,\"2044\":1,\"2054\":1,\"2068\":1,\"2079\":2,\"2086\":3,\"2087\":3,\"2090\":3,\"2095\":3,\"2198\":2,\"2243\":4,\"2244\":4,\"2255\":4,\"2263\":3,\"2264\":3,\"2279\":4,\"2357\":1,\"2371\":1,\"2380\":1,\"2384\":6,\"2385\":2,\"2387\":4,\"2392\":1,\"2394\":1,\"2397\":1,\"2398\":2,\"2399\":1,\"2401\":2,\"2406\":1,\"2410\":2,\"2411\":1,\"2412\":1,\"2422\":2,\"2425\":1,\"2429\":1,\"2432\":1,\"2433\":1,\"2437\":1,\"2447\":1,\"2450\":1,\"2451\":1,\"2463\":1,\"2467\":1,\"2480\":1,\"2492\":1,\"2494\":1,\"2502\":1,\"2506\":1,\"2525\":1,\"2528\":1,\"2530\":1,\"2533\":1,\"2534\":2,\"2535\":1,\"2537\":2,\"2545\":2,\"2548\":1,\"2554\":1,\"2558\":2,\"2559\":1,\"2563\":1,\"2566\":1,\"2568\":1,\"2569\":1,\"2578\":1,\"2583\":1,\"2584\":4,\"2585\":3,\"2599\":1,\"2600\":2,\"2612\":1,\"2628\":1,\"2630\":1,\"2635\":1,\"2638\":2}}],[\"wiki\",{\"1\":{\"1031\":1,\"1921\":1,\"1922\":1,\"1936\":1,\"1938\":1,\"1939\":1}}],[\"wikipedia\",{\"1\":{\"1001\":1,\"1031\":1,\"1921\":1,\"1922\":1,\"1936\":1,\"1938\":1,\"1939\":1}}],[\"wide\",{\"1\":{\"2398\":1,\"2452\":1,\"2534\":1,\"2585\":2}}],[\"widely\",{\"1\":{\"2385\":1,\"2388\":1,\"2411\":1,\"2421\":1,\"2462\":1,\"2524\":1,\"2544\":1}}],[\"wider\",{\"1\":{\"1904\":1}}],[\"widths\",{\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"widthofthe\",{\"1\":{\"1810\":1}}],[\"width=400\",{\"1\":{\"2440\":3,\"2564\":2,\"2572\":1}}],[\"width=none\",{\"1\":{\"2315\":1}}],[\"width=0\",{\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"width=100\",{\"1\":{\"968\":1}}],[\"width=27\",{\"1\":{\"968\":1}}],[\"width\",{\"1\":{\"778\":1,\"950\":4,\"956\":1,\"968\":4,\"973\":1,\"1009\":1,\"1011\":4,\"1018\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1025\":1,\"1037\":2,\"1039\":1,\"1257\":3,\"1914\":1,\"1915\":4,\"1940\":3}}],[\"widim\",{\"1\":{\"730\":1,\"1525\":1}}],[\"winodw\",{\"1\":{\"1132\":1}}],[\"win\",{\"1\":{\"247\":2,\"275\":2,\"295\":2,\"297\":2,\"509\":2,\"512\":2,\"519\":2,\"585\":2,\"684\":4,\"945\":1,\"947\":1,\"951\":1,\"953\":1,\"966\":1,\"967\":1,\"969\":1,\"970\":1,\"1158\":1,\"1207\":1,\"1220\":1,\"1255\":3,\"1289\":1,\"1373\":1,\"1558\":2,\"1560\":1,\"1643\":2,\"1644\":1,\"1777\":2,\"1778\":1,\"1799\":1,\"1804\":2,\"1805\":1,\"1818\":3,\"1850\":1,\"1852\":1,\"1859\":2,\"1877\":1,\"1910\":1,\"1918\":1,\"1929\":2,\"1941\":2,\"1947\":2,\"1987\":1,\"1989\":1,\"1991\":1,\"2084\":1,\"2089\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2317\":2,\"2325\":2,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"windowed\",{\"1\":{\"1778\":1,\"1851\":1,\"1852\":1}}],[\"windowing\",{\"0\":{\"1255\":1},\"1\":{\"705\":2,\"1198\":3,\"1255\":1,\"1917\":3,\"1929\":1,\"1941\":1,\"1947\":1}}],[\"window=\",{\"1\":{\"945\":1,\"947\":1,\"951\":1,\"953\":1,\"966\":1,\"967\":1,\"969\":1,\"970\":1,\"1643\":1,\"1644\":1,\"1660\":1,\"1661\":1,\"1719\":1,\"2498\":1,\"2616\":1,\"2634\":1}}],[\"window=2\",{\"1\":{\"939\":1,\"963\":1}}],[\"window=3\",{\"1\":{\"681\":1,\"682\":1,\"758\":1,\"2079\":1,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"window=1\",{\"1\":{\"681\":1,\"682\":1,\"758\":1,\"2079\":1,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"windowstreaminge2e\",{\"0\":{\"836\":1},\"1\":{\"836\":2}}],[\"windows\",{\"1\":{\"134\":1,\"1203\":2}}],[\"windows10\",{\"1\":{\"133\":1}}],[\"window\",{\"0\":{\"836\":1},\"1\":{\"21\":2,\"122\":1,\"217\":2,\"245\":4,\"247\":2,\"249\":5,\"263\":4,\"267\":4,\"275\":1,\"297\":1,\"301\":4,\"333\":2,\"399\":4,\"464\":4,\"470\":4,\"509\":1,\"512\":1,\"519\":1,\"681\":4,\"682\":4,\"684\":1,\"758\":6,\"836\":1,\"892\":1,\"956\":2,\"959\":1,\"964\":1,\"973\":2,\"1071\":6,\"1158\":1,\"1198\":2,\"1203\":1,\"1207\":1,\"1255\":5,\"1257\":1,\"1558\":1,\"1560\":4,\"1604\":4,\"1643\":1,\"1644\":1,\"1660\":3,\"1661\":3,\"1662\":3,\"1701\":2,\"1719\":3,\"1777\":1,\"1778\":1,\"1804\":2,\"1805\":1,\"1834\":3,\"1850\":1,\"1852\":1,\"1859\":5,\"1860\":1,\"1862\":3,\"1870\":3,\"1877\":1,\"1883\":2,\"1917\":5,\"1918\":1,\"1919\":2,\"1929\":3,\"1941\":3,\"1947\":3,\"1948\":2,\"1971\":1,\"1985\":2,\"1987\":1,\"1989\":1,\"1991\":1,\"2002\":6,\"2079\":4,\"2084\":1,\"2089\":1,\"2095\":6,\"2188\":2,\"2210\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2259\":3,\"2263\":6,\"2267\":1,\"2270\":1,\"2272\":1,\"2273\":3,\"2317\":4,\"2325\":4}}],[\"wise\",{\"0\":{\"1242\":1},\"1\":{\"115\":6,\"749\":1,\"1031\":2,\"1055\":1,\"1081\":2,\"1086\":4,\"1093\":3,\"1132\":1,\"1133\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1170\":1,\"1200\":1,\"1203\":3,\"1272\":1,\"1368\":1,\"1456\":1,\"1458\":1,\"1472\":1,\"1505\":1,\"1543\":2,\"1656\":3,\"1662\":1,\"1669\":1,\"1771\":3,\"1787\":3,\"1804\":2,\"1878\":2,\"2001\":1,\"2004\":1,\"2029\":1,\"2054\":1,\"2264\":2,\"2271\":1,\"2368\":1,\"2371\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2638\":1}}],[\"wireless\",{\"1\":{\"45\":3}}],[\"wip\",{\"1\":{\"11\":1,\"2447\":1}}],[\"within\",{\"1\":{\"5\":1,\"21\":3,\"79\":1,\"112\":1,\"115\":1,\"124\":1,\"148\":1,\"233\":1,\"594\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"940\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1255\":2,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1900\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1971\":2,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2156\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2387\":2}}],[\"without\",{\"0\":{\"97\":1,\"722\":1,\"1950\":1,\"2146\":1},\"1\":{\"3\":1,\"23\":1,\"48\":1,\"49\":1,\"74\":1,\"115\":1,\"119\":1,\"135\":1,\"461\":1,\"608\":1,\"672\":1,\"718\":2,\"722\":1,\"726\":2,\"1048\":1,\"1116\":1,\"1154\":1,\"1155\":2,\"1228\":1,\"1345\":1,\"1347\":1,\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"1670\":1,\"1671\":3,\"1854\":2,\"1947\":1,\"1950\":1,\"2099\":1,\"2102\":1,\"2146\":1,\"2585\":1}}],[\"with\",{\"0\":{\"34\":2,\"36\":1,\"40\":1,\"41\":1,\"42\":1,\"44\":1,\"51\":1,\"128\":1,\"185\":1,\"1698\":1,\"1705\":1,\"1707\":1,\"2140\":1,\"2353\":1,\"2483\":1,\"2499\":1,\"2596\":1,\"2617\":1,\"2620\":1,\"2635\":1},\"1\":{\"1\":2,\"2\":1,\"3\":2,\"4\":1,\"5\":1,\"6\":1,\"7\":1,\"16\":4,\"17\":1,\"20\":1,\"21\":2,\"22\":2,\"24\":2,\"28\":3,\"29\":1,\"30\":1,\"32\":7,\"34\":2,\"46\":2,\"47\":1,\"48\":2,\"49\":7,\"51\":2,\"59\":1,\"60\":1,\"62\":1,\"66\":2,\"69\":1,\"73\":1,\"75\":1,\"79\":2,\"80\":1,\"83\":1,\"84\":2,\"103\":1,\"110\":1,\"112\":1,\"113\":2,\"114\":1,\"118\":1,\"121\":1,\"122\":1,\"126\":1,\"133\":1,\"134\":1,\"141\":1,\"143\":1,\"148\":1,\"150\":1,\"155\":2,\"156\":1,\"158\":1,\"162\":1,\"164\":3,\"168\":1,\"171\":2,\"175\":2,\"179\":1,\"185\":1,\"193\":1,\"194\":2,\"203\":1,\"217\":1,\"218\":1,\"224\":1,\"225\":1,\"231\":1,\"232\":1,\"235\":1,\"238\":1,\"240\":1,\"245\":2,\"272\":2,\"286\":3,\"296\":1,\"301\":2,\"377\":1,\"461\":1,\"522\":1,\"551\":1,\"557\":1,\"564\":1,\"585\":1,\"596\":1,\"602\":1,\"603\":1,\"608\":1,\"622\":1,\"629\":1,\"632\":1,\"638\":1,\"640\":1,\"642\":2,\"646\":2,\"649\":1,\"650\":1,\"657\":1,\"661\":1,\"672\":1,\"676\":1,\"678\":1,\"682\":1,\"685\":1,\"691\":2,\"692\":3,\"693\":2,\"696\":1,\"697\":3,\"706\":1,\"708\":2,\"710\":1,\"726\":2,\"731\":3,\"734\":1,\"740\":1,\"741\":1,\"743\":1,\"745\":2,\"746\":2,\"750\":2,\"754\":2,\"762\":1,\"767\":1,\"770\":1,\"771\":2,\"773\":1,\"775\":1,\"776\":1,\"781\":1,\"787\":1,\"791\":1,\"793\":2,\"794\":2,\"797\":1,\"808\":1,\"809\":2,\"812\":1,\"815\":1,\"821\":1,\"826\":1,\"828\":1,\"830\":3,\"835\":6,\"836\":1,\"847\":1,\"857\":1,\"868\":1,\"869\":2,\"875\":1,\"877\":2,\"884\":1,\"893\":1,\"898\":2,\"900\":3,\"902\":3,\"905\":1,\"912\":1,\"913\":1,\"915\":1,\"919\":1,\"931\":1,\"933\":1,\"937\":1,\"938\":1,\"943\":1,\"950\":1,\"955\":1,\"956\":1,\"965\":2,\"968\":2,\"972\":2,\"973\":1,\"977\":1,\"984\":1,\"992\":1,\"995\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1011\":3,\"1015\":4,\"1016\":1,\"1019\":5,\"1025\":2,\"1028\":1,\"1037\":5,\"1039\":5,\"1040\":1,\"1044\":1,\"1048\":2,\"1065\":1,\"1066\":1,\"1076\":1,\"1081\":1,\"1093\":1,\"1130\":1,\"1133\":1,\"1142\":1,\"1150\":1,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1177\":2,\"1186\":2,\"1187\":2,\"1190\":1,\"1198\":7,\"1202\":2,\"1209\":1,\"1210\":2,\"1211\":2,\"1214\":1,\"1217\":2,\"1224\":2,\"1233\":1,\"1244\":1,\"1248\":1,\"1252\":4,\"1253\":4,\"1254\":3,\"1255\":5,\"1257\":1,\"1269\":2,\"1273\":1,\"1279\":1,\"1286\":1,\"1287\":3,\"1298\":1,\"1299\":1,\"1301\":2,\"1302\":1,\"1303\":1,\"1304\":2,\"1327\":1,\"1336\":1,\"1337\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1352\":3,\"1354\":1,\"1375\":1,\"1379\":1,\"1383\":1,\"1392\":1,\"1400\":1,\"1406\":1,\"1427\":8,\"1428\":2,\"1429\":3,\"1430\":2,\"1452\":1,\"1454\":1,\"1462\":2,\"1463\":2,\"1508\":1,\"1517\":1,\"1524\":1,\"1530\":1,\"1531\":1,\"1535\":1,\"1542\":1,\"1549\":1,\"1551\":1,\"1553\":1,\"1560\":2,\"1563\":1,\"1566\":1,\"1572\":1,\"1576\":1,\"1577\":1,\"1600\":1,\"1603\":1,\"1605\":1,\"1618\":2,\"1619\":2,\"1622\":1,\"1638\":1,\"1640\":1,\"1645\":1,\"1660\":5,\"1661\":7,\"1662\":5,\"1664\":1,\"1665\":2,\"1670\":2,\"1671\":4,\"1672\":1,\"1690\":1,\"1691\":1,\"1693\":3,\"1697\":1,\"1698\":2,\"1701\":2,\"1702\":2,\"1705\":2,\"1707\":2,\"1710\":2,\"1712\":2,\"1714\":1,\"1715\":3,\"1717\":1,\"1719\":8,\"1732\":1,\"1735\":2,\"1736\":1,\"1741\":1,\"1755\":3,\"1758\":1,\"1759\":1,\"1763\":1,\"1773\":1,\"1798\":1,\"1805\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1837\":1,\"1842\":1,\"1860\":2,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1880\":1,\"1914\":1,\"1915\":2,\"1917\":2,\"1929\":1,\"1940\":1,\"1941\":1,\"1944\":1,\"1946\":1,\"1947\":2,\"1957\":1,\"1958\":1,\"1959\":1,\"1960\":1,\"1971\":2,\"1973\":1,\"2001\":2,\"2002\":1,\"2003\":2,\"2004\":1,\"2010\":1,\"2011\":1,\"2040\":1,\"2078\":1,\"2087\":2,\"2090\":1,\"2095\":1,\"2099\":1,\"2102\":1,\"2125\":1,\"2140\":1,\"2142\":1,\"2151\":1,\"2154\":1,\"2155\":1,\"2170\":2,\"2193\":1,\"2236\":1,\"2243\":3,\"2244\":2,\"2253\":1,\"2254\":2,\"2255\":1,\"2259\":1,\"2263\":1,\"2264\":2,\"2270\":1,\"2271\":1,\"2272\":1,\"2279\":3,\"2342\":1,\"2353\":2,\"2355\":1,\"2360\":2,\"2363\":3,\"2365\":1,\"2372\":1,\"2373\":1,\"2385\":1,\"2386\":2,\"2401\":1,\"2403\":1,\"2406\":1,\"2415\":1,\"2416\":1,\"2420\":2,\"2430\":2,\"2431\":1,\"2433\":1,\"2450\":1,\"2452\":2,\"2457\":1,\"2458\":2,\"2467\":1,\"2468\":1,\"2481\":3,\"2482\":1,\"2494\":1,\"2497\":1,\"2501\":1,\"2506\":1,\"2508\":2,\"2510\":3,\"2514\":1,\"2515\":1,\"2518\":1,\"2522\":1,\"2523\":3,\"2537\":1,\"2539\":1,\"2542\":1,\"2543\":1,\"2555\":2,\"2558\":2,\"2559\":1,\"2568\":2,\"2571\":1,\"2574\":1,\"2581\":1,\"2582\":2,\"2584\":10,\"2585\":4,\"2586\":1,\"2592\":1,\"2598\":1,\"2599\":1,\"2600\":2,\"2614\":1,\"2615\":1,\"2618\":3,\"2630\":1,\"2632\":1,\"2633\":1,\"2638\":1,\"2639\":1,\"2653\":3,\"2655\":1,\"2659\":1,\"2660\":1}}],[\"ia\",{\"1\":{\"2618\":1}}],[\"iaxis=\",{\"1\":{\"1005\":1,\"1028\":1}}],[\"iaxis=0\",{\"1\":{\"799\":1,\"909\":1,\"1005\":2,\"1028\":2}}],[\"iaxis\",{\"1\":{\"909\":1,\"1005\":1,\"1028\":1}}],[\"i==int\",{\"1\":{\"2596\":1}}],[\"iemocap\",{\"1\":{\"2478\":8}}],[\"ieeexplore\",{\"1\":{\"700\":3,\"885\":1,\"1048\":3,\"1138\":3,\"1139\":3,\"1462\":1,\"1463\":1,\"1529\":1,\"1568\":1,\"1696\":1,\"1698\":1,\"1706\":1,\"1707\":1,\"1712\":1,\"1715\":1}}],[\"ieee\",{\"1\":{\"130\":6,\"198\":1,\"700\":3,\"885\":1,\"1048\":3,\"1138\":3,\"1139\":3,\"1462\":1,\"1463\":1,\"1529\":1,\"1568\":1,\"1696\":2,\"1698\":2,\"1706\":1,\"1707\":1,\"1712\":1,\"1715\":1,\"2030\":1,\"2064\":1,\"2070\":1}}],[\"iv\",{\"1\":{\"1705\":1}}],[\"i+1\",{\"1\":{\"1638\":1,\"2592\":2}}],[\"iii\",{\"1\":{\"1522\":1,\"1576\":1,\"1577\":1}}],[\"ij\",{\"1\":{\"1025\":2}}],[\"ikey\",{\"1\":{\"909\":1,\"1003\":1,\"1004\":1,\"1005\":1}}],[\"ikey=\",{\"1\":{\"799\":1,\"909\":1,\"1003\":3,\"1004\":3,\"1005\":3}}],[\"iu\",{\"1\":{\"231\":1}}],[\"iuysoiuwpn2atgnyhz\",{\"1\":{\"229\":1}}],[\"il3vd\",{\"1\":{\"223\":1}}],[\"ilens=none\",{\"1\":{\"1643\":1,\"2083\":1}}],[\"ilens\",{\"1\":{\"174\":2,\"676\":6,\"690\":3,\"701\":2,\"717\":1,\"729\":3,\"730\":3,\"735\":2,\"742\":4,\"747\":2,\"752\":1,\"754\":4,\"755\":2,\"756\":1,\"759\":1,\"760\":1,\"762\":2,\"763\":2,\"777\":1,\"778\":1,\"781\":4,\"782\":3,\"821\":4,\"826\":4,\"831\":1,\"932\":2,\"1111\":1,\"1114\":2,\"1119\":1,\"1140\":2,\"1148\":2,\"1149\":6,\"1150\":6,\"1169\":2,\"1178\":2,\"1179\":2,\"1180\":2,\"1181\":2,\"1200\":2,\"1203\":2,\"1215\":1,\"1216\":2,\"1222\":1,\"1269\":2,\"1272\":2,\"1282\":1,\"1285\":2,\"1360\":1,\"1362\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1373\":2,\"1374\":2,\"1375\":3,\"1376\":2,\"1377\":3,\"1431\":1,\"1432\":2,\"1433\":1,\"1435\":1,\"1439\":1,\"1440\":1,\"1441\":2,\"1445\":1,\"1454\":4,\"1455\":3,\"1463\":3,\"1505\":3,\"1510\":4,\"1511\":2,\"1515\":3,\"1516\":3,\"1523\":3,\"1524\":8,\"1525\":6,\"1528\":3,\"1529\":3,\"1534\":3,\"1539\":3,\"1558\":3,\"1595\":3,\"1611\":2,\"1616\":2,\"1617\":2,\"1626\":3,\"1643\":3,\"1644\":2,\"1645\":3,\"1654\":3,\"1658\":3,\"1659\":5,\"1660\":3,\"1661\":3,\"1662\":3,\"1669\":3,\"1671\":3,\"1719\":3,\"1841\":2,\"1879\":2,\"1906\":3,\"1910\":2,\"1912\":1,\"1918\":5,\"1920\":2,\"1949\":2,\"1999\":1,\"2029\":2,\"2083\":2,\"2091\":2,\"2245\":2,\"2256\":2,\"2280\":2,\"2285\":1,\"2287\":2,\"2498\":4,\"2616\":4,\"2634\":4}}],[\"ilen\",{\"1\":{\"26\":3,\"1145\":1}}],[\"icml\",{\"1\":{\"2438\":1,\"2564\":1}}],[\"ice\",{\"1\":{\"2125\":1}}],[\"icefall\",{\"1\":{\"120\":1,\"1047\":1,\"1076\":1,\"1101\":2,\"1102\":2}}],[\"ic\",{\"1\":{\"2040\":1}}],[\"icassp\",{\"1\":{\"130\":3,\"1132\":1,\"1462\":1,\"1463\":1,\"1604\":1,\"1655\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"2070\":1}}],[\"ipd\",{\"1\":{\"2386\":3}}],[\"ipykernel\",{\"1\":{\"2409\":1}}],[\"ipynb\",{\"1\":{\"155\":3,\"156\":2,\"157\":1,\"158\":1,\"159\":1,\"161\":9,\"162\":2,\"163\":1,\"164\":6,\"199\":1,\"202\":1,\"295\":1}}],[\"ipython\",{\"1\":{\"136\":2,\"194\":1,\"201\":1,\"202\":1,\"218\":1,\"225\":1,\"232\":1,\"240\":1,\"2359\":1,\"2360\":2,\"2365\":1,\"2367\":1,\"2372\":1,\"2384\":1,\"2386\":1,\"2415\":1,\"2416\":1,\"2440\":3,\"2456\":1,\"2458\":2,\"2460\":1,\"2470\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2497\":1,\"2501\":1,\"2508\":1,\"2510\":1,\"2515\":1,\"2521\":1,\"2522\":1,\"2523\":2,\"2564\":1,\"2572\":1,\"2580\":1,\"2581\":1,\"2582\":2,\"2604\":1,\"2607\":1,\"2614\":1,\"2615\":1,\"2621\":1,\"2624\":1,\"2632\":1,\"2633\":1,\"2647\":1,\"2655\":1,\"2660\":1}}],[\"ip\",{\"1\":{\"45\":1}}],[\"ibug\",{\"1\":{\"2040\":1}}],[\"ibm\",{\"1\":{\"1568\":1,\"1569\":1,\"1571\":1}}],[\"ib\",{\"1\":{\"45\":2}}],[\"i\",{\"0\":{\"1194\":1},\"1\":{\"24\":1,\"26\":1,\"30\":1,\"37\":1,\"38\":1,\"40\":1,\"48\":1,\"52\":2,\"76\":6,\"77\":1,\"78\":1,\"80\":2,\"82\":1,\"84\":1,\"106\":1,\"116\":1,\"121\":1,\"143\":1,\"201\":1,\"202\":1,\"217\":2,\"224\":2,\"231\":2,\"295\":1,\"493\":1,\"691\":2,\"697\":2,\"706\":2,\"711\":2,\"727\":1,\"728\":1,\"743\":1,\"744\":1,\"749\":2,\"797\":2,\"815\":2,\"875\":3,\"876\":1,\"880\":2,\"952\":2,\"1010\":2,\"1011\":7,\"1081\":1,\"1099\":1,\"1133\":2,\"1148\":2,\"1149\":2,\"1150\":2,\"1194\":1,\"1203\":2,\"1227\":2,\"1228\":2,\"1233\":1,\"1246\":1,\"1254\":2,\"1255\":1,\"1269\":1,\"1272\":2,\"1339\":1,\"1341\":2,\"1352\":1,\"1406\":1,\"1427\":2,\"1429\":1,\"1452\":2,\"1505\":2,\"1572\":1,\"1638\":5,\"1659\":2,\"1665\":2,\"1669\":2,\"1670\":1,\"1671\":1,\"1713\":1,\"1862\":1,\"1878\":1,\"1880\":1,\"1905\":1,\"1935\":3,\"1943\":3,\"1971\":1,\"1972\":1,\"2001\":2,\"2004\":2,\"2029\":2,\"2143\":2,\"2410\":1,\"2430\":1,\"2490\":1,\"2500\":6,\"2543\":1,\"2555\":1,\"2564\":1,\"2573\":1,\"2592\":6,\"2596\":1,\"2600\":6,\"2609\":1,\"2617\":6,\"2626\":1,\"2635\":6,\"2638\":1}}],[\"id0\",{\"1\":{\"2412\":3}}],[\"idxs\",{\"1\":{\"917\":2,\"1885\":2}}],[\"idx=1\",{\"1\":{\"2170\":1}}],[\"idx=\",{\"1\":{\"749\":1}}],[\"idx=0\",{\"1\":{\"711\":3,\"759\":1,\"1220\":1,\"2081\":1,\"2083\":1,\"2170\":1}}],[\"idx=none\",{\"1\":{\"681\":1,\"682\":1,\"758\":1,\"1169\":1,\"1670\":1,\"2299\":1}}],[\"idx\",{\"1\":{\"629\":2,\"681\":1,\"682\":1,\"691\":2,\"697\":6,\"699\":1,\"711\":2,\"725\":3,\"726\":2,\"749\":2,\"758\":1,\"759\":3,\"806\":2,\"818\":1,\"824\":3,\"858\":2,\"861\":2,\"918\":3,\"1046\":2,\"1048\":2,\"1066\":1,\"1073\":2,\"1075\":1,\"1083\":2,\"1140\":1,\"1148\":4,\"1149\":3,\"1150\":3,\"1155\":3,\"1169\":1,\"1200\":3,\"1203\":4,\"1270\":2,\"1272\":4,\"1306\":1,\"1505\":3,\"1670\":1,\"1773\":1,\"1778\":1,\"1805\":1,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"1996\":1,\"2026\":1,\"2029\":3,\"2054\":3,\"2170\":7,\"2500\":3,\"2514\":2,\"2617\":3,\"2635\":3,\"2659\":2}}],[\"id=0nqwnnwaori\",{\"1\":{\"1841\":1}}],[\"id=\",{\"1\":{\"768\":1,\"1567\":1}}],[\"id=bjywwy9ll\",{\"1\":{\"668\":1}}],[\"id=none\",{\"1\":{\"618\":1,\"706\":1}}],[\"id=1zf88brnbjhw9hnbq3nrdg8vnggibremg\",{\"1\":{\"277\":1}}],[\"id=1z8ksowvbjk\",{\"1\":{\"211\":1}}],[\"id=1t8thxkaxjgfpxpwptcklvhnd6lg0\",{\"1\":{\"230\":1}}],[\"id=1btsygvonv5ts6\",{\"1\":{\"229\":1}}],[\"id=10m6h88jeugbrwbmu1ff2vatmoael8cey\",{\"1\":{\"229\":1,\"230\":1}}],[\"id=1menzfbkqa4et6bn0erzup6lnzl\",{\"1\":{\"223\":1}}],[\"id=1kp5m4vvmagdmyckfja78wgqh1drb\",{\"1\":{\"222\":1}}],[\"id=1owruqzamvjj1x9cdhnzpp6dqtseqgejm\",{\"1\":{\"222\":1,\"223\":1}}],[\"id=1rgg5y15uy4wz\",{\"1\":{\"216\":1}}],[\"id=1\",{\"1\":{\"215\":1}}],[\"id=1grn7x9wd35ucdj5f7chwdtqta4u7devb\",{\"1\":{\"214\":1}}],[\"id=1p9i4qag8wacjitcpawt6wckbqufjftfp\",{\"1\":{\"212\":1}}],[\"id=1lffeyewyosxano\",{\"1\":{\"210\":1}}],[\"id=13y4tsygc8wtqzvavgk\",{\"1\":{\"200\":1}}],[\"identical\",{\"1\":{\"2564\":1}}],[\"identically\",{\"1\":{\"1598\":1,\"1652\":1,\"1654\":1}}],[\"identityfeatureextract\",{\"0\":{\"2281\":1},\"1\":{\"2281\":1}}],[\"identityencoder\",{\"0\":{\"2050\":1},\"1\":{\"2050\":1}}],[\"identity\",{\"0\":{\"946\":1,\"1325\":1,\"2050\":1,\"2281\":1},\"1\":{\"946\":2,\"1140\":1,\"1169\":1,\"1194\":1,\"1325\":1,\"1917\":1,\"2050\":2,\"2281\":1,\"2440\":1,\"2564\":1}}],[\"identifier\",{\"1\":{\"2385\":1}}],[\"identifiers\",{\"1\":{\"743\":1}}],[\"identification\",{\"1\":{\"150\":1,\"572\":1,\"2046\":1,\"2410\":1}}],[\"idea\",{\"1\":{\"56\":1,\"940\":1}}],[\"id3\",{\"1\":{\"75\":2,\"2395\":1,\"2412\":1,\"2531\":1}}],[\"id1\",{\"1\":{\"75\":2,\"1897\":1,\"2395\":1,\"2412\":3,\"2531\":1}}],[\"id2\",{\"1\":{\"60\":1,\"75\":2,\"1897\":1,\"2395\":1,\"2412\":1,\"2531\":1}}],[\"ida\",{\"1\":{\"54\":1}}],[\"id\",{\"0\":{\"572\":1,\"743\":1,\"745\":1,\"746\":1,\"875\":2,\"2123\":1,\"2128\":1,\"2135\":1},\"1\":{\"37\":1,\"47\":4,\"51\":2,\"52\":4,\"53\":4,\"54\":3,\"58\":11,\"60\":3,\"75\":1,\"88\":1,\"217\":6,\"224\":6,\"231\":5,\"237\":8,\"238\":2,\"239\":1,\"271\":2,\"272\":2,\"429\":2,\"572\":2,\"594\":3,\"604\":1,\"607\":1,\"618\":4,\"619\":1,\"621\":2,\"676\":2,\"691\":3,\"693\":2,\"697\":5,\"705\":2,\"706\":3,\"725\":13,\"726\":1,\"742\":1,\"743\":4,\"745\":1,\"746\":1,\"781\":1,\"797\":2,\"806\":11,\"815\":2,\"824\":6,\"825\":18,\"852\":2,\"857\":2,\"858\":1,\"861\":1,\"863\":3,\"865\":3,\"875\":4,\"876\":2,\"884\":4,\"895\":2,\"905\":2,\"908\":6,\"912\":1,\"918\":3,\"920\":2,\"922\":1,\"924\":2,\"925\":1,\"933\":2,\"934\":1,\"989\":2,\"1046\":5,\"1048\":1,\"1057\":9,\"1058\":1,\"1059\":3,\"1062\":4,\"1063\":1,\"1066\":4,\"1073\":6,\"1074\":2,\"1075\":4,\"1081\":4,\"1083\":6,\"1099\":8,\"1104\":3,\"1113\":1,\"1138\":1,\"1144\":1,\"1145\":1,\"1171\":3,\"1172\":2,\"1173\":3,\"1191\":1,\"1192\":1,\"1206\":1,\"1227\":1,\"1228\":2,\"1270\":9,\"1371\":1,\"1429\":1,\"1551\":1,\"1553\":1,\"1554\":1,\"1598\":2,\"1652\":2,\"1654\":2,\"1773\":7,\"1798\":2,\"1837\":4,\"1892\":2,\"1893\":2,\"1897\":1,\"1901\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1970\":2,\"1975\":2,\"1984\":1,\"2002\":2,\"2027\":2,\"2076\":4,\"2082\":7,\"2090\":2,\"2095\":2,\"2123\":1,\"2128\":1,\"2135\":1,\"2216\":1,\"2217\":1,\"2240\":7,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2278\":7,\"2279\":3,\"2280\":1,\"2367\":1,\"2368\":1,\"2372\":1,\"2385\":5,\"2386\":2,\"2387\":6,\"2412\":1,\"2414\":1,\"2419\":1,\"2470\":1,\"2474\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2486\":1,\"2490\":1,\"2492\":1,\"2494\":2,\"2497\":1,\"2500\":1,\"2506\":1,\"2510\":2,\"2512\":1,\"2514\":1,\"2515\":1,\"2567\":1,\"2604\":1,\"2605\":1,\"2609\":1,\"2614\":1,\"2621\":1,\"2622\":1,\"2626\":1,\"2628\":1,\"2630\":1,\"2632\":1,\"2647\":2,\"2649\":1,\"2659\":1}}],[\"ids=\",{\"1\":{\"2149\":1}}],[\"ids2tokens\",{\"1\":{\"2123\":1,\"2128\":1,\"2135\":1}}],[\"ids2text\",{\"1\":{\"1205\":1}}],[\"idseq\",{\"1\":{\"217\":6,\"224\":6,\"231\":5}}],[\"ids\",{\"1\":{\"22\":1,\"47\":1,\"60\":1,\"619\":1,\"691\":9,\"697\":10,\"705\":3,\"706\":4,\"726\":1,\"731\":1,\"734\":2,\"735\":1,\"742\":1,\"754\":2,\"767\":2,\"797\":4,\"815\":1,\"817\":2,\"826\":2,\"828\":2,\"875\":2,\"909\":1,\"933\":2,\"1133\":3,\"1204\":1,\"1205\":1,\"1214\":2,\"1244\":1,\"1273\":2,\"1778\":6,\"1804\":10,\"1805\":6,\"1897\":1,\"1957\":2,\"1960\":2,\"2001\":4,\"2002\":3,\"2004\":3,\"2024\":3,\"2028\":3,\"2083\":2,\"2086\":9,\"2087\":9,\"2090\":6,\"2095\":6,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2279\":3,\"2412\":1,\"2515\":1}}],[\"idim=80\",{\"1\":{\"2257\":1}}],[\"idim\",{\"1\":{\"21\":6,\"26\":3,\"173\":2,\"194\":2,\"217\":3,\"224\":3,\"231\":3,\"646\":2,\"676\":7,\"701\":4,\"712\":2,\"714\":3,\"715\":3,\"716\":3,\"717\":2,\"718\":3,\"719\":3,\"720\":3,\"721\":3,\"722\":3,\"726\":2,\"736\":1,\"737\":3,\"742\":5,\"747\":3,\"749\":2,\"754\":2,\"756\":1,\"764\":4,\"767\":1,\"777\":2,\"781\":5,\"782\":1,\"801\":1,\"802\":3,\"803\":1,\"804\":2,\"805\":2,\"808\":2,\"821\":5,\"826\":2,\"858\":2,\"878\":2,\"882\":1,\"890\":1,\"1595\":1,\"1778\":2,\"1805\":2,\"1850\":2,\"1851\":2,\"1852\":2,\"1877\":2,\"1917\":1,\"1994\":1,\"2001\":2,\"2002\":4,\"2003\":1,\"2004\":1,\"2078\":3,\"2081\":2,\"2083\":4,\"2086\":2,\"2087\":2,\"2090\":3,\"2095\":3,\"2243\":3,\"2244\":3,\"2255\":3,\"2257\":2,\"2260\":2,\"2261\":2,\"2263\":3,\"2264\":3,\"2265\":3,\"2279\":2}}],[\"io=false\",{\"1\":{\"1028\":1}}],[\"iou\",{\"1\":{\"231\":1}}],[\"io\",{\"0\":{\"987\":1,\"989\":1,\"1099\":1},\"1\":{\"13\":1,\"49\":1,\"166\":1,\"177\":1,\"199\":1,\"233\":1,\"825\":1,\"934\":1,\"987\":2,\"989\":2,\"1013\":1,\"1028\":1,\"1099\":1,\"1927\":3,\"2099\":1,\"2354\":1,\"2355\":2,\"2372\":3,\"2377\":1,\"2378\":1,\"2382\":1,\"2384\":1,\"2386\":1,\"2390\":3,\"2414\":1,\"2424\":3,\"2429\":1,\"2436\":1,\"2437\":1,\"2526\":3,\"2547\":3,\"2554\":1,\"2558\":1,\"2562\":1,\"2563\":1,\"2573\":1}}],[\"ignores\",{\"1\":{\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1}}],[\"ignored\",{\"1\":{\"73\":1,\"76\":1,\"691\":2,\"693\":2,\"697\":2,\"797\":2,\"857\":2,\"1214\":1,\"1618\":1,\"1619\":1}}],[\"ignore=d\",{\"1\":{\"11\":1}}],[\"ignore\",{\"1\":{\"11\":1,\"59\":1,\"167\":1,\"178\":1,\"234\":1,\"743\":3,\"744\":1,\"745\":1,\"746\":1,\"768\":1,\"825\":2,\"852\":2,\"875\":3,\"876\":1,\"884\":1,\"905\":2,\"920\":2,\"924\":2,\"925\":3,\"1057\":2,\"1099\":2,\"1145\":2,\"1171\":1,\"1172\":1,\"1206\":1,\"1567\":1,\"1892\":1,\"1893\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":2,\"2157\":1,\"2279\":1,\"2280\":1,\"2304\":1,\"2315\":1,\"2573\":1,\"2584\":1,\"2585\":4}}],[\"imcompatible\",{\"1\":{\"2362\":1,\"2393\":1,\"2427\":1,\"2504\":1,\"2529\":1,\"2550\":1,\"2576\":1,\"2651\":1}}],[\"im\",{\"1\":{\"1750\":1}}],[\"imics\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"imics=1\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"imitate\",{\"1\":{\"1013\":1}}],[\"imaginary\",{\"1\":{\"1452\":1,\"1522\":2,\"1671\":1}}],[\"imag=false\",{\"1\":{\"1314\":1}}],[\"imag\",{\"1\":{\"928\":2,\"1314\":1,\"1516\":3,\"1733\":1}}],[\"images\",{\"1\":{\"1\":1,\"5\":1,\"88\":1,\"247\":2,\"528\":2,\"1011\":1,\"1693\":2,\"1755\":2,\"2193\":1,\"2375\":2,\"2440\":3,\"2558\":2,\"2564\":2,\"2572\":1}}],[\"image\",{\"0\":{\"88\":1,\"1011\":1,\"1036\":1},\"1\":{\"1\":1,\"5\":2,\"7\":3,\"88\":1,\"240\":6,\"247\":2,\"629\":2,\"799\":1,\"909\":1,\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"1009\":2,\"1011\":12,\"1018\":2,\"1021\":2,\"1022\":2,\"1036\":1,\"1115\":4,\"1543\":4,\"1564\":2,\"1605\":1,\"1693\":2,\"1755\":2,\"2372\":1,\"2415\":3,\"2416\":2,\"2440\":6,\"2564\":4,\"2572\":3}}],[\"imshow\",{\"1\":{\"238\":2}}],[\"img\",{\"1\":{\"88\":2,\"1036\":1}}],[\"impulse\",{\"0\":{\"1809\":1},\"1\":{\"1809\":1}}],[\"imput\",{\"1\":{\"805\":1}}],[\"implemenation\",{\"1\":{\"1986\":1}}],[\"implements\",{\"1\":{\"627\":1,\"650\":1,\"792\":1,\"875\":1,\"1132\":1,\"1252\":1,\"1254\":1,\"1352\":1,\"2642\":1}}],[\"implementaion\",{\"1\":{\"1846\":1,\"1847\":1}}],[\"implementaiton\",{\"1\":{\"597\":1}}],[\"implementations\",{\"1\":{\"745\":2,\"746\":2}}],[\"implementation\",{\"1\":{\"14\":1,\"26\":1,\"74\":1,\"120\":1,\"628\":2,\"676\":2,\"691\":1,\"692\":1,\"693\":1,\"697\":1,\"700\":7,\"703\":1,\"706\":1,\"709\":1,\"733\":1,\"740\":1,\"741\":1,\"745\":1,\"746\":1,\"767\":1,\"775\":1,\"776\":1,\"781\":2,\"797\":1,\"809\":1,\"810\":1,\"812\":2,\"820\":1,\"836\":1,\"1037\":1,\"1048\":4,\"1138\":8,\"1139\":7,\"1203\":1,\"1207\":1,\"1214\":1,\"1257\":1,\"1274\":1,\"1332\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1598\":1,\"1648\":1,\"1652\":1,\"1697\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"1917\":1,\"1940\":1,\"2019\":2,\"2086\":1,\"2087\":1,\"2237\":1,\"2259\":1,\"2309\":1,\"2543\":1}}],[\"implement\",{\"1\":{\"170\":1,\"745\":2,\"746\":2,\"760\":1,\"778\":1,\"792\":1,\"831\":1,\"981\":2,\"1245\":1,\"1476\":1,\"1598\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2040\":1,\"2084\":1,\"2089\":1,\"2168\":1,\"2410\":1,\"2543\":2}}],[\"implementing\",{\"0\":{\"2641\":1},\"1\":{\"124\":1,\"2618\":1}}],[\"implemented\",{\"1\":{\"3\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":3,\"1897\":2,\"2156\":1,\"2270\":1,\"2272\":1,\"2543\":1}}],[\"impluse\",{\"1\":{\"1883\":1}}],[\"implants\",{\"1\":{\"1712\":1,\"1715\":1}}],[\"implicit\",{\"1\":{\"1558\":1}}],[\"improves\",{\"1\":{\"2473\":1}}],[\"improvedtransformerlayer\",{\"0\":{\"1581\":1},\"1\":{\"1581\":1}}],[\"improved\",{\"1\":{\"1430\":2,\"1581\":1,\"1670\":2}}],[\"improvement\",{\"1\":{\"19\":1,\"148\":1,\"2022\":1}}],[\"improve\",{\"1\":{\"11\":2,\"23\":1,\"119\":1,\"2435\":1,\"2473\":1,\"2561\":1,\"2564\":1}}],[\"important\",{\"0\":{\"30\":1},\"1\":{\"20\":1,\"79\":1,\"112\":1,\"169\":1,\"226\":1,\"2373\":1,\"2384\":1,\"2385\":2,\"2393\":1,\"2427\":1,\"2433\":2,\"2435\":1,\"2529\":1,\"2550\":1,\"2555\":2,\"2561\":1}}],[\"imported\",{\"1\":{\"5\":1,\"1012\":1}}],[\"import\",{\"0\":{\"665\":1,\"674\":1,\"872\":1,\"873\":1,\"874\":1,\"1012\":2,\"2591\":1},\"1\":{\"5\":1,\"49\":1,\"56\":2,\"60\":2,\"128\":1,\"171\":3,\"172\":1,\"173\":3,\"174\":6,\"175\":4,\"185\":3,\"193\":3,\"194\":8,\"198\":1,\"201\":2,\"202\":3,\"203\":2,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":13,\"218\":2,\"222\":1,\"223\":1,\"224\":11,\"225\":2,\"229\":1,\"230\":1,\"231\":12,\"232\":2,\"235\":1,\"238\":2,\"240\":1,\"665\":3,\"674\":2,\"872\":2,\"873\":3,\"874\":3,\"1012\":7,\"1017\":2,\"1951\":1,\"2096\":3,\"2098\":3,\"2099\":3,\"2100\":3,\"2101\":3,\"2102\":3,\"2103\":3,\"2104\":3,\"2105\":3,\"2107\":3,\"2108\":3,\"2109\":3,\"2110\":3,\"2111\":2,\"2112\":3,\"2113\":3,\"2114\":3,\"2115\":3,\"2116\":3,\"2117\":3,\"2118\":3,\"2143\":1,\"2168\":1,\"2170\":1,\"2176\":1,\"2209\":3,\"2320\":1,\"2328\":1,\"2335\":1,\"2340\":1,\"2358\":5,\"2359\":5,\"2360\":8,\"2364\":2,\"2365\":3,\"2367\":3,\"2368\":3,\"2371\":4,\"2372\":3,\"2386\":9,\"2392\":2,\"2400\":1,\"2415\":1,\"2416\":1,\"2425\":2,\"2432\":1,\"2440\":3,\"2455\":4,\"2456\":7,\"2458\":8,\"2460\":11,\"2470\":3,\"2472\":1,\"2474\":5,\"2476\":4,\"2478\":4,\"2482\":1,\"2485\":3,\"2486\":3,\"2490\":3,\"2494\":3,\"2497\":3,\"2498\":5,\"2500\":6,\"2501\":3,\"2507\":2,\"2508\":3,\"2510\":7,\"2513\":2,\"2514\":6,\"2515\":3,\"2520\":9,\"2521\":7,\"2522\":5,\"2523\":8,\"2528\":2,\"2536\":1,\"2548\":2,\"2564\":1,\"2568\":3,\"2572\":1,\"2579\":5,\"2580\":5,\"2581\":5,\"2582\":8,\"2584\":1,\"2591\":7,\"2595\":1,\"2599\":1,\"2600\":9,\"2604\":3,\"2605\":3,\"2607\":3,\"2609\":3,\"2612\":4,\"2614\":3,\"2615\":3,\"2616\":5,\"2617\":7,\"2621\":3,\"2622\":3,\"2624\":3,\"2626\":3,\"2630\":4,\"2632\":3,\"2633\":3,\"2634\":5,\"2635\":7,\"2647\":3,\"2648\":1,\"2649\":5,\"2654\":2,\"2655\":3,\"2658\":2,\"2659\":6,\"2660\":3}}],[\"isolated\",{\"1\":{\"2492\":5,\"2628\":5}}],[\"isotropic\",{\"1\":{\"948\":1,\"1252\":1}}],[\"isg2p\",{\"0\":{\"2125\":1},\"1\":{\"2125\":2}}],[\"isvalid=false\",{\"1\":{\"2082\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1}}],[\"isn\",{\"1\":{\"2019\":1,\"2121\":1,\"2122\":1}}],[\"isik16\",{\"1\":{\"1528\":1}}],[\"isik\",{\"1\":{\"1528\":1}}],[\"istft\",{\"0\":{\"945\":1,\"966\":1},\"1\":{\"247\":8,\"945\":2,\"966\":2,\"1643\":1}}],[\"isspace\",{\"1\":{\"217\":1,\"224\":1}}],[\"issues\",{\"1\":{\"734\":1,\"1255\":1,\"1860\":1,\"2090\":1,\"2389\":1,\"2393\":1,\"2408\":1,\"2422\":1,\"2427\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2529\":1,\"2545\":1,\"2550\":1}}],[\"issue\",{\"1\":{\"20\":2,\"34\":1,\"106\":2,\"112\":2,\"127\":1,\"1427\":1,\"2423\":1,\"2543\":1,\"2546\":1,\"2585\":1}}],[\"isca\",{\"1\":{\"130\":1,\"1462\":2,\"1463\":2,\"1528\":1,\"1529\":1,\"1568\":1,\"2032\":1}}],[\"is\",{\"0\":{\"48\":1,\"644\":1,\"895\":1,\"1027\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1723\":1,\"1724\":1,\"2220\":1,\"2221\":1},\"1\":{\"1\":1,\"2\":2,\"3\":1,\"4\":1,\"5\":5,\"11\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1,\"17\":2,\"19\":1,\"21\":4,\"22\":1,\"24\":4,\"25\":1,\"26\":2,\"28\":1,\"33\":2,\"38\":1,\"44\":3,\"45\":3,\"46\":4,\"48\":2,\"49\":3,\"51\":1,\"52\":3,\"53\":1,\"56\":4,\"57\":6,\"58\":3,\"59\":4,\"60\":12,\"62\":1,\"65\":1,\"69\":3,\"72\":4,\"73\":1,\"74\":2,\"75\":5,\"76\":4,\"77\":1,\"78\":1,\"79\":4,\"80\":4,\"82\":1,\"83\":1,\"84\":2,\"85\":6,\"88\":1,\"92\":2,\"95\":2,\"96\":1,\"98\":2,\"99\":1,\"101\":1,\"102\":5,\"109\":1,\"111\":1,\"112\":3,\"113\":1,\"114\":1,\"115\":4,\"116\":1,\"117\":1,\"118\":1,\"120\":1,\"121\":1,\"128\":1,\"132\":1,\"134\":2,\"135\":2,\"137\":2,\"142\":1,\"143\":2,\"144\":2,\"148\":5,\"150\":4,\"182\":1,\"195\":1,\"198\":1,\"216\":1,\"217\":2,\"221\":1,\"231\":1,\"233\":1,\"235\":2,\"236\":2,\"237\":4,\"238\":5,\"239\":2,\"286\":1,\"295\":4,\"315\":2,\"363\":2,\"461\":1,\"485\":2,\"493\":1,\"564\":1,\"595\":2,\"607\":1,\"608\":2,\"610\":2,\"612\":2,\"613\":1,\"626\":2,\"627\":1,\"628\":2,\"632\":1,\"643\":3,\"644\":1,\"652\":3,\"658\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":2,\"687\":1,\"688\":2,\"689\":2,\"691\":4,\"692\":4,\"693\":3,\"697\":5,\"698\":1,\"699\":2,\"700\":1,\"701\":1,\"704\":1,\"705\":1,\"706\":2,\"710\":4,\"711\":2,\"725\":1,\"726\":1,\"727\":6,\"728\":5,\"736\":1,\"737\":1,\"738\":1,\"739\":2,\"740\":2,\"741\":2,\"743\":3,\"745\":7,\"746\":6,\"754\":1,\"764\":1,\"766\":3,\"774\":1,\"775\":2,\"776\":2,\"778\":1,\"785\":2,\"786\":2,\"791\":1,\"794\":1,\"795\":1,\"797\":7,\"802\":1,\"803\":1,\"812\":1,\"816\":1,\"821\":1,\"823\":1,\"826\":1,\"834\":4,\"836\":2,\"837\":2,\"857\":2,\"864\":1,\"866\":3,\"875\":4,\"895\":3,\"905\":1,\"917\":1,\"918\":3,\"928\":1,\"933\":1,\"950\":1,\"952\":1,\"968\":1,\"999\":2,\"1008\":1,\"1011\":3,\"1013\":2,\"1015\":4,\"1025\":1,\"1027\":2,\"1034\":3,\"1048\":1,\"1071\":6,\"1085\":1,\"1095\":1,\"1097\":3,\"1103\":1,\"1113\":1,\"1116\":1,\"1132\":1,\"1133\":3,\"1138\":1,\"1141\":2,\"1142\":1,\"1143\":3,\"1144\":1,\"1149\":3,\"1150\":3,\"1154\":1,\"1160\":2,\"1161\":2,\"1164\":2,\"1165\":2,\"1169\":1,\"1171\":3,\"1172\":2,\"1182\":2,\"1187\":6,\"1190\":1,\"1194\":1,\"1198\":3,\"1202\":6,\"1204\":1,\"1206\":1,\"1211\":1,\"1214\":3,\"1224\":1,\"1227\":1,\"1228\":1,\"1241\":1,\"1244\":2,\"1245\":6,\"1248\":3,\"1252\":1,\"1253\":5,\"1254\":2,\"1255\":2,\"1269\":5,\"1273\":2,\"1274\":1,\"1279\":2,\"1286\":4,\"1287\":4,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1328\":2,\"1329\":2,\"1330\":1,\"1336\":1,\"1339\":1,\"1340\":1,\"1342\":1,\"1345\":1,\"1347\":1,\"1348\":1,\"1352\":1,\"1368\":3,\"1369\":1,\"1371\":3,\"1372\":3,\"1375\":2,\"1377\":3,\"1379\":1,\"1381\":3,\"1383\":1,\"1390\":1,\"1391\":1,\"1400\":1,\"1406\":6,\"1407\":1,\"1409\":1,\"1427\":1,\"1429\":1,\"1430\":1,\"1432\":1,\"1436\":1,\"1462\":1,\"1464\":1,\"1466\":2,\"1472\":3,\"1473\":1,\"1510\":1,\"1511\":1,\"1515\":2,\"1517\":2,\"1522\":1,\"1526\":1,\"1528\":2,\"1529\":2,\"1532\":3,\"1534\":2,\"1535\":3,\"1537\":3,\"1539\":2,\"1543\":1,\"1551\":2,\"1552\":3,\"1553\":2,\"1554\":1,\"1558\":1,\"1564\":1,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1570\":4,\"1571\":2,\"1572\":1,\"1575\":3,\"1581\":2,\"1598\":3,\"1603\":1,\"1604\":3,\"1618\":1,\"1619\":1,\"1622\":2,\"1626\":2,\"1639\":6,\"1640\":3,\"1641\":2,\"1643\":1,\"1644\":1,\"1648\":3,\"1650\":2,\"1652\":4,\"1654\":5,\"1655\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":2,\"1664\":2,\"1665\":2,\"1666\":2,\"1667\":4,\"1668\":2,\"1670\":5,\"1671\":12,\"1683\":3,\"1688\":3,\"1693\":4,\"1695\":2,\"1696\":4,\"1697\":4,\"1698\":1,\"1713\":1,\"1715\":1,\"1719\":2,\"1723\":1,\"1724\":1,\"1739\":2,\"1755\":4,\"1756\":3,\"1773\":1,\"1778\":11,\"1785\":2,\"1786\":4,\"1797\":3,\"1798\":1,\"1804\":2,\"1805\":12,\"1810\":2,\"1837\":1,\"1849\":1,\"1850\":5,\"1852\":4,\"1860\":1,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":5,\"1878\":2,\"1883\":1,\"1892\":2,\"1893\":2,\"1895\":1,\"1897\":1,\"1900\":1,\"1905\":2,\"1912\":1,\"1917\":5,\"1929\":1,\"1941\":2,\"1947\":1,\"1951\":1,\"1954\":1,\"1956\":2,\"1964\":1,\"1970\":2,\"1975\":2,\"1980\":2,\"1984\":1,\"1985\":1,\"1986\":1,\"2001\":2,\"2002\":1,\"2003\":1,\"2004\":1,\"2011\":3,\"2019\":4,\"2020\":2,\"2021\":1,\"2022\":2,\"2023\":1,\"2027\":2,\"2076\":2,\"2077\":2,\"2078\":1,\"2079\":3,\"2080\":1,\"2081\":2,\"2082\":2,\"2083\":2,\"2086\":10,\"2087\":10,\"2090\":11,\"2095\":10,\"2096\":5,\"2098\":5,\"2099\":9,\"2100\":5,\"2101\":5,\"2102\":7,\"2103\":5,\"2104\":5,\"2105\":5,\"2107\":5,\"2108\":5,\"2109\":5,\"2110\":5,\"2111\":4,\"2112\":5,\"2113\":5,\"2114\":5,\"2115\":5,\"2116\":5,\"2117\":5,\"2118\":5,\"2121\":1,\"2122\":1,\"2131\":1,\"2142\":1,\"2151\":1,\"2153\":2,\"2168\":1,\"2170\":3,\"2197\":4,\"2199\":1,\"2212\":2,\"2216\":1,\"2217\":1,\"2220\":1,\"2221\":1,\"2235\":2,\"2236\":1,\"2237\":2,\"2240\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2257\":1,\"2259\":2,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2270\":1,\"2272\":1,\"2277\":2,\"2278\":1,\"2279\":1,\"2281\":1,\"2294\":2,\"2301\":1,\"2302\":6,\"2303\":3,\"2304\":1,\"2305\":3,\"2309\":2,\"2354\":2,\"2355\":2,\"2372\":3,\"2373\":4,\"2384\":2,\"2385\":9,\"2386\":1,\"2387\":8,\"2388\":1,\"2389\":1,\"2394\":2,\"2395\":1,\"2396\":1,\"2398\":2,\"2400\":3,\"2403\":1,\"2410\":2,\"2411\":1,\"2412\":2,\"2414\":2,\"2415\":2,\"2418\":1,\"2420\":1,\"2421\":1,\"2424\":1,\"2429\":2,\"2430\":6,\"2431\":1,\"2432\":2,\"2433\":2,\"2435\":1,\"2437\":1,\"2438\":1,\"2439\":4,\"2440\":9,\"2441\":6,\"2450\":2,\"2451\":1,\"2452\":3,\"2461\":1,\"2462\":2,\"2467\":1,\"2468\":2,\"2480\":1,\"2499\":1,\"2500\":1,\"2502\":1,\"2510\":3,\"2514\":1,\"2518\":5,\"2524\":1,\"2525\":1,\"2530\":2,\"2531\":1,\"2532\":1,\"2534\":2,\"2536\":3,\"2539\":1,\"2543\":3,\"2544\":1,\"2547\":1,\"2554\":2,\"2555\":8,\"2558\":1,\"2561\":1,\"2563\":1,\"2564\":7,\"2565\":1,\"2568\":2,\"2573\":3,\"2574\":1,\"2583\":1,\"2584\":5,\"2585\":2,\"2592\":7,\"2596\":6,\"2597\":1,\"2598\":1,\"2599\":2,\"2600\":4,\"2617\":2,\"2618\":1,\"2635\":2,\"2639\":1,\"2641\":1,\"2659\":1}}],[\"ifasnet\",{\"0\":{\"1718\":2,\"1745\":1},\"1\":{\"1460\":1,\"1558\":2,\"1718\":4,\"1745\":2}}],[\"ifspk2utt\",{\"1\":{\"506\":1}}],[\"ifconfig\",{\"1\":{\"45\":1}}],[\"ifname\",{\"1\":{\"44\":1}}],[\"ifname=en\",{\"1\":{\"45\":1}}],[\"ifname=eth\",{\"1\":{\"45\":1}}],[\"ifname=^lo\",{\"1\":{\"45\":2}}],[\"ifname=\",{\"1\":{\"44\":2}}],[\"if\",{\"1\":{\"1\":2,\"4\":1,\"14\":1,\"18\":1,\"19\":3,\"20\":1,\"21\":2,\"22\":2,\"26\":2,\"27\":1,\"28\":1,\"33\":1,\"34\":1,\"38\":1,\"45\":1,\"47\":1,\"48\":1,\"49\":3,\"54\":2,\"56\":1,\"59\":2,\"60\":1,\"69\":1,\"73\":1,\"74\":1,\"80\":1,\"84\":1,\"85\":2,\"90\":1,\"95\":1,\"96\":1,\"101\":1,\"102\":2,\"111\":1,\"112\":1,\"115\":9,\"118\":1,\"124\":3,\"126\":1,\"127\":2,\"132\":1,\"133\":1,\"134\":3,\"135\":6,\"136\":1,\"141\":2,\"143\":1,\"148\":4,\"149\":1,\"150\":3,\"168\":2,\"179\":1,\"180\":1,\"198\":1,\"203\":1,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":2,\"222\":1,\"223\":1,\"224\":1,\"229\":1,\"230\":1,\"231\":3,\"235\":2,\"236\":1,\"276\":1,\"279\":1,\"295\":2,\"506\":1,\"607\":1,\"626\":1,\"629\":1,\"635\":1,\"691\":3,\"692\":1,\"693\":4,\"697\":6,\"706\":1,\"711\":2,\"727\":3,\"728\":3,\"734\":1,\"742\":1,\"743\":1,\"745\":2,\"746\":2,\"749\":2,\"750\":1,\"754\":1,\"768\":1,\"770\":1,\"778\":2,\"797\":5,\"808\":1,\"815\":1,\"820\":1,\"821\":1,\"826\":1,\"834\":3,\"835\":1,\"836\":1,\"857\":3,\"861\":1,\"863\":1,\"864\":1,\"865\":1,\"875\":1,\"895\":1,\"899\":1,\"901\":1,\"922\":1,\"943\":1,\"950\":1,\"955\":1,\"965\":1,\"968\":1,\"972\":1,\"987\":2,\"1003\":1,\"1004\":1,\"1005\":1,\"1007\":1,\"1008\":1,\"1011\":1,\"1019\":2,\"1025\":2,\"1028\":5,\"1034\":1,\"1037\":2,\"1039\":2,\"1046\":1,\"1051\":1,\"1052\":1,\"1054\":1,\"1055\":1,\"1061\":1,\"1063\":1,\"1067\":1,\"1082\":2,\"1084\":2,\"1095\":1,\"1133\":4,\"1141\":2,\"1142\":1,\"1148\":2,\"1149\":2,\"1150\":2,\"1154\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1177\":1,\"1178\":1,\"1187\":4,\"1190\":1,\"1198\":1,\"1202\":4,\"1203\":2,\"1204\":2,\"1209\":1,\"1214\":2,\"1218\":1,\"1222\":1,\"1228\":1,\"1241\":1,\"1243\":3,\"1244\":2,\"1245\":2,\"1251\":1,\"1252\":2,\"1253\":2,\"1254\":2,\"1255\":1,\"1269\":6,\"1272\":2,\"1273\":2,\"1282\":1,\"1286\":2,\"1287\":2,\"1345\":1,\"1347\":1,\"1352\":1,\"1356\":2,\"1371\":2,\"1406\":2,\"1430\":2,\"1462\":3,\"1463\":1,\"1464\":3,\"1478\":1,\"1482\":1,\"1505\":2,\"1516\":1,\"1524\":1,\"1603\":4,\"1622\":4,\"1638\":1,\"1639\":2,\"1643\":1,\"1644\":1,\"1664\":1,\"1665\":3,\"1669\":2,\"1670\":4,\"1671\":1,\"1693\":1,\"1719\":1,\"1741\":1,\"1755\":1,\"1765\":1,\"1777\":1,\"1785\":1,\"1800\":1,\"1803\":1,\"1804\":3,\"1810\":1,\"1833\":1,\"1838\":1,\"1840\":1,\"1844\":1,\"1848\":2,\"1849\":2,\"1851\":4,\"1855\":1,\"1856\":1,\"1857\":1,\"1859\":1,\"1861\":2,\"1862\":1,\"1865\":1,\"1868\":2,\"1871\":1,\"1878\":4,\"1880\":3,\"1912\":1,\"1921\":2,\"1924\":1,\"1929\":1,\"1941\":1,\"1947\":1,\"1973\":2,\"2001\":5,\"2002\":3,\"2004\":5,\"2019\":1,\"2022\":2,\"2023\":1,\"2029\":2,\"2079\":2,\"2086\":6,\"2087\":6,\"2090\":4,\"2095\":4,\"2096\":2,\"2098\":2,\"2099\":5,\"2100\":2,\"2101\":2,\"2102\":3,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2153\":1,\"2155\":1,\"2168\":1,\"2170\":3,\"2185\":1,\"2201\":2,\"2203\":1,\"2243\":5,\"2244\":5,\"2255\":5,\"2263\":4,\"2264\":4,\"2279\":5,\"2359\":1,\"2363\":1,\"2368\":1,\"2372\":3,\"2386\":1,\"2389\":2,\"2393\":1,\"2408\":2,\"2422\":2,\"2423\":1,\"2429\":3,\"2430\":1,\"2432\":1,\"2437\":1,\"2440\":2,\"2449\":2,\"2456\":1,\"2460\":1,\"2465\":2,\"2472\":1,\"2476\":1,\"2481\":2,\"2486\":1,\"2490\":1,\"2500\":7,\"2501\":1,\"2503\":2,\"2506\":1,\"2514\":4,\"2521\":1,\"2525\":2,\"2529\":1,\"2542\":1,\"2543\":1,\"2545\":2,\"2546\":1,\"2552\":2,\"2554\":1,\"2555\":1,\"2558\":1,\"2560\":1,\"2563\":1,\"2564\":3,\"2568\":6,\"2580\":1,\"2584\":5,\"2585\":2,\"2592\":5,\"2596\":3,\"2600\":1,\"2605\":1,\"2609\":1,\"2617\":7,\"2622\":1,\"2626\":1,\"2635\":9,\"2639\":1,\"2640\":1,\"2644\":1,\"2648\":1,\"2653\":1,\"2659\":4}}],[\"it1\",{\"1\":{\"2431\":3}}],[\"italian\",{\"1\":{\"461\":1}}],[\"items\",{\"1\":{\"171\":1,\"175\":1,\"185\":1,\"193\":1,\"629\":1,\"989\":1,\"1870\":1,\"2002\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2313\":1}}],[\"itertools\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"iter1\",{\"1\":{\"2431\":3}}],[\"iterrows\",{\"1\":{\"2359\":1,\"2456\":1,\"2460\":1,\"2521\":1,\"2580\":1}}],[\"iterfactory\",{\"1\":{\"1897\":1}}],[\"iter=none\",{\"1\":{\"1639\":1}}],[\"iterabledataset\",{\"1\":{\"2189\":1}}],[\"iterableespnetdataset\",{\"0\":{\"2189\":1},\"1\":{\"2099\":1,\"2189\":1,\"2190\":1}}],[\"iterable\",{\"0\":{\"2189\":1,\"2224\":1},\"1\":{\"1218\":4,\"1964\":2,\"1967\":1,\"2099\":1,\"2119\":1,\"2120\":3,\"2123\":2,\"2124\":1,\"2128\":2,\"2129\":1,\"2130\":2,\"2132\":1,\"2135\":3,\"2136\":2,\"2137\":3,\"2178\":4,\"2179\":4,\"2185\":2,\"2189\":1,\"2191\":3,\"2194\":3,\"2195\":4,\"2196\":3,\"2198\":2,\"2199\":1,\"2201\":3,\"2203\":2,\"2224\":2,\"2398\":1,\"2534\":1}}],[\"iterative\",{\"1\":{\"1639\":1,\"1719\":1}}],[\"iterating\",{\"0\":{\"174\":1},\"1\":{\"170\":1}}],[\"iterations=3\",{\"1\":{\"962\":1,\"1758\":1}}],[\"iterations\",{\"0\":{\"69\":1},\"1\":{\"249\":2,\"275\":1,\"321\":2,\"632\":2,\"668\":2,\"670\":1,\"672\":1,\"691\":1,\"697\":1,\"730\":1,\"797\":1,\"1205\":1,\"1524\":1,\"1525\":1,\"1528\":1,\"1611\":1,\"1698\":3,\"1704\":3,\"1707\":3,\"1712\":3,\"1713\":4,\"1714\":1,\"1715\":3,\"1758\":1,\"2099\":1,\"2102\":1,\"2151\":2,\"2199\":1,\"2317\":1,\"2325\":1}}],[\"iteration\",{\"0\":{\"1759\":1},\"1\":{\"17\":1,\"26\":2,\"57\":1,\"69\":1,\"80\":1,\"613\":1,\"627\":1,\"632\":2,\"652\":2,\"1528\":2,\"1759\":3,\"2151\":2}}],[\"iteratoroptions\",{\"0\":{\"2106\":1},\"1\":{\"2099\":5,\"2106\":2}}],[\"iterator|dict\",{\"1\":{\"727\":1,\"728\":1}}],[\"iterators\",{\"0\":{\"988\":1,\"997\":1,\"998\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"2693\":1},\"1\":{\"69\":1,\"727\":1,\"728\":1,\"988\":4,\"997\":1,\"998\":1,\"1371\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1}}],[\"iterator\",{\"1\":{\"57\":1,\"69\":2,\"79\":4,\"80\":1,\"240\":1,\"604\":2,\"607\":3,\"612\":4,\"626\":4,\"627\":2,\"727\":5,\"728\":5,\"975\":5,\"976\":4,\"979\":1,\"988\":2,\"997\":1,\"998\":1,\"1042\":5,\"1043\":4,\"1371\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1895\":1,\"1897\":1,\"1900\":1,\"2099\":5,\"2102\":2,\"2185\":2,\"2193\":1,\"2198\":2,\"2201\":3,\"2203\":2}}],[\"iters\",{\"1\":{\"17\":1,\"69\":2,\"242\":1,\"251\":4,\"253\":2,\"255\":4,\"259\":4,\"265\":2,\"269\":2,\"275\":2,\"519\":2,\"727\":1,\"742\":2,\"1895\":1,\"1897\":1,\"1900\":1,\"2099\":3,\"2102\":3,\"2106\":2,\"2317\":2}}],[\"iter\",{\"0\":{\"1894\":1,\"1895\":1,\"1896\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1},\"1\":{\"17\":1,\"69\":4,\"87\":2,\"148\":1,\"171\":1,\"174\":3,\"202\":1,\"251\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"429\":2,\"604\":2,\"607\":2,\"627\":2,\"667\":1,\"668\":1,\"669\":1,\"671\":1,\"672\":3,\"727\":1,\"728\":2,\"1371\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1639\":1,\"1894\":2,\"1895\":2,\"1896\":1,\"1897\":4,\"1898\":2,\"1899\":1,\"1900\":2,\"1901\":1,\"1964\":2,\"2099\":18,\"2102\":5,\"2199\":1,\"2201\":3,\"2209\":1,\"2325\":2,\"2345\":1}}],[\"ith\",{\"1\":{\"76\":1}}],[\"itself\",{\"1\":{\"120\":1,\"745\":1,\"746\":1,\"2387\":1}}],[\"its\",{\"1\":{\"22\":1,\"24\":1,\"27\":1,\"115\":1,\"150\":1,\"171\":1,\"533\":1,\"619\":1,\"621\":1,\"691\":5,\"693\":1,\"697\":8,\"797\":3,\"799\":1,\"857\":1,\"997\":1,\"998\":1,\"1334\":1,\"1352\":1,\"1522\":1,\"1523\":1,\"1551\":1,\"1553\":1,\"1693\":1,\"1712\":1,\"1715\":1,\"1755\":1,\"2168\":1,\"2170\":1,\"2176\":1,\"2411\":1,\"2468\":1}}],[\"it\",{\"0\":{\"1965\":1},\"1\":{\"1\":1,\"5\":2,\"7\":1,\"15\":1,\"17\":2,\"18\":1,\"21\":1,\"22\":1,\"25\":3,\"38\":1,\"45\":2,\"46\":1,\"49\":2,\"58\":1,\"59\":1,\"60\":4,\"62\":2,\"66\":1,\"69\":1,\"74\":1,\"76\":1,\"79\":4,\"82\":1,\"84\":4,\"85\":3,\"92\":1,\"96\":3,\"98\":1,\"102\":1,\"112\":1,\"113\":3,\"115\":1,\"126\":1,\"128\":1,\"135\":3,\"141\":1,\"142\":1,\"143\":1,\"144\":2,\"196\":1,\"200\":1,\"202\":1,\"203\":1,\"204\":1,\"234\":1,\"240\":1,\"285\":2,\"295\":1,\"595\":2,\"607\":1,\"608\":1,\"610\":2,\"613\":2,\"629\":1,\"638\":1,\"652\":1,\"684\":1,\"685\":1,\"689\":1,\"691\":1,\"692\":4,\"693\":5,\"697\":2,\"700\":1,\"710\":4,\"727\":2,\"728\":2,\"734\":1,\"738\":1,\"745\":5,\"746\":5,\"795\":1,\"797\":2,\"834\":3,\"857\":2,\"875\":2,\"927\":1,\"1007\":1,\"1015\":1,\"1132\":1,\"1138\":1,\"1139\":1,\"1143\":1,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1177\":1,\"1187\":4,\"1202\":4,\"1241\":1,\"1245\":3,\"1252\":1,\"1253\":3,\"1254\":2,\"1279\":2,\"1286\":3,\"1287\":3,\"1337\":1,\"1356\":1,\"1398\":1,\"1406\":1,\"1427\":2,\"1429\":1,\"1432\":2,\"1436\":2,\"1510\":2,\"1511\":2,\"1522\":1,\"1551\":1,\"1553\":1,\"1598\":1,\"1603\":1,\"1618\":1,\"1619\":1,\"1643\":2,\"1644\":2,\"1652\":1,\"1654\":1,\"1660\":3,\"1661\":3,\"1662\":3,\"1664\":1,\"1665\":1,\"1688\":1,\"1695\":1,\"1697\":1,\"1712\":1,\"1719\":2,\"1756\":1,\"1765\":1,\"1778\":1,\"1800\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1844\":1,\"1848\":2,\"1849\":2,\"1850\":1,\"1851\":1,\"1852\":1,\"1857\":1,\"1859\":1,\"1861\":1,\"1862\":1,\"1871\":1,\"1877\":1,\"1878\":1,\"1880\":1,\"1897\":3,\"1928\":1,\"1929\":1,\"1941\":1,\"1946\":1,\"1947\":2,\"1965\":1,\"2019\":1,\"2046\":1,\"2090\":1,\"2121\":1,\"2122\":1,\"2157\":1,\"2212\":2,\"2279\":1,\"2309\":1,\"2355\":1,\"2358\":1,\"2362\":1,\"2372\":6,\"2373\":2,\"2375\":2,\"2378\":1,\"2383\":1,\"2384\":1,\"2387\":2,\"2388\":1,\"2393\":1,\"2394\":2,\"2397\":1,\"2398\":1,\"2400\":2,\"2409\":1,\"2414\":1,\"2418\":2,\"2419\":1,\"2420\":1,\"2421\":1,\"2427\":1,\"2428\":1,\"2429\":4,\"2430\":3,\"2431\":1,\"2432\":1,\"2433\":1,\"2437\":1,\"2440\":1,\"2441\":3,\"2461\":1,\"2462\":1,\"2501\":1,\"2520\":1,\"2524\":1,\"2529\":1,\"2530\":2,\"2533\":1,\"2534\":1,\"2536\":2,\"2544\":1,\"2550\":1,\"2551\":1,\"2552\":2,\"2554\":2,\"2555\":3,\"2558\":3,\"2559\":1,\"2560\":1,\"2563\":1,\"2565\":2,\"2567\":1,\"2568\":3,\"2569\":1,\"2570\":1,\"2571\":1,\"2573\":1,\"2579\":1,\"2584\":7,\"2585\":4,\"2638\":3,\"2639\":1,\"2642\":1,\"2644\":1,\"2645\":1}}],[\"ingresaba\",{\"1\":{\"2457\":1}}],[\"inria\",{\"1\":{\"1712\":1,\"1715\":1}}],[\"ineube\",{\"0\":{\"1719\":2},\"1\":{\"1543\":2,\"1564\":1,\"1655\":3,\"1656\":2,\"1719\":5}}],[\"inline\",{\"1\":{\"1186\":1,\"1210\":1}}],[\"inner\",{\"0\":{\"1686\":1},\"1\":{\"1065\":1,\"1686\":1,\"1735\":1}}],[\"in=400\",{\"1\":{\"1198\":1}}],[\"in=0\",{\"1\":{\"1028\":1}}],[\"in=inf\",{\"1\":{\"1028\":1}}],[\"inherit\",{\"1\":{\"2168\":1,\"2185\":1,\"2201\":1,\"2203\":1}}],[\"inherits\",{\"1\":{\"2168\":1,\"2170\":1}}],[\"inheritance\",{\"1\":{\"1972\":1}}],[\"inherite\",{\"1\":{\"1552\":3}}],[\"inherited\",{\"1\":{\"812\":1,\"1603\":1,\"1622\":1}}],[\"inheriting\",{\"1\":{\"56\":1,\"1209\":1}}],[\"inyaml\",{\"1\":{\"503\":1,\"544\":1}}],[\"involve\",{\"1\":{\"2584\":1}}],[\"involved\",{\"1\":{\"2468\":1}}],[\"invoked\",{\"1\":{\"80\":1,\"85\":1,\"623\":1,\"1972\":1,\"2099\":1,\"2102\":1,\"2372\":1,\"2385\":1,\"2429\":1,\"2554\":1}}],[\"inv\",{\"1\":{\"1245\":2,\"1429\":1}}],[\"invert\",{\"1\":{\"1904\":1,\"1916\":1}}],[\"inverts\",{\"1\":{\"1720\":1}}],[\"inverted\",{\"1\":{\"1427\":1}}],[\"invers=false\",{\"1\":{\"1818\":1}}],[\"inversible\",{\"0\":{\"1909\":1},\"1\":{\"1909\":1}}],[\"inversibleinterface\",{\"0\":{\"1909\":1},\"1\":{\"1773\":3,\"1837\":3,\"1906\":1,\"1909\":1,\"1918\":1,\"2082\":3,\"2240\":3,\"2278\":2}}],[\"inversion\",{\"1\":{\"1719\":1}}],[\"inverse=false\",{\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"inversesqrt\",{\"1\":{\"670\":1}}],[\"inverse\",{\"0\":{\"1722\":1,\"1942\":1},\"1\":{\"242\":1,\"1701\":2,\"1702\":2,\"1722\":2,\"1759\":2,\"1833\":4,\"1838\":4,\"1840\":4,\"1855\":4,\"1864\":3,\"1865\":4,\"1868\":5,\"1906\":1,\"1909\":1,\"1918\":2,\"1942\":1}}],[\"investigated\",{\"1\":{\"1132\":1,\"2400\":1,\"2536\":1}}],[\"invalid\",{\"0\":{\"2312\":1},\"1\":{\"1008\":1,\"1025\":2,\"1225\":1,\"2312\":2}}],[\"invariant\",{\"1\":{\"794\":1,\"1466\":2,\"1552\":1,\"1600\":1,\"1603\":2,\"1622\":1,\"1660\":1,\"1661\":1,\"1662\":1}}],[\"inaguma\",{\"1\":{\"130\":2}}],[\"inoue\",{\"1\":{\"130\":1}}],[\"inout=0\",{\"1\":{\"1028\":1}}],[\"inout\",{\"1\":{\"26\":1,\"251\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"1004\":2}}],[\"inp\",{\"1\":{\"1506\":1,\"1564\":1,\"1656\":1}}],[\"inplanes\",{\"1\":{\"1134\":1,\"1312\":1,\"1313\":1,\"2042\":1,\"2047\":1,\"2059\":1}}],[\"inplace=true\",{\"1\":{\"968\":1}}],[\"inplace=false\",{\"1\":{\"965\":1,\"972\":1,\"973\":1}}],[\"inplace\",{\"1\":{\"943\":1,\"950\":1,\"955\":1,\"956\":1,\"965\":1,\"968\":1,\"972\":1,\"973\":1,\"1142\":1,\"1186\":2,\"1210\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1}}],[\"inproceedings\",{\"1\":{\"130\":7}}],[\"input1\",{\"1\":{\"987\":1}}],[\"input=\",{\"1\":{\"987\":1,\"1605\":1}}],[\"input=true\",{\"1\":{\"987\":1,\"2596\":1}}],[\"inputs\",{\"0\":{\"54\":1,\"1293\":1},\"1\":{\"108\":1,\"118\":1,\"272\":6,\"617\":1,\"681\":1,\"682\":1,\"701\":3,\"710\":1,\"742\":2,\"745\":4,\"746\":4,\"747\":1,\"754\":1,\"758\":1,\"764\":3,\"802\":1,\"803\":1,\"804\":1,\"808\":1,\"821\":1,\"825\":2,\"826\":1,\"838\":2,\"927\":2,\"1011\":1,\"1025\":1,\"1048\":2,\"1057\":1,\"1099\":1,\"1162\":1,\"1187\":1,\"1202\":1,\"1244\":1,\"1251\":1,\"1252\":3,\"1253\":1,\"1254\":2,\"1279\":1,\"1286\":1,\"1287\":1,\"1293\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1512\":1,\"1596\":1,\"1609\":1,\"1638\":1,\"1674\":1,\"1684\":1,\"1778\":1,\"1846\":1,\"1847\":1,\"1851\":1,\"1852\":1,\"1858\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1971\":1,\"2002\":1,\"2078\":1,\"2086\":2,\"2087\":2,\"2090\":3,\"2095\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2260\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2343\":2}}],[\"input+output\",{\"1\":{\"26\":1,\"1004\":1,\"1028\":1}}],[\"input\",{\"0\":{\"55\":1,\"57\":1,\"306\":1,\"309\":1,\"317\":1,\"323\":1,\"329\":1,\"335\":1,\"341\":1,\"345\":1,\"352\":1,\"359\":1,\"365\":1,\"370\":1,\"382\":1,\"386\":1,\"393\":1,\"401\":1,\"410\":1,\"418\":1,\"424\":1,\"431\":1,\"439\":1,\"445\":1,\"451\":1,\"457\":1,\"466\":1,\"472\":1,\"480\":1,\"487\":1,\"861\":1,\"884\":1,\"911\":1,\"1053\":1,\"1092\":1,\"1105\":1},\"1\":{\"21\":6,\"25\":1,\"26\":3,\"46\":1,\"47\":1,\"49\":3,\"52\":1,\"54\":1,\"56\":1,\"57\":1,\"59\":4,\"60\":1,\"73\":1,\"75\":1,\"76\":1,\"84\":2,\"102\":6,\"104\":1,\"108\":7,\"109\":5,\"110\":1,\"115\":8,\"116\":2,\"122\":1,\"150\":2,\"171\":1,\"173\":1,\"174\":1,\"175\":1,\"185\":1,\"193\":1,\"203\":6,\"218\":4,\"225\":3,\"232\":3,\"239\":2,\"265\":1,\"269\":1,\"271\":7,\"272\":1,\"274\":1,\"281\":1,\"286\":1,\"295\":3,\"296\":1,\"299\":2,\"397\":1,\"429\":2,\"461\":2,\"493\":2,\"501\":3,\"564\":5,\"566\":3,\"605\":2,\"619\":1,\"621\":1,\"629\":1,\"642\":1,\"646\":1,\"648\":1,\"658\":1,\"676\":7,\"680\":3,\"683\":3,\"691\":2,\"692\":3,\"693\":2,\"697\":4,\"701\":1,\"703\":2,\"705\":3,\"708\":1,\"710\":5,\"711\":10,\"712\":6,\"713\":2,\"714\":3,\"715\":3,\"716\":3,\"717\":2,\"718\":3,\"719\":3,\"720\":3,\"721\":3,\"722\":3,\"723\":1,\"725\":8,\"726\":10,\"727\":1,\"728\":1,\"731\":3,\"732\":1,\"734\":1,\"735\":1,\"737\":3,\"740\":1,\"741\":1,\"742\":4,\"743\":1,\"745\":16,\"746\":13,\"747\":6,\"748\":1,\"749\":12,\"754\":5,\"755\":1,\"762\":1,\"763\":1,\"767\":1,\"770\":3,\"771\":1,\"772\":2,\"774\":1,\"775\":1,\"776\":1,\"777\":2,\"781\":4,\"784\":2,\"786\":2,\"787\":1,\"797\":3,\"799\":1,\"800\":1,\"801\":2,\"802\":1,\"803\":1,\"804\":1,\"806\":1,\"808\":1,\"809\":1,\"810\":2,\"812\":1,\"813\":2,\"816\":2,\"817\":1,\"818\":2,\"821\":3,\"824\":1,\"826\":5,\"827\":1,\"828\":1,\"830\":1,\"835\":1,\"836\":2,\"838\":2,\"857\":2,\"858\":11,\"861\":4,\"875\":2,\"878\":1,\"884\":3,\"890\":1,\"903\":2,\"909\":4,\"911\":10,\"934\":1,\"944\":1,\"952\":1,\"987\":5,\"1001\":1,\"1003\":4,\"1004\":5,\"1005\":6,\"1011\":1,\"1019\":1,\"1028\":12,\"1031\":1,\"1037\":1,\"1039\":1,\"1040\":1,\"1047\":1,\"1049\":5,\"1050\":5,\"1051\":2,\"1052\":10,\"1053\":7,\"1054\":2,\"1055\":2,\"1056\":5,\"1058\":12,\"1062\":2,\"1065\":3,\"1066\":3,\"1068\":2,\"1069\":2,\"1070\":2,\"1071\":2,\"1072\":2,\"1073\":1,\"1074\":2,\"1075\":4,\"1076\":1,\"1077\":3,\"1079\":2,\"1080\":1,\"1081\":2,\"1092\":6,\"1094\":1,\"1095\":2,\"1097\":3,\"1098\":1,\"1103\":8,\"1104\":2,\"1105\":8,\"1111\":1,\"1113\":1,\"1114\":3,\"1115\":6,\"1116\":5,\"1121\":2,\"1123\":2,\"1125\":2,\"1132\":7,\"1133\":11,\"1140\":5,\"1141\":5,\"1142\":2,\"1144\":1,\"1148\":11,\"1149\":14,\"1150\":14,\"1158\":2,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":2,\"1167\":1,\"1168\":1,\"1169\":5,\"1170\":5,\"1171\":2,\"1172\":3,\"1174\":1,\"1177\":2,\"1178\":5,\"1179\":8,\"1180\":5,\"1181\":5,\"1184\":2,\"1186\":6,\"1187\":7,\"1190\":1,\"1191\":3,\"1192\":3,\"1194\":1,\"1195\":4,\"1196\":1,\"1197\":1,\"1198\":7,\"1199\":1,\"1200\":9,\"1201\":3,\"1202\":7,\"1203\":11,\"1204\":4,\"1206\":1,\"1207\":2,\"1209\":1,\"1210\":4,\"1214\":5,\"1215\":1,\"1216\":1,\"1222\":3,\"1226\":1,\"1228\":2,\"1233\":1,\"1237\":1,\"1239\":2,\"1244\":7,\"1251\":1,\"1252\":4,\"1253\":3,\"1254\":6,\"1255\":9,\"1256\":2,\"1261\":1,\"1263\":1,\"1269\":4,\"1270\":1,\"1271\":1,\"1272\":10,\"1273\":6,\"1276\":1,\"1279\":1,\"1280\":1,\"1282\":3,\"1284\":2,\"1285\":1,\"1286\":7,\"1287\":7,\"1337\":2,\"1345\":1,\"1347\":1,\"1349\":2,\"1350\":2,\"1352\":5,\"1360\":2,\"1362\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1369\":1,\"1371\":1,\"1373\":2,\"1374\":3,\"1375\":4,\"1376\":6,\"1377\":7,\"1381\":1,\"1429\":1,\"1430\":5,\"1431\":1,\"1432\":1,\"1433\":2,\"1435\":1,\"1436\":3,\"1439\":1,\"1440\":1,\"1441\":2,\"1445\":1,\"1446\":1,\"1447\":1,\"1449\":1,\"1454\":5,\"1460\":2,\"1462\":6,\"1463\":5,\"1464\":5,\"1470\":4,\"1471\":4,\"1473\":1,\"1478\":2,\"1482\":1,\"1501\":1,\"1505\":11,\"1510\":4,\"1511\":6,\"1512\":1,\"1515\":6,\"1516\":6,\"1517\":2,\"1522\":7,\"1523\":9,\"1528\":6,\"1529\":6,\"1531\":11,\"1532\":6,\"1534\":6,\"1535\":6,\"1537\":5,\"1539\":6,\"1543\":4,\"1545\":3,\"1551\":2,\"1552\":1,\"1553\":3,\"1554\":1,\"1558\":6,\"1559\":3,\"1560\":8,\"1561\":1,\"1564\":1,\"1576\":2,\"1577\":2,\"1581\":3,\"1586\":1,\"1588\":1,\"1590\":1,\"1594\":1,\"1596\":1,\"1602\":8,\"1605\":1,\"1609\":1,\"1611\":4,\"1616\":4,\"1617\":3,\"1626\":7,\"1629\":1,\"1643\":5,\"1644\":7,\"1645\":7,\"1648\":6,\"1650\":6,\"1652\":6,\"1653\":1,\"1654\":7,\"1656\":1,\"1658\":7,\"1659\":11,\"1660\":7,\"1661\":7,\"1662\":7,\"1669\":7,\"1670\":8,\"1671\":9,\"1674\":1,\"1683\":1,\"1688\":1,\"1693\":2,\"1712\":1,\"1715\":1,\"1718\":3,\"1719\":7,\"1728\":1,\"1741\":1,\"1743\":1,\"1746\":1,\"1752\":1,\"1753\":1,\"1754\":1,\"1755\":2,\"1756\":1,\"1765\":2,\"1766\":2,\"1768\":1,\"1771\":1,\"1772\":2,\"1773\":1,\"1774\":3,\"1776\":4,\"1777\":2,\"1778\":3,\"1781\":5,\"1785\":1,\"1786\":2,\"1787\":2,\"1788\":1,\"1791\":4,\"1797\":1,\"1798\":3,\"1800\":5,\"1803\":3,\"1804\":6,\"1805\":3,\"1807\":2,\"1808\":6,\"1811\":1,\"1816\":1,\"1833\":2,\"1834\":1,\"1835\":1,\"1837\":1,\"1838\":1,\"1840\":1,\"1841\":1,\"1844\":3,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":2,\"1849\":2,\"1850\":2,\"1851\":6,\"1852\":2,\"1855\":1,\"1856\":2,\"1857\":3,\"1858\":2,\"1860\":2,\"1861\":2,\"1862\":3,\"1863\":3,\"1864\":2,\"1865\":2,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":5,\"1872\":3,\"1873\":3,\"1874\":2,\"1876\":1,\"1877\":2,\"1878\":5,\"1879\":1,\"1880\":2,\"1884\":1,\"1885\":1,\"1892\":1,\"1893\":1,\"1902\":2,\"1907\":1,\"1909\":2,\"1910\":2,\"1915\":1,\"1917\":4,\"1918\":4,\"1921\":1,\"1922\":1,\"1923\":2,\"1925\":1,\"1926\":1,\"1928\":1,\"1929\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1946\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":3,\"1958\":1,\"1960\":3,\"1968\":1,\"1970\":3,\"1971\":10,\"1975\":1,\"1980\":3,\"1981\":2,\"1983\":2,\"1986\":2,\"1987\":2,\"1989\":2,\"1991\":2,\"2001\":10,\"2002\":5,\"2004\":8,\"2012\":1,\"2024\":1,\"2026\":4,\"2027\":1,\"2028\":1,\"2029\":10,\"2036\":1,\"2040\":3,\"2044\":4,\"2049\":5,\"2050\":3,\"2052\":3,\"2054\":9,\"2055\":3,\"2057\":1,\"2061\":1,\"2063\":1,\"2064\":3,\"2066\":1,\"2068\":4,\"2070\":3,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2078\":1,\"2079\":3,\"2082\":1,\"2083\":4,\"2084\":4,\"2086\":4,\"2087\":4,\"2090\":7,\"2091\":1,\"2095\":5,\"2096\":3,\"2098\":3,\"2099\":3,\"2100\":3,\"2101\":3,\"2102\":3,\"2103\":3,\"2104\":3,\"2105\":3,\"2107\":3,\"2108\":3,\"2109\":3,\"2110\":3,\"2111\":3,\"2112\":3,\"2113\":3,\"2114\":3,\"2115\":3,\"2116\":3,\"2117\":3,\"2118\":3,\"2142\":3,\"2168\":2,\"2170\":2,\"2183\":2,\"2190\":2,\"2224\":1,\"2233\":2,\"2237\":2,\"2239\":1,\"2240\":1,\"2241\":2,\"2243\":7,\"2244\":7,\"2245\":1,\"2246\":2,\"2248\":2,\"2250\":2,\"2252\":1,\"2253\":1,\"2255\":7,\"2256\":1,\"2257\":1,\"2258\":1,\"2260\":2,\"2261\":1,\"2262\":1,\"2263\":5,\"2264\":7,\"2265\":2,\"2266\":2,\"2271\":1,\"2275\":2,\"2278\":1,\"2279\":7,\"2280\":1,\"2281\":3,\"2290\":1,\"2292\":1,\"2304\":1,\"2322\":1,\"2343\":1,\"2348\":1,\"2359\":1,\"2365\":3,\"2369\":1,\"2372\":1,\"2375\":1,\"2430\":1,\"2439\":2,\"2440\":2,\"2467\":1,\"2487\":1,\"2491\":1,\"2497\":1,\"2501\":4,\"2508\":3,\"2510\":3,\"2515\":3,\"2521\":1,\"2522\":1,\"2555\":1,\"2558\":2,\"2564\":1,\"2580\":1,\"2581\":1,\"2584\":2,\"2606\":1,\"2607\":1,\"2610\":1,\"2614\":1,\"2615\":1,\"2623\":1,\"2624\":1,\"2627\":1,\"2632\":1,\"2633\":1,\"2638\":2,\"2655\":3,\"2660\":3}}],[\"inc\",{\"1\":{\"1818\":1}}],[\"inchannels\",{\"1\":{\"1688\":2,\"1756\":2}}],[\"inclusive\",{\"1\":{\"1142\":1,\"1186\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1}}],[\"included\",{\"1\":{\"48\":1,\"143\":1,\"231\":1,\"1057\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1529\":1,\"1530\":1,\"1534\":1,\"1539\":1,\"1558\":1,\"1611\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1669\":1,\"1671\":1}}],[\"include\",{\"1\":{\"17\":1,\"295\":1,\"1133\":2,\"1210\":1,\"1214\":1,\"1243\":1,\"1269\":1,\"1273\":1,\"1337\":2,\"1622\":1,\"1752\":1,\"1778\":1,\"1805\":1,\"1839\":3,\"1850\":1,\"1852\":1,\"1858\":1,\"1877\":1,\"1932\":1,\"2001\":1,\"2355\":1,\"2387\":1,\"2467\":1}}],[\"includes\",{\"1\":{\"7\":1,\"65\":1,\"295\":1,\"2372\":1}}],[\"including\",{\"1\":{\"16\":2,\"31\":1,\"92\":1,\"135\":1,\"149\":1,\"627\":1,\"794\":1,\"899\":1,\"901\":1,\"909\":1,\"1154\":1,\"1228\":1,\"1345\":1,\"1347\":1,\"1398\":3,\"1400\":1,\"1801\":1,\"2002\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1,\"2372\":1,\"2377\":1,\"2387\":2,\"2397\":1,\"2415\":1,\"2429\":1,\"2436\":1,\"2451\":1,\"2467\":1,\"2499\":1,\"2500\":1,\"2533\":1,\"2554\":1,\"2558\":1,\"2562\":1,\"2568\":1,\"2617\":2,\"2635\":2}}],[\"increasing\",{\"1\":{\"2468\":1}}],[\"increases\",{\"1\":{\"1406\":1}}],[\"increased\",{\"1\":{\"1245\":1}}],[\"increase\",{\"1\":{\"23\":1,\"33\":2,\"95\":2,\"119\":1,\"148\":3,\"150\":1,\"1171\":1,\"1206\":1,\"1241\":1,\"1427\":1,\"1552\":1,\"1953\":1,\"1955\":1,\"2440\":1,\"2468\":1,\"2558\":1}}],[\"incremental\",{\"1\":{\"449\":2,\"692\":1,\"699\":1,\"2460\":1,\"2461\":1}}],[\"incorporate\",{\"1\":{\"2473\":1}}],[\"incorrectly\",{\"1\":{\"1255\":1}}],[\"incorrect\",{\"1\":{\"44\":1}}],[\"inconsistencies\",{\"1\":{\"1717\":1}}],[\"inconvenient\",{\"1\":{\"25\":1}}],[\"incoming\",{\"1\":{\"778\":1,\"1912\":1}}],[\"infs\",{\"1\":{\"1603\":2}}],[\"infer\",{\"1\":{\"711\":2,\"1149\":4,\"1150\":4,\"1808\":1}}],[\"inferno\",{\"1\":{\"648\":1}}],[\"inferences\",{\"1\":{\"1603\":1}}],[\"inference\",{\"0\":{\"23\":1,\"119\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"368\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"436\":1,\"437\":1,\"443\":1,\"449\":1,\"455\":1,\"464\":1,\"470\":1,\"478\":1,\"485\":1,\"2402\":1,\"2450\":1,\"2538\":1,\"2592\":1},\"1\":{\"56\":2,\"59\":3,\"85\":1,\"93\":2,\"98\":2,\"106\":1,\"107\":1,\"108\":3,\"110\":1,\"111\":4,\"113\":1,\"116\":1,\"122\":2,\"148\":1,\"155\":2,\"156\":1,\"158\":1,\"217\":1,\"218\":3,\"224\":1,\"225\":3,\"231\":1,\"232\":3,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":3,\"350\":5,\"357\":5,\"363\":1,\"368\":5,\"384\":2,\"391\":2,\"399\":3,\"408\":2,\"416\":2,\"422\":2,\"437\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":3,\"470\":3,\"478\":2,\"485\":2,\"501\":1,\"700\":1,\"701\":2,\"737\":4,\"754\":4,\"785\":1,\"820\":1,\"821\":6,\"825\":1,\"826\":6,\"1048\":1,\"1066\":1,\"1075\":2,\"1081\":1,\"1138\":1,\"1139\":1,\"1149\":1,\"1150\":1,\"1171\":1,\"1172\":1,\"1205\":3,\"1209\":1,\"1216\":1,\"1218\":2,\"1219\":3,\"1375\":1,\"1552\":2,\"1603\":1,\"1773\":1,\"1778\":3,\"1800\":2,\"1804\":2,\"1805\":3,\"1844\":2,\"1850\":3,\"1851\":2,\"1852\":3,\"1857\":2,\"1862\":2,\"1871\":2,\"1877\":3,\"1878\":2,\"1892\":1,\"1893\":1,\"1970\":1,\"1975\":1,\"1980\":1,\"1984\":1,\"1985\":1,\"2001\":1,\"2002\":4,\"2027\":1,\"2076\":1,\"2077\":1,\"2079\":1,\"2081\":2,\"2082\":1,\"2083\":2,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":4,\"2096\":2,\"2097\":6,\"2098\":2,\"2099\":5,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2235\":1,\"2240\":1,\"2243\":2,\"2244\":1,\"2255\":1,\"2260\":3,\"2263\":4,\"2264\":4,\"2277\":1,\"2278\":1,\"2279\":1,\"2294\":1,\"2358\":1,\"2364\":1,\"2368\":1,\"2371\":1,\"2372\":3,\"2375\":6,\"2377\":2,\"2384\":1,\"2385\":1,\"2394\":2,\"2401\":1,\"2403\":4,\"2423\":1,\"2424\":1,\"2429\":2,\"2433\":1,\"2436\":2,\"2438\":1,\"2440\":3,\"2450\":1,\"2452\":1,\"2455\":1,\"2460\":1,\"2471\":1,\"2472\":1,\"2474\":2,\"2476\":2,\"2478\":2,\"2482\":1,\"2485\":1,\"2486\":1,\"2490\":2,\"2492\":2,\"2494\":1,\"2497\":1,\"2500\":3,\"2507\":1,\"2508\":1,\"2510\":2,\"2513\":1,\"2515\":1,\"2520\":2,\"2530\":2,\"2537\":1,\"2539\":4,\"2543\":1,\"2546\":1,\"2547\":1,\"2554\":2,\"2555\":1,\"2559\":9,\"2562\":2,\"2564\":4,\"2569\":4,\"2579\":1,\"2591\":1,\"2598\":1,\"2600\":7,\"2605\":1,\"2609\":1,\"2612\":1,\"2617\":1,\"2622\":1,\"2626\":1,\"2628\":2,\"2630\":1,\"2635\":1,\"2648\":1,\"2649\":1,\"2654\":1,\"2658\":1}}],[\"infinity\",{\"1\":{\"1132\":1,\"1145\":3}}],[\"infinite\",{\"1\":{\"78\":1,\"607\":1,\"1145\":1}}],[\"infile\",{\"1\":{\"538\":1}}],[\"inf\",{\"1\":{\"363\":2,\"501\":2,\"778\":1,\"1109\":1,\"1218\":7,\"1437\":1,\"1443\":1,\"1466\":2,\"1530\":2,\"1563\":2,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1571\":2,\"1600\":2,\"1622\":3,\"1641\":1,\"1666\":2,\"1668\":2,\"2644\":2}}],[\"info=none\",{\"1\":{\"2350\":1}}],[\"info=false\",{\"1\":{\"1781\":3}}],[\"info\",{\"1\":{\"44\":1,\"87\":3,\"171\":4,\"173\":2,\"174\":3,\"175\":4,\"185\":4,\"193\":3,\"194\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"728\":1,\"909\":1,\"1781\":3,\"1961\":6,\"2197\":2,\"2346\":1}}],[\"information\",{\"0\":{\"151\":1},\"1\":{\"1\":1,\"14\":1,\"17\":2,\"33\":1,\"73\":4,\"74\":2,\"75\":3,\"76\":1,\"77\":1,\"95\":1,\"113\":1,\"115\":1,\"124\":1,\"148\":1,\"237\":2,\"239\":1,\"294\":1,\"628\":1,\"641\":2,\"745\":1,\"760\":1,\"778\":1,\"831\":1,\"1144\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1228\":2,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1419\":1,\"1474\":1,\"1476\":1,\"1598\":1,\"1670\":1,\"1671\":1,\"1781\":3,\"1897\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1961\":1,\"1964\":1,\"2011\":1,\"2084\":1,\"2086\":1,\"2087\":1,\"2089\":1,\"2095\":1,\"2197\":1,\"2343\":1,\"2373\":3,\"2375\":1,\"2385\":4,\"2398\":1,\"2430\":3,\"2432\":1,\"2473\":2,\"2534\":1,\"2555\":3,\"2558\":2,\"2635\":1}}],[\"intra\",{\"1\":{\"1537\":1,\"1538\":1,\"1539\":1,\"1581\":1,\"1661\":1,\"1671\":1}}],[\"intrandomgeneratedataset\",{\"0\":{\"1386\":1},\"1\":{\"1386\":1,\"1387\":1}}],[\"introduce\",{\"1\":{\"2583\":1}}],[\"introduced\",{\"1\":{\"117\":1,\"121\":1,\"701\":1,\"764\":1,\"786\":1,\"1867\":1,\"2079\":2,\"2236\":1,\"2244\":1,\"2252\":1,\"2257\":1,\"2261\":1,\"2262\":1}}],[\"introduction\",{\"1\":{\"112\":1}}],[\"ints\",{\"1\":{\"1133\":1,\"1273\":2,\"1337\":2,\"1349\":2,\"1350\":2,\"2474\":2,\"2649\":2}}],[\"int=\",{\"1\":{\"1115\":21,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2180\":1}}],[\"int16\",{\"1\":{\"989\":2,\"1388\":1,\"1401\":1,\"1413\":1,\"2592\":1,\"2596\":1}}],[\"intput\",{\"1\":{\"827\":1}}],[\"int64\",{\"1\":{\"695\":2,\"696\":1,\"706\":2,\"731\":1,\"734\":2,\"773\":2,\"796\":1,\"815\":1,\"817\":1,\"828\":2,\"1133\":4,\"1190\":2,\"1204\":1,\"1214\":3,\"1221\":1,\"1244\":3,\"1273\":3,\"1386\":1,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"2001\":2,\"2474\":1,\"2649\":1}}],[\"int|none\",{\"1\":{\"680\":2,\"683\":2,\"703\":1}}],[\"int32\",{\"1\":{\"619\":1,\"1427\":1,\"2196\":2}}],[\"intelligent\",{\"1\":{\"2467\":1}}],[\"integers\",{\"1\":{\"2123\":1,\"2128\":1,\"2135\":1}}],[\"integer\",{\"1\":{\"875\":2,\"1063\":1,\"1688\":1,\"1693\":1,\"1735\":1,\"1755\":1,\"1756\":1,\"1927\":2,\"2154\":2,\"2357\":5,\"2371\":1,\"2578\":5,\"2612\":1,\"2630\":1}}],[\"integrating\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1,\"2479\":1}}],[\"integration\",{\"0\":{\"70\":1},\"1\":{\"31\":1,\"130\":1,\"754\":1,\"826\":1,\"1778\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"2001\":2,\"2002\":2,\"2003\":1,\"2004\":2,\"2086\":4,\"2087\":4,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":2,\"2263\":2,\"2264\":2,\"2270\":1,\"2272\":1,\"2279\":2}}],[\"integrated\",{\"1\":{\"2046\":1,\"2090\":1}}],[\"integrate\",{\"1\":{\"754\":1,\"826\":1,\"1851\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2086\":2,\"2087\":2,\"2090\":1,\"2095\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2279\":1}}],[\"integratable\",{\"1\":{\"130\":1}}],[\"intent\",{\"1\":{\"2467\":3,\"2471\":1,\"2472\":3,\"2473\":1,\"2474\":3,\"2476\":3,\"2648\":3,\"2649\":3}}],[\"intention\",{\"1\":{\"59\":1}}],[\"intended\",{\"1\":{\"1187\":2,\"1202\":2,\"1243\":1}}],[\"intend\",{\"1\":{\"74\":1,\"2168\":1}}],[\"interest\",{\"1\":{\"2468\":1}}],[\"interests\",{\"1\":{\"2452\":1}}],[\"interesting\",{\"1\":{\"186\":1}}],[\"intertopk\",{\"0\":{\"2040\":2},\"1\":{\"2040\":3}}],[\"inter\",{\"1\":{\"1515\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1537\":1,\"1538\":1,\"1539\":2,\"1581\":1,\"1626\":1,\"1661\":1,\"1671\":2,\"2040\":1,\"2304\":1}}],[\"intersection\",{\"1\":{\"2274\":1}}],[\"intersecting\",{\"1\":{\"1427\":1}}],[\"intersect\",{\"1\":{\"1427\":1}}],[\"interspeech\",{\"1\":{\"130\":8,\"705\":1,\"1462\":3,\"1463\":3,\"1528\":2,\"1529\":2,\"1568\":2,\"1581\":1,\"2032\":1,\"2049\":1,\"2054\":1,\"2055\":1}}],[\"interator\",{\"1\":{\"1371\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1}}],[\"interctc\",{\"1\":{\"1148\":2,\"1169\":3,\"1171\":1,\"1172\":1,\"1203\":2,\"1206\":1,\"1272\":2,\"1975\":1,\"2027\":1}}],[\"interchanged\",{\"1\":{\"38\":1}}],[\"interp2\",{\"1\":{\"1025\":1}}],[\"interpolate\",{\"0\":{\"1025\":1,\"1026\":1},\"1\":{\"1025\":2,\"1026\":1,\"1257\":1,\"1834\":1,\"1876\":2,\"1919\":2,\"1948\":2}}],[\"interpolated\",{\"1\":{\"1001\":1,\"1869\":1}}],[\"interpolation\",{\"0\":{\"1001\":1,\"1035\":1},\"1\":{\"1001\":8,\"1011\":1,\"1025\":1,\"1031\":2,\"1035\":1,\"1036\":1,\"1804\":1,\"1811\":2,\"1834\":1,\"1869\":1,\"1876\":1}}],[\"interpreted\",{\"1\":{\"697\":2,\"797\":1,\"1241\":1}}],[\"interpreter\",{\"1\":{\"85\":1,\"135\":1}}],[\"interm\",{\"1\":{\"1269\":4}}],[\"intermidiate\",{\"1\":{\"169\":1,\"181\":1}}],[\"intermediates\",{\"1\":{\"691\":1,\"693\":1,\"697\":1,\"797\":1,\"1133\":1,\"2452\":1}}],[\"intermediate\",{\"1\":{\"22\":1,\"115\":3,\"825\":1,\"950\":1,\"968\":1,\"1148\":1,\"1169\":1,\"1203\":1,\"1272\":1,\"1377\":1,\"1522\":1,\"1523\":1,\"1545\":1}}],[\"international\",{\"1\":{\"130\":2}}],[\"internal\",{\"1\":{\"114\":1,\"116\":1,\"124\":1,\"612\":1,\"613\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"708\":1,\"729\":1,\"730\":1,\"752\":1,\"756\":1,\"758\":1,\"760\":1,\"778\":1,\"782\":1,\"793\":1,\"819\":1,\"830\":1,\"831\":1,\"835\":1,\"1046\":1,\"1061\":1,\"1066\":1,\"1067\":1,\"1082\":1,\"1084\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1140\":1,\"1141\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1160\":2,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":2,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1174\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1188\":1,\"1190\":1,\"1196\":1,\"1197\":1,\"1200\":1,\"1203\":1,\"1204\":1,\"1206\":1,\"1207\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1244\":1,\"1247\":1,\"1249\":1,\"1252\":1,\"1253\":3,\"1254\":2,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1279\":2,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1378\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1524\":1,\"1525\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1552\":1,\"1554\":1,\"1555\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":1,\"1575\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1601\":1,\"1602\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1613\":1,\"1616\":1,\"1617\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":2,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1670\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1718\":1,\"1719\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1785\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1799\":1,\"1806\":1,\"1828\":1,\"1840\":1,\"1842\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1902\":1,\"1906\":1,\"1907\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1970\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2003\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2084\":1,\"2089\":1,\"2149\":1,\"2168\":1,\"2170\":1,\"2233\":1,\"2235\":1,\"2237\":1,\"2239\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2266\":1,\"2275\":1,\"2277\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1}}],[\"interval=none\",{\"1\":{\"835\":1}}],[\"interval\",{\"0\":{\"68\":1},\"1\":{\"68\":1,\"251\":4,\"253\":2,\"255\":4,\"259\":4,\"265\":6,\"269\":6,\"429\":2,\"595\":1,\"623\":1,\"632\":1,\"835\":2,\"1758\":1,\"1759\":1,\"1895\":1,\"1900\":1,\"1964\":1,\"2151\":1,\"2186\":3,\"2202\":6,\"2204\":3,\"2212\":1}}],[\"interface=<class\",{\"1\":{\"646\":1}}],[\"interfaces\",{\"1\":{\"45\":2,\"2481\":1,\"2618\":1}}],[\"interface\",{\"0\":{\"676\":1,\"695\":1,\"696\":1,\"709\":1,\"751\":1,\"767\":1,\"781\":1,\"783\":1,\"795\":1,\"812\":1,\"814\":1,\"820\":1,\"824\":1,\"872\":1,\"873\":1,\"874\":1,\"946\":1,\"957\":1,\"1909\":1},\"1\":{\"44\":3,\"45\":1,\"56\":1,\"57\":2,\"62\":1,\"143\":1,\"166\":1,\"177\":1,\"646\":3,\"672\":1,\"676\":3,\"695\":2,\"696\":2,\"698\":1,\"699\":1,\"706\":1,\"709\":2,\"745\":1,\"746\":1,\"751\":1,\"767\":3,\"781\":3,\"783\":1,\"795\":2,\"812\":3,\"814\":2,\"820\":3,\"824\":2,\"829\":1,\"872\":1,\"873\":2,\"874\":2,\"946\":1,\"957\":2,\"1217\":1,\"1245\":1,\"1253\":2,\"1909\":1}}],[\"int\",{\"0\":{\"2226\":1,\"2327\":1,\"2338\":1},\"1\":{\"21\":19,\"22\":1,\"23\":7,\"57\":4,\"58\":1,\"60\":3,\"80\":1,\"113\":1,\"115\":23,\"116\":15,\"117\":1,\"119\":4,\"121\":3,\"122\":2,\"174\":1,\"217\":1,\"224\":1,\"231\":1,\"604\":2,\"605\":1,\"606\":1,\"607\":1,\"614\":3,\"617\":3,\"618\":1,\"619\":3,\"623\":1,\"624\":1,\"626\":1,\"627\":1,\"632\":2,\"635\":1,\"641\":2,\"646\":2,\"648\":2,\"667\":1,\"668\":2,\"670\":1,\"671\":1,\"672\":2,\"676\":5,\"677\":3,\"678\":3,\"679\":5,\"680\":1,\"681\":8,\"682\":9,\"683\":3,\"684\":6,\"685\":5,\"686\":5,\"687\":5,\"688\":7,\"689\":7,\"691\":18,\"693\":16,\"697\":25,\"698\":4,\"699\":6,\"700\":8,\"701\":9,\"703\":1,\"705\":3,\"706\":4,\"710\":2,\"711\":4,\"712\":6,\"713\":3,\"714\":2,\"715\":2,\"716\":2,\"717\":2,\"718\":2,\"719\":2,\"720\":2,\"721\":2,\"722\":2,\"723\":2,\"724\":1,\"725\":6,\"726\":3,\"727\":1,\"728\":1,\"729\":1,\"730\":7,\"731\":5,\"732\":3,\"734\":1,\"737\":4,\"740\":3,\"741\":3,\"742\":4,\"743\":4,\"747\":7,\"748\":3,\"749\":7,\"751\":1,\"752\":4,\"754\":14,\"756\":13,\"758\":8,\"759\":2,\"764\":1,\"766\":5,\"767\":2,\"768\":1,\"770\":3,\"771\":2,\"772\":2,\"773\":2,\"775\":3,\"776\":3,\"777\":2,\"778\":5,\"781\":5,\"784\":3,\"785\":2,\"786\":3,\"793\":1,\"794\":2,\"797\":13,\"800\":2,\"801\":5,\"802\":5,\"804\":4,\"805\":4,\"806\":7,\"807\":3,\"808\":4,\"809\":2,\"810\":2,\"813\":2,\"815\":4,\"817\":1,\"818\":3,\"821\":30,\"823\":2,\"824\":3,\"825\":7,\"826\":22,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"833\":1,\"835\":10,\"852\":3,\"857\":9,\"858\":3,\"861\":1,\"865\":2,\"867\":1,\"869\":1,\"870\":3,\"875\":2,\"877\":1,\"878\":1,\"884\":2,\"890\":1,\"892\":7,\"895\":2,\"899\":1,\"901\":1,\"905\":3,\"908\":2,\"909\":2,\"911\":1,\"914\":1,\"918\":2,\"920\":1,\"921\":1,\"924\":1,\"925\":1,\"933\":3,\"934\":1,\"936\":1,\"943\":1,\"944\":2,\"950\":5,\"955\":1,\"956\":1,\"965\":1,\"968\":5,\"972\":1,\"973\":1,\"996\":1,\"997\":5,\"998\":1,\"1003\":4,\"1004\":6,\"1005\":6,\"1011\":1,\"1015\":1,\"1019\":2,\"1028\":10,\"1037\":5,\"1039\":2,\"1040\":1,\"1046\":3,\"1047\":1,\"1048\":7,\"1049\":4,\"1050\":3,\"1051\":2,\"1052\":8,\"1053\":4,\"1054\":2,\"1055\":2,\"1056\":4,\"1057\":4,\"1058\":3,\"1059\":2,\"1060\":1,\"1062\":7,\"1063\":2,\"1064\":4,\"1065\":9,\"1066\":15,\"1067\":1,\"1068\":4,\"1069\":4,\"1070\":2,\"1071\":3,\"1072\":1,\"1073\":8,\"1074\":6,\"1075\":11,\"1076\":6,\"1077\":4,\"1078\":2,\"1079\":5,\"1080\":1,\"1081\":9,\"1083\":6,\"1085\":2,\"1087\":1,\"1092\":2,\"1093\":3,\"1094\":1,\"1095\":2,\"1096\":3,\"1097\":2,\"1099\":2,\"1100\":1,\"1101\":3,\"1103\":1,\"1104\":2,\"1105\":1,\"1106\":1,\"1108\":1,\"1114\":1,\"1115\":21,\"1132\":3,\"1133\":2,\"1138\":10,\"1139\":9,\"1140\":7,\"1141\":2,\"1142\":14,\"1143\":3,\"1145\":2,\"1148\":18,\"1149\":11,\"1150\":10,\"1151\":3,\"1153\":2,\"1154\":4,\"1155\":7,\"1158\":7,\"1167\":8,\"1168\":8,\"1169\":12,\"1170\":4,\"1171\":5,\"1172\":3,\"1173\":1,\"1176\":1,\"1178\":3,\"1179\":7,\"1180\":7,\"1181\":7,\"1186\":7,\"1190\":2,\"1191\":2,\"1192\":3,\"1193\":2,\"1195\":4,\"1196\":8,\"1197\":8,\"1198\":7,\"1200\":3,\"1201\":2,\"1203\":17,\"1204\":5,\"1205\":2,\"1206\":4,\"1207\":6,\"1209\":2,\"1210\":8,\"1211\":1,\"1214\":2,\"1215\":1,\"1216\":2,\"1218\":1,\"1219\":8,\"1220\":5,\"1222\":5,\"1224\":1,\"1227\":4,\"1239\":2,\"1244\":3,\"1255\":4,\"1257\":7,\"1269\":15,\"1270\":6,\"1271\":5,\"1272\":8,\"1273\":5,\"1282\":5,\"1284\":1,\"1289\":14,\"1298\":6,\"1299\":6,\"1301\":6,\"1302\":7,\"1303\":7,\"1304\":7,\"1306\":1,\"1307\":2,\"1324\":3,\"1334\":7,\"1336\":1,\"1337\":2,\"1344\":2,\"1346\":2,\"1348\":1,\"1349\":2,\"1350\":2,\"1367\":1,\"1373\":2,\"1374\":2,\"1375\":5,\"1376\":3,\"1377\":10,\"1384\":1,\"1386\":3,\"1392\":1,\"1398\":1,\"1417\":1,\"1426\":3,\"1427\":1,\"1428\":2,\"1429\":2,\"1430\":5,\"1431\":1,\"1435\":1,\"1436\":1,\"1454\":2,\"1462\":5,\"1463\":12,\"1464\":1,\"1465\":2,\"1466\":1,\"1470\":3,\"1471\":1,\"1505\":16,\"1510\":5,\"1511\":6,\"1515\":8,\"1516\":11,\"1517\":3,\"1522\":6,\"1523\":13,\"1524\":10,\"1525\":8,\"1528\":10,\"1529\":8,\"1531\":10,\"1532\":4,\"1534\":7,\"1535\":4,\"1537\":5,\"1539\":8,\"1545\":4,\"1551\":1,\"1552\":4,\"1553\":3,\"1554\":1,\"1555\":2,\"1558\":10,\"1560\":3,\"1572\":3,\"1576\":5,\"1577\":6,\"1581\":3,\"1598\":1,\"1602\":4,\"1611\":13,\"1616\":2,\"1617\":3,\"1626\":6,\"1639\":1,\"1643\":6,\"1644\":7,\"1645\":12,\"1646\":2,\"1648\":2,\"1650\":2,\"1652\":3,\"1654\":7,\"1658\":10,\"1659\":17,\"1665\":4,\"1669\":12,\"1670\":13,\"1671\":27,\"1698\":3,\"1701\":2,\"1702\":2,\"1704\":3,\"1705\":1,\"1707\":3,\"1708\":1,\"1712\":3,\"1713\":3,\"1714\":2,\"1715\":3,\"1719\":2,\"1736\":2,\"1739\":2,\"1741\":4,\"1759\":2,\"1761\":1,\"1763\":2,\"1765\":20,\"1766\":4,\"1768\":9,\"1771\":16,\"1772\":4,\"1773\":1,\"1776\":12,\"1777\":12,\"1778\":11,\"1785\":6,\"1786\":1,\"1787\":16,\"1788\":16,\"1791\":3,\"1798\":14,\"1800\":8,\"1801\":4,\"1803\":20,\"1804\":76,\"1805\":9,\"1808\":9,\"1810\":6,\"1823\":1,\"1829\":2,\"1830\":8,\"1831\":10,\"1832\":8,\"1833\":10,\"1834\":7,\"1835\":6,\"1837\":1,\"1838\":2,\"1844\":18,\"1845\":2,\"1846\":2,\"1847\":3,\"1848\":13,\"1849\":14,\"1850\":7,\"1851\":79,\"1852\":9,\"1856\":12,\"1857\":14,\"1858\":14,\"1859\":14,\"1860\":4,\"1861\":12,\"1862\":20,\"1863\":16,\"1864\":15,\"1865\":14,\"1866\":6,\"1867\":6,\"1868\":10,\"1869\":4,\"1870\":6,\"1871\":16,\"1872\":8,\"1873\":9,\"1874\":14,\"1875\":2,\"1876\":4,\"1877\":9,\"1878\":62,\"1880\":23,\"1883\":2,\"1884\":2,\"1885\":2,\"1892\":2,\"1893\":2,\"1894\":1,\"1895\":4,\"1896\":8,\"1897\":1,\"1898\":2,\"1900\":4,\"1904\":1,\"1905\":2,\"1907\":2,\"1910\":2,\"1912\":5,\"1914\":4,\"1915\":2,\"1916\":1,\"1917\":9,\"1918\":3,\"1919\":1,\"1921\":4,\"1922\":4,\"1923\":3,\"1925\":2,\"1926\":2,\"1927\":1,\"1928\":2,\"1929\":1,\"1931\":2,\"1932\":4,\"1933\":1,\"1934\":2,\"1935\":2,\"1936\":4,\"1938\":4,\"1939\":4,\"1940\":3,\"1941\":6,\"1943\":2,\"1946\":2,\"1947\":2,\"1948\":1,\"1953\":4,\"1954\":2,\"1955\":4,\"1956\":2,\"1957\":1,\"1958\":5,\"1960\":6,\"1962\":2,\"1964\":2,\"1970\":3,\"1971\":2,\"1975\":2,\"1984\":4,\"1985\":2,\"1987\":3,\"1989\":7,\"1991\":3,\"1993\":2,\"1996\":2,\"2001\":11,\"2002\":39,\"2003\":17,\"2004\":11,\"2006\":3,\"2007\":3,\"2008\":2,\"2009\":2,\"2010\":1,\"2011\":1,\"2012\":4,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":6,\"2019\":3,\"2020\":2,\"2021\":2,\"2022\":1,\"2023\":4,\"2024\":2,\"2026\":8,\"2027\":2,\"2029\":7,\"2032\":1,\"2034\":1,\"2044\":1,\"2049\":4,\"2050\":1,\"2052\":1,\"2054\":17,\"2055\":4,\"2059\":6,\"2064\":5,\"2068\":1,\"2070\":3,\"2076\":6,\"2078\":10,\"2079\":2,\"2081\":2,\"2083\":7,\"2084\":4,\"2086\":36,\"2087\":43,\"2089\":4,\"2090\":41,\"2095\":66,\"2099\":3,\"2106\":10,\"2123\":1,\"2128\":1,\"2135\":1,\"2151\":1,\"2153\":1,\"2154\":1,\"2164\":1,\"2170\":1,\"2175\":2,\"2177\":3,\"2178\":2,\"2179\":2,\"2180\":9,\"2181\":1,\"2182\":3,\"2183\":2,\"2184\":5,\"2186\":10,\"2188\":4,\"2189\":1,\"2190\":2,\"2191\":2,\"2193\":15,\"2194\":1,\"2195\":2,\"2197\":10,\"2198\":1,\"2199\":9,\"2200\":6,\"2201\":1,\"2202\":20,\"2204\":12,\"2205\":6,\"2208\":3,\"2210\":2,\"2211\":2,\"2222\":1,\"2226\":2,\"2230\":2,\"2236\":6,\"2241\":5,\"2243\":53,\"2244\":67,\"2246\":3,\"2248\":7,\"2250\":3,\"2255\":69,\"2257\":13,\"2258\":6,\"2259\":4,\"2260\":14,\"2261\":20,\"2262\":8,\"2263\":64,\"2264\":65,\"2265\":8,\"2266\":7,\"2267\":2,\"2273\":2,\"2274\":2,\"2279\":52,\"2280\":1,\"2290\":5,\"2292\":6,\"2294\":3,\"2303\":1,\"2304\":4,\"2305\":1,\"2317\":8,\"2321\":1,\"2325\":4,\"2327\":3,\"2330\":5,\"2338\":2,\"2345\":1,\"2349\":2,\"2359\":1,\"2456\":1,\"2460\":1,\"2514\":2,\"2521\":1,\"2580\":1,\"2592\":3,\"2596\":4,\"2659\":2}}],[\"intornone\",{\"1\":{\"1927\":1}}],[\"intorfloat\",{\"1\":{\"903\":1}}],[\"intordict\",{\"1\":{\"728\":1}}],[\"into\",{\"0\":{\"1720\":1,\"1721\":1},\"1\":{\"13\":2,\"52\":1,\"80\":1,\"91\":1,\"99\":1,\"159\":1,\"201\":1,\"238\":3,\"242\":1,\"295\":1,\"299\":1,\"560\":1,\"568\":1,\"652\":1,\"684\":1,\"685\":1,\"701\":1,\"821\":1,\"826\":1,\"926\":1,\"1142\":1,\"1145\":1,\"1171\":1,\"1186\":1,\"1206\":1,\"1210\":2,\"1254\":1,\"1327\":2,\"1398\":1,\"1436\":1,\"1511\":1,\"1552\":1,\"1560\":2,\"1644\":1,\"1670\":1,\"1671\":1,\"1720\":2,\"1721\":3,\"1741\":1,\"1785\":1,\"1881\":1,\"1953\":1,\"1955\":1,\"2001\":1,\"2002\":1,\"2004\":1,\"2046\":3,\"2081\":1,\"2083\":1,\"2156\":1,\"2263\":1,\"2264\":1,\"2275\":1,\"2325\":1,\"2384\":1,\"2385\":1,\"2387\":1,\"2395\":1,\"2398\":1,\"2399\":1,\"2400\":2,\"2401\":1,\"2410\":1,\"2411\":1,\"2412\":2,\"2429\":1,\"2430\":1,\"2431\":1,\"2451\":1,\"2531\":1,\"2534\":1,\"2535\":1,\"2536\":2,\"2537\":1,\"2554\":1,\"2555\":1,\"2573\":1,\"2597\":1,\"2599\":1,\"2600\":1}}],[\"indim\",{\"1\":{\"1484\":1,\"1601\":1,\"1725\":1}}],[\"individual\",{\"1\":{\"1068\":1}}],[\"individually\",{\"1\":{\"21\":1}}],[\"indices\",{\"1\":{\"745\":2,\"746\":2,\"899\":2,\"901\":2,\"917\":1,\"1048\":1,\"1138\":4,\"1326\":1,\"1337\":1,\"1741\":1}}],[\"indicator\",{\"1\":{\"899\":1,\"900\":1,\"901\":1,\"902\":1}}],[\"indicating\",{\"1\":{\"737\":2,\"1327\":1,\"1417\":1,\"1422\":1,\"2265\":1}}],[\"indicates\",{\"1\":{\"57\":1,\"58\":1,\"69\":1,\"75\":1,\"143\":1,\"727\":1,\"1427\":1,\"2170\":2}}],[\"indicate\",{\"1\":{\"37\":2,\"870\":1,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1877\":1,\"2099\":1,\"2102\":1,\"2414\":1}}],[\"indent=none\",{\"1\":{\"2315\":1}}],[\"independent\",{\"1\":{\"1245\":3,\"1351\":1,\"1566\":1,\"1603\":2,\"1622\":2,\"1662\":2,\"1671\":1}}],[\"independently\",{\"1\":{\"22\":1,\"112\":1,\"150\":1}}],[\"indexer\",{\"1\":{\"1334\":4}}],[\"indexes\",{\"1\":{\"745\":4,\"746\":4}}],[\"indexing\",{\"1\":{\"1025\":2,\"1154\":1,\"1155\":1,\"1186\":1}}],[\"indexing=\",{\"1\":{\"1025\":1}}],[\"index=none\",{\"1\":{\"710\":1}}],[\"index\",{\"0\":{\"1154\":1,\"1326\":1},\"1\":{\"12\":1,\"239\":3,\"681\":1,\"682\":1,\"698\":2,\"699\":2,\"705\":2,\"706\":1,\"711\":2,\"758\":1,\"759\":2,\"815\":2,\"852\":3,\"905\":3,\"920\":1,\"924\":1,\"1011\":1,\"1062\":2,\"1074\":1,\"1081\":2,\"1136\":1,\"1138\":2,\"1142\":1,\"1144\":1,\"1154\":5,\"1155\":3,\"1186\":1,\"1210\":1,\"1254\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1326\":1,\"1337\":2,\"1349\":1,\"1350\":1,\"1430\":1,\"1470\":1,\"1471\":1,\"1523\":1,\"1524\":1,\"1538\":2,\"1551\":1,\"1553\":1,\"1659\":1,\"1665\":1,\"1670\":2,\"1671\":1,\"1773\":4,\"1778\":4,\"1798\":1,\"1804\":1,\"1805\":4,\"1837\":4,\"1850\":5,\"1851\":7,\"1852\":3,\"1874\":1,\"1877\":7,\"1878\":7,\"1884\":1,\"1885\":1,\"2018\":1,\"2082\":3,\"2170\":1,\"2240\":3,\"2278\":3,\"2294\":1,\"2304\":1,\"2359\":1,\"2456\":1,\"2460\":1,\"2482\":1,\"2521\":1,\"2580\":1}}],[\"init=false\",{\"1\":{\"1199\":1}}],[\"init=none\",{\"1\":{\"834\":1,\"1508\":1,\"1716\":1}}],[\"inits\",{\"1\":{\"988\":1}}],[\"initilialize\",{\"1\":{\"1863\":1}}],[\"initilize\",{\"1\":{\"737\":1,\"738\":1,\"774\":1,\"820\":1,\"893\":1,\"974\":1,\"975\":1,\"976\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1781\":1,\"1846\":1,\"1847\":1,\"1849\":1,\"1856\":1,\"1858\":1,\"1860\":1,\"1864\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"2257\":1,\"2261\":1,\"2262\":1,\"2265\":1}}],[\"initially\",{\"1\":{\"2354\":1}}],[\"initialzie\",{\"1\":{\"1865\":1}}],[\"initialw=w\",{\"1\":{\"744\":1}}],[\"initialw=none\",{\"1\":{\"717\":1,\"731\":1,\"732\":1,\"743\":1,\"747\":1,\"748\":1,\"777\":1,\"784\":1,\"801\":1}}],[\"initialw\",{\"1\":{\"731\":1,\"743\":1,\"747\":1,\"784\":1,\"801\":1}}],[\"initials\",{\"1\":{\"231\":2}}],[\"initial\",{\"0\":{\"892\":1},\"1\":{\"85\":1,\"104\":1,\"240\":1,\"691\":2,\"696\":2,\"697\":2,\"704\":2,\"706\":4,\"717\":1,\"725\":1,\"731\":2,\"732\":1,\"747\":2,\"748\":1,\"754\":1,\"777\":1,\"784\":2,\"797\":2,\"801\":2,\"806\":1,\"815\":2,\"817\":2,\"824\":1,\"826\":1,\"829\":4,\"834\":2,\"835\":1,\"838\":2,\"891\":1,\"892\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1073\":1,\"1083\":1,\"1149\":1,\"1150\":1,\"1162\":1,\"1221\":2,\"1247\":2,\"1248\":2,\"1252\":1,\"1253\":1,\"1254\":1,\"1270\":1,\"1279\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1804\":1,\"1844\":1,\"1848\":2,\"1849\":1,\"1851\":3,\"1856\":1,\"1857\":2,\"1858\":1,\"1878\":1,\"2090\":2,\"2216\":1,\"2243\":2,\"2244\":2,\"2255\":2,\"2264\":3,\"2279\":2,\"2473\":1}}],[\"initializated\",{\"1\":{\"825\":1}}],[\"initialization\",{\"0\":{\"434\":1,\"897\":1,\"919\":1,\"931\":1},\"1\":{\"24\":2,\"28\":2,\"30\":1,\"38\":4,\"40\":1,\"82\":1,\"643\":1,\"897\":1,\"919\":1,\"931\":1,\"1065\":1,\"1069\":1,\"1070\":1,\"1078\":1,\"1079\":1,\"1153\":1,\"1182\":1,\"1198\":3,\"1199\":1,\"1245\":1,\"1351\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1732\":1,\"1765\":1,\"1800\":1,\"1803\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1844\":1,\"1857\":1,\"1858\":1,\"1904\":1,\"1916\":1,\"2156\":3,\"2258\":1,\"2259\":1,\"2260\":1,\"2584\":1}}],[\"initializer=none\",{\"1\":{\"1162\":1,\"1177\":1,\"1199\":1,\"1279\":1}}],[\"initializer\",{\"0\":{\"894\":2,\"1322\":1},\"1\":{\"731\":4,\"743\":2,\"747\":2,\"766\":1,\"784\":2,\"801\":2,\"806\":1,\"894\":2,\"1322\":1}}],[\"initializes\",{\"1\":{\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"708\":1,\"729\":1,\"730\":1,\"752\":1,\"756\":1,\"758\":1,\"760\":1,\"778\":1,\"782\":1,\"793\":1,\"819\":1,\"830\":1,\"831\":1,\"835\":1,\"1046\":1,\"1061\":1,\"1067\":1,\"1082\":1,\"1084\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1140\":1,\"1141\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1174\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1188\":1,\"1190\":1,\"1196\":1,\"1197\":1,\"1200\":1,\"1203\":1,\"1204\":1,\"1206\":1,\"1207\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1244\":1,\"1247\":1,\"1249\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1376\":1,\"1378\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":1,\"1482\":1,\"1484\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1517\":1,\"1518\":1,\"1520\":1,\"1524\":1,\"1525\":1,\"1530\":1,\"1531\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1552\":1,\"1554\":1,\"1555\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":1,\"1575\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":1,\"1601\":1,\"1602\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1611\":1,\"1613\":1,\"1616\":1,\"1617\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1670\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1718\":1,\"1719\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1785\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1799\":1,\"1806\":1,\"1828\":1,\"1840\":1,\"1842\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1890\":1,\"1892\":1,\"1893\":1,\"1902\":1,\"1906\":1,\"1907\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1970\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2003\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2029\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2084\":1,\"2089\":1,\"2149\":1,\"2168\":1,\"2170\":1,\"2233\":1,\"2235\":1,\"2237\":1,\"2239\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2266\":1,\"2275\":1,\"2277\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1}}],[\"initialized\",{\"1\":{\"646\":1,\"754\":1,\"2156\":1,\"2585\":1}}],[\"initialize\",{\"0\":{\"893\":1,\"2156\":2},\"1\":{\"38\":1,\"593\":1,\"594\":1,\"625\":1,\"629\":1,\"659\":1,\"660\":1,\"661\":2,\"662\":1,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":2,\"676\":1,\"691\":1,\"692\":1,\"693\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"706\":1,\"710\":1,\"713\":1,\"717\":1,\"724\":1,\"725\":1,\"727\":1,\"728\":1,\"731\":3,\"732\":1,\"734\":1,\"735\":1,\"743\":1,\"747\":3,\"748\":1,\"754\":2,\"755\":1,\"762\":1,\"763\":1,\"764\":1,\"767\":1,\"768\":1,\"769\":1,\"770\":1,\"772\":1,\"773\":1,\"777\":1,\"781\":1,\"784\":3,\"786\":1,\"787\":1,\"794\":1,\"797\":1,\"800\":1,\"801\":3,\"802\":1,\"804\":1,\"806\":1,\"807\":2,\"813\":1,\"817\":1,\"821\":1,\"822\":1,\"824\":1,\"825\":1,\"826\":2,\"828\":1,\"829\":1,\"834\":3,\"838\":1,\"871\":1,\"879\":1,\"891\":1,\"893\":1,\"894\":1,\"897\":1,\"919\":1,\"931\":1,\"999\":1,\"1046\":1,\"1049\":1,\"1050\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1066\":1,\"1068\":1,\"1073\":1,\"1075\":1,\"1083\":1,\"1130\":1,\"1132\":1,\"1138\":1,\"1139\":1,\"1166\":1,\"1191\":1,\"1192\":1,\"1195\":1,\"1198\":2,\"1201\":1,\"1205\":1,\"1219\":1,\"1241\":1,\"1244\":1,\"1248\":1,\"1251\":1,\"1255\":1,\"1256\":1,\"1270\":1,\"1323\":1,\"1765\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1798\":1,\"1800\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1808\":1,\"1829\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1841\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1848\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"1857\":1,\"1859\":1,\"1861\":1,\"1862\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1911\":1,\"1917\":2,\"1959\":1,\"1971\":1,\"2002\":1,\"2028\":1,\"2078\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2086\":2,\"2087\":2,\"2088\":1,\"2090\":2,\"2091\":1,\"2092\":1,\"2093\":1,\"2095\":1,\"2156\":5,\"2240\":1,\"2243\":2,\"2244\":2,\"2245\":1,\"2253\":1,\"2254\":1,\"2255\":2,\"2256\":1,\"2263\":1,\"2264\":2,\"2278\":1,\"2279\":2,\"2280\":1,\"2317\":1,\"2584\":2}}],[\"init\",{\"0\":{\"38\":1,\"635\":1,\"637\":1,\"639\":1,\"640\":1,\"642\":1,\"643\":1,\"645\":1,\"646\":1,\"658\":1,\"871\":1,\"879\":1,\"891\":1,\"897\":1,\"931\":1,\"1692\":1,\"1818\":1,\"1901\":1,\"2092\":1,\"2093\":1},\"1\":{\"11\":1,\"28\":7,\"38\":4,\"40\":2,\"41\":2,\"42\":2,\"66\":7,\"104\":1,\"124\":1,\"231\":2,\"251\":8,\"255\":8,\"259\":8,\"265\":8,\"269\":8,\"377\":2,\"429\":2,\"635\":1,\"637\":1,\"639\":1,\"640\":1,\"642\":1,\"643\":1,\"645\":1,\"646\":1,\"658\":1,\"691\":1,\"696\":1,\"697\":1,\"706\":2,\"725\":1,\"754\":1,\"770\":1,\"797\":2,\"806\":1,\"815\":1,\"817\":1,\"824\":1,\"826\":1,\"829\":2,\"834\":1,\"871\":1,\"879\":1,\"891\":1,\"897\":1,\"931\":1,\"981\":1,\"996\":1,\"997\":1,\"998\":1,\"1000\":1,\"1046\":1,\"1066\":1,\"1073\":1,\"1075\":1,\"1083\":1,\"1149\":2,\"1150\":2,\"1182\":1,\"1221\":1,\"1244\":1,\"1253\":2,\"1270\":1,\"1454\":1,\"1458\":1,\"1542\":1,\"1605\":1,\"1607\":1,\"1631\":1,\"1635\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1731\":1,\"1732\":1,\"1778\":3,\"1818\":1,\"1850\":3,\"1851\":6,\"1852\":3,\"1901\":1,\"1917\":1,\"2003\":3,\"2018\":1,\"2086\":2,\"2087\":2,\"2090\":6,\"2092\":1,\"2093\":1,\"2155\":1,\"2156\":2,\"2157\":3,\"2176\":2,\"2180\":4,\"2194\":1,\"2243\":6,\"2244\":6,\"2255\":6,\"2264\":6,\"2279\":6,\"2394\":3,\"2440\":1,\"2530\":3,\"2558\":1,\"2573\":1,\"2584\":2,\"2585\":4}}],[\"insensitive\",{\"1\":{\"2155\":1}}],[\"inserted\",{\"1\":{\"1198\":1}}],[\"insert\",{\"1\":{\"175\":1,\"194\":1,\"2500\":1,\"2617\":1,\"2635\":1}}],[\"insertion\",{\"1\":{\"150\":1}}],[\"inspect\",{\"1\":{\"944\":1}}],[\"inspired\",{\"1\":{\"56\":1,\"1917\":1}}],[\"ins\",{\"1\":{\"864\":1,\"2444\":1,\"2445\":1,\"2446\":1}}],[\"instructions\",{\"1\":{\"2584\":1}}],[\"instabilities\",{\"1\":{\"1619\":1}}],[\"instancia\",{\"1\":{\"2457\":1}}],[\"instancenorm2d\",{\"1\":{\"1629\":1}}],[\"instancenorm2dplus\",{\"0\":{\"1583\":1},\"1\":{\"1583\":1}}],[\"instancenorm\",{\"1\":{\"1629\":1}}],[\"instances\",{\"1\":{\"1452\":1,\"2152\":1}}],[\"instance\",{\"1\":{\"654\":1,\"672\":1,\"676\":1,\"694\":1,\"711\":8,\"753\":1,\"757\":1,\"761\":1,\"765\":1,\"767\":1,\"779\":1,\"781\":1,\"798\":1,\"832\":1,\"914\":1,\"975\":2,\"976\":2,\"1042\":2,\"1043\":2,\"1049\":2,\"1050\":4,\"1056\":5,\"1110\":1,\"1112\":1,\"1116\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1506\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1600\":1,\"1603\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1622\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1951\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1962\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2078\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2584\":2,\"2585\":1,\"2600\":2}}],[\"instantiated\",{\"1\":{\"2121\":1,\"2122\":1}}],[\"instantiate\",{\"0\":{\"1327\":1},\"1\":{\"1327\":3,\"2012\":1,\"2137\":1}}],[\"instantiates\",{\"1\":{\"892\":1}}],[\"instantiations\",{\"1\":{\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1245\":1,\"1253\":2,\"1254\":1,\"1279\":1}}],[\"installs\",{\"1\":{\"2429\":1,\"2552\":1}}],[\"installing\",{\"1\":{\"132\":1,\"134\":1}}],[\"installer\",{\"1\":{\"127\":1}}],[\"installers\",{\"1\":{\"101\":2,\"127\":1,\"136\":2,\"2372\":1,\"2384\":1,\"2409\":2,\"2429\":4,\"2552\":4}}],[\"installed\",{\"1\":{\"1\":1,\"85\":1,\"101\":1,\"126\":1,\"132\":1,\"135\":1,\"136\":1,\"2372\":2,\"2553\":1,\"2591\":1,\"2598\":1}}],[\"install\",{\"0\":{\"10\":1,\"134\":1,\"200\":1,\"207\":1,\"220\":1,\"227\":1,\"2394\":1,\"2429\":1,\"2482\":1,\"2530\":1,\"2552\":1,\"2595\":1,\"2602\":1,\"2619\":1},\"1\":{\"10\":2,\"17\":4,\"99\":1,\"101\":3,\"126\":2,\"127\":4,\"132\":5,\"134\":7,\"135\":2,\"136\":10,\"137\":1,\"141\":1,\"167\":3,\"178\":3,\"196\":3,\"200\":3,\"207\":3,\"220\":1,\"227\":1,\"234\":3,\"2355\":2,\"2362\":1,\"2372\":7,\"2384\":6,\"2387\":1,\"2396\":1,\"2409\":3,\"2429\":6,\"2450\":10,\"2466\":4,\"2481\":1,\"2482\":4,\"2499\":1,\"2504\":8,\"2517\":9,\"2552\":7,\"2554\":2,\"2576\":2,\"2585\":1,\"2598\":4,\"2602\":2,\"2618\":1,\"2619\":2,\"2635\":1,\"2646\":4,\"2651\":1}}],[\"installations\",{\"1\":{\"2372\":1}}],[\"installation\",{\"0\":{\"128\":2,\"131\":1,\"135\":1,\"136\":1,\"137\":1,\"167\":1,\"178\":1,\"2362\":1,\"2409\":1,\"2450\":1,\"2466\":1,\"2504\":1,\"2553\":1,\"2651\":1},\"1\":{\"5\":1,\"7\":1,\"17\":2,\"70\":1,\"113\":1,\"126\":1,\"127\":3,\"128\":1,\"132\":1,\"133\":2,\"134\":1,\"136\":1,\"137\":2,\"148\":1,\"2355\":4,\"2372\":8,\"2382\":2,\"2384\":3,\"2390\":2,\"2409\":1,\"2424\":6,\"2428\":1,\"2450\":2,\"2466\":1,\"2482\":2,\"2526\":2,\"2547\":6,\"2551\":1}}],[\"insteadof\",{\"1\":{\"1252\":1,\"1253\":1,\"1254\":1}}],[\"instead\",{\"1\":{\"25\":2,\"62\":1,\"75\":1,\"77\":1,\"78\":1,\"79\":2,\"109\":1,\"112\":1,\"115\":1,\"144\":1,\"148\":1,\"295\":1,\"753\":1,\"757\":1,\"761\":1,\"778\":1,\"779\":1,\"832\":1,\"952\":2,\"1013\":1,\"1034\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1187\":1,\"1189\":1,\"1198\":2,\"1202\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1241\":1,\"1245\":1,\"1246\":1,\"1250\":1,\"1251\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1398\":1,\"1400\":1,\"1430\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1462\":1,\"1463\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1524\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1551\":1,\"1553\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1639\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1664\":1,\"1665\":1,\"1670\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1798\":1,\"1802\":1,\"1807\":1,\"1859\":1,\"1874\":1,\"1878\":1,\"1891\":1,\"1897\":1,\"1903\":1,\"1908\":1,\"1912\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2244\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2371\":1,\"2372\":1,\"2393\":1,\"2410\":1,\"2412\":1,\"2429\":1,\"2433\":1,\"2529\":1,\"2554\":1,\"2570\":1,\"2573\":1,\"2612\":1,\"2630\":1}}],[\"inside\",{\"1\":{\"1\":2,\"608\":1,\"1298\":1,\"1299\":1,\"1302\":1,\"1303\":1,\"1797\":1,\"2479\":1}}],[\"in\",{\"0\":{\"1\":1,\"18\":1,\"49\":1,\"69\":1,\"88\":1,\"90\":1,\"182\":1,\"2220\":1,\"2221\":1,\"2369\":1,\"2372\":1,\"2374\":1,\"2386\":1,\"2434\":1,\"2487\":1,\"2491\":1,\"2496\":1,\"2557\":1,\"2583\":1,\"2606\":1,\"2610\":1,\"2614\":1,\"2623\":1,\"2627\":1,\"2632\":1},\"1\":{\"1\":3,\"3\":2,\"4\":1,\"5\":4,\"6\":1,\"7\":1,\"11\":2,\"14\":1,\"17\":3,\"18\":2,\"19\":4,\"20\":2,\"21\":8,\"22\":3,\"23\":4,\"25\":2,\"26\":5,\"27\":1,\"29\":1,\"33\":3,\"37\":1,\"38\":1,\"40\":1,\"45\":4,\"46\":3,\"47\":1,\"48\":5,\"49\":2,\"52\":2,\"56\":2,\"57\":4,\"60\":3,\"62\":2,\"63\":2,\"64\":2,\"68\":1,\"69\":3,\"74\":2,\"75\":1,\"76\":3,\"77\":4,\"78\":3,\"80\":1,\"82\":1,\"84\":1,\"85\":4,\"90\":1,\"91\":1,\"92\":1,\"94\":1,\"95\":3,\"98\":3,\"100\":2,\"102\":7,\"104\":1,\"107\":2,\"108\":3,\"109\":3,\"110\":1,\"111\":1,\"112\":6,\"113\":6,\"114\":1,\"115\":14,\"116\":3,\"117\":2,\"118\":2,\"119\":6,\"120\":2,\"121\":5,\"122\":3,\"124\":8,\"126\":1,\"127\":2,\"130\":1,\"132\":1,\"135\":1,\"136\":2,\"137\":1,\"141\":2,\"143\":2,\"144\":3,\"148\":5,\"150\":1,\"152\":1,\"161\":3,\"166\":1,\"167\":1,\"168\":1,\"169\":2,\"173\":1,\"174\":8,\"175\":2,\"179\":1,\"181\":2,\"185\":2,\"193\":1,\"194\":2,\"199\":1,\"203\":2,\"204\":1,\"206\":1,\"217\":4,\"218\":1,\"224\":4,\"231\":8,\"237\":2,\"238\":10,\"239\":2,\"240\":3,\"242\":2,\"251\":3,\"255\":3,\"259\":3,\"265\":3,\"269\":3,\"275\":2,\"276\":1,\"282\":1,\"286\":2,\"294\":2,\"295\":1,\"296\":1,\"297\":4,\"429\":3,\"496\":1,\"506\":1,\"522\":1,\"538\":1,\"607\":2,\"612\":1,\"617\":1,\"618\":1,\"619\":1,\"625\":1,\"627\":1,\"628\":1,\"629\":1,\"635\":2,\"637\":1,\"648\":1,\"668\":2,\"674\":1,\"676\":1,\"681\":3,\"682\":3,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"690\":2,\"691\":7,\"692\":1,\"693\":2,\"697\":8,\"700\":1,\"701\":3,\"702\":1,\"704\":1,\"705\":2,\"706\":6,\"708\":2,\"710\":2,\"712\":2,\"713\":3,\"723\":1,\"727\":1,\"728\":3,\"731\":2,\"732\":1,\"734\":2,\"736\":2,\"737\":9,\"738\":5,\"739\":2,\"742\":1,\"743\":2,\"745\":4,\"746\":2,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"754\":11,\"755\":2,\"758\":2,\"760\":1,\"762\":2,\"763\":1,\"764\":1,\"766\":1,\"767\":2,\"771\":1,\"772\":2,\"773\":2,\"774\":1,\"778\":3,\"785\":1,\"786\":5,\"797\":5,\"802\":2,\"803\":2,\"804\":2,\"808\":1,\"809\":1,\"810\":2,\"814\":1,\"815\":2,\"817\":2,\"821\":14,\"822\":2,\"825\":1,\"826\":21,\"828\":2,\"831\":1,\"833\":2,\"836\":1,\"837\":1,\"857\":1,\"858\":2,\"872\":1,\"873\":1,\"874\":1,\"875\":2,\"880\":1,\"890\":2,\"897\":1,\"899\":2,\"901\":2,\"909\":1,\"913\":1,\"919\":1,\"920\":1,\"922\":1,\"924\":1,\"926\":1,\"929\":1,\"940\":1,\"950\":2,\"952\":2,\"959\":1,\"968\":2,\"975\":1,\"976\":1,\"979\":1,\"981\":1,\"987\":3,\"999\":1,\"1001\":2,\"1004\":2,\"1005\":2,\"1008\":1,\"1011\":2,\"1013\":1,\"1014\":2,\"1015\":3,\"1016\":1,\"1028\":9,\"1042\":1,\"1043\":1,\"1048\":1,\"1049\":2,\"1050\":2,\"1052\":8,\"1056\":2,\"1057\":4,\"1058\":4,\"1062\":2,\"1065\":2,\"1066\":2,\"1068\":2,\"1071\":1,\"1074\":1,\"1077\":2,\"1081\":2,\"1096\":2,\"1099\":1,\"1101\":1,\"1117\":2,\"1132\":3,\"1133\":13,\"1137\":1,\"1138\":4,\"1139\":1,\"1142\":4,\"1144\":1,\"1148\":3,\"1149\":4,\"1150\":5,\"1154\":2,\"1155\":1,\"1170\":1,\"1171\":2,\"1172\":1,\"1178\":1,\"1180\":2,\"1181\":1,\"1182\":1,\"1186\":5,\"1187\":3,\"1190\":7,\"1198\":16,\"1200\":1,\"1202\":3,\"1203\":4,\"1204\":6,\"1206\":2,\"1209\":1,\"1210\":5,\"1214\":9,\"1216\":1,\"1220\":2,\"1222\":1,\"1227\":1,\"1242\":1,\"1244\":6,\"1245\":4,\"1252\":1,\"1269\":11,\"1272\":2,\"1273\":12,\"1282\":2,\"1286\":1,\"1287\":1,\"1298\":6,\"1299\":6,\"1301\":6,\"1302\":5,\"1303\":5,\"1304\":6,\"1305\":1,\"1334\":2,\"1337\":4,\"1349\":2,\"1350\":2,\"1356\":1,\"1370\":1,\"1371\":1,\"1375\":4,\"1377\":2,\"1378\":1,\"1379\":5,\"1392\":2,\"1398\":1,\"1406\":2,\"1409\":1,\"1427\":1,\"1430\":4,\"1432\":2,\"1436\":2,\"1451\":2,\"1452\":1,\"1454\":2,\"1455\":2,\"1462\":2,\"1463\":5,\"1470\":1,\"1476\":1,\"1478\":4,\"1480\":2,\"1484\":1,\"1487\":1,\"1491\":1,\"1505\":7,\"1506\":1,\"1508\":1,\"1510\":3,\"1511\":3,\"1514\":1,\"1515\":1,\"1516\":3,\"1518\":1,\"1520\":1,\"1522\":9,\"1523\":8,\"1529\":2,\"1530\":1,\"1531\":5,\"1534\":2,\"1539\":2,\"1543\":7,\"1545\":7,\"1546\":1,\"1547\":1,\"1551\":10,\"1552\":2,\"1553\":12,\"1554\":2,\"1557\":1,\"1558\":7,\"1564\":1,\"1572\":2,\"1576\":9,\"1577\":10,\"1581\":1,\"1585\":1,\"1592\":1,\"1598\":2,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":4,\"1607\":1,\"1611\":2,\"1612\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1622\":4,\"1623\":1,\"1626\":2,\"1627\":1,\"1631\":1,\"1633\":2,\"1635\":1,\"1637\":1,\"1638\":1,\"1639\":4,\"1640\":1,\"1643\":3,\"1644\":3,\"1645\":5,\"1646\":3,\"1652\":1,\"1654\":3,\"1655\":10,\"1656\":6,\"1658\":3,\"1659\":3,\"1660\":9,\"1661\":11,\"1662\":9,\"1663\":1,\"1664\":6,\"1665\":6,\"1669\":3,\"1670\":7,\"1671\":4,\"1690\":1,\"1691\":1,\"1692\":1,\"1696\":1,\"1698\":2,\"1704\":1,\"1707\":1,\"1712\":4,\"1713\":2,\"1715\":4,\"1719\":17,\"1731\":1,\"1732\":1,\"1741\":1,\"1752\":3,\"1757\":1,\"1765\":5,\"1766\":5,\"1768\":2,\"1769\":1,\"1771\":3,\"1772\":1,\"1773\":3,\"1776\":2,\"1778\":5,\"1782\":1,\"1785\":2,\"1786\":4,\"1787\":1,\"1788\":2,\"1797\":1,\"1798\":2,\"1800\":6,\"1801\":4,\"1803\":5,\"1804\":48,\"1805\":11,\"1820\":1,\"1829\":1,\"1830\":1,\"1831\":2,\"1832\":1,\"1833\":2,\"1834\":1,\"1835\":1,\"1842\":1,\"1844\":6,\"1845\":1,\"1846\":1,\"1847\":2,\"1848\":5,\"1849\":3,\"1850\":6,\"1851\":34,\"1852\":5,\"1856\":3,\"1857\":6,\"1858\":3,\"1860\":1,\"1861\":3,\"1862\":6,\"1863\":7,\"1864\":7,\"1865\":6,\"1866\":2,\"1867\":2,\"1868\":2,\"1869\":2,\"1870\":1,\"1871\":7,\"1872\":7,\"1873\":8,\"1874\":2,\"1876\":1,\"1877\":10,\"1878\":46,\"1879\":2,\"1880\":6,\"1897\":3,\"1901\":1,\"1905\":3,\"1906\":1,\"1910\":1,\"1912\":3,\"1914\":1,\"1915\":1,\"1917\":10,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1922\":1,\"1923\":2,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":4,\"1932\":1,\"1935\":1,\"1936\":2,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":4,\"1943\":1,\"1946\":1,\"1947\":5,\"1954\":1,\"1956\":2,\"1968\":1,\"1970\":1,\"1973\":1,\"1975\":1,\"1984\":1,\"1999\":1,\"2001\":5,\"2002\":8,\"2003\":4,\"2004\":2,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":2,\"2011\":1,\"2012\":2,\"2019\":2,\"2021\":1,\"2022\":2,\"2023\":1,\"2027\":1,\"2029\":2,\"2030\":1,\"2040\":1,\"2044\":2,\"2046\":5,\"2049\":2,\"2054\":4,\"2055\":1,\"2064\":1,\"2068\":1,\"2070\":1,\"2076\":1,\"2078\":2,\"2079\":5,\"2080\":1,\"2081\":1,\"2082\":4,\"2083\":2,\"2084\":1,\"2086\":4,\"2087\":5,\"2088\":2,\"2089\":1,\"2090\":17,\"2091\":2,\"2095\":19,\"2096\":1,\"2098\":1,\"2099\":3,\"2100\":1,\"2101\":1,\"2102\":3,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":2,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2153\":2,\"2155\":1,\"2168\":2,\"2186\":1,\"2190\":1,\"2193\":1,\"2197\":3,\"2199\":1,\"2202\":2,\"2204\":1,\"2209\":1,\"2216\":2,\"2217\":2,\"2220\":1,\"2221\":1,\"2236\":1,\"2243\":32,\"2244\":34,\"2245\":2,\"2252\":1,\"2255\":33,\"2256\":2,\"2257\":8,\"2261\":9,\"2262\":4,\"2263\":19,\"2264\":30,\"2265\":1,\"2268\":1,\"2270\":1,\"2272\":1,\"2279\":25,\"2280\":3,\"2317\":2,\"2325\":2,\"2342\":1,\"2354\":2,\"2355\":2,\"2357\":3,\"2359\":1,\"2365\":1,\"2371\":2,\"2372\":4,\"2373\":4,\"2374\":2,\"2375\":1,\"2377\":2,\"2380\":4,\"2384\":7,\"2385\":14,\"2386\":1,\"2387\":7,\"2388\":4,\"2389\":2,\"2390\":1,\"2391\":2,\"2392\":1,\"2393\":1,\"2394\":8,\"2395\":1,\"2398\":4,\"2399\":1,\"2400\":5,\"2401\":2,\"2403\":1,\"2406\":1,\"2408\":2,\"2411\":2,\"2412\":1,\"2413\":1,\"2414\":3,\"2415\":2,\"2418\":1,\"2419\":2,\"2420\":1,\"2421\":2,\"2422\":2,\"2423\":1,\"2425\":1,\"2427\":1,\"2429\":5,\"2430\":6,\"2431\":2,\"2432\":1,\"2434\":2,\"2436\":3,\"2437\":2,\"2439\":1,\"2440\":5,\"2441\":2,\"2446\":1,\"2447\":2,\"2448\":1,\"2449\":2,\"2450\":2,\"2451\":6,\"2452\":2,\"2456\":1,\"2457\":2,\"2460\":1,\"2461\":1,\"2462\":2,\"2463\":2,\"2464\":1,\"2465\":2,\"2467\":6,\"2468\":5,\"2472\":2,\"2473\":1,\"2476\":3,\"2479\":1,\"2480\":2,\"2481\":2,\"2492\":1,\"2494\":1,\"2500\":5,\"2501\":2,\"2502\":3,\"2503\":2,\"2506\":1,\"2508\":1,\"2510\":1,\"2514\":5,\"2515\":1,\"2518\":1,\"2521\":1,\"2522\":2,\"2524\":3,\"2525\":2,\"2526\":1,\"2527\":2,\"2528\":1,\"2529\":1,\"2530\":8,\"2531\":1,\"2534\":3,\"2535\":1,\"2536\":5,\"2537\":2,\"2539\":1,\"2542\":1,\"2543\":6,\"2544\":2,\"2545\":2,\"2546\":1,\"2548\":1,\"2550\":1,\"2552\":1,\"2554\":4,\"2555\":7,\"2557\":2,\"2558\":6,\"2559\":1,\"2562\":3,\"2563\":2,\"2564\":1,\"2566\":1,\"2567\":2,\"2568\":9,\"2569\":3,\"2570\":1,\"2573\":3,\"2574\":1,\"2575\":1,\"2578\":2,\"2580\":1,\"2581\":2,\"2583\":2,\"2584\":6,\"2585\":7,\"2586\":1,\"2592\":5,\"2596\":3,\"2598\":2,\"2599\":1,\"2600\":2,\"2601\":1,\"2607\":2,\"2612\":2,\"2615\":2,\"2617\":5,\"2618\":1,\"2624\":2,\"2628\":1,\"2630\":1,\"2633\":2,\"2635\":6,\"2638\":7,\"2640\":1,\"2642\":2,\"2643\":1,\"2644\":1,\"2648\":2,\"2650\":1,\"2655\":1,\"2659\":5,\"2660\":1}}],[\"df\",{\"1\":{\"2268\":2}}],[\"dfs\",{\"1\":{\"794\":1,\"2269\":1}}],[\"d2\",{\"1\":{\"1963\":1}}],[\"ddropout\",{\"1\":{\"2086\":2,\"2087\":2}}],[\"dds\",{\"1\":{\"1868\":3,\"1877\":1,\"1878\":3}}],[\"ddsp\",{\"0\":{\"1809\":1,\"1812\":1,\"1813\":1,\"1814\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1822\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1827\":1},\"1\":{\"1803\":2,\"1809\":1,\"1812\":1,\"1813\":1,\"1814\":1,\"1816\":2,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":2,\"1821\":1,\"1822\":1,\"1823\":2,\"1824\":2,\"1825\":1,\"1827\":2}}],[\"ddpm\",{\"0\":{\"1690\":1,\"1691\":1},\"1\":{\"1458\":1,\"1633\":1,\"1635\":1,\"1690\":2,\"1691\":2,\"1692\":1}}],[\"ddp=false\",{\"1\":{\"626\":1,\"627\":1}}],[\"ddp\",{\"1\":{\"32\":1,\"35\":1,\"251\":1,\"429\":2,\"626\":2,\"627\":2,\"999\":2,\"2186\":1,\"2202\":2,\"2204\":1}}],[\"dl\",{\"1\":{\"1715\":1,\"2431\":2,\"2432\":2}}],[\"dlayers=2\",{\"1\":{\"2078\":1}}],[\"dlayers\",{\"1\":{\"1\":1,\"754\":1,\"806\":2,\"821\":1,\"826\":1,\"1778\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"2002\":2,\"2078\":1,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":1,\"2263\":2,\"2264\":2,\"2279\":2}}],[\"dw\",{\"1\":{\"1618\":1,\"1619\":1}}],[\"dc\",{\"0\":{\"1522\":2,\"1523\":2,\"1545\":1,\"1572\":1,\"1576\":1,\"1577\":1},\"1\":{\"1522\":6,\"1523\":4,\"1545\":1,\"1572\":2,\"1576\":2,\"1577\":1}}],[\"dccrnseparator\",{\"0\":{\"1516\":1},\"1\":{\"1516\":1}}],[\"dccrn\",{\"0\":{\"1516\":1},\"1\":{\"1516\":2}}],[\"dcunetcomplexencoderblock\",{\"0\":{\"1520\":1},\"1\":{\"1520\":1}}],[\"dcunetcomplexdecoderblock\",{\"0\":{\"1518\":1},\"1\":{\"1518\":1}}],[\"dcunet\",{\"0\":{\"1452\":1,\"1465\":1,\"1482\":1,\"1517\":2,\"1518\":1,\"1520\":1,\"1547\":1,\"1561\":1,\"1620\":1,\"1700\":1,\"1726\":1,\"1750\":1,\"1752\":1},\"1\":{\"1452\":1,\"1465\":2,\"1482\":2,\"1517\":14,\"1518\":1,\"1520\":1,\"1547\":1,\"1561\":2,\"1620\":2,\"1700\":2,\"1726\":1,\"1750\":1,\"1752\":1}}],[\"dcs\",{\"1\":{\"940\":1}}],[\"d1\",{\"1\":{\"1391\":1,\"1963\":1}}],[\"d=modeldownloader\",{\"1\":{\"2592\":1}}],[\"d=4\",{\"1\":{\"2314\":2}}],[\"d=win\",{\"1\":{\"1255\":1}}],[\"d=c=1\",{\"1\":{\"1255\":1}}],[\"dt\",{\"1\":{\"1243\":2,\"1245\":6,\"1247\":1,\"1248\":4,\"1618\":1,\"1619\":1,\"2568\":1}}],[\"dtype=none\",{\"1\":{\"1390\":1,\"1405\":1,\"1411\":1,\"1426\":1,\"1465\":1,\"2166\":1,\"2174\":1,\"2222\":1,\"2225\":1,\"2229\":1,\"2231\":1}}],[\"dtype=np\",{\"1\":{\"928\":1,\"989\":1,\"2386\":1,\"2474\":1,\"2649\":1}}],[\"dtype=<class\",{\"1\":{\"1388\":1,\"1401\":1,\"1409\":1,\"1413\":1}}],[\"dtype=\",{\"1\":{\"989\":1,\"2592\":1,\"2596\":1}}],[\"dtype=int32\",{\"1\":{\"744\":1,\"876\":1}}],[\"dtype=float32\",{\"1\":{\"744\":2,\"876\":3}}],[\"dtype=float\",{\"1\":{\"629\":1}}],[\"dtype=torch\",{\"1\":{\"624\":1,\"625\":1,\"899\":2,\"900\":4,\"901\":2,\"902\":4,\"921\":1,\"936\":1,\"1133\":4,\"1214\":2,\"1273\":2,\"1314\":1,\"1339\":1,\"1342\":1,\"1757\":1,\"2001\":2,\"2521\":2,\"2522\":2,\"2523\":2}}],[\"dtype\",{\"0\":{\"1315\":1},\"1\":{\"21\":5,\"245\":1,\"249\":2,\"251\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"301\":1,\"307\":2,\"315\":1,\"321\":1,\"327\":1,\"333\":2,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"380\":1,\"384\":2,\"391\":1,\"399\":1,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":1,\"443\":1,\"449\":1,\"455\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":2,\"485\":1,\"624\":2,\"625\":2,\"697\":1,\"806\":2,\"818\":1,\"920\":3,\"921\":3,\"924\":3,\"936\":2,\"989\":1,\"1315\":1,\"1384\":2,\"1386\":2,\"1407\":1,\"1427\":1,\"2099\":1,\"2165\":1,\"2182\":2,\"2186\":1,\"2189\":2,\"2202\":2,\"2204\":1,\"2210\":1}}],[\"d$\",{\"1\":{\"875\":2}}],[\"dnsmos\",{\"0\":{\"366\":1,\"1526\":2,\"1527\":2,\"1738\":1},\"1\":{\"363\":13,\"1526\":4,\"1527\":4,\"1738\":2}}],[\"dnn2\",{\"1\":{\"1719\":2}}],[\"dnn1=false\",{\"1\":{\"1719\":1}}],[\"dnn1\",{\"1\":{\"1719\":4}}],[\"dnn\",{\"0\":{\"690\":1,\"729\":2,\"730\":2,\"1455\":1,\"1524\":2,\"1525\":2},\"1\":{\"46\":1,\"48\":1,\"63\":1,\"85\":1,\"92\":1,\"251\":2,\"690\":1,\"729\":3,\"730\":3,\"756\":1,\"1158\":1,\"1239\":1,\"1455\":1,\"1524\":4,\"1525\":6,\"1611\":1,\"1791\":1,\"2068\":1,\"2070\":1,\"2168\":1,\"2170\":1}}],[\"dr\",{\"1\":{\"2134\":1}}],[\"drift\",{\"1\":{\"1619\":1}}],[\"drive\",{\"0\":{\"277\":1},\"1\":{\"200\":2,\"210\":2,\"211\":2,\"212\":2,\"214\":2,\"215\":2,\"216\":2,\"222\":4,\"223\":4,\"229\":4,\"230\":4,\"277\":3,\"2396\":1,\"2532\":1,\"2567\":1,\"2593\":3}}],[\"draw\",{\"1\":{\"629\":1,\"648\":2}}],[\"dropinp=0\",{\"1\":{\"1252\":1}}],[\"dropinp\",{\"1\":{\"1244\":2,\"1252\":1}}],[\"drop\",{\"1\":{\"429\":2,\"787\":3,\"914\":2,\"1140\":1,\"1141\":2,\"1148\":1,\"1169\":1,\"1244\":3,\"1252\":2,\"1254\":3,\"1269\":3,\"1271\":1,\"1272\":1,\"2001\":1,\"2004\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":2,\"2054\":1,\"2142\":3,\"2440\":1,\"2564\":1,\"2584\":1}}],[\"dropping\",{\"1\":{\"115\":1,\"787\":1,\"914\":1,\"1068\":1,\"1093\":1,\"1141\":1,\"1352\":1}}],[\"dropout2d\",{\"1\":{\"1244\":1,\"1252\":1,\"1254\":1}}],[\"dropout=true\",{\"1\":{\"1242\":1}}],[\"dropout=false\",{\"1\":{\"1177\":1,\"1241\":1,\"1252\":1,\"1254\":1}}],[\"dropout=0\",{\"1\":{\"717\":1,\"732\":1,\"748\":1,\"777\":1,\"784\":1,\"800\":1,\"801\":1,\"1177\":1,\"1209\":1,\"1241\":1,\"1252\":1,\"1254\":1,\"1430\":1,\"1460\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1560\":1,\"1581\":1,\"1598\":1,\"1605\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1670\":1,\"1800\":1,\"1806\":1,\"1994\":1}}],[\"dropout1d\",{\"1\":{\"1166\":1,\"1242\":1,\"1244\":1,\"1252\":1,\"1254\":1}}],[\"dropoutnd\",{\"0\":{\"1166\":1},\"1\":{\"1166\":1}}],[\"dropout\",{\"1\":{\"21\":18,\"22\":4,\"115\":22,\"116\":16,\"118\":2,\"703\":3,\"711\":3,\"712\":3,\"713\":3,\"714\":3,\"715\":3,\"716\":3,\"717\":2,\"718\":3,\"719\":3,\"720\":3,\"721\":3,\"722\":3,\"725\":3,\"726\":6,\"729\":1,\"730\":1,\"731\":2,\"732\":2,\"736\":1,\"737\":2,\"740\":3,\"741\":3,\"747\":5,\"748\":2,\"749\":9,\"754\":14,\"770\":3,\"771\":3,\"772\":3,\"775\":3,\"776\":3,\"777\":2,\"782\":1,\"784\":4,\"785\":3,\"786\":3,\"800\":2,\"801\":4,\"802\":3,\"803\":1,\"804\":1,\"805\":3,\"806\":6,\"807\":2,\"808\":3,\"809\":3,\"810\":3,\"813\":3,\"818\":3,\"821\":2,\"825\":6,\"826\":20,\"827\":3,\"858\":6,\"911\":6,\"940\":1,\"1049\":3,\"1050\":3,\"1052\":3,\"1054\":3,\"1056\":3,\"1057\":3,\"1065\":9,\"1066\":15,\"1070\":3,\"1073\":6,\"1074\":6,\"1075\":9,\"1076\":3,\"1077\":3,\"1083\":3,\"1093\":3,\"1115\":20,\"1133\":6,\"1140\":3,\"1141\":3,\"1145\":3,\"1148\":9,\"1149\":9,\"1150\":9,\"1151\":1,\"1153\":1,\"1166\":2,\"1167\":4,\"1168\":4,\"1169\":3,\"1170\":3,\"1179\":7,\"1180\":9,\"1181\":7,\"1182\":1,\"1195\":1,\"1196\":4,\"1197\":4,\"1198\":8,\"1200\":3,\"1201\":1,\"1203\":9,\"1204\":4,\"1209\":2,\"1214\":1,\"1215\":1,\"1220\":1,\"1222\":3,\"1242\":3,\"1244\":7,\"1252\":5,\"1254\":4,\"1256\":6,\"1269\":12,\"1270\":6,\"1271\":4,\"1272\":9,\"1273\":4,\"1282\":3,\"1376\":1,\"1430\":2,\"1505\":9,\"1515\":3,\"1524\":1,\"1525\":1,\"1528\":3,\"1529\":3,\"1531\":3,\"1532\":2,\"1534\":3,\"1535\":2,\"1537\":2,\"1539\":3,\"1558\":3,\"1581\":2,\"1595\":1,\"1598\":2,\"1602\":3,\"1626\":3,\"1648\":2,\"1650\":2,\"1652\":2,\"1654\":3,\"1669\":9,\"1670\":2,\"1671\":3,\"1769\":1,\"1771\":9,\"1772\":3,\"1776\":3,\"1777\":3,\"1778\":8,\"1787\":9,\"1788\":9,\"1798\":9,\"1804\":15,\"1805\":5,\"1835\":3,\"1850\":11,\"1851\":33,\"1852\":12,\"1862\":4,\"1863\":3,\"1864\":3,\"1865\":3,\"1868\":3,\"1874\":9,\"1877\":6,\"1878\":18,\"1880\":4,\"1932\":3,\"1934\":1,\"1958\":1,\"1960\":3,\"1971\":3,\"1993\":1,\"2001\":8,\"2002\":3,\"2003\":2,\"2004\":8,\"2026\":3,\"2029\":9,\"2054\":9,\"2078\":3,\"2081\":2,\"2083\":2,\"2086\":8,\"2087\":11,\"2090\":24,\"2095\":3,\"2243\":24,\"2244\":36,\"2253\":1,\"2255\":33,\"2260\":3,\"2262\":3,\"2263\":3,\"2264\":30,\"2265\":3,\"2279\":36,\"2290\":1,\"2292\":1,\"2440\":7,\"2558\":7,\"2564\":3,\"2584\":3}}],[\"dx\",{\"1\":{\"130\":1,\"1618\":1,\"1619\":1}}],[\"dynamicmixingpreprocessor\",{\"0\":{\"2181\":1},\"1\":{\"2181\":1}}],[\"dynamics\",{\"1\":{\"1451\":1}}],[\"dynamicconvolutiontransformerdecoder\",{\"0\":{\"1168\":1},\"1\":{\"1168\":1}}],[\"dynamicconvolution2dtransformerdecoder\",{\"0\":{\"1167\":1},\"1\":{\"1167\":1}}],[\"dynamicconvolution2d\",{\"0\":{\"741\":1},\"1\":{\"741\":1}}],[\"dynamicconvolution\",{\"0\":{\"740\":1},\"1\":{\"740\":1}}],[\"dynamic\",{\"0\":{\"665\":1,\"674\":1,\"740\":1,\"741\":1,\"872\":1,\"873\":1,\"874\":1,\"1012\":2},\"1\":{\"120\":1,\"121\":6,\"217\":3,\"224\":3,\"231\":3,\"665\":2,\"674\":1,\"700\":1,\"740\":4,\"741\":4,\"758\":1,\"766\":1,\"872\":1,\"873\":2,\"874\":2,\"1012\":5,\"1093\":5,\"2079\":3,\"2083\":1,\"2095\":6,\"2181\":1}}],[\"dynamically\",{\"1\":{\"62\":1,\"665\":1,\"674\":1,\"872\":1,\"873\":1,\"874\":1}}],[\"dberrebb\",{\"1\":{\"2583\":1}}],[\"dbidirectional\",{\"1\":{\"2086\":2,\"2087\":2}}],[\"db=\",{\"1\":{\"1799\":1}}],[\"db=20\",{\"1\":{\"1799\":1}}],[\"db=none\",{\"1\":{\"1639\":1,\"1640\":1}}],[\"db=true\",{\"1\":{\"1526\":1}}],[\"dbunit=true\",{\"1\":{\"948\":1,\"961\":1}}],[\"db\",{\"1\":{\"85\":3,\"297\":1,\"648\":2,\"1639\":3,\"1640\":3,\"1936\":1,\"2178\":1,\"2179\":1,\"2181\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2197\":3,\"2200\":1,\"2372\":1,\"2385\":1,\"2394\":1,\"2429\":1,\"2498\":3,\"2530\":1,\"2554\":1,\"2566\":1,\"2567\":3,\"2568\":2,\"2616\":3,\"2634\":3}}],[\"dsp\",{\"1\":{\"1695\":1}}],[\"ds\",{\"1\":{\"755\":2,\"774\":2,\"1842\":2,\"1879\":2,\"1881\":2,\"2091\":2,\"2245\":2,\"2256\":2,\"2280\":2,\"2360\":1,\"2458\":1,\"2523\":1,\"2582\":1}}],[\"dset\",{\"1\":{\"476\":2}}],[\"dst\",{\"1\":{\"66\":1,\"2152\":2,\"2584\":3}}],[\"dsd\",{\"1\":{\"48\":1}}],[\"dprnnseparator\",{\"0\":{\"1534\":1},\"1\":{\"1534\":1}}],[\"dprnn\",{\"0\":{\"1532\":2,\"1534\":1,\"1535\":2,\"1650\":1,\"1728\":1,\"1743\":1},\"1\":{\"1532\":4,\"1534\":2,\"1535\":4,\"1558\":2,\"1650\":2,\"1728\":2,\"1743\":2,\"2371\":1,\"2612\":1,\"2630\":1,\"2641\":1}}],[\"dprenet\",{\"1\":{\"826\":3,\"2264\":6}}],[\"dpmulcat\",{\"0\":{\"1531\":2,\"1602\":1},\"1\":{\"1531\":6,\"1602\":2}}],[\"dpclsolver\",{\"0\":{\"1530\":1},\"1\":{\"1530\":1}}],[\"dpclseparator\",{\"0\":{\"1529\":1},\"1\":{\"1529\":1}}],[\"dpcle2eseparator\",{\"0\":{\"1528\":1},\"1\":{\"1528\":1}}],[\"dpcl\",{\"0\":{\"1528\":1,\"1529\":1,\"1530\":1},\"1\":{\"1528\":1,\"1529\":1,\"1530\":2,\"1568\":1}}],[\"dptnetseparator\",{\"0\":{\"1539\":1},\"1\":{\"1539\":1}}],[\"dptnet\",{\"0\":{\"1537\":2,\"1539\":1,\"1581\":1},\"1\":{\"1454\":1,\"1537\":4,\"1539\":2,\"1581\":1}}],[\"dplr\",{\"0\":{\"1314\":1},\"1\":{\"1314\":2,\"1351\":1}}],[\"dp\",{\"0\":{\"2087\":1},\"1\":{\"32\":1,\"698\":2,\"699\":2,\"2087\":1}}],[\"d\",{\"1\":{\"21\":8,\"135\":4,\"174\":4,\"194\":2,\"203\":1,\"503\":1,\"628\":1,\"676\":4,\"677\":3,\"678\":3,\"679\":3,\"681\":3,\"684\":3,\"685\":3,\"686\":3,\"687\":3,\"688\":3,\"689\":3,\"691\":4,\"692\":2,\"693\":1,\"697\":5,\"700\":6,\"712\":8,\"725\":11,\"726\":4,\"731\":1,\"732\":2,\"734\":1,\"740\":4,\"741\":4,\"747\":1,\"748\":2,\"755\":2,\"758\":3,\"766\":3,\"770\":2,\"771\":1,\"772\":2,\"774\":2,\"775\":4,\"776\":4,\"781\":2,\"785\":6,\"794\":1,\"797\":2,\"801\":2,\"806\":27,\"809\":1,\"810\":2,\"812\":2,\"813\":2,\"818\":2,\"825\":11,\"827\":3,\"834\":1,\"857\":1,\"863\":1,\"865\":2,\"880\":2,\"888\":1,\"925\":2,\"932\":2,\"1001\":3,\"1010\":2,\"1011\":3,\"1025\":3,\"1047\":2,\"1048\":4,\"1049\":8,\"1050\":8,\"1051\":4,\"1052\":11,\"1053\":2,\"1054\":3,\"1055\":3,\"1056\":8,\"1057\":2,\"1058\":2,\"1060\":1,\"1063\":2,\"1064\":6,\"1065\":2,\"1066\":3,\"1068\":5,\"1069\":8,\"1071\":3,\"1072\":2,\"1073\":25,\"1074\":2,\"1075\":24,\"1076\":10,\"1080\":2,\"1081\":8,\"1083\":3,\"1086\":10,\"1116\":1,\"1132\":2,\"1138\":7,\"1139\":6,\"1145\":1,\"1149\":3,\"1150\":3,\"1153\":3,\"1160\":4,\"1161\":4,\"1162\":4,\"1163\":4,\"1164\":4,\"1165\":3,\"1177\":4,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1198\":6,\"1199\":2,\"1200\":1,\"1208\":1,\"1209\":6,\"1212\":1,\"1233\":3,\"1235\":1,\"1241\":4,\"1243\":1,\"1245\":2,\"1248\":2,\"1251\":1,\"1252\":7,\"1253\":11,\"1254\":8,\"1255\":4,\"1256\":1,\"1269\":1,\"1270\":27,\"1272\":1,\"1274\":2,\"1276\":3,\"1279\":5,\"1280\":2,\"1340\":1,\"1389\":1,\"1395\":1,\"1397\":1,\"1406\":1,\"1414\":1,\"1416\":1,\"1418\":2,\"1423\":2,\"1427\":3,\"1478\":4,\"1515\":2,\"1528\":2,\"1529\":3,\"1530\":1,\"1568\":1,\"1572\":2,\"1761\":6,\"1763\":6,\"1768\":6,\"1773\":4,\"1778\":1,\"1781\":3,\"1805\":7,\"1837\":3,\"1842\":2,\"1850\":1,\"1852\":1,\"1877\":1,\"1879\":2,\"1917\":2,\"1949\":1,\"1971\":4,\"1993\":1,\"2029\":1,\"2068\":1,\"2070\":1,\"2082\":3,\"2091\":2,\"2170\":1,\"2185\":1,\"2201\":1,\"2203\":1,\"2240\":3,\"2245\":2,\"2256\":2,\"2278\":3,\"2280\":2,\"2314\":3,\"2358\":2,\"2360\":1,\"2368\":1,\"2371\":2,\"2392\":1,\"2425\":1,\"2458\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":4,\"2506\":1,\"2510\":2,\"2512\":1,\"2514\":2,\"2520\":2,\"2523\":1,\"2528\":1,\"2548\":1,\"2579\":2,\"2582\":1,\"2590\":1,\"2592\":1,\"2593\":2,\"2600\":2,\"2605\":1,\"2609\":1,\"2612\":2,\"2617\":5,\"2622\":1,\"2626\":1,\"2630\":2,\"2635\":5,\"2659\":2}}],[\"day\",{\"1\":{\"2467\":1}}],[\"days\",{\"1\":{\"17\":1}}],[\"dal\",{\"1\":{\"2461\":1,\"2462\":1}}],[\"dalmia\",{\"1\":{\"130\":1}}],[\"darle\",{\"1\":{\"2457\":1}}],[\"dampening\",{\"1\":{\"1972\":1}}],[\"damped\",{\"0\":{\"1069\":1},\"1\":{\"1069\":2}}],[\"damping\",{\"1\":{\"1069\":3}}],[\"datetime\",{\"1\":{\"2392\":3,\"2425\":3,\"2528\":3,\"2548\":3}}],[\"date\",{\"0\":{\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1},\"1\":{\"2392\":4,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2425\":4,\"2440\":3,\"2441\":3,\"2442\":2,\"2528\":4,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2542\":1,\"2543\":1,\"2548\":4,\"2554\":1,\"2556\":1,\"2559\":1,\"2560\":1,\"2564\":2,\"2568\":1,\"2572\":1,\"2583\":1}}],[\"dateutil\",{\"1\":{\"203\":1}}],[\"data=data\",{\"1\":{\"2596\":1}}],[\"data=stream\",{\"1\":{\"2596\":1}}],[\"data=np\",{\"1\":{\"2592\":1,\"2596\":1}}],[\"data=none\",{\"1\":{\"989\":1}}],[\"dataaugmentation\",{\"0\":{\"1905\":1},\"1\":{\"1905\":1}}],[\"datatype\",{\"1\":{\"1688\":1,\"1756\":1}}],[\"datadirwriter\",{\"0\":{\"1382\":1},\"1\":{\"1382\":1,\"1383\":1}}],[\"datadir\",{\"0\":{\"1382\":1},\"1\":{\"1382\":1,\"2432\":3}}],[\"dataclass\",{\"0\":{\"2318\":2},\"1\":{\"1060\":1,\"2186\":1,\"2204\":1,\"2318\":4}}],[\"data2json\",{\"0\":{\"276\":1},\"1\":{\"276\":2}}],[\"dataloader=none\",{\"1\":{\"2350\":2}}],[\"dataloader\",{\"0\":{\"2345\":2,\"2718\":1},\"1\":{\"60\":3,\"174\":3,\"981\":3,\"1895\":1,\"1900\":1,\"1901\":1,\"2096\":3,\"2098\":3,\"2099\":4,\"2100\":3,\"2101\":3,\"2102\":3,\"2103\":3,\"2104\":3,\"2105\":3,\"2107\":3,\"2108\":3,\"2109\":3,\"2110\":3,\"2112\":3,\"2113\":3,\"2114\":3,\"2115\":3,\"2116\":3,\"2117\":3,\"2118\":3,\"2345\":4,\"2351\":2,\"2440\":1,\"2558\":1}}],[\"dataset=none\",{\"1\":{\"2350\":2}}],[\"datasets\",{\"0\":{\"174\":1,\"185\":1},\"1\":{\"170\":1,\"172\":1,\"185\":1,\"2468\":1}}],[\"dataset\",{\"0\":{\"171\":1,\"619\":1,\"981\":1,\"999\":1,\"1000\":1,\"1384\":1,\"1386\":1,\"2167\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2182\":1,\"2187\":1,\"2189\":1,\"2222\":1,\"2223\":1,\"2224\":1,\"2225\":1,\"2226\":1,\"2228\":1,\"2229\":1,\"2231\":1,\"2346\":1,\"2353\":1,\"2396\":1,\"2411\":1,\"2532\":1,\"2719\":1},\"1\":{\"57\":1,\"60\":6,\"74\":1,\"170\":1,\"201\":1,\"204\":1,\"233\":1,\"236\":3,\"243\":2,\"604\":1,\"607\":2,\"612\":2,\"619\":4,\"626\":1,\"627\":1,\"727\":3,\"728\":3,\"975\":1,\"976\":1,\"981\":2,\"989\":1,\"997\":3,\"998\":3,\"999\":2,\"1000\":4,\"1042\":1,\"1043\":1,\"1375\":1,\"1384\":1,\"1385\":3,\"1386\":1,\"1387\":3,\"1819\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1900\":1,\"2099\":3,\"2167\":3,\"2172\":1,\"2173\":1,\"2174\":1,\"2182\":3,\"2183\":2,\"2187\":2,\"2189\":2,\"2190\":2,\"2209\":4,\"2222\":2,\"2223\":2,\"2224\":2,\"2225\":1,\"2226\":2,\"2228\":2,\"2229\":2,\"2231\":1,\"2343\":4,\"2346\":3,\"2351\":2,\"2352\":1,\"2353\":2,\"2363\":2,\"2411\":1,\"2444\":1,\"2445\":1,\"2446\":1,\"2492\":3,\"2506\":2,\"2564\":2,\"2565\":2,\"2566\":1,\"2567\":2,\"2568\":1,\"2573\":1,\"2585\":1,\"2628\":3,\"2653\":2}}],[\"dataparallel\",{\"1\":{\"32\":1,\"2148\":1,\"2149\":1,\"2153\":2}}],[\"data\",{\"0\":{\"52\":1,\"55\":1,\"57\":1,\"60\":1,\"168\":1,\"180\":1,\"182\":1,\"185\":1,\"193\":1,\"236\":1,\"237\":1,\"287\":1,\"309\":1,\"317\":1,\"323\":1,\"329\":1,\"335\":1,\"341\":1,\"345\":1,\"347\":1,\"352\":1,\"353\":1,\"355\":1,\"359\":1,\"361\":1,\"365\":1,\"370\":1,\"371\":1,\"373\":1,\"382\":1,\"386\":1,\"393\":1,\"401\":1,\"410\":1,\"418\":1,\"424\":1,\"431\":1,\"439\":1,\"445\":1,\"451\":1,\"457\":1,\"466\":1,\"472\":1,\"480\":1,\"487\":1,\"1300\":1,\"1306\":1,\"2343\":1,\"2344\":1,\"2373\":1,\"2380\":1,\"2385\":1,\"2387\":1,\"2395\":1,\"2397\":1,\"2412\":1,\"2413\":1,\"2430\":1,\"2433\":1,\"2531\":1,\"2533\":1,\"2555\":1,\"2567\":1,\"2568\":1,\"2638\":1,\"2717\":1},\"1\":{\"16\":3,\"17\":1,\"32\":1,\"34\":1,\"47\":2,\"48\":3,\"49\":2,\"56\":2,\"57\":10,\"59\":18,\"60\":17,\"69\":2,\"74\":5,\"75\":4,\"76\":9,\"77\":4,\"78\":4,\"79\":2,\"84\":2,\"85\":2,\"91\":3,\"93\":1,\"96\":2,\"110\":1,\"130\":1,\"140\":1,\"148\":2,\"149\":1,\"161\":2,\"168\":3,\"169\":5,\"171\":3,\"174\":5,\"175\":3,\"179\":3,\"181\":4,\"183\":1,\"184\":1,\"185\":3,\"191\":1,\"193\":2,\"194\":2,\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"235\":6,\"237\":8,\"238\":13,\"239\":7,\"244\":1,\"245\":2,\"265\":2,\"269\":2,\"274\":2,\"275\":5,\"276\":3,\"279\":5,\"281\":3,\"282\":5,\"283\":5,\"284\":5,\"285\":2,\"286\":1,\"287\":1,\"290\":1,\"292\":1,\"294\":2,\"296\":1,\"297\":2,\"298\":3,\"307\":4,\"315\":4,\"321\":4,\"327\":4,\"333\":4,\"339\":4,\"343\":4,\"350\":4,\"357\":4,\"368\":4,\"380\":4,\"384\":4,\"391\":4,\"399\":4,\"408\":4,\"416\":4,\"422\":4,\"429\":4,\"437\":2,\"443\":4,\"449\":4,\"455\":4,\"464\":4,\"470\":4,\"476\":4,\"478\":4,\"485\":4,\"515\":1,\"560\":1,\"564\":7,\"618\":2,\"624\":1,\"625\":1,\"627\":1,\"629\":3,\"691\":1,\"694\":1,\"697\":1,\"727\":1,\"729\":2,\"730\":3,\"734\":1,\"744\":1,\"745\":1,\"765\":2,\"797\":1,\"798\":2,\"799\":1,\"876\":2,\"909\":2,\"936\":1,\"959\":1,\"981\":1,\"987\":3,\"998\":1,\"999\":1,\"1000\":2,\"1001\":1,\"1003\":3,\"1004\":2,\"1005\":3,\"1006\":1,\"1028\":8,\"1037\":1,\"1071\":6,\"1143\":1,\"1179\":1,\"1187\":1,\"1198\":2,\"1202\":1,\"1228\":1,\"1255\":1,\"1257\":1,\"1300\":1,\"1306\":1,\"1345\":1,\"1347\":1,\"1375\":1,\"1382\":1,\"1397\":2,\"1398\":1,\"1404\":2,\"1407\":1,\"1408\":8,\"1412\":1,\"1416\":2,\"1454\":2,\"1463\":2,\"1505\":2,\"1515\":2,\"1516\":2,\"1523\":1,\"1524\":6,\"1525\":4,\"1528\":2,\"1529\":2,\"1530\":1,\"1534\":2,\"1539\":2,\"1558\":2,\"1611\":2,\"1622\":1,\"1626\":2,\"1645\":2,\"1654\":2,\"1658\":2,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":2,\"1671\":2,\"1719\":2,\"1771\":1,\"1776\":3,\"1905\":4,\"1917\":1,\"1964\":1,\"2005\":1,\"2024\":1,\"2028\":1,\"2096\":7,\"2097\":7,\"2098\":7,\"2099\":10,\"2100\":7,\"2101\":7,\"2102\":7,\"2103\":7,\"2104\":7,\"2105\":7,\"2106\":2,\"2107\":7,\"2108\":7,\"2109\":7,\"2110\":7,\"2111\":6,\"2112\":7,\"2113\":7,\"2114\":7,\"2115\":7,\"2116\":7,\"2117\":7,\"2118\":7,\"2153\":1,\"2166\":1,\"2178\":3,\"2179\":3,\"2183\":1,\"2184\":3,\"2190\":2,\"2191\":3,\"2195\":3,\"2200\":3,\"2208\":1,\"2268\":1,\"2270\":3,\"2272\":3,\"2275\":1,\"2292\":1,\"2315\":1,\"2316\":1,\"2342\":1,\"2343\":5,\"2344\":2,\"2346\":1,\"2350\":1,\"2354\":1,\"2360\":1,\"2363\":1,\"2372\":2,\"2373\":17,\"2375\":2,\"2380\":1,\"2381\":1,\"2382\":4,\"2384\":5,\"2385\":17,\"2386\":1,\"2387\":9,\"2388\":1,\"2394\":2,\"2395\":1,\"2396\":2,\"2397\":2,\"2398\":4,\"2411\":1,\"2412\":1,\"2413\":1,\"2421\":1,\"2423\":1,\"2424\":1,\"2429\":1,\"2430\":14,\"2431\":4,\"2432\":5,\"2433\":3,\"2452\":1,\"2458\":1,\"2468\":2,\"2492\":4,\"2506\":1,\"2523\":1,\"2524\":1,\"2530\":2,\"2531\":1,\"2532\":2,\"2533\":2,\"2534\":4,\"2542\":2,\"2544\":1,\"2546\":1,\"2547\":1,\"2554\":1,\"2555\":18,\"2556\":1,\"2558\":2,\"2564\":3,\"2565\":1,\"2568\":27,\"2569\":3,\"2574\":1,\"2582\":1,\"2584\":4,\"2585\":1,\"2592\":1,\"2596\":1,\"2628\":4,\"2638\":7,\"2653\":1}}],[\"database\",{\"1\":{\"16\":1}}],[\"danseparator\",{\"0\":{\"1515\":1},\"1\":{\"1515\":1}}],[\"daniel\",{\"1\":{\"1257\":1}}],[\"dan\",{\"0\":{\"1515\":1},\"1\":{\"130\":1,\"1515\":1,\"2583\":1}}],[\"dual\",{\"1\":{\"1523\":1,\"1531\":1,\"1532\":1,\"1534\":2,\"1535\":1,\"1537\":1,\"1539\":2,\"1558\":1,\"1581\":1,\"1645\":1}}],[\"dunits=1024\",{\"1\":{\"2078\":1}}],[\"dunits\",{\"1\":{\"677\":2,\"678\":2,\"679\":2,\"680\":2,\"681\":2,\"682\":4,\"683\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"754\":1,\"758\":2,\"806\":2,\"821\":1,\"826\":1,\"892\":2,\"1289\":1,\"1778\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"2002\":2,\"2078\":1,\"2086\":2,\"2087\":2,\"2090\":2,\"2095\":2,\"2243\":2,\"2244\":2,\"2255\":1,\"2263\":2,\"2264\":2,\"2279\":2}}],[\"dutch\",{\"1\":{\"461\":1}}],[\"dummy\",{\"1\":{\"196\":1,\"200\":1,\"234\":1,\"237\":1,\"615\":1,\"677\":1,\"686\":1,\"687\":1,\"754\":3,\"791\":1,\"975\":1,\"1042\":1,\"1209\":1,\"1622\":1,\"2398\":1,\"2534\":1}}],[\"dumping\",{\"1\":{\"636\":1}}],[\"dumpdir\",{\"1\":{\"278\":1}}],[\"dumped\",{\"0\":{\"2432\":1},\"1\":{\"238\":1,\"2432\":2,\"2514\":1,\"2659\":1}}],[\"dump\",{\"0\":{\"278\":1,\"279\":1,\"525\":1,\"2315\":1,\"2342\":2,\"2343\":2,\"2344\":1,\"2433\":1},\"1\":{\"168\":1,\"169\":1,\"171\":2,\"175\":1,\"181\":1,\"185\":2,\"193\":1,\"238\":2,\"239\":3,\"253\":2,\"278\":1,\"279\":3,\"525\":3,\"734\":1,\"2315\":1,\"2342\":3,\"2343\":8,\"2344\":11,\"2347\":4,\"2349\":2,\"2350\":2,\"2351\":2,\"2373\":5,\"2415\":1,\"2416\":1,\"2429\":4,\"2430\":2,\"2431\":2,\"2432\":11,\"2433\":4,\"2492\":1,\"2514\":2,\"2555\":5,\"2584\":1,\"2628\":1,\"2659\":2}}],[\"dumps\",{\"0\":{\"2344\":1},\"1\":{\"46\":1,\"2344\":2,\"2430\":1,\"2555\":1}}],[\"duh\",{\"1\":{\"130\":1}}],[\"dur=0\",{\"1\":{\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"dur\",{\"1\":{\"249\":2,\"429\":2,\"455\":2,\"464\":2,\"470\":2,\"1778\":3,\"1798\":2,\"1804\":10,\"1805\":4,\"1877\":4,\"1878\":4,\"2090\":2}}],[\"durationpredictorloss\",{\"0\":{\"738\":1},\"1\":{\"738\":1}}],[\"durationpredictor\",{\"0\":{\"736\":1,\"1772\":1,\"1983\":1},\"1\":{\"736\":1,\"1772\":1,\"1983\":1}}],[\"durationcalculator\",{\"0\":{\"735\":1,\"2239\":1},\"1\":{\"735\":1,\"2239\":1}}],[\"duration=100\",{\"1\":{\"632\":1}}],[\"durations=\",{\"1\":{\"1336\":1}}],[\"durations\",{\"1\":{\"263\":2,\"267\":2,\"632\":1,\"735\":1,\"737\":2,\"738\":2,\"754\":2,\"755\":1,\"774\":3,\"1138\":2,\"1171\":1,\"1211\":3,\"1286\":3,\"1302\":2,\"1303\":2,\"1304\":2,\"1336\":2,\"1337\":4,\"1837\":6,\"1842\":1,\"1877\":2,\"1878\":1,\"1879\":1,\"1881\":1,\"1986\":3,\"2081\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2091\":1,\"2151\":1,\"2237\":2,\"2239\":1,\"2240\":8,\"2241\":2,\"2243\":7,\"2244\":7,\"2245\":1,\"2255\":7,\"2256\":1,\"2266\":2,\"2278\":8,\"2279\":7,\"2280\":1}}],[\"duration\",{\"0\":{\"735\":1,\"736\":1,\"738\":1,\"1772\":1,\"1868\":1,\"1881\":1,\"2081\":1,\"2239\":1},\"1\":{\"79\":1,\"110\":1,\"203\":2,\"245\":2,\"429\":2,\"632\":1,\"735\":3,\"736\":5,\"737\":2,\"738\":3,\"754\":7,\"755\":2,\"758\":3,\"1138\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1400\":2,\"1643\":1,\"1644\":1,\"1772\":4,\"1773\":40,\"1778\":11,\"1781\":9,\"1798\":2,\"1804\":6,\"1805\":8,\"1837\":6,\"1842\":1,\"1850\":6,\"1851\":15,\"1852\":4,\"1868\":5,\"1877\":9,\"1878\":16,\"1879\":2,\"1881\":3,\"1889\":2,\"1946\":1,\"1947\":1,\"1983\":2,\"2003\":3,\"2078\":1,\"2079\":1,\"2081\":3,\"2082\":48,\"2083\":1,\"2084\":4,\"2086\":9,\"2087\":23,\"2089\":6,\"2090\":31,\"2091\":2,\"2094\":1,\"2095\":11,\"2151\":2,\"2197\":3,\"2239\":3,\"2240\":7,\"2243\":18,\"2244\":17,\"2245\":2,\"2255\":17,\"2256\":2,\"2265\":1,\"2278\":7,\"2279\":17,\"2280\":2}}],[\"during\",{\"1\":{\"5\":1,\"33\":1,\"72\":1,\"80\":1,\"82\":1,\"84\":1,\"95\":1,\"106\":3,\"113\":4,\"116\":1,\"121\":2,\"122\":1,\"148\":1,\"150\":1,\"691\":1,\"693\":1,\"697\":1,\"700\":1,\"750\":1,\"754\":3,\"785\":1,\"797\":1,\"820\":2,\"821\":2,\"826\":2,\"857\":1,\"997\":1,\"998\":1,\"1048\":1,\"1057\":2,\"1093\":1,\"1138\":4,\"1139\":4,\"1187\":2,\"1202\":2,\"1243\":1,\"1269\":1,\"1286\":1,\"1287\":1,\"1719\":1,\"1778\":1,\"1805\":1,\"1850\":2,\"1852\":1,\"1877\":2,\"2243\":1,\"2259\":1,\"2260\":2,\"2384\":1,\"2430\":1,\"2555\":1,\"2558\":1}}],[\"duplicates\",{\"1\":{\"933\":1}}],[\"duplicated\",{\"1\":{\"56\":1}}],[\"duplication\",{\"1\":{\"49\":1}}],[\"due\",{\"1\":{\"14\":1,\"119\":1,\"1257\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1717\":1,\"2362\":1,\"2504\":1,\"2576\":1,\"2651\":1}}],[\"dig\",{\"1\":{\"2385\":1}}],[\"digits\",{\"1\":{\"1398\":1}}],[\"dio\",{\"0\":{\"2236\":2},\"1\":{\"2236\":6}}],[\"dic\",{\"1\":{\"2306\":1}}],[\"dicriminative\",{\"1\":{\"2303\":1}}],[\"dicussion\",{\"1\":{\"1860\":1}}],[\"dict=\",{\"1\":{\"2476\":1}}],[\"dictornone\",{\"1\":{\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1558\":1,\"1611\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":1,\"1671\":1,\"1719\":1}}],[\"dictconfig\",{\"1\":{\"1340\":1}}],[\"dicts\",{\"1\":{\"987\":1,\"2152\":1}}],[\"dictsummary\",{\"1\":{\"607\":1,\"628\":1}}],[\"dictrionary\",{\"1\":{\"239\":2}}],[\"dict\",{\"0\":{\"64\":1,\"292\":1,\"557\":1,\"635\":1,\"640\":1,\"642\":1,\"643\":1,\"913\":1,\"1328\":1,\"1355\":1,\"1966\":1,\"2152\":1,\"2313\":1,\"2316\":1,\"2324\":1},\"1\":{\"24\":2,\"56\":1,\"57\":2,\"60\":3,\"64\":2,\"84\":1,\"115\":3,\"116\":2,\"173\":1,\"174\":1,\"175\":1,\"194\":2,\"210\":1,\"211\":1,\"212\":1,\"217\":1,\"222\":1,\"223\":1,\"224\":1,\"229\":1,\"230\":1,\"231\":2,\"238\":3,\"245\":2,\"249\":2,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"276\":1,\"285\":3,\"290\":1,\"292\":1,\"298\":1,\"546\":1,\"549\":1,\"551\":1,\"554\":3,\"557\":2,\"600\":3,\"608\":1,\"609\":2,\"611\":2,\"616\":1,\"619\":3,\"620\":2,\"621\":3,\"629\":2,\"633\":5,\"635\":10,\"637\":5,\"640\":10,\"641\":1,\"642\":10,\"643\":4,\"647\":1,\"658\":8,\"674\":1,\"676\":2,\"691\":16,\"692\":4,\"693\":6,\"694\":6,\"697\":24,\"698\":4,\"699\":4,\"710\":1,\"725\":2,\"734\":4,\"742\":1,\"751\":1,\"754\":2,\"765\":5,\"792\":5,\"797\":8,\"798\":3,\"806\":2,\"824\":2,\"826\":2,\"857\":6,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"866\":1,\"909\":5,\"910\":1,\"911\":1,\"913\":4,\"918\":1,\"927\":1,\"934\":1,\"987\":5,\"1003\":4,\"1004\":4,\"1005\":3,\"1008\":1,\"1012\":1,\"1028\":5,\"1029\":1,\"1046\":3,\"1049\":1,\"1050\":1,\"1051\":1,\"1054\":1,\"1056\":1,\"1057\":1,\"1058\":3,\"1060\":1,\"1063\":2,\"1065\":1,\"1066\":13,\"1068\":1,\"1069\":1,\"1071\":1,\"1074\":1,\"1075\":2,\"1087\":2,\"1088\":2,\"1089\":2,\"1090\":1,\"1091\":2,\"1092\":1,\"1094\":1,\"1103\":2,\"1104\":1,\"1105\":2,\"1113\":1,\"1158\":1,\"1171\":1,\"1176\":1,\"1179\":2,\"1181\":3,\"1193\":2,\"1215\":1,\"1218\":1,\"1219\":2,\"1220\":1,\"1239\":1,\"1248\":1,\"1270\":2,\"1328\":2,\"1355\":3,\"1406\":1,\"1420\":1,\"1424\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1529\":1,\"1530\":1,\"1534\":1,\"1539\":1,\"1553\":1,\"1554\":1,\"1558\":1,\"1563\":1,\"1600\":3,\"1603\":2,\"1611\":1,\"1622\":2,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1669\":1,\"1671\":2,\"1719\":2,\"1739\":1,\"1761\":3,\"1763\":4,\"1765\":2,\"1773\":9,\"1778\":42,\"1791\":1,\"1800\":1,\"1801\":8,\"1803\":2,\"1805\":36,\"1834\":2,\"1837\":6,\"1844\":2,\"1845\":2,\"1846\":4,\"1847\":6,\"1848\":2,\"1849\":2,\"1850\":21,\"1851\":2,\"1852\":27,\"1856\":4,\"1857\":4,\"1858\":6,\"1861\":2,\"1862\":2,\"1866\":2,\"1867\":4,\"1870\":2,\"1871\":2,\"1876\":2,\"1877\":21,\"1895\":1,\"1905\":3,\"1930\":2,\"1963\":3,\"1964\":2,\"1966\":1,\"1967\":2,\"1968\":1,\"1980\":1,\"1984\":1,\"2001\":1,\"2002\":2,\"2014\":2,\"2015\":2,\"2016\":2,\"2017\":2,\"2022\":3,\"2077\":1,\"2082\":7,\"2086\":22,\"2087\":22,\"2090\":22,\"2095\":22,\"2099\":2,\"2102\":1,\"2125\":1,\"2132\":1,\"2137\":1,\"2152\":5,\"2153\":1,\"2170\":5,\"2176\":1,\"2182\":2,\"2185\":2,\"2189\":2,\"2191\":1,\"2193\":5,\"2196\":1,\"2198\":2,\"2199\":2,\"2201\":3,\"2203\":2,\"2208\":1,\"2209\":1,\"2235\":1,\"2240\":7,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2277\":1,\"2278\":7,\"2279\":3,\"2290\":1,\"2292\":1,\"2294\":2,\"2295\":1,\"2313\":2,\"2316\":3,\"2324\":2,\"2343\":9,\"2476\":2,\"2500\":1,\"2521\":3,\"2522\":2,\"2523\":2,\"2617\":1,\"2635\":1}}],[\"dictionary\",{\"0\":{\"239\":1},\"1\":{\"16\":1,\"59\":1,\"235\":1,\"239\":2,\"551\":1,\"557\":1,\"607\":2,\"608\":2,\"619\":1,\"621\":1,\"626\":1,\"727\":2,\"728\":2,\"1003\":1,\"1004\":1,\"1005\":2,\"1028\":2,\"1181\":1,\"1327\":2,\"1355\":1,\"1739\":1,\"2125\":1,\"2343\":1,\"2373\":2,\"2433\":1,\"2476\":1,\"2555\":2}}],[\"dildcunet\",{\"1\":{\"1517\":1}}],[\"dilateddepthseparableconv\",{\"0\":{\"1835\":1},\"1\":{\"1835\":2}}],[\"dilated\",{\"1\":{\"708\":1,\"835\":1,\"1782\":1,\"1835\":1,\"1856\":1,\"1857\":2,\"1858\":1,\"1862\":1,\"1867\":1,\"1880\":1}}],[\"dilations=\",{\"1\":{\"1800\":1,\"2074\":1,\"2075\":1}}],[\"dilations\",{\"1\":{\"1761\":1,\"1763\":1,\"1765\":3,\"1778\":1,\"1782\":1,\"1795\":1,\"1800\":1,\"1803\":3,\"1804\":3,\"1805\":2,\"1844\":3,\"1850\":1,\"1851\":3,\"1852\":1,\"1866\":2,\"1877\":1,\"1878\":3,\"2070\":1,\"2258\":1}}],[\"dilation=none\",{\"1\":{\"1501\":1,\"2042\":1,\"2047\":1}}],[\"dilation=1\",{\"1\":{\"708\":1,\"1478\":1,\"1629\":1,\"1656\":1,\"1687\":1,\"1689\":1,\"1691\":1,\"1731\":1,\"1732\":1,\"1815\":1}}],[\"dilation\",{\"1\":{\"21\":2,\"115\":1,\"712\":2,\"835\":7,\"1052\":2,\"1203\":3,\"1370\":1,\"1378\":1,\"1518\":1,\"1520\":1,\"1546\":1,\"1656\":2,\"1663\":1,\"1800\":1,\"1804\":6,\"1805\":2,\"1806\":1,\"1808\":3,\"1830\":1,\"1831\":1,\"1832\":1,\"1861\":5,\"1862\":1,\"1863\":3,\"1864\":3,\"1865\":3,\"1866\":2,\"1867\":4,\"1871\":3,\"1873\":1,\"1877\":2,\"1878\":6,\"1880\":4,\"1917\":2,\"2258\":2,\"2290\":1,\"2292\":1}}],[\"diff\",{\"1\":{\"2500\":10,\"2617\":10,\"2635\":10}}],[\"difflib\",{\"1\":{\"2500\":2,\"2617\":2,\"2635\":2}}],[\"diffwave\",{\"1\":{\"1547\":1}}],[\"diffusionstepembedding\",{\"0\":{\"1547\":1},\"1\":{\"1547\":1}}],[\"diffusion\",{\"0\":{\"1433\":2,\"1451\":1,\"1514\":1,\"1551\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1623\":1,\"1637\":1,\"1638\":1,\"1646\":2,\"1679\":1},\"1\":{\"1433\":2,\"1451\":1,\"1514\":1,\"1547\":1,\"1551\":2,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1618\":2,\"1619\":2,\"1623\":1,\"1637\":1,\"1638\":3,\"1646\":4,\"1679\":2,\"2255\":7,\"2258\":2,\"2260\":5}}],[\"differ\",{\"1\":{\"112\":1,\"629\":1,\"799\":1}}],[\"differs\",{\"1\":{\"99\":1}}],[\"differencefunctiontorch\",{\"0\":{\"2271\":1},\"1\":{\"2271\":1}}],[\"differencefunction\",{\"0\":{\"2270\":1,\"2272\":1},\"1\":{\"2270\":1,\"2271\":1,\"2272\":1}}],[\"differences\",{\"1\":{\"24\":1,\"2412\":1,\"2419\":1,\"2420\":1,\"2461\":1,\"2569\":1}}],[\"difference\",{\"1\":{\"23\":1,\"82\":2,\"119\":1,\"700\":1,\"917\":1,\"1048\":1,\"1138\":1,\"1139\":1,\"1400\":1,\"2021\":1,\"2153\":1,\"2267\":2,\"2268\":3,\"2270\":2,\"2272\":2,\"2387\":1,\"2441\":1}}],[\"differents\",{\"1\":{\"2585\":1}}],[\"differentiating\",{\"1\":{\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1,\"2452\":1}}],[\"differentiation\",{\"1\":{\"745\":1,\"746\":1,\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1}}],[\"differentiable\",{\"1\":{\"745\":2,\"746\":2,\"875\":1,\"950\":1,\"956\":2,\"968\":1,\"973\":2}}],[\"different\",{\"1\":{\"4\":1,\"21\":3,\"30\":1,\"33\":1,\"46\":1,\"72\":1,\"79\":1,\"95\":1,\"102\":2,\"109\":2,\"120\":1,\"148\":1,\"164\":1,\"689\":1,\"737\":1,\"743\":1,\"1066\":1,\"1245\":1,\"1357\":1,\"1390\":1,\"1392\":1,\"1409\":1,\"1551\":3,\"1553\":3,\"1670\":1,\"1671\":1,\"1917\":1,\"1951\":1,\"2046\":1,\"2253\":1,\"2371\":1,\"2384\":1,\"2395\":2,\"2419\":1,\"2450\":1,\"2451\":1,\"2482\":1,\"2494\":1,\"2508\":1,\"2510\":1,\"2531\":2,\"2564\":1,\"2584\":1,\"2585\":3,\"2612\":1,\"2630\":1}}],[\"dialog\",{\"1\":{\"2504\":1}}],[\"dialect\",{\"1\":{\"2125\":1}}],[\"diag=none\",{\"1\":{\"1639\":1}}],[\"diag\",{\"1\":{\"1245\":8,\"1248\":1,\"1524\":1,\"1525\":1,\"1611\":2,\"1639\":1,\"1696\":2,\"1697\":2,\"1698\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1712\":2,\"1714\":1,\"1715\":2}}],[\"diagonalize\",{\"1\":{\"1339\":1}}],[\"diagonal=true\",{\"1\":{\"1314\":1}}],[\"diagonal\",{\"1\":{\"762\":2,\"763\":2,\"803\":1,\"1247\":1,\"1248\":1,\"1524\":1,\"1525\":1,\"1611\":1,\"1639\":1,\"1696\":3,\"1697\":3,\"1698\":3,\"1704\":3,\"1705\":3,\"1706\":3,\"1707\":3,\"1708\":3,\"1712\":3,\"1714\":1,\"1715\":3,\"1719\":2}}],[\"diarize\",{\"0\":{\"348\":1}}],[\"diarizationtask\",{\"0\":{\"2100\":1},\"1\":{\"2100\":2}}],[\"diarization\",{\"1\":{\"343\":1,\"1114\":1,\"1371\":2,\"1373\":1,\"1374\":1,\"1376\":1,\"1377\":1,\"1551\":1,\"1553\":1,\"2046\":1,\"2288\":1,\"2385\":1}}],[\"diar\",{\"0\":{\"343\":1,\"349\":1,\"1360\":1,\"1362\":1,\"1364\":2,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":1,\"2100\":1,\"2686\":1},\"1\":{\"343\":4,\"397\":1,\"398\":2,\"1113\":1,\"1360\":1,\"1362\":1,\"1364\":4,\"1366\":2,\"1368\":1,\"1369\":2,\"1370\":1,\"1371\":3,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":2,\"1376\":1,\"1377\":2,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":2,\"1552\":2,\"2100\":2}}],[\"dispatcher\",{\"1\":{\"1351\":1}}],[\"display\",{\"1\":{\"194\":1,\"201\":3,\"202\":3,\"218\":3,\"225\":3,\"232\":3,\"240\":7,\"333\":2,\"1085\":1,\"2359\":5,\"2360\":7,\"2365\":3,\"2367\":3,\"2369\":2,\"2372\":5,\"2386\":4,\"2415\":3,\"2416\":3,\"2440\":9,\"2456\":5,\"2458\":7,\"2460\":5,\"2470\":3,\"2476\":3,\"2478\":3,\"2485\":3,\"2487\":2,\"2491\":2,\"2497\":5,\"2501\":4,\"2508\":3,\"2510\":3,\"2515\":3,\"2521\":7,\"2522\":7,\"2523\":9,\"2564\":3,\"2572\":4,\"2580\":5,\"2581\":5,\"2582\":7,\"2604\":3,\"2606\":2,\"2607\":4,\"2610\":2,\"2614\":5,\"2615\":5,\"2621\":3,\"2623\":2,\"2624\":4,\"2627\":2,\"2632\":5,\"2633\":5,\"2647\":3,\"2655\":3,\"2660\":3}}],[\"discussion\",{\"1\":{\"2400\":1,\"2536\":1}}],[\"discussed\",{\"1\":{\"2395\":1,\"2531\":1}}],[\"discuss\",{\"1\":{\"1973\":1,\"2387\":1,\"2418\":1,\"2479\":1,\"2501\":1,\"2508\":1,\"2510\":1}}],[\"disciminator\",{\"1\":{\"1870\":1}}],[\"disc\",{\"1\":{\"1763\":1,\"1801\":2}}],[\"discrimininative\",{\"1\":{\"2302\":1}}],[\"discriminatoraversarialloss\",{\"1\":{\"1836\":1}}],[\"discriminatoradversarialloss\",{\"0\":{\"1836\":1},\"1\":{\"1836\":1}}],[\"discriminators\",{\"1\":{\"1778\":3,\"1805\":3,\"1836\":3,\"1839\":3,\"1843\":3,\"1846\":1,\"1847\":1,\"1850\":3,\"1852\":3,\"1870\":1,\"1877\":3}}],[\"discriminator\",{\"0\":{\"2283\":2,\"2290\":2,\"2297\":2,\"2301\":1},\"1\":{\"1760\":1,\"1761\":1,\"1763\":1,\"1766\":3,\"1767\":1,\"1768\":1,\"1773\":1,\"1778\":12,\"1786\":5,\"1793\":1,\"1795\":1,\"1801\":10,\"1805\":14,\"1828\":1,\"1836\":10,\"1837\":1,\"1839\":5,\"1843\":3,\"1845\":5,\"1846\":7,\"1847\":10,\"1848\":1,\"1849\":2,\"1850\":13,\"1851\":1,\"1852\":12,\"1856\":1,\"1858\":2,\"1861\":1,\"1870\":4,\"1877\":13,\"2170\":3,\"2283\":2,\"2290\":3,\"2294\":2,\"2297\":2,\"2301\":2,\"2302\":2,\"2303\":1,\"2305\":1}}],[\"discriminative\",{\"1\":{\"1529\":1,\"1568\":1,\"2294\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":2}}],[\"discrimiantor\",{\"1\":{\"2170\":4}}],[\"discretize\",{\"1\":{\"1638\":2}}],[\"discretized\",{\"1\":{\"1248\":1}}],[\"discretization\",{\"1\":{\"1618\":1,\"1619\":1,\"1638\":2}}],[\"discrete\",{\"0\":{\"405\":1,\"1172\":1,\"2001\":1,\"2279\":1},\"1\":{\"1172\":1,\"2001\":7,\"2004\":5,\"2275\":1,\"2277\":1,\"2278\":13,\"2279\":10,\"2280\":1,\"2281\":1,\"2452\":1,\"2518\":1,\"2519\":1,\"2520\":2}}],[\"disc=\",{\"1\":{\"1247\":1}}],[\"discarded\",{\"1\":{\"745\":1,\"746\":1,\"1255\":1}}],[\"discard\",{\"1\":{\"84\":1,\"1896\":1}}],[\"discarding\",{\"1\":{\"84\":2}}],[\"disk\",{\"1\":{\"148\":1,\"2373\":1,\"2430\":2,\"2431\":1,\"2555\":2}}],[\"disributed\",{\"1\":{\"49\":1}}],[\"disabled\",{\"1\":{\"60\":1,\"1061\":1,\"2363\":1,\"2506\":1,\"2653\":1}}],[\"disable\",{\"1\":{\"34\":1,\"70\":1,\"134\":1,\"327\":2,\"449\":2,\"692\":1,\"1406\":1,\"2461\":1,\"2592\":1}}],[\"distortions\",{\"1\":{\"1717\":1}}],[\"distortion=none\",{\"1\":{\"1524\":1}}],[\"distortion\",{\"1\":{\"1466\":1,\"1524\":1,\"1604\":1,\"1639\":2,\"1655\":1,\"1712\":2,\"1715\":4,\"1719\":1,\"1739\":2,\"1923\":1,\"1925\":1}}],[\"distortionless\",{\"1\":{\"885\":1,\"1696\":1,\"1698\":1,\"1706\":1,\"1707\":1}}],[\"distinguished\",{\"1\":{\"2573\":1}}],[\"distinguish\",{\"1\":{\"1149\":1,\"1150\":1}}],[\"distinction\",{\"1\":{\"57\":1}}],[\"dists\",{\"1\":{\"1010\":2}}],[\"distances\",{\"1\":{\"1010\":1}}],[\"distance\",{\"0\":{\"1010\":1},\"1\":{\"1010\":2,\"2040\":1,\"2500\":1,\"2617\":1,\"2635\":1}}],[\"distribution\",{\"1\":{\"632\":1,\"896\":1,\"905\":1,\"931\":1,\"1618\":3,\"1619\":3,\"1638\":3,\"1757\":1,\"2151\":1,\"2156\":1}}],[\"distributions\",{\"0\":{\"133\":1},\"1\":{\"133\":1,\"1269\":2}}],[\"distribute\",{\"1\":{\"148\":1}}],[\"distributedoption\",{\"0\":{\"2180\":1},\"1\":{\"2099\":4,\"2102\":1,\"2180\":1,\"2185\":2,\"2198\":2,\"2201\":3,\"2203\":2}}],[\"distributeddictsummary\",{\"0\":{\"628\":1},\"1\":{\"628\":1}}],[\"distributeddataparallel\",{\"1\":{\"32\":4,\"84\":1}}],[\"distributed\",{\"0\":{\"32\":1,\"34\":1,\"36\":1,\"40\":1,\"42\":1,\"94\":1,\"433\":1,\"593\":1,\"594\":1,\"595\":1,\"596\":1,\"598\":1,\"2180\":1,\"2212\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2227\":2,\"2668\":1},\"1\":{\"32\":5,\"34\":5,\"35\":1,\"36\":3,\"38\":1,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"48\":1,\"71\":2,\"84\":1,\"94\":2,\"127\":1,\"148\":1,\"377\":3,\"429\":2,\"593\":1,\"594\":1,\"595\":1,\"596\":1,\"598\":1,\"628\":1,\"870\":1,\"875\":1,\"2099\":4,\"2102\":1,\"2106\":2,\"2161\":1,\"2163\":1,\"2180\":6,\"2185\":2,\"2198\":2,\"2201\":3,\"2203\":2,\"2212\":2,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":3,\"2217\":3,\"2218\":2,\"2219\":1,\"2220\":1,\"2221\":1,\"2227\":2}}],[\"dist\",{\"0\":{\"896\":1},\"1\":{\"32\":3,\"36\":8,\"37\":2,\"38\":6,\"39\":12,\"40\":3,\"41\":3,\"42\":3,\"429\":13,\"896\":1,\"2180\":14,\"2500\":2,\"2617\":2,\"2635\":2}}],[\"diversity\",{\"0\":{\"2303\":1},\"1\":{\"2303\":2}}],[\"diverse\",{\"1\":{\"1670\":1,\"1671\":1,\"2046\":1}}],[\"divergence\",{\"1\":{\"22\":4,\"825\":4,\"933\":1,\"1805\":1,\"1853\":3,\"1854\":2,\"1877\":1}}],[\"divisor\",{\"1\":{\"2013\":1}}],[\"divisors=\",{\"1\":{\"1766\":1,\"1786\":1}}],[\"divisors\",{\"1\":{\"1763\":1,\"1766\":2,\"1786\":2,\"1801\":1}}],[\"divisible\",{\"1\":{\"1517\":2}}],[\"dividing\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1}}],[\"divide\",{\"0\":{\"2162\":1},\"1\":{\"778\":1,\"1245\":1,\"1248\":1,\"1810\":1,\"2162\":1}}],[\"divided\",{\"1\":{\"91\":1,\"1211\":1,\"1224\":1,\"1336\":1,\"1348\":1}}],[\"div\",{\"0\":{\"1307\":1},\"1\":{\"22\":2,\"825\":6,\"933\":2,\"1307\":1}}],[\"dimention\",{\"1\":{\"1660\":1,\"1661\":1}}],[\"dimenstions\",{\"1\":{\"821\":2,\"826\":1}}],[\"dimenstion\",{\"1\":{\"801\":1}}],[\"dimensionality\",{\"1\":{\"1245\":1,\"1659\":1,\"1665\":1,\"2030\":1,\"2044\":2,\"2049\":1,\"2052\":1,\"2055\":1,\"2064\":1,\"2068\":2,\"2070\":1}}],[\"dimensional\",{\"1\":{\"741\":3,\"776\":3,\"875\":1,\"1069\":1,\"1211\":1,\"1224\":1,\"1287\":1,\"1336\":1,\"1348\":1}}],[\"dimensions\",{\"1\":{\"731\":3,\"754\":2,\"826\":1,\"898\":2,\"1011\":1,\"1198\":1,\"1248\":1,\"1255\":1,\"1352\":1,\"1662\":1,\"1679\":1,\"1735\":2,\"1971\":1,\"2258\":1,\"2264\":1}}],[\"dimension\",{\"1\":{\"21\":8,\"22\":1,\"73\":1,\"74\":2,\"75\":1,\"102\":1,\"115\":6,\"238\":1,\"239\":1,\"646\":2,\"677\":1,\"678\":1,\"679\":1,\"680\":3,\"681\":1,\"682\":2,\"683\":3,\"684\":1,\"685\":1,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"701\":2,\"703\":4,\"711\":1,\"712\":2,\"714\":2,\"715\":2,\"716\":2,\"718\":2,\"719\":2,\"720\":2,\"721\":2,\"722\":2,\"725\":1,\"726\":1,\"730\":1,\"731\":2,\"732\":1,\"737\":1,\"742\":4,\"743\":1,\"747\":3,\"748\":1,\"749\":1,\"754\":2,\"758\":1,\"764\":1,\"766\":4,\"770\":1,\"772\":1,\"793\":1,\"801\":2,\"802\":2,\"804\":2,\"805\":1,\"806\":2,\"808\":1,\"810\":1,\"813\":1,\"818\":1,\"821\":4,\"825\":5,\"826\":3,\"827\":1,\"835\":1,\"858\":2,\"867\":1,\"870\":1,\"878\":1,\"892\":1,\"899\":1,\"900\":1,\"901\":1,\"902\":1,\"909\":2,\"910\":1,\"911\":1,\"934\":1,\"1001\":1,\"1005\":2,\"1028\":2,\"1052\":2,\"1053\":1,\"1130\":1,\"1132\":1,\"1133\":1,\"1141\":1,\"1142\":2,\"1145\":1,\"1148\":2,\"1149\":1,\"1150\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1170\":1,\"1178\":1,\"1180\":1,\"1181\":2,\"1186\":1,\"1190\":1,\"1198\":2,\"1200\":1,\"1203\":2,\"1208\":1,\"1210\":1,\"1228\":1,\"1241\":1,\"1243\":2,\"1244\":1,\"1251\":2,\"1252\":1,\"1253\":4,\"1254\":3,\"1255\":1,\"1269\":2,\"1270\":1,\"1272\":1,\"1274\":1,\"1276\":1,\"1279\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1,\"1345\":1,\"1347\":1,\"1377\":2,\"1430\":3,\"1447\":1,\"1449\":1,\"1454\":1,\"1462\":1,\"1463\":1,\"1470\":2,\"1471\":1,\"1478\":1,\"1505\":2,\"1515\":3,\"1516\":2,\"1522\":1,\"1523\":1,\"1525\":1,\"1528\":3,\"1529\":3,\"1531\":3,\"1532\":3,\"1534\":2,\"1535\":3,\"1537\":3,\"1539\":2,\"1558\":3,\"1577\":1,\"1581\":2,\"1588\":1,\"1590\":1,\"1598\":1,\"1602\":2,\"1626\":2,\"1645\":2,\"1648\":2,\"1650\":2,\"1652\":3,\"1654\":2,\"1658\":2,\"1659\":3,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":2,\"1670\":5,\"1671\":5,\"1679\":1,\"1741\":1,\"1771\":2,\"1778\":2,\"1785\":1,\"1787\":1,\"1788\":1,\"1798\":1,\"1804\":1,\"1805\":2,\"1829\":2,\"1850\":2,\"1851\":3,\"1852\":2,\"1874\":1,\"1877\":2,\"1878\":1,\"1917\":1,\"1935\":1,\"1943\":1,\"1971\":1,\"2001\":2,\"2002\":5,\"2004\":2,\"2012\":1,\"2029\":1,\"2049\":2,\"2050\":1,\"2054\":2,\"2055\":2,\"2064\":2,\"2070\":2,\"2078\":2,\"2086\":5,\"2087\":5,\"2090\":5,\"2095\":5,\"2153\":2,\"2243\":3,\"2244\":3,\"2253\":1,\"2255\":3,\"2257\":1,\"2260\":2,\"2261\":2,\"2262\":2,\"2263\":5,\"2264\":4,\"2265\":1,\"2279\":3,\"2439\":1}}],[\"dim0\",{\"1\":{\"1427\":1}}],[\"dim=0\",{\"1\":{\"1740\":1}}],[\"dim=48\",{\"1\":{\"1660\":1,\"1661\":1,\"1662\":1}}],[\"dim=481\",{\"1\":{\"1462\":1}}],[\"dim=4\",{\"1\":{\"1586\":1}}],[\"dim=1\",{\"1\":{\"1586\":1,\"1797\":1}}],[\"dim=100\",{\"1\":{\"1184\":1,\"1774\":1}}],[\"dim=512\",{\"1\":{\"1578\":1,\"1579\":1,\"1660\":1,\"1661\":1,\"2081\":1,\"2083\":1}}],[\"dim=\",{\"1\":{\"899\":1,\"901\":1,\"1129\":1,\"1685\":1,\"1710\":1,\"2498\":3,\"2616\":3,\"2634\":3}}],[\"dim=none\",{\"1\":{\"770\":1,\"1263\":1,\"1518\":1,\"1520\":1,\"1609\":1,\"1631\":1,\"1633\":1,\"1635\":1}}],[\"dim=256\",{\"1\":{\"747\":1,\"749\":1,\"1430\":1,\"1470\":1,\"1670\":1}}],[\"dims\",{\"1\":{\"717\":1,\"769\":1,\"777\":1,\"1517\":2,\"2260\":3}}],[\"dim1\",{\"1\":{\"75\":1,\"1474\":1,\"1531\":2,\"1875\":1}}],[\"dim2\",{\"1\":{\"60\":1,\"75\":1,\"1474\":1,\"1531\":2,\"1875\":1}}],[\"dim\",{\"0\":{\"1295\":1},\"1\":{\"22\":1,\"60\":2,\"171\":2,\"175\":2,\"562\":2,\"676\":1,\"677\":2,\"678\":2,\"679\":2,\"680\":2,\"681\":2,\"682\":2,\"683\":2,\"684\":2,\"685\":6,\"686\":4,\"687\":4,\"688\":4,\"689\":4,\"690\":1,\"702\":2,\"717\":2,\"735\":1,\"749\":2,\"754\":4,\"758\":2,\"760\":1,\"770\":2,\"777\":2,\"781\":1,\"800\":1,\"806\":2,\"821\":7,\"825\":10,\"826\":5,\"858\":1,\"863\":1,\"899\":1,\"901\":1,\"911\":2,\"1010\":1,\"1019\":1,\"1028\":1,\"1037\":1,\"1039\":1,\"1040\":1,\"1066\":1,\"1106\":3,\"1108\":3,\"1115\":12,\"1133\":1,\"1149\":1,\"1150\":1,\"1166\":1,\"1171\":2,\"1178\":1,\"1179\":4,\"1180\":1,\"1181\":1,\"1182\":3,\"1200\":1,\"1206\":2,\"1220\":1,\"1241\":2,\"1244\":2,\"1245\":2,\"1248\":2,\"1252\":1,\"1254\":1,\"1269\":6,\"1272\":1,\"1289\":1,\"1295\":2,\"1373\":2,\"1375\":4,\"1377\":8,\"1430\":1,\"1436\":1,\"1454\":2,\"1455\":1,\"1460\":4,\"1462\":1,\"1463\":2,\"1464\":1,\"1470\":3,\"1471\":2,\"1482\":2,\"1501\":2,\"1505\":2,\"1511\":1,\"1512\":2,\"1515\":2,\"1516\":2,\"1517\":1,\"1522\":2,\"1523\":2,\"1528\":2,\"1529\":2,\"1534\":2,\"1539\":2,\"1547\":1,\"1552\":2,\"1555\":1,\"1558\":8,\"1560\":4,\"1561\":2,\"1578\":1,\"1579\":1,\"1580\":1,\"1586\":1,\"1594\":1,\"1596\":2,\"1602\":2,\"1607\":1,\"1611\":1,\"1617\":1,\"1626\":2,\"1629\":2,\"1644\":1,\"1645\":3,\"1654\":2,\"1658\":6,\"1659\":14,\"1660\":4,\"1661\":4,\"1662\":3,\"1665\":4,\"1669\":2,\"1670\":1,\"1671\":4,\"1674\":2,\"1771\":3,\"1778\":5,\"1781\":6,\"1787\":3,\"1788\":3,\"1795\":1,\"1797\":1,\"1798\":5,\"1804\":4,\"1805\":3,\"1850\":2,\"1851\":4,\"1852\":1,\"1874\":5,\"1877\":3,\"1878\":4,\"1910\":2,\"1914\":1,\"1915\":1,\"1940\":1,\"1971\":3,\"1983\":1,\"1986\":2,\"2001\":4,\"2002\":6,\"2003\":1,\"2004\":4,\"2029\":1,\"2032\":1,\"2064\":1,\"2081\":1,\"2083\":1,\"2084\":2,\"2086\":8,\"2087\":9,\"2090\":8,\"2095\":8,\"2243\":4,\"2244\":4,\"2253\":3,\"2255\":4,\"2261\":3,\"2262\":6,\"2263\":6,\"2264\":6,\"2279\":4,\"2290\":1,\"2292\":2,\"2304\":2}}],[\"dirname\",{\"1\":{\"2514\":1,\"2568\":1,\"2659\":1}}],[\"dirs\",{\"0\":{\"299\":1},\"1\":{\"299\":2,\"2431\":1}}],[\"dir=exp\",{\"1\":{\"2375\":1}}],[\"dir=none\",{\"1\":{\"2350\":2}}],[\"dir=\",{\"1\":{\"134\":1}}],[\"direct\",{\"1\":{\"1581\":1,\"1639\":1,\"2001\":1,\"2002\":1,\"2003\":2,\"2452\":1,\"2494\":1,\"2630\":1}}],[\"direction\",{\"1\":{\"1406\":1,\"1834\":1,\"1876\":1,\"1940\":1}}],[\"directions\",{\"1\":{\"701\":1}}],[\"directly\",{\"1\":{\"25\":1,\"27\":1,\"84\":1,\"92\":1,\"144\":1,\"1187\":1,\"1198\":1,\"1202\":1,\"2086\":1,\"2087\":1,\"2270\":1,\"2272\":1,\"2414\":1,\"2415\":1,\"2418\":1,\"2419\":2,\"2420\":1,\"2459\":1,\"2461\":1,\"2462\":1,\"2467\":2,\"2468\":1,\"2473\":1,\"2569\":1,\"2588\":1}}],[\"directories\",{\"0\":{\"169\":1},\"1\":{\"3\":2,\"169\":1,\"237\":2,\"299\":1,\"2385\":1,\"2430\":1,\"2555\":1,\"2566\":1,\"2638\":1}}],[\"directory\",{\"0\":{\"15\":1,\"96\":1,\"181\":1,\"2566\":1,\"2637\":1},\"1\":{\"1\":2,\"16\":4,\"47\":2,\"57\":1,\"85\":4,\"96\":4,\"108\":1,\"124\":1,\"135\":1,\"197\":2,\"198\":2,\"201\":1,\"235\":4,\"236\":1,\"237\":1,\"238\":5,\"239\":1,\"240\":1,\"277\":1,\"286\":2,\"296\":2,\"299\":1,\"629\":1,\"1178\":1,\"1180\":1,\"1382\":1,\"1962\":1,\"2344\":2,\"2347\":1,\"2349\":1,\"2372\":2,\"2373\":2,\"2382\":1,\"2384\":1,\"2385\":3,\"2387\":1,\"2411\":1,\"2429\":2,\"2430\":2,\"2492\":1,\"2514\":1,\"2554\":2,\"2555\":2,\"2558\":1,\"2559\":1,\"2564\":1,\"2566\":1,\"2584\":1,\"2599\":1,\"2628\":1,\"2637\":2,\"2638\":1,\"2659\":1}}],[\"dir\",{\"0\":{\"287\":1},\"1\":{\"3\":1,\"47\":2,\"51\":1,\"54\":4,\"102\":1,\"108\":4,\"110\":1,\"135\":1,\"194\":3,\"203\":2,\"207\":1,\"247\":2,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"274\":1,\"275\":7,\"276\":1,\"277\":2,\"279\":7,\"280\":1,\"281\":1,\"282\":7,\"283\":7,\"284\":7,\"286\":1,\"287\":1,\"290\":1,\"292\":1,\"294\":2,\"296\":1,\"297\":2,\"298\":1,\"299\":4,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"363\":2,\"368\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":2,\"441\":2,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"501\":2,\"536\":1,\"619\":1,\"909\":1,\"1178\":2,\"1179\":3,\"1180\":2,\"1214\":1,\"1215\":1,\"1239\":1,\"1284\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1791\":1,\"1962\":2,\"1964\":1,\"2186\":1,\"2193\":1,\"2198\":1,\"2201\":1,\"2202\":2,\"2204\":1,\"2343\":2,\"2344\":2,\"2350\":2,\"2351\":2,\"2396\":1,\"2431\":2,\"2432\":5,\"2450\":1,\"2514\":1,\"2517\":1,\"2568\":2,\"2638\":2,\"2659\":3}}],[\"deemphasis\",{\"0\":{\"1935\":1},\"1\":{\"1935\":2}}],[\"deep\",{\"1\":{\"762\":1,\"1252\":2,\"1352\":1,\"1515\":2,\"1523\":1,\"1528\":2,\"1529\":3,\"1532\":1,\"1535\":1,\"1566\":1,\"1568\":3,\"2030\":1,\"2079\":1,\"2354\":1,\"2388\":1,\"2421\":1,\"2524\":1,\"2544\":1}}],[\"deeplearning\",{\"1\":{\"45\":1}}],[\"deriving\",{\"1\":{\"1964\":1}}],[\"derive\",{\"1\":{\"1963\":1,\"2046\":1}}],[\"derived\",{\"1\":{\"135\":1,\"2019\":1}}],[\"derivative=0\",{\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"derivatives\",{\"1\":{\"1886\":1,\"1887\":1,\"1888\":1}}],[\"dereverberated\",{\"1\":{\"2638\":2}}],[\"dereverberation\",{\"1\":{\"1525\":1,\"1670\":2,\"1671\":5,\"1696\":1,\"1698\":1,\"2640\":1}}],[\"dereverb1\",{\"1\":{\"1611\":2}}],[\"dereverb\",{\"1\":{\"1466\":1,\"1553\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":2,\"1571\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1667\":2,\"1668\":1,\"1671\":4,\"2184\":2,\"2200\":2,\"2638\":1,\"2640\":1}}],[\"deocoder\",{\"1\":{\"754\":2,\"826\":2}}],[\"deng\",{\"1\":{\"2030\":1,\"2586\":1}}],[\"denoised\",{\"1\":{\"2260\":1,\"2501\":1}}],[\"denoise\",{\"1\":{\"2260\":1}}],[\"denoiser\",{\"0\":{\"2252\":1,\"2258\":1,\"2260\":1,\"2274\":1},\"1\":{\"2252\":2,\"2255\":2,\"2258\":3,\"2260\":2,\"2274\":1}}],[\"denoising\",{\"1\":{\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"1670\":2,\"1671\":5,\"1696\":1,\"1698\":1,\"1712\":3,\"1715\":3,\"2260\":1,\"2441\":1}}],[\"denoting\",{\"1\":{\"1735\":1}}],[\"denotes\",{\"1\":{\"1248\":1}}],[\"denoted\",{\"1\":{\"1241\":2,\"1251\":1}}],[\"denom\",{\"1\":{\"1186\":4,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":3,\"1344\":1,\"1346\":1}}],[\"denominator\",{\"1\":{\"1047\":1,\"1072\":1,\"1080\":1,\"1098\":1,\"1186\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":2}}],[\"denorm\",{\"1\":{\"242\":2}}],[\"dense=\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"dense=32\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"densenet\",{\"1\":{\"1543\":3,\"1655\":2,\"1719\":2}}],[\"denseblock\",{\"0\":{\"1543\":1},\"1\":{\"1543\":2}}],[\"denselyconnectedblock\",{\"0\":{\"1545\":1},\"1\":{\"1522\":12,\"1523\":12,\"1545\":2}}],[\"densely\",{\"1\":{\"1522\":1,\"1523\":1,\"1545\":1}}],[\"dense\",{\"0\":{\"1009\":1,\"1011\":1,\"1542\":1},\"1\":{\"1009\":1,\"1011\":3,\"1136\":1,\"1245\":1,\"1427\":1,\"1542\":1,\"1543\":2,\"1655\":2,\"1719\":2,\"1996\":1,\"2301\":2,\"2303\":2,\"2305\":4}}],[\"density\",{\"0\":{\"887\":1,\"1711\":1},\"1\":{\"887\":2,\"1618\":2,\"1619\":2,\"1638\":2,\"1711\":2}}],[\"denisov\",{\"1\":{\"130\":1}}],[\"de\",{\"1\":{\"242\":1,\"274\":1,\"294\":1,\"1800\":1,\"1935\":3,\"2457\":6}}],[\"dewy9isz6qb9zs1ur\",{\"1\":{\"210\":1}}],[\"del\",{\"1\":{\"2444\":1,\"2445\":1,\"2446\":1}}],[\"deliberation\",{\"1\":{\"2473\":1,\"2474\":2,\"2649\":2}}],[\"deliberationencoder\",{\"1\":{\"2027\":1}}],[\"delimiter\",{\"1\":{\"461\":2,\"1406\":1,\"2136\":1,\"2137\":1,\"2178\":1,\"2179\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2196\":1}}],[\"delta=0\",{\"1\":{\"1842\":1}}],[\"delta\",{\"0\":{\"964\":1},\"1\":{\"959\":1,\"964\":2,\"1245\":1}}],[\"deltas\",{\"0\":{\"939\":1,\"963\":2,\"964\":1},\"1\":{\"939\":2,\"963\":4,\"964\":2,\"1012\":1}}],[\"deltafalse\",{\"1\":{\"171\":2,\"175\":1,\"185\":1,\"193\":1}}],[\"delegate\",{\"1\":{\"1951\":1,\"2168\":1,\"2170\":1}}],[\"delegates\",{\"1\":{\"745\":1,\"746\":1}}],[\"deleforge\",{\"1\":{\"1132\":1}}],[\"delete\",{\"1\":{\"503\":1,\"2500\":1,\"2617\":1,\"2635\":1}}],[\"deletion\",{\"1\":{\"150\":1}}],[\"delay=3\",{\"1\":{\"962\":1,\"1758\":1}}],[\"delay\",{\"1\":{\"251\":2,\"730\":1,\"756\":1,\"1158\":1,\"1239\":1,\"1525\":1,\"1611\":1,\"1701\":3,\"1737\":1,\"1739\":1,\"1741\":1,\"1758\":2,\"1759\":3,\"1791\":1}}],[\"dell\",{\"1\":{\"45\":1}}],[\"debian11\",{\"1\":{\"133\":1}}],[\"debugdir\",{\"1\":{\"251\":2,\"255\":2,\"259\":2}}],[\"debugmode\",{\"1\":{\"245\":2,\"247\":2,\"249\":2,\"251\":2,\"253\":2,\"255\":2,\"257\":2,\"259\":2,\"261\":2,\"263\":2,\"265\":2,\"267\":2,\"269\":2}}],[\"debug\",{\"1\":{\"98\":1,\"299\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"339\":1,\"343\":1,\"350\":1,\"357\":1,\"363\":1,\"368\":1,\"375\":1,\"380\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"416\":1,\"422\":1,\"429\":1,\"437\":1,\"441\":1,\"443\":1,\"449\":1,\"455\":1,\"461\":1,\"464\":1,\"470\":1,\"476\":1,\"478\":1,\"485\":1,\"491\":1,\"691\":1,\"693\":1,\"697\":1,\"797\":1,\"857\":1,\"1003\":1,\"1004\":1,\"1028\":1,\"1623\":1,\"2568\":1,\"2569\":1}}],[\"debugging\",{\"1\":{\"17\":1}}],[\"demonstrate\",{\"1\":{\"2451\":1,\"2600\":1}}],[\"demonstration\",{\"0\":{\"199\":1,\"206\":1,\"2516\":1,\"2576\":1,\"2586\":1,\"2597\":1,\"2601\":1,\"2618\":1,\"2646\":1,\"2650\":1},\"1\":{\"161\":1,\"164\":2,\"196\":1,\"198\":1,\"206\":1,\"240\":1,\"295\":2,\"2357\":1,\"2361\":1,\"2371\":1,\"2380\":1,\"2381\":1,\"2384\":2,\"2385\":1,\"2387\":1,\"2406\":1,\"2407\":1,\"2411\":1,\"2415\":1,\"2447\":1,\"2448\":1,\"2463\":1,\"2464\":1,\"2480\":1,\"2494\":1,\"2502\":1,\"2516\":1,\"2518\":1,\"2576\":1,\"2578\":1,\"2586\":1,\"2597\":1,\"2600\":1,\"2601\":1,\"2612\":1,\"2618\":1,\"2630\":1,\"2646\":1,\"2650\":1}}],[\"demonstrations\",{\"1\":{\"130\":1,\"2447\":1,\"2480\":1,\"2502\":1}}],[\"demo\",{\"0\":{\"154\":1,\"165\":1,\"208\":1,\"219\":1,\"226\":1,\"244\":1,\"2356\":1,\"2361\":1,\"2366\":1,\"2443\":1,\"2505\":1,\"2511\":1,\"2577\":1,\"2588\":1,\"2652\":1,\"2656\":1,\"2728\":1},\"1\":{\"139\":2,\"140\":9,\"155\":4,\"156\":3,\"157\":1,\"158\":1,\"159\":1,\"161\":1,\"164\":2,\"165\":2,\"199\":2,\"202\":2,\"244\":9,\"295\":2,\"2355\":1,\"2373\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2429\":2,\"2430\":1,\"2440\":9,\"2441\":1,\"2447\":1,\"2449\":1,\"2465\":1,\"2467\":1,\"2473\":1,\"2481\":1,\"2503\":1,\"2504\":1,\"2506\":1,\"2525\":1,\"2545\":1,\"2552\":1,\"2555\":1,\"2558\":7,\"2559\":6,\"2560\":1,\"2564\":5,\"2569\":1,\"2570\":2,\"2572\":3,\"2573\":1,\"2593\":3,\"2598\":3,\"2600\":1}}],[\"demos\",{\"1\":{\"130\":1,\"138\":1,\"2354\":1,\"2355\":1}}],[\"dedicated\",{\"1\":{\"112\":1,\"1274\":1}}],[\"detok\",{\"1\":{\"2461\":1}}],[\"detection=true\",{\"1\":{\"2592\":1}}],[\"detection=false\",{\"1\":{\"692\":1}}],[\"detection\",{\"1\":{\"327\":2,\"449\":2,\"880\":1,\"1705\":1,\"2210\":1,\"2461\":1}}],[\"detect\",{\"0\":{\"128\":1,\"880\":1,\"2210\":1},\"1\":{\"429\":2,\"692\":1,\"693\":1,\"697\":1,\"797\":1,\"857\":1,\"880\":2,\"2210\":5}}],[\"determinant\",{\"1\":{\"1833\":1,\"1838\":1,\"1840\":1,\"1855\":1,\"1865\":1}}],[\"determining\",{\"1\":{\"1154\":1,\"1551\":1,\"1553\":1}}],[\"deterministic=false\",{\"1\":{\"1245\":1}}],[\"deterministic\",{\"0\":{\"743\":1,\"745\":1,\"746\":1,\"875\":1,\"1032\":2,\"1033\":2},\"1\":{\"82\":4,\"429\":2,\"743\":1,\"745\":1,\"746\":1,\"875\":1,\"1032\":3,\"1033\":3}}],[\"determinization\",{\"0\":{\"82\":1}}],[\"determine\",{\"1\":{\"77\":1,\"78\":1,\"79\":1,\"1054\":1,\"1055\":1,\"1618\":1,\"1619\":1,\"1638\":1}}],[\"determines\",{\"1\":{\"59\":1,\"1766\":2}}],[\"determined\",{\"1\":{\"40\":1,\"60\":1,\"96\":1,\"745\":1,\"746\":1,\"2044\":1,\"2052\":1,\"2068\":1}}],[\"detailed\",{\"1\":{\"1211\":1,\"1286\":1,\"1336\":1,\"1337\":1,\"2429\":1,\"2552\":1,\"2635\":1}}],[\"details\",{\"0\":{\"205\":1},\"1\":{\"103\":1,\"106\":1,\"124\":1,\"133\":1,\"203\":1,\"295\":2,\"650\":1,\"771\":1,\"772\":1,\"809\":1,\"810\":1,\"1148\":1,\"1150\":1,\"1203\":2,\"1371\":1,\"1551\":1,\"1553\":1,\"1927\":3,\"2054\":1,\"2267\":1,\"2372\":2,\"2387\":1,\"2401\":1,\"2411\":1,\"2424\":1,\"2437\":1,\"2537\":1,\"2547\":1,\"2563\":1,\"2584\":1,\"2586\":1}}],[\"detail\",{\"1\":{\"92\":1,\"197\":1,\"198\":1,\"612\":2,\"802\":1,\"804\":1,\"981\":2,\"1015\":1}}],[\"dealing\",{\"1\":{\"1661\":1}}],[\"deals\",{\"1\":{\"638\":1}}],[\"dealt\",{\"1\":{\"84\":1}}],[\"deal\",{\"1\":{\"80\":1}}],[\"despite\",{\"1\":{\"2462\":1}}],[\"desplanques\",{\"1\":{\"2044\":1,\"2049\":1}}],[\"desperate\",{\"1\":{\"1241\":1}}],[\"deserialize\",{\"1\":{\"981\":1}}],[\"descending\",{\"1\":{\"429\":2,\"987\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":2,\"2012\":1}}],[\"descritive=false\",{\"1\":{\"2122\":1}}],[\"descriptinc\",{\"1\":{\"1857\":1,\"1858\":1}}],[\"description\",{\"1\":{\"117\":1,\"1905\":1}}],[\"describing\",{\"1\":{\"84\":1,\"85\":1,\"2012\":1}}],[\"describes\",{\"1\":{\"75\":1}}],[\"describe\",{\"1\":{\"57\":1,\"114\":1,\"2430\":1,\"2431\":1,\"2462\":1,\"2555\":1}}],[\"described\",{\"1\":{\"23\":1,\"74\":1,\"112\":1,\"113\":1,\"119\":1,\"736\":1,\"754\":1,\"762\":1,\"774\":1,\"802\":1,\"803\":1,\"826\":1,\"837\":1,\"880\":1,\"1198\":1,\"1798\":1,\"1805\":1,\"1841\":1,\"1850\":1,\"1863\":1,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2078\":1,\"2083\":1,\"2095\":1,\"2154\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2585\":1}}],[\"dest\",{\"1\":{\"1036\":1,\"1300\":1,\"1306\":1,\"2313\":1}}],[\"destdir\",{\"1\":{\"287\":1}}],[\"destination\",{\"1\":{\"143\":1,\"294\":1,\"629\":1,\"728\":1}}],[\"desbele\",{\"1\":{\"133\":1}}],[\"design\",{\"0\":{\"1883\":1},\"1\":{\"119\":1,\"1132\":1,\"1883\":3,\"2090\":1}}],[\"designed\",{\"1\":{\"79\":1,\"112\":2,\"130\":1,\"786\":1,\"1638\":1,\"2198\":1,\"2372\":1,\"2429\":1,\"2450\":1,\"2482\":1,\"2554\":1}}],[\"desired\",{\"1\":{\"3\":1,\"1936\":1}}],[\"decribed\",{\"1\":{\"2584\":1}}],[\"decrease\",{\"1\":{\"150\":1,\"2018\":1,\"2440\":1,\"2558\":1}}],[\"decompress\",{\"1\":{\"2567\":1}}],[\"decomposition\",{\"0\":{\"1695\":1},\"1\":{\"1695\":4,\"1704\":2,\"1713\":1}}],[\"decompose\",{\"1\":{\"1339\":1}}],[\"deconstruct\",{\"1\":{\"2299\":1}}],[\"deconv1d\",{\"1\":{\"1660\":2,\"1661\":3,\"1662\":2}}],[\"deconvglu\",{\"1\":{\"1577\":1}}],[\"deconvolution\",{\"1\":{\"830\":1}}],[\"decorator\",{\"1\":{\"1343\":2}}],[\"decode=true\",{\"1\":{\"2460\":1}}],[\"decode=false\",{\"1\":{\"692\":1}}],[\"decoded\",{\"1\":{\"107\":1,\"731\":1,\"1133\":1,\"1190\":1,\"1204\":1,\"1214\":1,\"1244\":1,\"1273\":1}}],[\"decode\",{\"0\":{\"263\":1,\"267\":1,\"271\":1,\"869\":1,\"977\":1,\"1044\":1,\"1889\":1},\"1\":{\"23\":1,\"25\":3,\"98\":4,\"108\":3,\"109\":1,\"110\":2,\"119\":1,\"189\":2,\"191\":2,\"197\":2,\"198\":1,\"202\":2,\"203\":12,\"235\":1,\"241\":2,\"242\":2,\"263\":2,\"267\":2,\"271\":2,\"285\":1,\"286\":8,\"296\":7,\"449\":2,\"455\":1,\"501\":1,\"603\":1,\"649\":1,\"650\":1,\"699\":1,\"836\":1,\"869\":1,\"938\":1,\"977\":3,\"1044\":3,\"1149\":1,\"1150\":1,\"1773\":1,\"1889\":1,\"2082\":1,\"2240\":1,\"2278\":1,\"2401\":1,\"2440\":1,\"2444\":2,\"2445\":2,\"2446\":2,\"2461\":1,\"2537\":1,\"2559\":5,\"2564\":1}}],[\"decoders\",{\"0\":{\"870\":1},\"1\":{\"815\":1,\"870\":1,\"1133\":2,\"1214\":1,\"1273\":1,\"1752\":1,\"2001\":1,\"2452\":1,\"2643\":1}}],[\"decoderlayer\",{\"0\":{\"732\":1},\"1\":{\"732\":2,\"827\":1}}],[\"decoder\",{\"0\":{\"116\":1,\"725\":1,\"731\":2,\"732\":1,\"751\":1,\"802\":1,\"803\":1,\"806\":1,\"824\":1,\"827\":1,\"837\":1,\"841\":1,\"845\":1,\"848\":1,\"850\":1,\"870\":1,\"871\":2,\"884\":1,\"1046\":2,\"1062\":1,\"1065\":1,\"1066\":2,\"1069\":1,\"1070\":1,\"1073\":2,\"1074\":1,\"1075\":2,\"1078\":1,\"1079\":1,\"1081\":1,\"1083\":2,\"1086\":1,\"1100\":1,\"1111\":2,\"1114\":2,\"1117\":2,\"1133\":2,\"1167\":2,\"1168\":2,\"1174\":2,\"1190\":2,\"1196\":2,\"1197\":2,\"1204\":2,\"1214\":2,\"1220\":2,\"1244\":2,\"1270\":2,\"1271\":2,\"1273\":2,\"1289\":2,\"1320\":2,\"1321\":2,\"1362\":2,\"1374\":2,\"1431\":2,\"1510\":2,\"1540\":1,\"1616\":2,\"1643\":2,\"1726\":1,\"1752\":1,\"1771\":1,\"1788\":1,\"1808\":1,\"2078\":2,\"2092\":2,\"2401\":1,\"2537\":1},\"1\":{\"21\":4,\"24\":3,\"28\":2,\"29\":1,\"30\":5,\"64\":1,\"66\":13,\"67\":1,\"114\":1,\"116\":11,\"307\":4,\"327\":2,\"443\":4,\"449\":4,\"640\":1,\"677\":2,\"678\":2,\"679\":2,\"680\":1,\"681\":2,\"682\":3,\"683\":1,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"691\":2,\"692\":1,\"693\":2,\"697\":2,\"698\":3,\"699\":3,\"700\":3,\"706\":4,\"707\":1,\"725\":19,\"731\":6,\"732\":3,\"749\":1,\"751\":1,\"754\":8,\"758\":2,\"766\":5,\"796\":1,\"797\":2,\"802\":2,\"803\":3,\"806\":23,\"815\":5,\"821\":3,\"824\":16,\"825\":7,\"826\":12,\"827\":4,\"829\":1,\"836\":2,\"837\":1,\"841\":2,\"845\":2,\"848\":1,\"850\":2,\"857\":2,\"858\":2,\"860\":1,\"862\":2,\"863\":3,\"865\":3,\"870\":2,\"871\":3,\"884\":3,\"886\":1,\"892\":1,\"910\":1,\"924\":1,\"934\":1,\"1046\":16,\"1048\":3,\"1057\":3,\"1059\":3,\"1060\":1,\"1062\":3,\"1063\":1,\"1064\":5,\"1065\":3,\"1066\":19,\"1069\":1,\"1070\":1,\"1073\":22,\"1074\":3,\"1075\":21,\"1078\":1,\"1079\":1,\"1081\":4,\"1083\":18,\"1086\":1,\"1099\":2,\"1100\":1,\"1111\":2,\"1113\":2,\"1114\":3,\"1115\":22,\"1117\":4,\"1127\":1,\"1133\":5,\"1138\":3,\"1139\":3,\"1148\":1,\"1149\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1167\":2,\"1168\":2,\"1171\":5,\"1172\":5,\"1173\":3,\"1174\":2,\"1190\":4,\"1196\":2,\"1197\":2,\"1198\":1,\"1203\":1,\"1204\":5,\"1206\":5,\"1214\":4,\"1219\":1,\"1220\":4,\"1244\":6,\"1253\":1,\"1254\":1,\"1270\":24,\"1271\":2,\"1272\":1,\"1273\":3,\"1279\":1,\"1289\":2,\"1320\":2,\"1321\":2,\"1362\":2,\"1371\":2,\"1374\":3,\"1376\":2,\"1429\":1,\"1431\":4,\"1510\":3,\"1516\":1,\"1540\":1,\"1551\":4,\"1552\":3,\"1553\":4,\"1554\":2,\"1616\":3,\"1643\":3,\"1645\":1,\"1726\":2,\"1752\":3,\"1771\":4,\"1778\":3,\"1788\":3,\"1804\":28,\"1805\":7,\"1808\":3,\"1850\":3,\"1851\":16,\"1852\":3,\"1877\":7,\"1878\":22,\"1970\":5,\"1975\":3,\"1984\":2,\"2001\":2,\"2002\":2,\"2004\":2,\"2027\":3,\"2029\":1,\"2046\":1,\"2076\":5,\"2078\":9,\"2086\":4,\"2087\":4,\"2090\":15,\"2092\":3,\"2095\":2,\"2158\":9,\"2243\":16,\"2244\":16,\"2255\":14,\"2263\":2,\"2264\":16,\"2279\":16,\"2394\":5,\"2399\":1,\"2401\":4,\"2440\":2,\"2530\":5,\"2535\":1,\"2537\":4,\"2558\":3,\"2584\":4,\"2592\":1,\"2641\":5,\"2642\":2,\"2643\":1,\"2645\":1}}],[\"decoding\",{\"0\":{\"105\":1,\"122\":1,\"189\":1,\"203\":1,\"241\":1,\"325\":1,\"403\":1,\"459\":1,\"468\":1,\"474\":1},\"1\":{\"19\":5,\"23\":8,\"24\":1,\"25\":1,\"91\":1,\"105\":1,\"106\":1,\"107\":2,\"108\":2,\"110\":2,\"111\":1,\"113\":1,\"119\":8,\"120\":1,\"122\":5,\"124\":1,\"150\":6,\"168\":1,\"179\":1,\"197\":1,\"198\":1,\"203\":3,\"235\":1,\"241\":2,\"271\":1,\"286\":1,\"296\":1,\"301\":1,\"307\":1,\"315\":3,\"321\":1,\"327\":1,\"333\":3,\"339\":1,\"384\":1,\"391\":1,\"408\":1,\"416\":1,\"422\":1,\"443\":1,\"449\":1,\"476\":1,\"478\":1,\"485\":5,\"676\":2,\"692\":2,\"693\":6,\"696\":1,\"697\":2,\"700\":2,\"706\":6,\"754\":1,\"781\":2,\"785\":3,\"797\":1,\"798\":1,\"812\":2,\"815\":1,\"817\":1,\"829\":2,\"857\":1,\"869\":1,\"870\":1,\"1048\":3,\"1071\":7,\"1138\":2,\"1139\":2,\"1221\":1,\"1253\":1,\"1428\":2,\"2011\":1,\"2243\":1,\"2375\":1,\"2423\":1,\"2546\":1,\"2559\":4,\"2569\":2,\"2570\":1}}],[\"decided\",{\"1\":{\"1895\":1,\"1900\":1,\"2099\":1,\"2102\":1}}],[\"decides\",{\"1\":{\"1132\":1,\"1142\":1}}],[\"decide\",{\"1\":{\"623\":1,\"1005\":2,\"1028\":2,\"1962\":1,\"2365\":1,\"2508\":1,\"2510\":1,\"2514\":1,\"2515\":1,\"2655\":1,\"2659\":1,\"2660\":1}}],[\"decision\",{\"1\":{\"80\":1}}],[\"decayfalse\",{\"1\":{\"1973\":1}}],[\"decaytrue\",{\"1\":{\"1973\":1}}],[\"decay==false\",{\"1\":{\"1973\":1}}],[\"decay=0\",{\"1\":{\"1973\":1}}],[\"decay=false\",{\"1\":{\"1973\":2}}],[\"decayresidual\",{\"0\":{\"1156\":1},\"1\":{\"1156\":1}}],[\"decay\",{\"0\":{\"630\":1,\"631\":1},\"1\":{\"62\":1,\"251\":5,\"255\":6,\"259\":6,\"265\":2,\"269\":2,\"630\":6,\"631\":6,\"670\":1,\"1081\":3,\"1086\":5,\"1156\":1,\"1217\":1,\"1254\":1,\"1972\":1,\"1973\":3,\"1974\":1,\"2294\":1}}],[\"dec\",{\"1\":{\"21\":4,\"24\":1,\"28\":2,\"29\":2,\"30\":1,\"249\":2,\"251\":4,\"255\":4,\"259\":4,\"265\":4,\"269\":4,\"285\":2,\"677\":3,\"678\":3,\"679\":3,\"681\":3,\"682\":2,\"684\":3,\"685\":3,\"686\":3,\"687\":3,\"688\":3,\"689\":3,\"725\":30,\"751\":3,\"754\":4,\"758\":3,\"766\":3,\"806\":34,\"824\":9,\"825\":12,\"826\":4,\"827\":3,\"863\":2,\"865\":2,\"1060\":5,\"1063\":3,\"1064\":5,\"1066\":3,\"1073\":24,\"1075\":14,\"1083\":1,\"1133\":1,\"1176\":3,\"1193\":2,\"1270\":34,\"1273\":2,\"1360\":1,\"1376\":2,\"1778\":5,\"1850\":5,\"1851\":10,\"1852\":5,\"2002\":1,\"2003\":2,\"2090\":9,\"2095\":1,\"2243\":10,\"2244\":10,\"2255\":6,\"2263\":1,\"2264\":10,\"2279\":10}}],[\"defend\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"defalut\",{\"1\":{\"1377\":1,\"1658\":1,\"1659\":1}}],[\"defaultfrontend\",{\"0\":{\"1158\":1},\"1\":{\"1158\":1}}],[\"defaultrnnlm\",{\"0\":{\"606\":1,\"733\":1},\"1\":{\"606\":2,\"733\":1,\"815\":1}}],[\"default=none\",{\"1\":{\"2313\":1}}],[\"default=5\",{\"1\":{\"684\":1}}],[\"default=32\",{\"1\":{\"297\":1}}],[\"default=run\",{\"1\":{\"297\":1}}],[\"default=0\",{\"1\":{\"297\":1}}],[\"default=60\",{\"1\":{\"297\":1}}],[\"default=64\",{\"1\":{\"275\":1}}],[\"default=1\",{\"1\":{\"1454\":1}}],[\"default=16\",{\"1\":{\"297\":1}}],[\"default=16000\",{\"1\":{\"297\":1}}],[\"default=1024\",{\"1\":{\"275\":1,\"282\":1,\"297\":1}}],[\"default=22050\",{\"1\":{\"282\":1}}],[\"default=256\",{\"1\":{\"275\":1,\"282\":1,\"297\":1}}],[\"default=zip\",{\"1\":{\"277\":1}}],[\"default=downloads\",{\"1\":{\"277\":1}}],[\"default=80\",{\"1\":{\"275\":1}}],[\"default=\",{\"1\":{\"275\":1,\"1203\":1,\"2176\":1,\"2314\":1}}],[\"defaults\",{\"1\":{\"275\":2,\"279\":2,\"282\":2,\"283\":2,\"284\":2,\"1516\":8,\"1638\":1,\"1766\":3,\"1788\":18,\"1808\":2,\"1932\":3,\"2099\":1,\"2259\":4,\"2260\":12,\"2274\":3,\"2349\":3}}],[\"default\",{\"0\":{\"710\":1,\"733\":1,\"807\":1,\"1158\":1,\"1692\":1,\"2312\":1,\"2322\":2},\"1\":{\"1\":2,\"3\":1,\"11\":1,\"17\":3,\"19\":1,\"21\":21,\"22\":8,\"23\":5,\"24\":1,\"25\":1,\"44\":1,\"45\":1,\"49\":1,\"69\":1,\"79\":1,\"82\":1,\"91\":1,\"96\":1,\"109\":1,\"113\":7,\"115\":51,\"116\":22,\"117\":2,\"118\":5,\"119\":7,\"121\":4,\"122\":2,\"134\":1,\"135\":1,\"136\":1,\"141\":1,\"142\":1,\"144\":2,\"150\":5,\"240\":1,\"249\":1,\"276\":2,\"281\":1,\"286\":2,\"295\":2,\"296\":1,\"298\":2,\"506\":1,\"564\":1,\"606\":1,\"610\":1,\"692\":1,\"693\":1,\"697\":1,\"700\":3,\"710\":1,\"727\":1,\"728\":1,\"733\":2,\"745\":2,\"746\":2,\"797\":1,\"807\":1,\"815\":1,\"857\":1,\"944\":3,\"950\":1,\"956\":1,\"968\":1,\"973\":1,\"996\":1,\"1008\":2,\"1016\":1,\"1048\":2,\"1061\":1,\"1063\":1,\"1138\":4,\"1139\":4,\"1158\":1,\"1162\":1,\"1193\":1,\"1198\":2,\"1209\":1,\"1211\":1,\"1224\":2,\"1243\":1,\"1245\":2,\"1247\":1,\"1248\":2,\"1252\":1,\"1253\":2,\"1254\":1,\"1279\":1,\"1336\":1,\"1348\":2,\"1352\":1,\"1371\":1,\"1406\":1,\"1430\":1,\"1515\":2,\"1528\":2,\"1529\":2,\"1531\":4,\"1532\":3,\"1534\":2,\"1535\":3,\"1537\":3,\"1539\":2,\"1542\":1,\"1551\":1,\"1552\":3,\"1553\":1,\"1554\":1,\"1558\":1,\"1581\":1,\"1598\":2,\"1602\":2,\"1604\":1,\"1619\":1,\"1626\":2,\"1639\":1,\"1643\":1,\"1644\":1,\"1645\":8,\"1648\":2,\"1650\":2,\"1652\":2,\"1654\":3,\"1670\":1,\"1671\":1,\"1688\":3,\"1692\":1,\"1693\":3,\"1715\":1,\"1755\":3,\"1756\":3,\"1785\":5,\"1786\":4,\"1797\":5,\"1810\":1,\"1896\":1,\"1972\":2,\"2018\":6,\"2099\":1,\"2170\":1,\"2176\":1,\"2312\":2,\"2315\":2,\"2322\":3,\"2323\":1,\"2440\":2,\"2599\":1}}],[\"def\",{\"1\":{\"56\":7,\"59\":2,\"60\":4,\"174\":1,\"175\":1,\"217\":1,\"224\":1,\"231\":1,\"944\":1,\"1253\":1,\"2096\":2,\"2098\":2,\"2099\":5,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2149\":1,\"2168\":2,\"2170\":2,\"2176\":2,\"2201\":2,\"2323\":1,\"2335\":4,\"2337\":2,\"2358\":1,\"2360\":1,\"2392\":1,\"2425\":1,\"2458\":1,\"2472\":1,\"2476\":1,\"2500\":1,\"2520\":1,\"2523\":1,\"2528\":1,\"2548\":1,\"2579\":1,\"2582\":1,\"2592\":2,\"2617\":1,\"2635\":1,\"2644\":1,\"2648\":1}}],[\"definite\",{\"1\":{\"1695\":1}}],[\"definitely\",{\"1\":{\"127\":1}}],[\"definition\",{\"1\":{\"21\":2,\"117\":1,\"124\":2,\"751\":1,\"861\":1,\"1031\":1,\"1047\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1060\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1076\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1084\":1,\"1086\":1,\"1176\":1,\"1193\":1,\"1797\":1,\"2097\":1,\"2099\":1,\"2102\":1,\"2410\":1}}],[\"defining\",{\"1\":{\"115\":1,\"1072\":1,\"1098\":1}}],[\"defines\",{\"1\":{\"727\":1,\"752\":1,\"756\":1,\"760\":1,\"778\":1,\"831\":1,\"1084\":1,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1134\":1,\"1136\":1,\"1151\":1,\"1156\":1,\"1158\":1,\"1174\":1,\"1184\":1,\"1187\":1,\"1188\":1,\"1202\":1,\"1207\":1,\"1212\":1,\"1215\":1,\"1220\":1,\"1222\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1245\":1,\"1249\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1274\":1,\"1276\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":1,\"1518\":1,\"1520\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1543\":1,\"1547\":1,\"1549\":1,\"1555\":1,\"1561\":1,\"1564\":1,\"1573\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1596\":1,\"1598\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1613\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1641\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1656\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1761\":1,\"1763\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1801\":1,\"1806\":1,\"1890\":1,\"1902\":1,\"1905\":1,\"1907\":1,\"1912\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1976\":1,\"1978\":1,\"1981\":1,\"1984\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1994\":1,\"1997\":1,\"2024\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2050\":1,\"2052\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2149\":1,\"2168\":2,\"2233\":1,\"2237\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2275\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1}}],[\"define\",{\"1\":{\"22\":1,\"27\":1,\"113\":1,\"116\":1,\"118\":1,\"217\":4,\"224\":4,\"231\":4,\"840\":1,\"841\":1,\"842\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"853\":1,\"1011\":1,\"1031\":1,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2131\":1,\"2392\":1,\"2399\":1,\"2425\":1,\"2528\":1,\"2535\":1,\"2548\":1}}],[\"defined\",{\"1\":{\"21\":2,\"22\":2,\"115\":3,\"116\":2,\"118\":1,\"121\":1,\"648\":1,\"742\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"870\":1,\"1001\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1603\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1622\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1719\":1,\"1739\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1905\":2,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2349\":3,\"2410\":1,\"2437\":1,\"2563\":1}}],[\"dev|200|1130|99\",{\"1\":{\"2572\":1}}],[\"dev|200|665|99\",{\"1\":{\"2572\":1}}],[\"dev|100|2015|88\",{\"1\":{\"2564\":1}}],[\"dev|100|2015|79\",{\"1\":{\"2441\":1}}],[\"dev|100|2015|95\",{\"1\":{\"2440\":1}}],[\"dev|100|1915|87\",{\"1\":{\"2564\":1}}],[\"dev|100|1915|78\",{\"1\":{\"2441\":1}}],[\"dev|100|1915|95\",{\"1\":{\"2440\":1}}],[\"dev|100|591|77\",{\"1\":{\"2564\":1}}],[\"dev|100|591|59\",{\"1\":{\"2441\":1}}],[\"dev|100|591|92\",{\"1\":{\"2440\":1}}],[\"dev=\",{\"1\":{\"2568\":1}}],[\"deviation\",{\"1\":{\"632\":1,\"762\":1,\"763\":2,\"1065\":1,\"1069\":2,\"1070\":1,\"1078\":1,\"1079\":1,\"2151\":1}}],[\"device=\",{\"1\":{\"728\":2,\"742\":1,\"921\":1,\"1757\":1,\"2358\":1,\"2364\":1,\"2368\":1,\"2371\":1,\"2455\":1,\"2460\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2520\":2,\"2579\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2617\":1,\"2622\":1,\"2626\":1,\"2630\":1,\"2635\":1,\"2654\":1,\"2658\":1}}],[\"device=none\",{\"1\":{\"617\":1,\"628\":1,\"979\":1,\"1162\":1,\"1243\":1,\"1252\":1,\"1253\":1,\"1279\":1,\"1465\":1,\"2166\":1}}],[\"device\",{\"0\":{\"926\":1,\"2153\":1,\"2166\":2},\"1\":{\"71\":1,\"80\":1,\"217\":6,\"224\":6,\"231\":6,\"604\":3,\"607\":3,\"617\":2,\"626\":4,\"627\":4,\"629\":4,\"697\":1,\"725\":6,\"727\":4,\"728\":4,\"799\":1,\"806\":6,\"818\":1,\"921\":2,\"926\":2,\"975\":4,\"976\":4,\"1021\":1,\"1022\":1,\"1042\":4,\"1043\":4,\"1046\":5,\"1049\":4,\"1050\":4,\"1052\":4,\"1056\":4,\"1058\":4,\"1066\":6,\"1068\":4,\"1071\":4,\"1073\":6,\"1075\":6,\"1083\":6,\"1101\":4,\"1143\":1,\"1270\":6,\"1427\":1,\"1428\":1,\"1785\":2,\"2099\":3,\"2109\":1,\"2113\":1,\"2115\":1,\"2116\":1,\"2149\":1,\"2153\":3,\"2166\":5,\"2592\":1}}],[\"devices=2\",{\"1\":{\"71\":1}}],[\"devices=0\",{\"1\":{\"19\":1}}],[\"devices\",{\"1\":{\"19\":1,\"71\":1,\"72\":2,\"727\":1,\"742\":1,\"2467\":1}}],[\"devset\",{\"1\":{\"172\":3,\"174\":1}}],[\"dev\",{\"0\":{\"171\":1},\"1\":{\"57\":2,\"134\":1,\"170\":1,\"171\":2,\"172\":1,\"174\":4,\"200\":1,\"210\":2,\"211\":2,\"212\":2,\"222\":2,\"223\":2,\"229\":2,\"230\":2,\"238\":2,\"239\":1,\"2372\":1,\"2373\":12,\"2375\":12,\"2377\":4,\"2385\":2,\"2387\":1,\"2397\":1,\"2398\":2,\"2400\":2,\"2401\":2,\"2403\":1,\"2405\":1,\"2430\":10,\"2431\":2,\"2432\":8,\"2433\":6,\"2436\":4,\"2440\":2,\"2444\":1,\"2445\":1,\"2446\":1,\"2533\":1,\"2534\":2,\"2536\":2,\"2537\":2,\"2539\":1,\"2541\":1,\"2555\":14,\"2558\":6,\"2559\":6,\"2562\":4,\"2564\":2,\"2568\":3,\"2569\":2,\"2584\":4,\"2585\":2}}],[\"devtalk\",{\"1\":{\"44\":1}}],[\"developed\",{\"1\":{\"2373\":1,\"2433\":1,\"2468\":1,\"2555\":1,\"2584\":1}}],[\"developer\",{\"1\":{\"45\":1,\"1144\":1,\"1228\":1,\"1345\":1,\"1347\":1}}],[\"developing\",{\"1\":{\"83\":1}}],[\"develop\",{\"1\":{\"48\":1}}],[\"development\",{\"1\":{\"14\":1,\"84\":1,\"2201\":1,\"2411\":1,\"2450\":1}}],[\"devel\",{\"1\":{\"8\":1}}],[\"depth=10\",{\"1\":{\"835\":1}}],[\"depthwiseseparableconv\",{\"0\":{\"1370\":1,\"1546\":1},\"1\":{\"1370\":1,\"1546\":1}}],[\"depthwiseconvolution\",{\"0\":{\"1055\":1},\"1\":{\"115\":1,\"1055\":5,\"1056\":1}}],[\"depthwise\",{\"1\":{\"115\":1,\"1056\":2,\"1198\":11}}],[\"depth\",{\"0\":{\"1352\":1},\"1\":{\"48\":1,\"793\":3,\"835\":2,\"1055\":1,\"1140\":1,\"1141\":3,\"1148\":1,\"1156\":1,\"1169\":1,\"1170\":1,\"1244\":1,\"1252\":1,\"1254\":1,\"1261\":2,\"1352\":5,\"1656\":3,\"1835\":1,\"1927\":1,\"2054\":1,\"2290\":1,\"2372\":1,\"2383\":1,\"2393\":1,\"2409\":1,\"2427\":1,\"2440\":1,\"2450\":2,\"2504\":1,\"2517\":2,\"2529\":1,\"2550\":1,\"2564\":1,\"2584\":1}}],[\"dependency\",{\"1\":{\"2598\":1}}],[\"dependencies\",{\"0\":{\"220\":1,\"227\":1},\"1\":{\"135\":1,\"167\":2,\"178\":2,\"196\":2,\"234\":2}}],[\"dependent\",{\"1\":{\"23\":1,\"119\":1}}],[\"depends\",{\"1\":{\"102\":1,\"167\":1,\"178\":1,\"944\":1}}],[\"depend\",{\"1\":{\"49\":1,\"90\":1,\"134\":1,\"150\":1}}],[\"depending\",{\"1\":{\"22\":1,\"49\":1,\"84\":1,\"112\":1,\"115\":1,\"144\":1,\"1032\":1,\"1033\":1,\"1156\":1,\"2046\":1,\"2097\":3,\"2638\":1}}],[\"deploy\",{\"0\":{\"13\":1},\"1\":{\"13\":1}}],[\"deprecated\",{\"0\":{\"6\":1},\"1\":{\"6\":1,\"52\":1,\"97\":1,\"1148\":1,\"1203\":1,\"1551\":3,\"1553\":3,\"1640\":1,\"2054\":1}}],[\"doens\",{\"1\":{\"1245\":1}}],[\"doesn\",{\"1\":{\"57\":1,\"74\":1,\"85\":2,\"135\":1,\"1028\":1,\"1245\":1,\"1257\":1,\"1897\":1,\"1972\":1,\"2019\":1,\"2046\":1,\"2099\":1,\"2102\":1}}],[\"does\",{\"1\":{\"24\":1,\"79\":1,\"150\":1,\"295\":1,\"669\":1,\"677\":1,\"686\":1,\"687\":1,\"745\":1,\"746\":1,\"754\":1,\"1371\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1612\":1,\"1615\":1,\"1758\":1,\"1759\":1,\"2050\":1,\"2125\":1,\"2243\":1,\"2355\":1,\"2430\":1,\"2555\":1,\"2584\":1}}],[\"double\",{\"0\":{\"1748\":1},\"1\":{\"950\":1,\"968\":1,\"1524\":1,\"1525\":1,\"1748\":2,\"1763\":1,\"1786\":2,\"1801\":1,\"2044\":1,\"2068\":1,\"2521\":1,\"2522\":1,\"2523\":1}}],[\"doubled\",{\"1\":{\"727\":1,\"728\":1}}],[\"domain=\",{\"1\":{\"1786\":1,\"1799\":1}}],[\"domain\",{\"0\":{\"1466\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1667\":1,\"1668\":1},\"1\":{\"736\":1,\"737\":6,\"738\":4,\"739\":2,\"885\":1,\"1198\":2,\"1466\":1,\"1551\":3,\"1553\":3,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":2,\"1571\":1,\"1604\":5,\"1639\":1,\"1640\":1,\"1641\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1666\":2,\"1667\":2,\"1668\":2,\"1706\":1,\"1707\":1,\"1719\":1,\"1763\":1,\"1786\":2,\"1801\":1,\"1917\":4,\"2086\":1,\"2087\":1,\"2584\":1}}],[\"doing\",{\"1\":{\"1243\":1,\"2410\":1}}],[\"doi\",{\"1\":{\"130\":1,\"1696\":1,\"1698\":1,\"1715\":1}}],[\"doi=\",{\"1\":{\"130\":1}}],[\"down=1\",{\"1\":{\"1753\":1}}],[\"down=false\",{\"1\":{\"1508\":1,\"1631\":1}}],[\"downstream\",{\"1\":{\"2441\":1,\"2467\":1,\"2468\":2}}],[\"downspectralpool\",{\"0\":{\"1165\":1},\"1\":{\"1165\":1}}],[\"downsamples\",{\"1\":{\"1693\":1}}],[\"downsample=false\",{\"1\":{\"1231\":1}}],[\"downsample=none\",{\"1\":{\"1134\":1}}],[\"downsample\",{\"0\":{\"1164\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1549\":1,\"1688\":1,\"1693\":1,\"1729\":1},\"1\":{\"1164\":2,\"1311\":2,\"1312\":1,\"1313\":1,\"1549\":1,\"1551\":1,\"1553\":1,\"1688\":2,\"1693\":2,\"1729\":1,\"1778\":6,\"1800\":2,\"1801\":8,\"1804\":4,\"1805\":6,\"1845\":2,\"1846\":6,\"1847\":8,\"1848\":4,\"1849\":4,\"1850\":6,\"1852\":6,\"1856\":4,\"1858\":8,\"1870\":2,\"1877\":6,\"2304\":1}}],[\"downsampling\",{\"1\":{\"102\":1,\"1508\":1,\"1688\":1,\"1693\":2,\"1801\":2,\"1804\":2,\"1846\":1,\"1847\":1,\"1848\":2,\"1849\":3,\"1856\":2,\"1858\":3,\"2188\":1}}],[\"downpool2d\",{\"0\":{\"1163\":1},\"1\":{\"1163\":2}}],[\"downpool\",{\"0\":{\"1162\":1},\"1\":{\"1162\":2}}],[\"downlinearpool\",{\"0\":{\"1161\":1},\"1\":{\"1161\":1}}],[\"download=1\",{\"1\":{\"2617\":1,\"2635\":1}}],[\"downloader\",{\"1\":{\"2358\":1,\"2371\":1,\"2514\":1,\"2520\":1,\"2579\":1,\"2591\":1,\"2612\":1,\"2617\":1,\"2630\":1,\"2635\":1,\"2659\":1}}],[\"downloaded\",{\"1\":{\"3\":2,\"5\":1,\"85\":1,\"277\":2,\"2368\":1,\"2384\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2500\":1,\"2542\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1}}],[\"downloading\",{\"1\":{\"134\":1,\"204\":1,\"1179\":1,\"2387\":1,\"2585\":2}}],[\"downloads\",{\"1\":{\"85\":1,\"210\":4,\"211\":4,\"212\":4,\"214\":3,\"215\":3,\"216\":3,\"222\":6,\"223\":6,\"229\":6,\"230\":6,\"236\":3,\"277\":1,\"2387\":2,\"2431\":3,\"2432\":3,\"2567\":4}}],[\"download\",{\"0\":{\"209\":1,\"213\":1,\"221\":1,\"228\":1,\"236\":1,\"277\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"2368\":1,\"2383\":1,\"2393\":1,\"2396\":1,\"2427\":1,\"2454\":1,\"2470\":1,\"2472\":1,\"2486\":1,\"2489\":1,\"2519\":1,\"2529\":1,\"2532\":1,\"2550\":1,\"2567\":1,\"2588\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2647\":1,\"2648\":1,\"2649\":1},\"1\":{\"1\":1,\"3\":1,\"16\":1,\"97\":1,\"102\":1,\"110\":1,\"168\":1,\"179\":1,\"198\":1,\"200\":2,\"210\":3,\"211\":3,\"212\":3,\"214\":3,\"215\":3,\"216\":3,\"217\":1,\"222\":4,\"223\":4,\"229\":4,\"230\":4,\"235\":2,\"277\":4,\"1178\":1,\"1179\":1,\"1180\":1,\"1214\":1,\"1215\":1,\"1239\":1,\"1284\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1791\":1,\"2358\":2,\"2367\":1,\"2371\":1,\"2372\":1,\"2383\":1,\"2393\":1,\"2396\":1,\"2427\":1,\"2430\":1,\"2431\":2,\"2454\":1,\"2460\":1,\"2482\":1,\"2485\":1,\"2490\":1,\"2494\":1,\"2506\":1,\"2510\":1,\"2512\":1,\"2514\":1,\"2519\":2,\"2520\":2,\"2529\":1,\"2532\":1,\"2550\":1,\"2555\":1,\"2567\":1,\"2579\":2,\"2585\":2,\"2588\":1,\"2592\":1,\"2593\":1,\"2599\":1,\"2604\":1,\"2609\":1,\"2612\":1,\"2617\":1,\"2621\":1,\"2626\":1,\"2630\":2,\"2635\":1,\"2657\":1,\"2659\":1}}],[\"downavgpool\",{\"0\":{\"1160\":1},\"1\":{\"1160\":2}}],[\"down\",{\"0\":{\"1508\":1,\"1688\":1,\"1693\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1755\":1,\"1756\":1,\"2013\":1},\"1\":{\"1143\":1,\"1508\":1,\"1688\":1,\"1693\":1,\"1716\":1,\"1729\":1,\"1730\":1,\"1754\":2,\"1755\":1,\"1756\":1,\"1941\":1,\"2013\":1,\"2375\":1,\"2559\":1}}],[\"donwloaded\",{\"1\":{\"236\":1}}],[\"dongji\",{\"1\":{\"130\":1}}],[\"done\",{\"1\":{\"98\":1,\"150\":1,\"196\":1,\"200\":2,\"692\":1,\"693\":1,\"1198\":1,\"1917\":1,\"2011\":1,\"2198\":1,\"2360\":1,\"2440\":2,\"2458\":1,\"2523\":1,\"2568\":2,\"2582\":1,\"2583\":1,\"2637\":1}}],[\"don\",{\"1\":{\"45\":1,\"47\":1,\"57\":1,\"69\":1,\"76\":1,\"82\":1,\"84\":1,\"85\":2,\"115\":1,\"126\":1,\"135\":1,\"148\":1}}],[\"dot\",{\"1\":{\"31\":1,\"680\":1,\"687\":1,\"771\":1,\"785\":1,\"809\":1,\"1076\":1,\"1209\":1}}],[\"doctor\",{\"1\":{\"2134\":1}}],[\"documetation\",{\"1\":{\"166\":1,\"177\":1}}],[\"documents\",{\"1\":{\"2040\":1}}],[\"documentaion\",{\"1\":{\"233\":1}}],[\"documentations\",{\"1\":{\"2354\":1}}],[\"documentation\",{\"1\":{\"10\":1,\"126\":1,\"134\":1,\"199\":1,\"1015\":1,\"2380\":1,\"2384\":1,\"2388\":1,\"2406\":1,\"2414\":1,\"2421\":1,\"2430\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":1,\"2544\":1,\"2555\":1,\"2573\":1}}],[\"document\",{\"0\":{\"9\":1},\"1\":{\"14\":1,\"700\":3,\"885\":1,\"1048\":3,\"1138\":3,\"1139\":3,\"1462\":1,\"1463\":1,\"1529\":1,\"1568\":1,\"1696\":1,\"1698\":1,\"1706\":1,\"1707\":1,\"1712\":2,\"1715\":2,\"2564\":1}}],[\"docs\",{\"1\":{\"38\":1,\"45\":2,\"70\":2,\"82\":1,\"1400\":1,\"1454\":1,\"2401\":2,\"2537\":2}}],[\"docstring\",{\"1\":{\"11\":1,\"2199\":1}}],[\"docstrings\",{\"0\":{\"11\":1},\"1\":{\"10\":1,\"11\":4}}],[\"doc\",{\"1\":{\"10\":1,\"11\":1,\"12\":2,\"33\":1,\"95\":1,\"148\":1,\"195\":1,\"528\":2,\"564\":1,\"944\":2,\"2040\":1,\"2155\":1,\"2372\":1,\"2373\":1,\"2382\":1,\"2384\":1,\"2429\":1,\"2430\":1,\"2554\":1,\"2555\":1}}],[\"dockerfile\",{\"1\":{\"5\":1,\"8\":3}}],[\"docker\",{\"0\":{\"0\":1,\"1\":1},\"1\":{\"1\":18,\"2\":3,\"3\":15,\"4\":4,\"5\":8,\"6\":2,\"8\":3,\"45\":4,\"2372\":1}}],[\"do\",{\"1\":{\"5\":1,\"11\":1,\"29\":1,\"56\":1,\"84\":1,\"92\":1,\"98\":1,\"111\":1,\"118\":1,\"144\":2,\"235\":1,\"608\":1,\"1011\":2,\"1215\":1,\"1452\":1,\"1601\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1719\":1,\"1741\":3,\"2040\":3,\"2294\":1,\"2362\":1,\"2363\":1,\"2373\":2,\"2387\":1,\"2394\":1,\"2430\":1,\"2440\":1,\"2441\":1,\"2468\":1,\"2504\":1,\"2506\":1,\"2530\":1,\"2555\":2,\"2558\":2,\"2565\":1,\"2568\":3,\"2573\":2,\"2576\":1,\"2584\":1,\"2585\":1,\"2651\":1,\"2653\":1}}],[\"t4\",{\"1\":{\"2558\":1,\"2559\":1,\"2571\":1}}],[\"tqdm\",{\"1\":{\"2384\":1,\"2386\":3}}],[\"tyc\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"typ\",{\"1\":{\"805\":1,\"807\":1,\"808\":1}}],[\"typ=\",{\"1\":{\"614\":1,\"805\":1,\"807\":1,\"808\":1}}],[\"typical\",{\"1\":{\"46\":1,\"48\":1,\"2410\":1,\"2451\":1}}],[\"typically\",{\"1\":{\"23\":1,\"48\":1,\"119\":1,\"168\":1,\"179\":1,\"785\":1,\"1935\":1,\"1943\":1}}],[\"typeguard==2\",{\"1\":{\"2384\":1,\"2450\":1,\"2504\":1,\"2517\":1}}],[\"type2\",{\"1\":{\"1905\":2}}],[\"type2=tuple\",{\"1\":{\"1905\":1}}],[\"type1\",{\"1\":{\"1905\":3}}],[\"type1=tuple\",{\"1\":{\"1905\":1}}],[\"typeerror\",{\"1\":{\"1008\":2}}],[\"typeinfotuple\",{\"1\":{\"745\":1}}],[\"type>\",{\"1\":{\"564\":1}}],[\"type=str\",{\"1\":{\"2340\":1}}],[\"type=str2pair\",{\"1\":{\"2335\":1}}],[\"type=int\",{\"1\":{\"2328\":1}}],[\"type=float\",{\"1\":{\"2320\":1}}],[\"type=filepath\",{\"1\":{\"59\":5,\"60\":1}}],[\"type=none\",{\"1\":{\"1289\":1,\"1566\":1,\"1567\":1,\"1818\":1,\"2592\":1}}],[\"type=numel\",{\"1\":{\"73\":1}}],[\"type=length\",{\"1\":{\"73\":1}}],[\"type=\",{\"1\":{\"57\":4,\"729\":1,\"749\":1,\"952\":1,\"1134\":1,\"1231\":1,\"1247\":1,\"1248\":1,\"1370\":1,\"1378\":1,\"1379\":1,\"1430\":2,\"1460\":1,\"1518\":1,\"1520\":1,\"1537\":1,\"1546\":1,\"1568\":2,\"1569\":1,\"1571\":1,\"1598\":2,\"1605\":2,\"1646\":1,\"1648\":1,\"1652\":2,\"1663\":1,\"1664\":1,\"1665\":1,\"1670\":2,\"1739\":1,\"1923\":1,\"2099\":1}}],[\"type\",{\"0\":{\"64\":1,\"73\":1,\"124\":1,\"935\":1,\"1296\":1,\"1720\":1,\"2426\":1,\"2549\":1},\"1\":{\"21\":10,\"23\":11,\"49\":2,\"57\":4,\"60\":4,\"64\":1,\"73\":1,\"74\":6,\"75\":9,\"76\":12,\"77\":5,\"78\":5,\"79\":11,\"113\":2,\"115\":30,\"116\":11,\"117\":2,\"119\":8,\"124\":2,\"210\":1,\"211\":1,\"212\":1,\"217\":1,\"249\":1,\"251\":1,\"259\":1,\"295\":2,\"301\":1,\"307\":3,\"315\":3,\"321\":3,\"327\":3,\"333\":3,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"380\":2,\"384\":3,\"391\":3,\"399\":3,\"408\":3,\"416\":2,\"422\":3,\"429\":4,\"437\":2,\"443\":4,\"449\":3,\"455\":2,\"461\":1,\"464\":2,\"470\":2,\"476\":2,\"478\":3,\"485\":3,\"579\":1,\"608\":1,\"614\":2,\"618\":1,\"619\":1,\"624\":1,\"625\":1,\"629\":2,\"633\":1,\"635\":4,\"637\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"645\":1,\"646\":1,\"658\":1,\"672\":2,\"674\":2,\"676\":8,\"677\":2,\"678\":2,\"679\":2,\"681\":2,\"682\":2,\"684\":2,\"685\":2,\"686\":2,\"687\":2,\"688\":2,\"689\":2,\"690\":1,\"691\":7,\"692\":3,\"693\":2,\"694\":1,\"695\":1,\"696\":1,\"697\":10,\"700\":9,\"701\":2,\"702\":1,\"703\":1,\"706\":3,\"708\":1,\"711\":2,\"712\":3,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"725\":20,\"726\":18,\"729\":1,\"730\":1,\"731\":2,\"732\":1,\"734\":4,\"735\":1,\"737\":2,\"738\":2,\"740\":1,\"741\":1,\"742\":3,\"743\":1,\"745\":4,\"746\":1,\"747\":4,\"749\":2,\"754\":5,\"755\":1,\"758\":2,\"759\":3,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":4,\"767\":2,\"768\":1,\"770\":1,\"771\":2,\"772\":1,\"773\":2,\"774\":1,\"775\":1,\"776\":1,\"781\":5,\"782\":2,\"784\":1,\"785\":3,\"786\":1,\"793\":1,\"796\":1,\"797\":6,\"798\":1,\"799\":1,\"802\":1,\"804\":1,\"805\":1,\"806\":6,\"807\":1,\"808\":1,\"809\":2,\"810\":1,\"812\":2,\"813\":1,\"815\":3,\"817\":2,\"818\":1,\"820\":3,\"821\":4,\"822\":1,\"824\":5,\"825\":6,\"826\":6,\"827\":1,\"828\":3,\"830\":1,\"835\":2,\"838\":1,\"852\":2,\"855\":1,\"856\":1,\"857\":1,\"858\":16,\"859\":11,\"860\":1,\"862\":6,\"863\":1,\"865\":1,\"866\":1,\"869\":1,\"870\":1,\"872\":2,\"873\":2,\"874\":2,\"875\":1,\"877\":1,\"884\":1,\"885\":1,\"886\":7,\"889\":1,\"891\":1,\"892\":1,\"896\":2,\"898\":1,\"899\":1,\"901\":1,\"903\":1,\"906\":1,\"908\":1,\"909\":1,\"911\":4,\"912\":1,\"914\":1,\"915\":1,\"917\":1,\"918\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":2,\"933\":1,\"934\":1,\"935\":3,\"936\":1,\"959\":3,\"989\":1,\"1011\":3,\"1013\":1,\"1016\":1,\"1025\":1,\"1046\":2,\"1048\":9,\"1049\":2,\"1050\":2,\"1051\":1,\"1052\":4,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":2,\"1057\":3,\"1058\":2,\"1059\":1,\"1062\":1,\"1064\":4,\"1065\":6,\"1066\":13,\"1068\":2,\"1069\":4,\"1070\":1,\"1071\":3,\"1072\":1,\"1073\":8,\"1074\":1,\"1075\":7,\"1076\":3,\"1077\":1,\"1078\":1,\"1079\":2,\"1081\":2,\"1083\":3,\"1086\":2,\"1093\":12,\"1096\":3,\"1097\":1,\"1098\":3,\"1099\":1,\"1101\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1115\":8,\"1132\":1,\"1133\":5,\"1138\":10,\"1139\":9,\"1140\":4,\"1141\":1,\"1145\":5,\"1148\":15,\"1149\":5,\"1150\":3,\"1153\":1,\"1169\":6,\"1170\":1,\"1173\":1,\"1182\":1,\"1190\":3,\"1198\":9,\"1200\":1,\"1203\":15,\"1204\":1,\"1209\":3,\"1214\":3,\"1220\":2,\"1221\":1,\"1222\":1,\"1229\":1,\"1244\":4,\"1255\":1,\"1270\":8,\"1272\":3,\"1273\":3,\"1282\":1,\"1296\":1,\"1352\":1,\"1368\":1,\"1370\":1,\"1372\":1,\"1373\":1,\"1375\":1,\"1376\":1,\"1377\":3,\"1379\":2,\"1380\":1,\"1381\":1,\"1384\":1,\"1386\":1,\"1392\":1,\"1417\":1,\"1430\":5,\"1432\":1,\"1436\":1,\"1451\":1,\"1454\":4,\"1455\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1466\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1505\":13,\"1510\":1,\"1511\":2,\"1514\":1,\"1515\":3,\"1516\":2,\"1517\":2,\"1522\":1,\"1523\":1,\"1524\":4,\"1525\":2,\"1528\":3,\"1529\":3,\"1530\":1,\"1532\":2,\"1534\":3,\"1535\":2,\"1537\":4,\"1539\":6,\"1545\":1,\"1546\":1,\"1551\":4,\"1552\":1,\"1553\":7,\"1557\":1,\"1558\":3,\"1563\":1,\"1566\":2,\"1567\":2,\"1568\":2,\"1569\":2,\"1570\":1,\"1571\":2,\"1572\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1581\":3,\"1585\":1,\"1594\":1,\"1595\":2,\"1598\":2,\"1600\":1,\"1602\":1,\"1603\":1,\"1604\":3,\"1611\":5,\"1612\":1,\"1615\":1,\"1622\":1,\"1623\":1,\"1626\":3,\"1637\":1,\"1639\":1,\"1640\":1,\"1643\":2,\"1644\":3,\"1645\":1,\"1646\":2,\"1648\":1,\"1650\":2,\"1652\":3,\"1654\":4,\"1655\":1,\"1658\":3,\"1659\":6,\"1660\":3,\"1661\":3,\"1662\":3,\"1664\":2,\"1665\":5,\"1666\":1,\"1668\":1,\"1669\":3,\"1670\":6,\"1671\":7,\"1680\":1,\"1682\":1,\"1683\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1717\":1,\"1719\":4,\"1720\":2,\"1725\":1,\"1736\":1,\"1739\":2,\"1741\":1,\"1746\":1,\"1758\":1,\"1759\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1771\":13,\"1772\":1,\"1773\":3,\"1776\":1,\"1777\":1,\"1778\":23,\"1781\":3,\"1785\":4,\"1786\":1,\"1787\":13,\"1788\":13,\"1798\":13,\"1800\":2,\"1801\":1,\"1803\":1,\"1804\":21,\"1805\":17,\"1808\":3,\"1810\":1,\"1811\":1,\"1829\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":4,\"1837\":2,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":4,\"1844\":2,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":19,\"1851\":25,\"1852\":22,\"1853\":1,\"1855\":1,\"1856\":1,\"1857\":2,\"1858\":1,\"1859\":2,\"1860\":2,\"1861\":1,\"1862\":2,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":2,\"1872\":1,\"1873\":2,\"1874\":13,\"1876\":1,\"1877\":14,\"1878\":15,\"1879\":1,\"1880\":1,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1889\":1,\"1904\":1,\"1910\":1,\"1916\":1,\"1917\":2,\"1918\":2,\"1921\":1,\"1922\":1,\"1923\":2,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1930\":1,\"1932\":3,\"1935\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1947\":1,\"1957\":2,\"1958\":2,\"1960\":2,\"1963\":1,\"1971\":1,\"1984\":1,\"1986\":1,\"1993\":1,\"1999\":1,\"2000\":2,\"2001\":5,\"2002\":4,\"2003\":8,\"2004\":3,\"2012\":3,\"2024\":1,\"2026\":5,\"2028\":1,\"2029\":3,\"2049\":2,\"2054\":15,\"2055\":1,\"2064\":1,\"2078\":1,\"2079\":1,\"2081\":1,\"2082\":3,\"2083\":3,\"2084\":2,\"2086\":11,\"2087\":8,\"2088\":1,\"2089\":1,\"2090\":20,\"2091\":4,\"2095\":7,\"2097\":3,\"2099\":2,\"2106\":4,\"2128\":1,\"2129\":1,\"2130\":1,\"2137\":2,\"2142\":1,\"2155\":1,\"2170\":1,\"2176\":3,\"2178\":2,\"2179\":2,\"2182\":1,\"2184\":1,\"2186\":1,\"2189\":1,\"2191\":2,\"2194\":2,\"2195\":2,\"2196\":2,\"2197\":2,\"2200\":1,\"2202\":2,\"2204\":1,\"2226\":1,\"2239\":1,\"2240\":3,\"2243\":25,\"2244\":25,\"2245\":1,\"2252\":1,\"2254\":1,\"2255\":25,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":3,\"2260\":5,\"2261\":1,\"2262\":1,\"2263\":7,\"2264\":11,\"2265\":1,\"2267\":3,\"2268\":1,\"2270\":1,\"2272\":1,\"2273\":1,\"2274\":4,\"2278\":3,\"2279\":25,\"2280\":1,\"2309\":1,\"2317\":1,\"2325\":1,\"2349\":3,\"2357\":10,\"2363\":6,\"2371\":1,\"2426\":1,\"2431\":5,\"2432\":4,\"2433\":4,\"2436\":2,\"2440\":5,\"2512\":2,\"2549\":1,\"2558\":2,\"2564\":3,\"2569\":1,\"2573\":1,\"2578\":10,\"2584\":2,\"2612\":1,\"2630\":1,\"2653\":6,\"2657\":2}}],[\"types=1\",{\"1\":{\"1670\":1}}],[\"types\",{\"0\":{\"2319\":1,\"2326\":1,\"2327\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2336\":1,\"2338\":1,\"2339\":1},\"1\":{\"21\":2,\"31\":1,\"73\":1,\"116\":1,\"745\":4,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1187\":1,\"1202\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1670\":1,\"1671\":2,\"1739\":1,\"2133\":1,\"2319\":2,\"2326\":1,\"2327\":2,\"2331\":2,\"2332\":2,\"2333\":2,\"2334\":2,\"2336\":2,\"2338\":2,\"2339\":2,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2558\":1,\"2638\":1,\"2644\":1,\"2654\":1,\"2658\":1}}],[\"ttranslatotron\",{\"1\":{\"2002\":1}}],[\"ttstask\",{\"0\":{\"2116\":1},\"1\":{\"2116\":2}}],[\"ttsinterface\",{\"0\":{\"820\":1},\"1\":{\"754\":1,\"820\":2,\"821\":1,\"826\":1}}],[\"tts2task\",{\"0\":{\"2115\":1},\"1\":{\"2115\":2}}],[\"tts2\",{\"0\":{\"464\":1,\"2115\":1,\"2275\":1,\"2277\":2,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2712\":1},\"1\":{\"464\":2,\"2115\":2,\"2275\":1,\"2277\":5,\"2278\":2,\"2279\":1,\"2280\":1,\"2281\":1}}],[\"tts\",{\"0\":{\"158\":1,\"206\":1,\"263\":1,\"265\":1,\"470\":1,\"754\":1,\"755\":1,\"762\":1,\"763\":1,\"820\":1,\"822\":1,\"826\":1,\"974\":2,\"975\":2,\"976\":2,\"977\":2,\"978\":2,\"1828\":2,\"1829\":1,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1882\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"2104\":1,\"2116\":1,\"2233\":1,\"2235\":2,\"2236\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2274\":1,\"2505\":1,\"2506\":1,\"2509\":1,\"2650\":1,\"2679\":1,\"2691\":1,\"2711\":1,\"2732\":1},\"1\":{\"15\":1,\"31\":1,\"56\":1,\"92\":2,\"96\":3,\"130\":3,\"136\":1,\"139\":2,\"140\":2,\"158\":2,\"161\":2,\"164\":6,\"165\":2,\"199\":2,\"202\":3,\"204\":1,\"206\":2,\"207\":1,\"217\":1,\"224\":1,\"231\":1,\"233\":2,\"235\":4,\"237\":1,\"244\":2,\"263\":3,\"265\":3,\"295\":2,\"397\":1,\"398\":2,\"464\":1,\"470\":3,\"646\":1,\"735\":1,\"754\":2,\"755\":1,\"762\":1,\"763\":1,\"802\":1,\"803\":1,\"804\":1,\"820\":6,\"822\":1,\"826\":2,\"974\":2,\"975\":2,\"976\":2,\"977\":5,\"978\":5,\"987\":4,\"1003\":2,\"1004\":2,\"1005\":4,\"1017\":1,\"1028\":2,\"1828\":5,\"1829\":2,\"1830\":1,\"1831\":1,\"1832\":1,\"1833\":2,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":2,\"1838\":1,\"1839\":1,\"1840\":2,\"1841\":2,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":2,\"1851\":1,\"1852\":1,\"1853\":2,\"1854\":1,\"1855\":2,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":2,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":2,\"1876\":1,\"1877\":2,\"1878\":1,\"1879\":2,\"1880\":2,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1980\":1,\"1989\":1,\"2104\":2,\"2116\":2,\"2142\":1,\"2233\":1,\"2235\":5,\"2236\":2,\"2239\":1,\"2240\":3,\"2241\":2,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":1,\"2248\":2,\"2250\":1,\"2252\":2,\"2253\":1,\"2254\":1,\"2255\":2,\"2256\":2,\"2257\":1,\"2258\":2,\"2259\":2,\"2260\":1,\"2261\":2,\"2262\":1,\"2263\":3,\"2264\":2,\"2265\":1,\"2266\":2,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":2,\"2274\":1,\"2277\":1,\"2278\":1,\"2361\":3,\"2363\":1,\"2364\":1,\"2394\":1,\"2492\":1,\"2506\":7,\"2507\":5,\"2508\":2,\"2510\":19,\"2512\":3,\"2513\":5,\"2514\":2,\"2515\":1,\"2518\":1,\"2530\":1,\"2628\":1,\"2650\":3,\"2653\":1,\"2654\":1,\"2658\":1}}],[\"tts1\",{\"1\":{\"15\":1,\"85\":1,\"198\":2,\"235\":3,\"243\":2,\"2279\":1,\"2361\":1,\"2385\":1,\"2650\":1}}],[\"tdspeakerbeamextractor\",{\"0\":{\"1659\":1},\"1\":{\"1659\":1}}],[\"td\",{\"0\":{\"1659\":1},\"1\":{\"1659\":2}}],[\"tdnn\",{\"0\":{\"2042\":1,\"2049\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":1,\"2066\":1,\"2074\":1,\"2075\":1},\"1\":{\"21\":4,\"2042\":1,\"2044\":2,\"2049\":5,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":3,\"2066\":1,\"2074\":1,\"2075\":1,\"2415\":1,\"2417\":1,\"2419\":2}}],[\"tzw\",{\"1\":{\"1523\":1}}],[\"tlg\",{\"1\":{\"1428\":1}}],[\"t=0\",{\"1\":{\"1619\":1}}],[\"t=20\",{\"1\":{\"1354\":1}}],[\"t=5\",{\"1\":{\"1354\":1}}],[\"t=40\",{\"1\":{\"972\":1,\"1037\":1,\"1039\":1}}],[\"tvm\",{\"1\":{\"1203\":1}}],[\"tfgridnet\",{\"0\":{\"1578\":1,\"1588\":1,\"1660\":2},\"1\":{\"1578\":1,\"1588\":1,\"1660\":5,\"1661\":1,\"2494\":4}}],[\"tfgridnetv2\",{\"0\":{\"1449\":1,\"1579\":1,\"1590\":1,\"1661\":2},\"1\":{\"1449\":1,\"1579\":1,\"1590\":1,\"1661\":6,\"1662\":1}}],[\"tfgridnetv3\",{\"0\":{\"1447\":1,\"1580\":1,\"1662\":2},\"1\":{\"1447\":1,\"1580\":1,\"1662\":6}}],[\"tf\",{\"0\":{\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1},\"1\":{\"1011\":3,\"1515\":1,\"1528\":1,\"1529\":3,\"1530\":1,\"1551\":2,\"1553\":2,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1655\":2,\"1660\":2,\"1661\":2,\"1662\":2,\"1688\":1,\"1719\":1,\"1756\":1,\"2371\":1,\"2494\":4,\"2497\":1,\"2612\":1,\"2630\":1}}],[\"t2\",{\"1\":{\"676\":2,\"742\":1,\"781\":1}}],[\"t1\",{\"1\":{\"676\":2,\"742\":1,\"781\":1}}],[\"tmax\",{\"1\":{\"629\":3,\"676\":4,\"682\":2,\"701\":2,\"732\":1,\"735\":2,\"737\":6,\"742\":2,\"747\":1,\"754\":4,\"755\":2,\"759\":1,\"774\":1,\"781\":3,\"799\":3,\"802\":2,\"812\":1,\"820\":1,\"821\":3,\"826\":2,\"906\":1,\"1145\":7,\"1778\":11,\"1804\":14,\"2078\":3,\"2081\":2,\"2083\":3,\"2086\":12,\"2087\":12,\"2090\":10,\"2095\":10,\"2264\":1,\"2265\":3}}],[\"tgt\",{\"0\":{\"1981\":2,\"1987\":1,\"1989\":1,\"1991\":1},\"1\":{\"255\":2,\"257\":2,\"261\":2,\"1133\":10,\"1214\":4,\"1273\":4,\"1661\":2,\"1662\":2,\"1965\":1,\"1981\":2,\"1984\":10,\"1985\":2,\"1987\":1,\"1989\":1,\"1991\":1,\"2001\":4,\"2076\":5,\"2456\":2,\"2460\":2,\"2520\":1,\"2521\":2}}],[\"tw\",{\"1\":{\"2618\":1}}],[\"twice\",{\"1\":{\"727\":1,\"728\":1}}],[\"tweaks\",{\"1\":{\"2142\":1}}],[\"tweak\",{\"1\":{\"187\":1}}],[\"twooptimizertrainer\",{\"1\":{\"2201\":1}}],[\"two\",{\"0\":{\"2473\":1},\"1\":{\"23\":1,\"27\":1,\"38\":1,\"45\":1,\"58\":1,\"62\":1,\"63\":1,\"92\":1,\"94\":1,\"112\":2,\"115\":1,\"118\":1,\"119\":1,\"121\":1,\"122\":1,\"157\":1,\"213\":1,\"238\":1,\"691\":2,\"697\":3,\"734\":1,\"767\":1,\"817\":1,\"828\":1,\"875\":1,\"1010\":1,\"1143\":1,\"1241\":1,\"1336\":1,\"1735\":1,\"1849\":2,\"1856\":3,\"1858\":2,\"1922\":1,\"2027\":1,\"2099\":1,\"2102\":1,\"2154\":1,\"2385\":1,\"2394\":1,\"2397\":1,\"2400\":1,\"2429\":1,\"2467\":1,\"2473\":1,\"2530\":1,\"2533\":1,\"2536\":1,\"2552\":1,\"2646\":1}}],[\"tc4000\",{\"1\":{\"2454\":1,\"2455\":2,\"2460\":3,\"2461\":2}}],[\"tcnresblock\",{\"0\":{\"1656\":1},\"1\":{\"1656\":1}}],[\"tcndensenet\",{\"1\":{\"1655\":1}}],[\"tcndenseunet\",{\"0\":{\"1506\":1,\"1543\":1,\"1564\":1,\"1655\":2,\"1656\":1},\"1\":{\"1506\":1,\"1543\":2,\"1564\":1,\"1655\":2,\"1656\":1}}],[\"tcn=3\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"tcnseparatornomask\",{\"0\":{\"1377\":1},\"1\":{\"1377\":1}}],[\"tcnseparator\",{\"0\":{\"1658\":1},\"1\":{\"1375\":1,\"1377\":1,\"1658\":1}}],[\"tcn\",{\"0\":{\"1368\":1,\"1369\":1,\"1370\":1,\"1372\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":1,\"1472\":1,\"1473\":1,\"1546\":1,\"1575\":1,\"1658\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1682\":1,\"1683\":1},\"1\":{\"1368\":1,\"1369\":2,\"1370\":1,\"1372\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":2,\"1472\":1,\"1473\":2,\"1546\":1,\"1575\":2,\"1655\":11,\"1656\":2,\"1658\":1,\"1663\":2,\"1664\":2,\"1665\":1,\"1682\":2,\"1683\":2,\"1719\":11,\"2641\":2}}],[\"tc\",{\"1\":{\"144\":1,\"294\":1}}],[\"tcp\",{\"1\":{\"38\":3}}],[\"tsne\",{\"1\":{\"2415\":1,\"2416\":1}}],[\"tsnormalization\",{\"0\":{\"1267\":1},\"1\":{\"1267\":1}}],[\"tsinversenormalization\",{\"0\":{\"1265\":1},\"1\":{\"1265\":1}}],[\"tsukuyomi\",{\"1\":{\"2363\":2,\"2506\":1,\"2653\":2}}],[\"tsunoo\",{\"1\":{\"692\":1,\"693\":1,\"1150\":1,\"2357\":2,\"2578\":2,\"2589\":1}}],[\"tsubasa\",{\"1\":{\"130\":1}}],[\"tsepreprocessor\",{\"0\":{\"2200\":1},\"1\":{\"2200\":2}}],[\"tse\",{\"0\":{\"368\":1,\"1554\":1,\"2117\":1},\"1\":{\"363\":2,\"368\":2,\"1554\":1,\"2117\":1}}],[\"tsao\",{\"1\":{\"130\":1}}],[\"tsd\",{\"1\":{\"23\":2,\"119\":2,\"249\":1,\"700\":1,\"1048\":1,\"1138\":2,\"1139\":2}}],[\"txt\",{\"1\":{\"75\":4,\"76\":2,\"77\":4,\"78\":4,\"79\":2,\"175\":1,\"194\":1,\"198\":2,\"201\":1,\"202\":3,\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"239\":1,\"276\":1,\"295\":7,\"298\":1,\"570\":1,\"1015\":1,\"1024\":1,\"1181\":1,\"1383\":2,\"1384\":1,\"1385\":2,\"1386\":1,\"1387\":2,\"2128\":1,\"2129\":1,\"2347\":2,\"2349\":1,\"2373\":1,\"2375\":1,\"2500\":12,\"2556\":1,\"2559\":1,\"2617\":12,\"2635\":12}}],[\"t\",{\"1\":{\"40\":1,\"45\":1,\"47\":1,\"57\":2,\"69\":2,\"74\":1,\"76\":1,\"82\":1,\"83\":1,\"84\":1,\"85\":4,\"113\":1,\"115\":1,\"126\":1,\"135\":2,\"144\":2,\"148\":1,\"171\":1,\"175\":1,\"185\":1,\"193\":1,\"202\":1,\"203\":3,\"238\":2,\"301\":1,\"676\":4,\"677\":2,\"678\":1,\"679\":1,\"681\":2,\"684\":3,\"685\":3,\"686\":2,\"687\":2,\"688\":3,\"689\":3,\"691\":4,\"692\":2,\"693\":1,\"697\":5,\"700\":7,\"701\":2,\"705\":2,\"708\":2,\"712\":12,\"713\":2,\"726\":4,\"729\":4,\"730\":3,\"731\":2,\"734\":6,\"738\":2,\"745\":7,\"746\":7,\"754\":1,\"758\":4,\"762\":2,\"763\":2,\"766\":2,\"767\":6,\"774\":2,\"781\":2,\"782\":3,\"786\":2,\"793\":2,\"797\":2,\"806\":1,\"812\":2,\"817\":6,\"820\":1,\"821\":2,\"825\":17,\"826\":2,\"828\":6,\"830\":4,\"835\":5,\"857\":1,\"887\":2,\"889\":1,\"906\":3,\"932\":2,\"950\":2,\"968\":2,\"999\":1,\"1015\":1,\"1019\":1,\"1024\":1,\"1028\":1,\"1037\":2,\"1039\":2,\"1040\":1,\"1047\":2,\"1048\":4,\"1049\":13,\"1050\":13,\"1051\":2,\"1052\":19,\"1053\":4,\"1054\":2,\"1055\":2,\"1056\":13,\"1057\":2,\"1058\":4,\"1061\":1,\"1064\":5,\"1068\":9,\"1072\":2,\"1076\":35,\"1077\":3,\"1080\":2,\"1099\":1,\"1106\":2,\"1108\":2,\"1114\":1,\"1132\":2,\"1138\":8,\"1139\":7,\"1142\":14,\"1153\":3,\"1155\":4,\"1186\":4,\"1187\":3,\"1198\":3,\"1202\":3,\"1210\":2,\"1228\":3,\"1245\":3,\"1255\":3,\"1257\":1,\"1270\":1,\"1286\":3,\"1287\":3,\"1296\":1,\"1298\":4,\"1299\":4,\"1301\":6,\"1302\":4,\"1303\":4,\"1304\":6,\"1334\":7,\"1337\":2,\"1345\":3,\"1347\":3,\"1349\":2,\"1350\":2,\"1354\":2,\"1374\":1,\"1376\":1,\"1377\":2,\"1429\":1,\"1432\":1,\"1436\":1,\"1451\":2,\"1454\":5,\"1462\":2,\"1463\":2,\"1464\":2,\"1478\":4,\"1505\":2,\"1510\":2,\"1511\":1,\"1514\":2,\"1515\":3,\"1516\":7,\"1517\":3,\"1518\":1,\"1520\":1,\"1522\":2,\"1523\":3,\"1524\":11,\"1525\":7,\"1528\":2,\"1529\":4,\"1530\":2,\"1534\":2,\"1539\":2,\"1545\":2,\"1547\":1,\"1557\":2,\"1558\":1,\"1559\":1,\"1560\":2,\"1566\":4,\"1567\":4,\"1568\":2,\"1569\":4,\"1571\":4,\"1572\":2,\"1578\":2,\"1579\":2,\"1580\":2,\"1585\":2,\"1594\":3,\"1595\":3,\"1604\":2,\"1612\":2,\"1615\":2,\"1618\":7,\"1619\":9,\"1623\":3,\"1626\":2,\"1637\":2,\"1638\":9,\"1643\":2,\"1644\":2,\"1645\":2,\"1646\":2,\"1647\":1,\"1654\":2,\"1655\":2,\"1658\":2,\"1659\":3,\"1660\":2,\"1661\":2,\"1662\":3,\"1666\":4,\"1668\":4,\"1669\":2,\"1671\":3,\"1696\":2,\"1698\":1,\"1701\":2,\"1702\":2,\"1710\":2,\"1711\":2,\"1718\":1,\"1719\":9,\"1736\":2,\"1737\":1,\"1739\":4,\"1741\":3,\"1758\":2,\"1759\":3,\"1765\":2,\"1767\":4,\"1768\":3,\"1771\":3,\"1772\":3,\"1773\":30,\"1776\":4,\"1777\":3,\"1778\":5,\"1781\":6,\"1786\":1,\"1788\":3,\"1798\":7,\"1800\":6,\"1803\":3,\"1804\":11,\"1805\":19,\"1808\":8,\"1829\":5,\"1833\":2,\"1834\":4,\"1835\":3,\"1837\":4,\"1838\":2,\"1840\":2,\"1841\":2,\"1842\":5,\"1844\":4,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":9,\"1851\":20,\"1852\":6,\"1853\":5,\"1854\":4,\"1855\":3,\"1856\":1,\"1857\":4,\"1858\":1,\"1859\":3,\"1860\":4,\"1861\":2,\"1862\":6,\"1863\":5,\"1864\":2,\"1865\":2,\"1866\":2,\"1867\":2,\"1868\":4,\"1869\":2,\"1870\":1,\"1871\":4,\"1872\":4,\"1873\":4,\"1874\":5,\"1876\":2,\"1877\":10,\"1878\":19,\"1879\":6,\"1880\":6,\"1881\":3,\"1884\":1,\"1885\":1,\"1889\":3,\"1897\":3,\"1918\":2,\"1949\":1,\"1971\":3,\"1972\":2,\"1973\":1,\"2001\":2,\"2002\":6,\"2004\":2,\"2019\":2,\"2046\":1,\"2063\":4,\"2074\":4,\"2075\":4,\"2079\":2,\"2082\":30,\"2083\":3,\"2086\":1,\"2087\":1,\"2090\":6,\"2091\":9,\"2095\":7,\"2099\":1,\"2102\":1,\"2121\":2,\"2122\":2,\"2148\":1,\"2154\":3,\"2239\":5,\"2240\":6,\"2243\":8,\"2244\":16,\"2245\":9,\"2254\":2,\"2255\":16,\"2256\":9,\"2263\":7,\"2264\":6,\"2267\":2,\"2278\":8,\"2279\":16,\"2280\":9,\"2325\":1,\"2330\":2,\"2367\":1,\"2384\":1,\"2415\":2,\"2416\":1,\"2430\":1,\"2470\":1,\"2476\":1,\"2478\":1,\"2485\":1,\"2491\":1,\"2498\":1,\"2555\":1,\"2604\":1,\"2610\":1,\"2616\":1,\"2621\":1,\"2627\":1,\"2634\":1,\"2647\":1}}],[\"treated\",{\"1\":{\"710\":1,\"760\":2,\"1929\":1,\"1941\":1,\"1947\":1}}],[\"trees\",{\"1\":{\"1705\":1}}],[\"tree\",{\"0\":{\"620\":1},\"1\":{\"167\":1,\"169\":1,\"178\":1,\"181\":1,\"196\":1,\"200\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"242\":1,\"620\":3,\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"2294\":1,\"2361\":1,\"2366\":1,\"2379\":1,\"2382\":1,\"2384\":1,\"2430\":1,\"2555\":1,\"2564\":1,\"2574\":1,\"2576\":1,\"2601\":1,\"2618\":1,\"2646\":1,\"2650\":1}}],[\"trn2stm\",{\"0\":{\"590\":1},\"1\":{\"590\":2}}],[\"trn2ctm\",{\"0\":{\"588\":1},\"1\":{\"588\":2}}],[\"trn\",{\"1\":{\"588\":2,\"590\":2,\"2461\":1}}],[\"trg\",{\"1\":{\"560\":2}}],[\"trgspk\",{\"1\":{\"269\":2}}],[\"trillion\",{\"1\":{\"2154\":2}}],[\"trillions\",{\"1\":{\"2154\":1}}],[\"triton\",{\"1\":{\"1274\":1}}],[\"triangles\",{\"1\":{\"778\":1}}],[\"triangular\",{\"1\":{\"771\":1,\"778\":1,\"809\":1,\"1148\":1,\"1203\":1,\"1695\":1,\"1785\":1,\"1810\":2,\"2054\":1}}],[\"trial\",{\"1\":{\"429\":2}}],[\"triu\",{\"0\":{\"930\":1},\"1\":{\"771\":1,\"809\":1,\"930\":1,\"1140\":1,\"1148\":2,\"1169\":1,\"1203\":2,\"1778\":1,\"1850\":1,\"1851\":3,\"1852\":1,\"2003\":1,\"2026\":1,\"2054\":2,\"2090\":1,\"2243\":3,\"2244\":3,\"2255\":3,\"2279\":3}}],[\"triu=false\",{\"1\":{\"771\":1,\"809\":1}}],[\"trigger\",{\"1\":{\"623\":3,\"1007\":1,\"1034\":1}}],[\"trigger=\",{\"1\":{\"623\":1}}],[\"trimmed\",{\"1\":{\"929\":1}}],[\"trim\",{\"0\":{\"297\":1,\"585\":1,\"929\":1},\"1\":{\"286\":1,\"296\":1,\"297\":3,\"585\":3,\"863\":1,\"865\":1,\"929\":2,\"1148\":1,\"1215\":1,\"1216\":2,\"1409\":1}}],[\"tries\",{\"1\":{\"135\":2}}],[\"truncated\",{\"1\":{\"1462\":2,\"1464\":2}}],[\"truncation\",{\"1\":{\"116\":2,\"1065\":3,\"1066\":3,\"1069\":3}}],[\"truths\",{\"1\":{\"2499\":1,\"2617\":1,\"2635\":1}}],[\"truth\",{\"1\":{\"710\":2,\"1106\":1,\"1107\":1,\"1108\":1,\"1142\":1,\"1155\":1,\"1337\":1,\"1349\":1,\"1350\":1,\"1767\":1,\"1804\":2,\"1808\":3,\"1851\":3,\"1877\":1,\"1878\":1}}],[\"true|false\",{\"1\":{\"279\":1}}],[\"true\",{\"1\":{\"21\":5,\"22\":5,\"32\":2,\"34\":1,\"35\":2,\"36\":2,\"40\":1,\"42\":1,\"54\":2,\"59\":2,\"65\":1,\"70\":1,\"81\":1,\"91\":4,\"96\":4,\"97\":1,\"98\":1,\"102\":1,\"104\":2,\"105\":2,\"106\":1,\"110\":2,\"113\":2,\"115\":2,\"121\":1,\"122\":1,\"217\":1,\"274\":1,\"279\":1,\"629\":1,\"691\":1,\"692\":1,\"693\":1,\"697\":1,\"700\":1,\"711\":1,\"712\":2,\"730\":1,\"742\":1,\"749\":1,\"750\":3,\"752\":2,\"756\":1,\"760\":2,\"768\":2,\"785\":1,\"797\":1,\"825\":1,\"831\":1,\"868\":1,\"932\":1,\"943\":1,\"950\":1,\"955\":1,\"959\":1,\"965\":1,\"968\":1,\"972\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1019\":1,\"1028\":3,\"1037\":1,\"1039\":1,\"1051\":1,\"1052\":3,\"1053\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1064\":1,\"1097\":1,\"1115\":2,\"1133\":4,\"1138\":1,\"1139\":1,\"1140\":2,\"1145\":2,\"1148\":3,\"1149\":6,\"1150\":5,\"1158\":4,\"1167\":2,\"1168\":2,\"1171\":3,\"1172\":2,\"1179\":2,\"1180\":1,\"1187\":1,\"1190\":1,\"1196\":2,\"1197\":2,\"1198\":1,\"1200\":1,\"1202\":1,\"1203\":3,\"1204\":3,\"1206\":3,\"1207\":1,\"1214\":1,\"1218\":1,\"1222\":3,\"1239\":1,\"1241\":1,\"1243\":1,\"1244\":2,\"1248\":1,\"1251\":1,\"1257\":4,\"1269\":3,\"1271\":2,\"1272\":2,\"1273\":4,\"1282\":3,\"1284\":1,\"1286\":1,\"1287\":1,\"1340\":1,\"1352\":3,\"1373\":1,\"1376\":1,\"1462\":1,\"1463\":2,\"1465\":2,\"1505\":3,\"1516\":3,\"1524\":3,\"1525\":3,\"1531\":2,\"1532\":1,\"1534\":1,\"1537\":1,\"1539\":2,\"1545\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1581\":1,\"1602\":2,\"1603\":1,\"1611\":7,\"1622\":2,\"1638\":1,\"1639\":1,\"1640\":1,\"1643\":2,\"1644\":3,\"1645\":2,\"1654\":1,\"1658\":1,\"1659\":1,\"1669\":2,\"1671\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":2,\"1714\":1,\"1715\":2,\"1759\":1,\"1761\":2,\"1763\":3,\"1765\":4,\"1771\":1,\"1773\":1,\"1778\":17,\"1786\":1,\"1787\":1,\"1788\":2,\"1791\":1,\"1797\":1,\"1798\":1,\"1800\":1,\"1801\":5,\"1803\":4,\"1804\":8,\"1805\":16,\"1836\":1,\"1837\":1,\"1839\":2,\"1843\":1,\"1844\":4,\"1845\":2,\"1846\":1,\"1847\":4,\"1848\":4,\"1849\":4,\"1850\":17,\"1851\":10,\"1852\":18,\"1856\":1,\"1857\":4,\"1858\":2,\"1859\":2,\"1861\":3,\"1862\":4,\"1863\":2,\"1864\":3,\"1865\":3,\"1866\":2,\"1867\":1,\"1870\":2,\"1871\":3,\"1872\":1,\"1873\":1,\"1874\":1,\"1877\":14,\"1878\":8,\"1879\":1,\"1880\":3,\"1896\":1,\"1906\":2,\"1910\":1,\"1914\":1,\"1915\":1,\"1918\":2,\"1920\":1,\"1921\":1,\"1940\":1,\"1949\":1,\"1961\":1,\"1968\":1,\"1970\":2,\"1975\":3,\"1984\":4,\"1987\":2,\"1989\":2,\"1991\":2,\"2000\":1,\"2001\":3,\"2002\":3,\"2003\":2,\"2004\":3,\"2008\":1,\"2009\":1,\"2012\":1,\"2026\":2,\"2027\":3,\"2029\":2,\"2046\":1,\"2054\":2,\"2076\":4,\"2084\":1,\"2086\":3,\"2087\":3,\"2089\":1,\"2090\":6,\"2091\":1,\"2095\":5,\"2096\":2,\"2097\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2125\":2,\"2142\":1,\"2170\":2,\"2180\":2,\"2184\":1,\"2188\":2,\"2200\":1,\"2201\":1,\"2211\":2,\"2236\":3,\"2241\":3,\"2243\":7,\"2244\":7,\"2245\":1,\"2246\":2,\"2248\":2,\"2250\":2,\"2255\":6,\"2256\":1,\"2259\":1,\"2263\":5,\"2264\":5,\"2265\":1,\"2279\":7,\"2280\":1,\"2290\":1,\"2292\":2,\"2294\":1,\"2295\":1,\"2296\":1,\"2360\":1,\"2403\":1,\"2431\":4,\"2432\":3,\"2440\":4,\"2458\":1,\"2461\":2,\"2492\":1,\"2523\":1,\"2539\":1,\"2558\":2,\"2559\":2,\"2564\":3,\"2569\":1,\"2582\":1,\"2584\":1,\"2585\":5,\"2628\":1,\"2640\":2}}],[\"tr\",{\"1\":{\"57\":2,\"202\":1,\"285\":4,\"885\":1,\"1696\":1,\"1706\":1,\"1712\":1,\"2514\":1,\"2659\":1}}],[\"troubleshooting\",{\"0\":{\"44\":1}}],[\"trying\",{\"1\":{\"295\":1}}],[\"try\",{\"1\":{\"26\":2,\"34\":1,\"48\":2,\"127\":2,\"194\":1,\"243\":3,\"2363\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2506\":1,\"2525\":1,\"2542\":1,\"2545\":1,\"2653\":1}}],[\"tralsate\",{\"1\":{\"2458\":1,\"2523\":1}}],[\"trace\",{\"0\":{\"1751\":1},\"1\":{\"1751\":2}}],[\"track\",{\"1\":{\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1244\":2,\"1252\":2,\"1253\":2,\"1254\":1,\"1279\":1,\"1465\":2,\"1476\":1,\"2492\":5,\"2628\":5}}],[\"traducir\",{\"1\":{\"2387\":2}}],[\"trade\",{\"1\":{\"1712\":1,\"1715\":1}}],[\"traditional\",{\"1\":{\"100\":1,\"1917\":1}}],[\"trailing\",{\"1\":{\"1255\":1}}],[\"train=true\",{\"1\":{\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1}}],[\"trainable\",{\"1\":{\"754\":1,\"762\":1,\"826\":1,\"1245\":1,\"1248\":1,\"1851\":1,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2264\":1,\"2279\":1}}],[\"trainset\",{\"1\":{\"172\":1,\"174\":2}}],[\"traininig\",{\"1\":{\"106\":1}}],[\"trainings\",{\"1\":{\"2046\":1}}],[\"training=true\",{\"1\":{\"645\":1}}],[\"training\",{\"0\":{\"25\":1,\"32\":1,\"35\":1,\"55\":1,\"61\":1,\"65\":1,\"81\":1,\"86\":1,\"88\":1,\"92\":1,\"94\":2,\"97\":1,\"104\":1,\"121\":1,\"186\":1,\"204\":1,\"240\":1,\"433\":1,\"724\":1,\"727\":1,\"728\":1,\"834\":1,\"847\":1,\"923\":1,\"979\":1,\"988\":1,\"996\":1,\"997\":1,\"998\":1,\"1003\":1,\"1004\":1,\"1005\":1,\"1006\":1,\"1007\":1,\"1028\":1,\"1034\":1,\"2376\":1,\"2417\":1,\"2570\":1,\"2639\":1},\"1\":{\"16\":1,\"17\":2,\"19\":1,\"20\":1,\"21\":1,\"22\":2,\"24\":3,\"25\":1,\"26\":2,\"27\":1,\"32\":1,\"33\":1,\"35\":1,\"49\":1,\"56\":2,\"57\":3,\"62\":1,\"63\":1,\"65\":2,\"68\":1,\"69\":5,\"71\":2,\"72\":2,\"74\":3,\"80\":1,\"82\":3,\"84\":6,\"85\":2,\"91\":2,\"92\":6,\"94\":2,\"95\":1,\"106\":1,\"113\":4,\"120\":1,\"121\":4,\"124\":1,\"130\":1,\"148\":2,\"149\":1,\"150\":2,\"161\":1,\"164\":1,\"168\":3,\"170\":1,\"171\":1,\"172\":2,\"174\":2,\"179\":2,\"180\":1,\"235\":2,\"237\":1,\"238\":3,\"239\":3,\"240\":7,\"627\":5,\"645\":2,\"654\":1,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":1,\"693\":2,\"697\":1,\"698\":1,\"699\":1,\"701\":1,\"702\":1,\"708\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":3,\"728\":2,\"729\":1,\"730\":1,\"734\":1,\"735\":1,\"737\":1,\"739\":1,\"740\":1,\"741\":1,\"742\":2,\"747\":1,\"749\":1,\"750\":1,\"753\":1,\"754\":3,\"755\":1,\"757\":1,\"758\":1,\"761\":1,\"762\":1,\"763\":1,\"764\":1,\"766\":1,\"768\":1,\"770\":1,\"771\":1,\"772\":1,\"774\":1,\"775\":1,\"776\":1,\"779\":1,\"781\":1,\"782\":1,\"785\":1,\"786\":1,\"793\":1,\"794\":1,\"797\":1,\"802\":1,\"804\":1,\"806\":1,\"807\":1,\"809\":1,\"810\":1,\"813\":1,\"817\":1,\"818\":1,\"819\":1,\"820\":2,\"821\":3,\"822\":1,\"825\":4,\"826\":3,\"827\":1,\"828\":1,\"830\":1,\"832\":1,\"834\":1,\"835\":1,\"838\":1,\"847\":2,\"868\":2,\"923\":1,\"952\":1,\"976\":2,\"979\":1,\"988\":1,\"996\":1,\"997\":2,\"998\":2,\"1003\":1,\"1004\":1,\"1005\":1,\"1006\":1,\"1007\":2,\"1028\":1,\"1034\":2,\"1043\":2,\"1046\":1,\"1047\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1061\":1,\"1062\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1093\":4,\"1106\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1135\":1,\"1137\":1,\"1140\":1,\"1141\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1152\":1,\"1153\":1,\"1157\":1,\"1159\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1175\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1185\":1,\"1189\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1200\":1,\"1201\":1,\"1203\":1,\"1204\":1,\"1205\":1,\"1206\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1211\":2,\"1213\":1,\"1214\":1,\"1216\":2,\"1217\":1,\"1218\":1,\"1219\":1,\"1221\":1,\"1223\":1,\"1224\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1243\":1,\"1244\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1269\":2,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1275\":1,\"1277\":1,\"1279\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1286\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":1,\"1352\":2,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":2,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1398\":1,\"1430\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1454\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1466\":1,\"1467\":1,\"1469\":1,\"1470\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1484\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1505\":1,\"1507\":1,\"1509\":1,\"1510\":1,\"1511\":1,\"1513\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1521\":1,\"1522\":1,\"1523\":1,\"1524\":1,\"1525\":1,\"1528\":1,\"1529\":1,\"1530\":2,\"1531\":2,\"1533\":1,\"1534\":1,\"1536\":1,\"1538\":1,\"1539\":1,\"1541\":1,\"1542\":1,\"1544\":1,\"1545\":1,\"1546\":1,\"1548\":1,\"1550\":1,\"1551\":4,\"1552\":2,\"1553\":4,\"1554\":1,\"1556\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1562\":1,\"1563\":2,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1574\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1597\":1,\"1599\":1,\"1600\":3,\"1601\":1,\"1602\":1,\"1603\":4,\"1604\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1611\":1,\"1614\":1,\"1616\":1,\"1617\":1,\"1621\":1,\"1622\":3,\"1625\":1,\"1626\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1639\":1,\"1640\":1,\"1642\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1654\":1,\"1655\":1,\"1657\":1,\"1658\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1663\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1718\":1,\"1719\":3,\"1760\":1,\"1762\":1,\"1764\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1775\":1,\"1776\":1,\"1777\":1,\"1778\":2,\"1780\":1,\"1781\":1,\"1783\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1802\":1,\"1803\":1,\"1804\":1,\"1805\":2,\"1807\":1,\"1808\":1,\"1828\":1,\"1829\":1,\"1831\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":4,\"1851\":1,\"1852\":2,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":3,\"1878\":1,\"1879\":1,\"1880\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1895\":2,\"1900\":2,\"1903\":1,\"1906\":1,\"1908\":1,\"1910\":1,\"1911\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1932\":4,\"1952\":1,\"1954\":1,\"1956\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"1970\":1,\"1971\":1,\"1975\":1,\"1977\":1,\"1979\":1,\"1980\":1,\"1982\":1,\"1983\":1,\"1985\":1,\"1986\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"1996\":1,\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2025\":1,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2030\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2046\":1,\"2048\":1,\"2049\":1,\"2051\":1,\"2053\":1,\"2054\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2063\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2086\":2,\"2087\":6,\"2088\":1,\"2089\":1,\"2090\":6,\"2091\":1,\"2095\":5,\"2097\":4,\"2099\":3,\"2102\":1,\"2150\":1,\"2168\":1,\"2169\":1,\"2170\":1,\"2185\":1,\"2197\":1,\"2198\":1,\"2201\":2,\"2203\":1,\"2234\":1,\"2235\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2242\":1,\"2243\":5,\"2244\":5,\"2245\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":5,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":2,\"2261\":1,\"2262\":1,\"2263\":5,\"2264\":5,\"2265\":1,\"2267\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":5,\"2280\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2287\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2294\":2,\"2295\":1,\"2296\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":5,\"2303\":2,\"2304\":1,\"2305\":2,\"2347\":1,\"2350\":1,\"2355\":1,\"2372\":3,\"2373\":7,\"2375\":3,\"2377\":2,\"2378\":1,\"2384\":1,\"2385\":4,\"2399\":1,\"2401\":4,\"2403\":1,\"2420\":1,\"2423\":2,\"2424\":1,\"2429\":3,\"2430\":6,\"2433\":2,\"2436\":2,\"2437\":1,\"2438\":1,\"2440\":4,\"2452\":2,\"2468\":1,\"2535\":1,\"2537\":4,\"2539\":1,\"2542\":2,\"2543\":1,\"2546\":2,\"2547\":1,\"2554\":2,\"2555\":9,\"2558\":12,\"2559\":1,\"2562\":2,\"2563\":1,\"2564\":2,\"2565\":1,\"2569\":1,\"2571\":1,\"2573\":1,\"2583\":1,\"2584\":5,\"2600\":1,\"2639\":1}}],[\"traineroptions\",{\"0\":{\"2202\":1},\"1\":{\"2186\":1,\"2198\":2,\"2201\":3,\"2202\":2,\"2204\":1}}],[\"trainer\",{\"0\":{\"434\":1,\"2185\":1,\"2186\":1,\"2198\":1,\"2201\":2,\"2202\":1,\"2203\":1,\"2204\":1,\"2350\":2,\"2351\":1,\"2721\":1},\"1\":{\"87\":3,\"610\":1,\"615\":1,\"652\":2,\"654\":4,\"656\":1,\"834\":2,\"1007\":3,\"1034\":4,\"2096\":2,\"2097\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":1,\"2104\":1,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":1,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":1,\"2185\":6,\"2186\":2,\"2198\":4,\"2201\":8,\"2202\":2,\"2203\":6,\"2204\":2,\"2350\":5,\"2351\":2}}],[\"trained\",{\"0\":{\"99\":1,\"643\":1,\"645\":1,\"646\":1,\"2415\":1,\"2573\":1,\"2574\":1,\"2584\":1,\"2585\":1,\"2588\":1},\"1\":{\"20\":1,\"22\":1,\"28\":1,\"30\":2,\"96\":1,\"106\":1,\"112\":1,\"150\":1,\"155\":3,\"156\":1,\"157\":1,\"158\":1,\"164\":2,\"192\":1,\"198\":1,\"201\":1,\"235\":1,\"241\":1,\"245\":1,\"635\":5,\"637\":1,\"640\":1,\"642\":1,\"643\":4,\"645\":4,\"646\":2,\"658\":1,\"1179\":2,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"2254\":1,\"2355\":2,\"2377\":2,\"2415\":1,\"2417\":1,\"2429\":1,\"2446\":1,\"2479\":1,\"2500\":1,\"2518\":2,\"2552\":1,\"2573\":2,\"2574\":3,\"2575\":1,\"2583\":3,\"2584\":18,\"2585\":5,\"2588\":1,\"2597\":1,\"2600\":1}}],[\"train2\",{\"1\":{\"25\":1,\"75\":1,\"76\":2,\"77\":1,\"78\":1}}],[\"train\",{\"0\":{\"171\":1,\"251\":1,\"253\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"602\":1,\"622\":1,\"657\":1,\"937\":1,\"978\":1,\"1007\":1,\"1034\":1,\"1045\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2171\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2176\":1,\"2177\":1,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":1,\"2184\":1,\"2185\":1,\"2186\":1,\"2187\":1,\"2188\":1,\"2189\":1,\"2191\":1,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2202\":1,\"2203\":1,\"2204\":1,\"2205\":1,\"2206\":1,\"2207\":1,\"2208\":1,\"2210\":1,\"2211\":1,\"2212\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":1,\"2223\":1,\"2224\":1,\"2225\":1,\"2226\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2230\":1,\"2231\":1,\"2232\":1,\"2349\":1,\"2443\":1,\"2587\":1,\"2710\":1},\"1\":{\"4\":1,\"17\":1,\"20\":1,\"25\":14,\"27\":2,\"28\":2,\"34\":2,\"35\":1,\"36\":2,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"56\":2,\"57\":4,\"58\":1,\"59\":5,\"60\":3,\"62\":5,\"63\":3,\"64\":2,\"65\":1,\"66\":6,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":2,\"72\":4,\"74\":5,\"75\":6,\"76\":14,\"77\":8,\"78\":8,\"79\":6,\"80\":1,\"81\":1,\"82\":1,\"87\":6,\"88\":2,\"89\":1,\"91\":1,\"92\":2,\"96\":2,\"97\":1,\"106\":1,\"110\":6,\"121\":2,\"169\":1,\"170\":2,\"171\":4,\"172\":1,\"173\":2,\"174\":10,\"175\":1,\"183\":1,\"184\":1,\"186\":3,\"187\":2,\"191\":2,\"192\":2,\"194\":3,\"210\":3,\"211\":3,\"212\":3,\"215\":1,\"216\":1,\"217\":3,\"222\":3,\"223\":3,\"224\":3,\"229\":3,\"230\":3,\"231\":3,\"235\":1,\"237\":2,\"238\":15,\"239\":5,\"240\":37,\"241\":8,\"242\":6,\"251\":6,\"253\":6,\"255\":6,\"259\":6,\"265\":5,\"269\":5,\"274\":1,\"275\":2,\"276\":2,\"279\":2,\"281\":3,\"282\":2,\"283\":2,\"284\":2,\"285\":3,\"294\":1,\"297\":2,\"298\":2,\"301\":2,\"307\":6,\"315\":6,\"321\":2,\"327\":6,\"333\":4,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"380\":2,\"384\":4,\"391\":6,\"398\":28,\"399\":2,\"408\":6,\"416\":2,\"422\":6,\"429\":3,\"437\":2,\"443\":10,\"449\":6,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":6,\"485\":6,\"602\":3,\"604\":3,\"622\":3,\"626\":1,\"627\":2,\"645\":1,\"657\":3,\"711\":1,\"727\":2,\"728\":2,\"742\":1,\"889\":2,\"937\":3,\"942\":1,\"959\":1,\"978\":3,\"1001\":2,\"1007\":1,\"1026\":2,\"1034\":1,\"1035\":2,\"1045\":3,\"1149\":3,\"1150\":3,\"1551\":1,\"1553\":1,\"1639\":1,\"1778\":1,\"1852\":1,\"1962\":1,\"1964\":2,\"2046\":1,\"2096\":17,\"2097\":14,\"2098\":14,\"2099\":9,\"2100\":15,\"2101\":32,\"2102\":14,\"2103\":18,\"2104\":15,\"2105\":14,\"2106\":2,\"2107\":10,\"2108\":15,\"2109\":22,\"2110\":17,\"2111\":17,\"2112\":21,\"2113\":17,\"2114\":16,\"2115\":14,\"2116\":15,\"2117\":12,\"2118\":13,\"2167\":2,\"2168\":1,\"2170\":1,\"2171\":3,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":2,\"2176\":2,\"2177\":2,\"2178\":2,\"2179\":2,\"2180\":1,\"2181\":2,\"2182\":2,\"2184\":3,\"2185\":6,\"2186\":2,\"2187\":2,\"2188\":2,\"2189\":1,\"2191\":2,\"2192\":2,\"2193\":4,\"2194\":3,\"2195\":3,\"2196\":3,\"2197\":4,\"2198\":2,\"2199\":2,\"2200\":4,\"2201\":7,\"2202\":4,\"2203\":6,\"2204\":2,\"2205\":2,\"2206\":2,\"2207\":2,\"2208\":2,\"2209\":1,\"2210\":1,\"2211\":2,\"2212\":2,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2217\":1,\"2218\":2,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":2,\"2223\":2,\"2224\":2,\"2225\":1,\"2226\":2,\"2227\":1,\"2228\":2,\"2229\":2,\"2230\":2,\"2231\":1,\"2232\":2,\"2347\":2,\"2349\":3,\"2350\":5,\"2351\":3,\"2357\":9,\"2368\":3,\"2371\":6,\"2373\":25,\"2375\":31,\"2377\":15,\"2385\":8,\"2387\":4,\"2394\":1,\"2397\":2,\"2398\":2,\"2400\":2,\"2401\":2,\"2403\":2,\"2405\":2,\"2411\":2,\"2415\":1,\"2416\":1,\"2429\":1,\"2430\":18,\"2431\":8,\"2432\":14,\"2433\":13,\"2436\":14,\"2440\":15,\"2441\":4,\"2444\":1,\"2445\":1,\"2446\":1,\"2454\":1,\"2455\":3,\"2460\":4,\"2461\":3,\"2472\":4,\"2474\":4,\"2476\":4,\"2478\":4,\"2486\":3,\"2490\":3,\"2492\":1,\"2494\":5,\"2500\":5,\"2507\":4,\"2510\":6,\"2513\":4,\"2514\":1,\"2519\":2,\"2520\":5,\"2530\":1,\"2533\":2,\"2534\":2,\"2536\":2,\"2537\":2,\"2539\":2,\"2541\":2,\"2542\":1,\"2543\":1,\"2555\":27,\"2558\":23,\"2559\":17,\"2560\":1,\"2562\":14,\"2564\":12,\"2568\":12,\"2569\":9,\"2570\":2,\"2572\":5,\"2578\":9,\"2584\":21,\"2585\":14,\"2587\":1,\"2589\":1,\"2590\":1,\"2599\":1,\"2600\":4,\"2605\":3,\"2609\":3,\"2612\":6,\"2617\":1,\"2622\":3,\"2626\":3,\"2628\":1,\"2630\":7,\"2635\":1,\"2639\":2,\"2648\":4,\"2649\":4,\"2653\":1,\"2659\":1}}],[\"tranining\",{\"1\":{\"169\":1,\"181\":1}}],[\"traning\",{\"1\":{\"168\":1}}],[\"tranducer\",{\"1\":{\"112\":1}}],[\"transription\",{\"1\":{\"2385\":1}}],[\"transducuer\",{\"1\":{\"1302\":1}}],[\"transduction\",{\"1\":{\"1298\":1,\"1299\":1,\"1301\":1}}],[\"transducerdecoder\",{\"0\":{\"1270\":1},\"1\":{\"1270\":1}}],[\"transducerdecoderinterface\",{\"0\":{\"824\":1},\"1\":{\"725\":1,\"806\":1,\"824\":1}}],[\"transducers\",{\"1\":{\"1210\":1,\"1211\":1,\"1286\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":1,\"2452\":1}}],[\"transducertasks\",{\"0\":{\"825\":1},\"1\":{\"825\":1}}],[\"transducer\",{\"0\":{\"20\":1,\"112\":1,\"333\":1,\"635\":1,\"700\":1,\"712\":1,\"725\":1,\"726\":1,\"751\":1,\"766\":1,\"806\":1,\"824\":1,\"825\":2,\"827\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"853\":2,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"865\":1,\"866\":1,\"868\":1,\"884\":1,\"886\":1,\"891\":1,\"894\":1,\"895\":1,\"908\":1,\"910\":1,\"911\":1,\"912\":1,\"915\":1,\"917\":1,\"918\":1,\"922\":1,\"933\":1,\"934\":1,\"1046\":1,\"1047\":1,\"1048\":2,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":2,\"1058\":1,\"1059\":1,\"1060\":2,\"1061\":1,\"1062\":1,\"1063\":2,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1086\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":2,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1138\":2,\"1139\":2,\"1142\":1,\"1143\":1,\"1154\":1,\"1155\":1,\"1173\":1,\"1176\":2,\"1186\":1,\"1193\":2,\"1194\":1,\"1202\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1227\":1,\"1270\":1,\"1286\":1,\"1287\":1,\"1288\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1306\":1,\"1307\":1,\"1315\":1,\"1316\":1,\"1318\":1,\"1324\":1,\"1325\":1,\"1331\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1344\":1,\"1346\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1353\":1,\"1359\":1,\"2097\":1,\"2682\":1},\"1\":{\"20\":5,\"21\":6,\"22\":18,\"23\":2,\"24\":4,\"30\":1,\"112\":5,\"113\":4,\"115\":1,\"118\":3,\"119\":1,\"120\":1,\"124\":1,\"127\":1,\"136\":3,\"307\":2,\"333\":3,\"422\":2,\"443\":2,\"449\":2,\"635\":4,\"643\":3,\"645\":1,\"692\":1,\"700\":3,\"712\":1,\"725\":2,\"726\":2,\"751\":1,\"766\":2,\"806\":3,\"824\":2,\"825\":27,\"827\":2,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"853\":3,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"865\":1,\"866\":1,\"868\":3,\"884\":1,\"886\":1,\"891\":1,\"894\":3,\"895\":1,\"908\":1,\"910\":1,\"911\":1,\"912\":1,\"915\":1,\"917\":1,\"918\":1,\"922\":1,\"933\":1,\"934\":1,\"1046\":1,\"1047\":1,\"1048\":3,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":7,\"1058\":1,\"1059\":2,\"1060\":2,\"1061\":2,\"1062\":1,\"1063\":3,\"1064\":2,\"1065\":1,\"1066\":1,\"1067\":2,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":2,\"1083\":2,\"1084\":2,\"1085\":1,\"1086\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":2,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":3,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1138\":6,\"1139\":4,\"1142\":2,\"1143\":1,\"1154\":1,\"1155\":1,\"1171\":2,\"1173\":2,\"1176\":2,\"1186\":2,\"1193\":3,\"1194\":1,\"1202\":1,\"1210\":2,\"1211\":2,\"1224\":1,\"1225\":1,\"1226\":1,\"1227\":1,\"1270\":2,\"1286\":3,\"1287\":1,\"1288\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":2,\"1304\":2,\"1306\":1,\"1307\":1,\"1315\":1,\"1316\":1,\"1318\":1,\"1324\":1,\"1325\":1,\"1331\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":3,\"1337\":3,\"1338\":1,\"1344\":1,\"1346\":1,\"1348\":2,\"1349\":3,\"1350\":3,\"1353\":1,\"1359\":1,\"2097\":5}}],[\"transposelast\",{\"0\":{\"2299\":1},\"1\":{\"2299\":1}}],[\"transpose\",{\"0\":{\"1875\":1},\"1\":{\"1182\":2,\"1244\":1,\"1252\":1,\"1254\":1,\"1278\":1,\"1761\":1,\"1763\":1,\"1805\":1,\"1875\":5,\"2498\":3,\"2616\":3,\"2634\":3}}],[\"transposedmodule\",{\"0\":{\"1278\":1},\"1\":{\"1278\":1}}],[\"transposedlinear\",{\"0\":{\"1276\":1},\"1\":{\"1276\":1}}],[\"transposedln\",{\"0\":{\"1274\":1},\"1\":{\"1274\":1}}],[\"transposed=false\",{\"1\":{\"1177\":1,\"1188\":1,\"1199\":1,\"1209\":1,\"1212\":1,\"1235\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1311\":1,\"1358\":1,\"1545\":1}}],[\"transposed=true\",{\"1\":{\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1241\":1,\"1279\":1,\"1280\":1,\"1545\":1}}],[\"transposed\",{\"1\":{\"1130\":1,\"1156\":1,\"1233\":1,\"1243\":2,\"1244\":2,\"1251\":1,\"1252\":1,\"1254\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1510\":1,\"1545\":1,\"1830\":1,\"1831\":1,\"1832\":1}}],[\"transition\",{\"0\":{\"1357\":1},\"1\":{\"682\":1,\"758\":1,\"1357\":3,\"2078\":1,\"2079\":1,\"2081\":2}}],[\"translatotron\",{\"0\":{\"2002\":2},\"1\":{\"2002\":3,\"2452\":1}}],[\"translatotron2\",{\"0\":{\"1983\":1,\"1986\":1,\"1994\":1,\"2003\":2},\"1\":{\"1983\":1,\"1986\":1,\"1994\":1,\"2003\":4}}],[\"translated\",{\"0\":{\"202\":1},\"1\":{\"201\":2,\"202\":8}}],[\"translate\",{\"0\":{\"296\":1,\"2456\":1,\"2458\":1,\"2521\":1,\"2522\":1,\"2523\":1},\"1\":{\"201\":2,\"203\":1,\"205\":1,\"257\":1,\"261\":1,\"296\":4,\"781\":3,\"812\":2,\"2358\":1,\"2451\":1,\"2520\":1,\"2522\":1,\"2579\":1}}],[\"translation\",{\"0\":{\"199\":1,\"201\":1,\"2447\":1,\"2451\":1,\"2453\":1,\"2460\":1,\"2518\":1},\"1\":{\"130\":2,\"161\":2,\"164\":2,\"203\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"554\":1,\"812\":1,\"1984\":1,\"2001\":2,\"2002\":2,\"2003\":2,\"2004\":2,\"2447\":1,\"2448\":1,\"2451\":5,\"2452\":5,\"2456\":1,\"2457\":1,\"2459\":2,\"2460\":1,\"2516\":1,\"2518\":2}}],[\"trans\",{\"0\":{\"257\":1,\"261\":1,\"603\":1,\"938\":1},\"1\":{\"118\":2,\"210\":1,\"211\":1,\"212\":1,\"217\":1,\"257\":4,\"261\":4,\"296\":2,\"579\":1,\"603\":2,\"758\":2,\"781\":4,\"812\":4,\"825\":3,\"938\":2,\"2078\":2,\"2079\":2}}],[\"transcripciones\",{\"1\":{\"2387\":2}}],[\"transcript=entity\",{\"1\":{\"2476\":1}}],[\"transcript=sub\",{\"1\":{\"2476\":1}}],[\"transcript=text\",{\"1\":{\"2472\":1,\"2474\":1,\"2476\":1,\"2648\":1,\"2649\":1}}],[\"transcript=none\",{\"1\":{\"896\":1}}],[\"transcripts\",{\"1\":{\"2467\":1}}],[\"transcript\",{\"1\":{\"870\":1,\"896\":1,\"2024\":4,\"2027\":7,\"2028\":4,\"2195\":1,\"2472\":10,\"2474\":6,\"2476\":19,\"2568\":1,\"2648\":10,\"2649\":6}}],[\"transcriptions\",{\"1\":{\"237\":1}}],[\"transcription\",{\"1\":{\"58\":1,\"171\":1,\"237\":2,\"239\":3,\"551\":1,\"554\":1,\"557\":1,\"936\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"2373\":1,\"2385\":2,\"2412\":1,\"2430\":1,\"2518\":2,\"2555\":1}}],[\"transcribe\",{\"1\":{\"98\":2,\"249\":1,\"2128\":1,\"2129\":1}}],[\"transfomer\",{\"1\":{\"1133\":1,\"2001\":1,\"2004\":1}}],[\"transforner\",{\"1\":{\"786\":1}}],[\"transforms\",{\"1\":{\"1253\":1,\"1721\":1}}],[\"transformdataset\",{\"0\":{\"1000\":1},\"1\":{\"1000\":2}}],[\"transforminterface\",{\"0\":{\"957\":1},\"1\":{\"944\":1,\"946\":1,\"957\":1}}],[\"transformed\",{\"1\":{\"785\":5,\"1076\":9,\"1209\":5}}],[\"transformerpostencoder\",{\"0\":{\"2029\":1},\"1\":{\"2029\":1}}],[\"transformerdiscretesynthesizer\",{\"0\":{\"2001\":1},\"1\":{\"2001\":1}}],[\"transformerdecoder\",{\"0\":{\"1271\":1},\"1\":{\"1271\":1}}],[\"transformerdecoderlayer\",{\"0\":{\"827\":1},\"1\":{\"827\":1}}],[\"transformermddecoder\",{\"0\":{\"1273\":1},\"1\":{\"1273\":1}}],[\"transformerencoder\",{\"0\":{\"1272\":1},\"1\":{\"1272\":1}}],[\"transformersentenceencoderlayer\",{\"1\":{\"1933\":1}}],[\"transformerseparator\",{\"0\":{\"1669\":1},\"1\":{\"1669\":1}}],[\"transformers\",{\"0\":{\"1190\":1,\"1191\":1,\"1192\":1,\"1320\":1,\"1321\":1,\"2028\":1},\"1\":{\"1190\":3,\"1191\":2,\"1192\":2,\"1252\":1,\"1320\":1,\"1321\":1,\"2026\":1,\"2028\":2,\"2466\":1,\"2474\":1,\"2646\":1,\"2649\":1}}],[\"transformerlm\",{\"0\":{\"828\":1,\"1960\":1},\"1\":{\"286\":1,\"815\":1,\"828\":1,\"1960\":2}}],[\"transformer2\",{\"1\":{\"110\":2}}],[\"transformer\",{\"0\":{\"211\":1,\"223\":1,\"229\":1,\"703\":1,\"713\":1,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"724\":1,\"727\":1,\"728\":1,\"731\":1,\"732\":1,\"740\":1,\"741\":1,\"747\":1,\"748\":1,\"749\":1,\"763\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"775\":1,\"776\":1,\"777\":1,\"780\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"792\":1,\"799\":1,\"800\":1,\"801\":1,\"809\":1,\"810\":1,\"813\":1,\"818\":1,\"823\":1,\"826\":2,\"827\":1,\"828\":1,\"834\":1,\"843\":2,\"852\":1,\"862\":1,\"864\":1,\"888\":1,\"898\":1,\"909\":1,\"914\":1,\"916\":1,\"921\":1,\"923\":1,\"924\":1,\"1133\":1,\"1150\":1,\"1167\":1,\"1168\":1,\"1196\":1,\"1197\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1669\":1,\"1960\":1,\"2029\":1,\"2264\":3,\"2586\":1,\"2587\":1},\"1\":{\"21\":12,\"24\":2,\"30\":2,\"31\":1,\"103\":1,\"104\":4,\"173\":1,\"201\":2,\"203\":1,\"211\":5,\"221\":1,\"223\":7,\"228\":1,\"229\":7,\"235\":2,\"286\":8,\"295\":13,\"296\":4,\"692\":1,\"693\":1,\"703\":1,\"713\":2,\"714\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"724\":1,\"727\":1,\"728\":1,\"731\":1,\"732\":1,\"735\":3,\"740\":1,\"741\":1,\"747\":2,\"748\":1,\"749\":3,\"754\":16,\"755\":2,\"763\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"784\":1,\"785\":1,\"786\":2,\"787\":1,\"792\":1,\"799\":1,\"800\":1,\"801\":1,\"809\":1,\"810\":1,\"813\":1,\"815\":2,\"818\":1,\"823\":1,\"826\":17,\"827\":6,\"828\":2,\"834\":1,\"839\":1,\"843\":3,\"852\":1,\"862\":4,\"864\":1,\"888\":1,\"889\":1,\"898\":1,\"909\":1,\"914\":1,\"916\":1,\"921\":1,\"923\":1,\"924\":1,\"1116\":1,\"1133\":2,\"1149\":1,\"1150\":4,\"1160\":2,\"1161\":2,\"1162\":2,\"1163\":2,\"1164\":2,\"1167\":2,\"1168\":2,\"1171\":2,\"1177\":2,\"1196\":2,\"1197\":2,\"1204\":1,\"1206\":2,\"1214\":1,\"1215\":1,\"1252\":2,\"1253\":2,\"1254\":2,\"1269\":3,\"1271\":2,\"1272\":3,\"1273\":2,\"1279\":2,\"1429\":1,\"1430\":3,\"1505\":1,\"1537\":2,\"1539\":2,\"1552\":2,\"1581\":2,\"1669\":3,\"1670\":3,\"1778\":8,\"1798\":1,\"1850\":8,\"1851\":17,\"1852\":6,\"1874\":1,\"1878\":1,\"1953\":1,\"1955\":1,\"1960\":2,\"1971\":1,\"2001\":1,\"2004\":1,\"2029\":3,\"2086\":1,\"2087\":1,\"2090\":17,\"2091\":1,\"2243\":19,\"2244\":17,\"2245\":1,\"2255\":13,\"2256\":1,\"2264\":24,\"2279\":17,\"2280\":1,\"2357\":4,\"2363\":4,\"2364\":1,\"2377\":4,\"2400\":2,\"2436\":4,\"2440\":1,\"2500\":4,\"2506\":1,\"2507\":1,\"2510\":2,\"2512\":5,\"2513\":1,\"2536\":2,\"2558\":9,\"2559\":6,\"2560\":1,\"2562\":4,\"2564\":1,\"2574\":1,\"2578\":4,\"2584\":2,\"2586\":2,\"2587\":1,\"2589\":1,\"2590\":1,\"2600\":2,\"2617\":1,\"2635\":1,\"2641\":1,\"2653\":4,\"2654\":1,\"2657\":5,\"2658\":1}}],[\"transformation\",{\"0\":{\"958\":2},\"1\":{\"754\":1,\"826\":1,\"944\":1,\"958\":2,\"959\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1252\":2,\"1253\":2,\"1254\":2,\"1279\":1,\"1917\":1,\"2264\":1}}],[\"transform\",{\"0\":{\"752\":1,\"760\":1,\"778\":1,\"831\":1,\"881\":2,\"932\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"946\":2,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"954\":1,\"955\":1,\"956\":1,\"957\":2,\"958\":1,\"960\":1,\"961\":1,\"962\":1,\"963\":1,\"964\":1,\"965\":1,\"966\":1,\"967\":1,\"968\":1,\"969\":1,\"970\":1,\"971\":1,\"972\":1,\"973\":1,\"999\":1,\"1886\":2,\"1887\":1,\"1888\":1,\"2678\":1},\"1\":{\"60\":1,\"629\":1,\"752\":1,\"760\":1,\"778\":1,\"785\":1,\"799\":1,\"831\":1,\"881\":2,\"932\":1,\"939\":2,\"940\":1,\"941\":2,\"942\":1,\"943\":2,\"944\":2,\"945\":2,\"946\":2,\"947\":1,\"948\":2,\"949\":2,\"950\":2,\"951\":2,\"952\":2,\"953\":2,\"954\":1,\"955\":2,\"956\":2,\"957\":3,\"958\":1,\"959\":2,\"960\":2,\"961\":1,\"962\":2,\"963\":2,\"964\":2,\"965\":2,\"966\":2,\"967\":1,\"968\":2,\"969\":2,\"970\":2,\"971\":1,\"972\":2,\"973\":2,\"999\":3,\"1000\":4,\"1012\":1,\"1076\":1,\"1209\":1,\"1471\":1,\"1643\":1,\"1644\":2,\"1799\":1,\"1886\":2,\"1887\":1,\"1888\":1}}],[\"transferred\",{\"1\":{\"754\":1}}],[\"transfer\",{\"0\":{\"28\":1,\"66\":1,\"658\":1,\"2583\":1},\"1\":{\"27\":1,\"28\":5,\"30\":1,\"140\":1,\"155\":1,\"244\":1,\"635\":1,\"637\":1,\"642\":1,\"658\":3,\"754\":2,\"1466\":1,\"1713\":1,\"1714\":1,\"2257\":1,\"2261\":1,\"2262\":1,\"2573\":4,\"2584\":2,\"2585\":4}}],[\"tuve\",{\"1\":{\"2457\":1}}],[\"tutorail\",{\"1\":{\"2380\":1}}],[\"tutorial1\",{\"1\":{\"2388\":1,\"2524\":1}}],[\"tutorial2\",{\"0\":{\"2388\":1,\"2524\":1}}],[\"tutorials\",{\"1\":{\"138\":1,\"2401\":1,\"2409\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2466\":1,\"2481\":2,\"2537\":1,\"2618\":2,\"2635\":1}}],[\"tutorial\",{\"0\":{\"83\":1,\"2354\":1,\"2374\":1,\"2421\":1,\"2429\":1,\"2434\":1,\"2544\":1,\"2557\":1},\"1\":{\"14\":1,\"15\":1,\"85\":1,\"112\":1,\"140\":4,\"146\":1,\"147\":1,\"161\":2,\"162\":4,\"163\":2,\"164\":1,\"195\":2,\"244\":4,\"2354\":1,\"2355\":1,\"2372\":2,\"2374\":1,\"2378\":1,\"2380\":1,\"2384\":1,\"2388\":3,\"2389\":3,\"2390\":1,\"2391\":1,\"2393\":1,\"2394\":2,\"2395\":2,\"2396\":2,\"2398\":2,\"2401\":3,\"2408\":1,\"2421\":1,\"2422\":4,\"2423\":1,\"2424\":1,\"2429\":9,\"2434\":1,\"2437\":1,\"2449\":1,\"2451\":1,\"2465\":1,\"2467\":1,\"2481\":1,\"2503\":1,\"2524\":2,\"2525\":4,\"2526\":1,\"2527\":1,\"2529\":2,\"2530\":2,\"2531\":2,\"2532\":2,\"2534\":1,\"2537\":3,\"2542\":2,\"2544\":1,\"2545\":4,\"2546\":1,\"2547\":1,\"2554\":2,\"2557\":1,\"2563\":1,\"2572\":1,\"2573\":1,\"2583\":1,\"2600\":1}}],[\"tunethresholdfromscore\",{\"0\":{\"2341\":1},\"1\":{\"2341\":2}}],[\"tune\",{\"0\":{\"2573\":1},\"1\":{\"1241\":1,\"1427\":1,\"2573\":1}}],[\"tuning\",{\"0\":{\"66\":1},\"1\":{\"155\":1,\"1180\":1,\"1243\":1,\"2099\":1,\"2377\":4,\"2436\":2,\"2562\":2,\"2564\":1,\"2573\":2,\"2639\":2}}],[\"turns\",{\"1\":{\"1245\":1}}],[\"turnlist\",{\"1\":{\"287\":1}}],[\"turned\",{\"1\":{\"82\":1}}],[\"turn\",{\"1\":{\"59\":1,\"135\":1,\"1785\":1,\"2373\":1,\"2430\":1,\"2555\":1}}],[\"tupleofint\",{\"1\":{\"745\":1,\"746\":1}}],[\"tuples\",{\"1\":{\"658\":1,\"1412\":2,\"1752\":2,\"2197\":1}}],[\"tuple\",{\"0\":{\"1721\":1},\"1\":{\"21\":3,\"60\":2,\"115\":4,\"605\":1,\"617\":1,\"618\":2,\"619\":2,\"623\":1,\"629\":1,\"647\":1,\"685\":2,\"691\":5,\"692\":2,\"694\":1,\"695\":2,\"696\":2,\"697\":5,\"698\":1,\"699\":1,\"706\":4,\"710\":1,\"712\":5,\"734\":6,\"745\":7,\"746\":7,\"749\":2,\"751\":1,\"765\":1,\"767\":2,\"773\":4,\"782\":1,\"796\":2,\"797\":3,\"798\":1,\"806\":5,\"815\":2,\"817\":4,\"824\":4,\"828\":6,\"838\":2,\"1003\":1,\"1004\":1,\"1005\":1,\"1013\":1,\"1028\":1,\"1046\":3,\"1052\":4,\"1053\":1,\"1057\":1,\"1060\":1,\"1063\":2,\"1073\":4,\"1081\":1,\"1115\":2,\"1133\":4,\"1141\":2,\"1170\":2,\"1171\":1,\"1172\":2,\"1176\":1,\"1187\":1,\"1190\":6,\"1193\":2,\"1202\":1,\"1204\":2,\"1206\":1,\"1214\":4,\"1221\":2,\"1244\":6,\"1256\":2,\"1270\":5,\"1273\":4,\"1286\":1,\"1287\":1,\"1356\":1,\"1427\":1,\"1484\":4,\"1522\":8,\"1523\":16,\"1545\":6,\"1576\":3,\"1577\":4,\"1595\":1,\"1601\":4,\"1720\":1,\"1721\":4,\"1733\":1,\"1752\":2,\"1767\":2,\"1768\":2,\"1804\":1,\"1830\":5,\"1831\":5,\"1832\":5,\"1878\":1,\"1892\":1,\"1893\":1,\"1905\":4,\"1955\":1,\"1957\":4,\"1958\":2,\"1959\":2,\"1960\":4,\"1964\":2,\"1970\":2,\"1975\":1,\"1984\":3,\"2001\":2,\"2007\":1,\"2008\":1,\"2009\":1,\"2012\":1,\"2027\":2,\"2076\":2,\"2153\":1,\"2182\":1,\"2185\":1,\"2189\":1,\"2197\":10,\"2201\":2,\"2203\":1,\"2205\":2,\"2208\":1}}],[\"timit\",{\"1\":{\"2599\":1,\"2600\":2}}],[\"timezone\",{\"1\":{\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1}}],[\"timedomainmse\",{\"0\":{\"1668\":1},\"1\":{\"1668\":1}}],[\"timedomainl1\",{\"0\":{\"1666\":1},\"1\":{\"1666\":1}}],[\"timedomainloss\",{\"0\":{\"1667\":1},\"1\":{\"1466\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1667\":1,\"1668\":1}}],[\"timewarp\",{\"0\":{\"956\":1,\"1919\":1},\"1\":{\"956\":2,\"1919\":2}}],[\"timemask\",{\"0\":{\"955\":1},\"1\":{\"955\":2}}],[\"time2\",{\"1\":{\"740\":3,\"741\":3,\"771\":5,\"775\":3,\"776\":3,\"785\":13,\"809\":4,\"1209\":13,\"1993\":4}}],[\"time1\",{\"1\":{\"740\":3,\"741\":3,\"771\":5,\"775\":3,\"776\":3,\"785\":9,\"809\":7,\"1209\":9,\"1993\":3}}],[\"timescale\",{\"1\":{\"1248\":1,\"2255\":1,\"2260\":3}}],[\"timesync\",{\"0\":{\"698\":1,\"699\":1,\"707\":1},\"1\":{\"698\":1,\"699\":1,\"707\":1}}],[\"times\",{\"1\":{\"107\":1,\"108\":4,\"109\":2,\"110\":2,\"115\":1,\"203\":3,\"501\":2,\"725\":1,\"726\":1,\"858\":1,\"875\":2,\"914\":1,\"1244\":1,\"1252\":1}}],[\"timesteps\",{\"1\":{\"2260\":3,\"2274\":2}}],[\"timestep\",{\"1\":{\"23\":1,\"113\":1,\"1069\":3,\"1334\":2}}],[\"time=4\",{\"1\":{\"87\":1}}],[\"time=0\",{\"1\":{\"87\":9}}],[\"time\",{\"0\":{\"107\":1,\"206\":1,\"972\":1,\"973\":1,\"1039\":1,\"1040\":1,\"1354\":1,\"1466\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1919\":1,\"1947\":1,\"1948\":2,\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1},\"1\":{\"17\":2,\"23\":7,\"51\":2,\"84\":1,\"87\":1,\"102\":1,\"107\":3,\"108\":1,\"110\":1,\"113\":1,\"119\":7,\"203\":4,\"204\":1,\"218\":5,\"225\":5,\"232\":5,\"301\":1,\"307\":2,\"408\":2,\"449\":2,\"501\":1,\"595\":1,\"648\":1,\"692\":3,\"698\":3,\"699\":3,\"700\":4,\"711\":10,\"714\":8,\"715\":8,\"716\":8,\"718\":8,\"719\":8,\"720\":8,\"721\":8,\"722\":8,\"723\":2,\"729\":1,\"730\":1,\"770\":2,\"772\":3,\"810\":2,\"813\":2,\"816\":1,\"818\":2,\"825\":9,\"836\":1,\"914\":1,\"943\":1,\"950\":8,\"955\":1,\"956\":6,\"965\":1,\"968\":11,\"972\":3,\"973\":9,\"1037\":5,\"1039\":3,\"1040\":4,\"1048\":4,\"1057\":1,\"1059\":1,\"1081\":7,\"1086\":10,\"1099\":1,\"1133\":2,\"1138\":4,\"1139\":4,\"1141\":7,\"1162\":1,\"1170\":7,\"1182\":5,\"1198\":2,\"1214\":1,\"1243\":1,\"1248\":1,\"1257\":8,\"1273\":1,\"1279\":1,\"1354\":1,\"1400\":2,\"1423\":6,\"1430\":4,\"1451\":1,\"1462\":3,\"1463\":1,\"1466\":2,\"1470\":2,\"1471\":2,\"1478\":1,\"1514\":1,\"1517\":7,\"1518\":1,\"1520\":1,\"1523\":1,\"1524\":1,\"1525\":1,\"1551\":1,\"1553\":1,\"1557\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1571\":1,\"1585\":1,\"1602\":2,\"1604\":5,\"1605\":1,\"1612\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1623\":1,\"1637\":1,\"1638\":5,\"1639\":1,\"1640\":1,\"1641\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":2,\"1666\":2,\"1667\":2,\"1668\":2,\"1670\":2,\"1671\":1,\"1719\":1,\"1741\":1,\"1766\":1,\"1785\":2,\"1797\":1,\"1869\":1,\"1905\":2,\"1914\":1,\"1915\":1,\"1917\":2,\"1919\":5,\"1921\":2,\"1922\":2,\"1923\":2,\"1925\":2,\"1927\":2,\"1928\":2,\"1929\":2,\"1935\":3,\"1936\":2,\"1938\":2,\"1939\":2,\"1941\":2,\"1943\":3,\"1946\":2,\"1947\":6,\"1948\":7,\"2001\":1,\"2082\":1,\"2086\":1,\"2087\":1,\"2194\":3,\"2199\":2,\"2210\":1,\"2212\":1,\"2236\":1,\"2259\":2,\"2260\":5,\"2267\":1,\"2274\":1,\"2358\":1,\"2360\":4,\"2365\":5,\"2387\":3,\"2389\":2,\"2392\":4,\"2397\":1,\"2398\":1,\"2400\":1,\"2401\":2,\"2403\":1,\"2405\":1,\"2408\":2,\"2420\":1,\"2422\":1,\"2425\":4,\"2440\":4,\"2441\":1,\"2449\":2,\"2452\":1,\"2455\":1,\"2458\":4,\"2460\":2,\"2461\":1,\"2465\":2,\"2481\":2,\"2498\":1,\"2503\":2,\"2508\":5,\"2510\":5,\"2515\":5,\"2520\":2,\"2523\":5,\"2525\":2,\"2528\":4,\"2533\":1,\"2534\":1,\"2536\":1,\"2537\":2,\"2539\":1,\"2541\":1,\"2542\":1,\"2543\":1,\"2545\":1,\"2548\":4,\"2554\":1,\"2556\":1,\"2558\":1,\"2559\":1,\"2560\":1,\"2564\":2,\"2572\":1,\"2579\":1,\"2582\":4,\"2616\":1,\"2634\":1,\"2641\":1,\"2655\":5,\"2660\":5}}],[\"tight\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"tikhonov\",{\"1\":{\"1746\":1}}],[\"tik\",{\"0\":{\"1746\":1},\"1\":{\"1719\":4,\"1746\":2}}],[\"tidigits=\",{\"1\":{\"2567\":2}}],[\"tidigits\",{\"1\":{\"2565\":1,\"2566\":2,\"2567\":7,\"2568\":5,\"2571\":1}}],[\"tid\",{\"1\":{\"1143\":1,\"1144\":1}}],[\"tile\",{\"1\":{\"1078\":1}}],[\"tick\",{\"1\":{\"648\":1}}],[\"ticks\",{\"1\":{\"648\":1}}],[\"tiene\",{\"1\":{\"2457\":1}}],[\"tied\",{\"1\":{\"1245\":1}}],[\"ties\",{\"1\":{\"1242\":1}}],[\"tie=true\",{\"1\":{\"1166\":1}}],[\"tie\",{\"1\":{\"255\":4,\"807\":1,\"1166\":2,\"1177\":1,\"1241\":1,\"1242\":1,\"1244\":3,\"1252\":3,\"1254\":3,\"1958\":1}}],[\"tips\",{\"0\":{\"148\":1,\"182\":1,\"185\":1,\"187\":1,\"188\":1}}],[\"tian\",{\"1\":{\"130\":1}}],[\"title\",{\"1\":{\"130\":1,\"171\":1,\"175\":1,\"185\":1,\"193\":1,\"238\":2,\"2357\":5,\"2363\":3,\"2371\":1,\"2498\":6,\"2506\":1,\"2510\":1,\"2512\":1,\"2578\":5,\"2612\":1,\"2616\":6,\"2630\":1,\"2634\":6,\"2653\":3,\"2657\":1}}],[\"title=\",{\"1\":{\"130\":9}}],[\"tiny\",{\"1\":{\"15\":1,\"85\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1715\":1,\"1739\":1,\"2411\":1}}],[\"tau\",{\"1\":{\"2266\":1,\"2267\":2,\"2270\":2,\"2271\":2,\"2272\":2}}],[\"tadelayer\",{\"0\":{\"1872\":1},\"1\":{\"1872\":2}}],[\"taderesblock\",{\"0\":{\"1873\":1},\"1\":{\"1871\":1,\"1873\":3}}],[\"tade\",{\"0\":{\"1872\":1,\"1873\":1},\"1\":{\"1871\":1,\"1872\":2,\"1873\":1}}],[\"tas\",{\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"taslp\",{\"1\":{\"1661\":1,\"1662\":1}}],[\"taslp21\",{\"1\":{\"1523\":1}}],[\"tasnet\",{\"0\":{\"2368\":1,\"2486\":1,\"2605\":1,\"2622\":1},\"1\":{\"1375\":1,\"1379\":2,\"1664\":2,\"1665\":2,\"2368\":4,\"2371\":3,\"2486\":4,\"2605\":4,\"2612\":3,\"2622\":4,\"2630\":4,\"2641\":1}}],[\"task5\",{\"0\":{\"2501\":1}}],[\"task4\",{\"0\":{\"2500\":1}}],[\"task3\",{\"0\":{\"2497\":1}}],[\"task2\",{\"0\":{\"2459\":1,\"2490\":1}}],[\"task1\",{\"0\":{\"2457\":1,\"2485\":1}}],[\"tasks\",{\"0\":{\"825\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2492\":1,\"2628\":1,\"2707\":1},\"1\":{\"56\":1,\"79\":1,\"102\":1,\"118\":3,\"136\":1,\"150\":2,\"162\":1,\"766\":1,\"825\":5,\"997\":1,\"1132\":1,\"1269\":1,\"1327\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"2046\":2,\"2096\":2,\"2097\":1,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2106\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":1,\"2118\":2,\"2168\":2,\"2170\":1,\"2197\":1,\"2209\":1,\"2384\":1,\"2385\":1,\"2387\":4,\"2388\":1,\"2389\":1,\"2394\":2,\"2400\":2,\"2408\":1,\"2421\":1,\"2422\":1,\"2441\":1,\"2449\":1,\"2465\":1,\"2468\":1,\"2474\":1,\"2481\":2,\"2503\":1,\"2524\":1,\"2525\":1,\"2530\":2,\"2536\":2,\"2544\":1,\"2545\":1,\"2618\":1,\"2643\":1,\"2649\":1}}],[\"task\",{\"0\":{\"22\":1,\"55\":1,\"56\":1,\"118\":1,\"844\":1,\"1099\":1,\"2099\":1,\"2106\":1,\"2352\":2,\"2353\":2,\"2388\":1,\"2524\":1,\"2589\":1,\"2590\":1,\"2722\":1},\"1\":{\"22\":2,\"23\":1,\"41\":1,\"47\":1,\"49\":1,\"56\":8,\"57\":3,\"59\":6,\"60\":4,\"62\":1,\"69\":1,\"74\":1,\"83\":1,\"92\":1,\"98\":1,\"99\":3,\"100\":1,\"102\":1,\"112\":2,\"113\":1,\"118\":3,\"119\":1,\"140\":1,\"148\":1,\"161\":1,\"162\":1,\"244\":1,\"307\":2,\"343\":2,\"350\":2,\"357\":2,\"408\":2,\"443\":2,\"455\":2,\"491\":2,\"627\":1,\"812\":2,\"825\":1,\"844\":2,\"987\":1,\"1057\":2,\"1099\":1,\"1377\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1600\":1,\"1603\":1,\"1622\":1,\"1773\":1,\"1837\":1,\"1951\":1,\"2044\":1,\"2046\":3,\"2052\":1,\"2068\":1,\"2082\":1,\"2096\":5,\"2097\":11,\"2098\":5,\"2099\":14,\"2100\":5,\"2101\":5,\"2102\":5,\"2103\":6,\"2104\":6,\"2105\":5,\"2106\":2,\"2107\":5,\"2108\":5,\"2109\":5,\"2110\":5,\"2111\":5,\"2112\":5,\"2113\":5,\"2114\":5,\"2115\":5,\"2116\":5,\"2117\":5,\"2118\":5,\"2128\":1,\"2129\":1,\"2137\":1,\"2168\":4,\"2170\":2,\"2178\":2,\"2179\":2,\"2184\":1,\"2191\":1,\"2196\":1,\"2209\":1,\"2240\":1,\"2278\":1,\"2307\":1,\"2308\":1,\"2350\":1,\"2352\":5,\"2353\":5,\"2372\":1,\"2385\":3,\"2388\":1,\"2390\":2,\"2391\":2,\"2394\":7,\"2395\":2,\"2397\":1,\"2399\":1,\"2400\":1,\"2403\":1,\"2410\":2,\"2429\":1,\"2451\":1,\"2452\":1,\"2467\":1,\"2468\":1,\"2487\":1,\"2491\":1,\"2497\":1,\"2500\":1,\"2501\":1,\"2524\":1,\"2526\":2,\"2527\":2,\"2529\":1,\"2530\":7,\"2531\":2,\"2533\":1,\"2535\":1,\"2536\":1,\"2539\":1,\"2554\":1,\"2568\":1,\"2593\":2}}],[\"tac\",{\"0\":{\"1535\":1,\"1559\":1},\"1\":{\"1430\":1,\"1471\":1,\"1535\":3,\"1559\":2,\"1670\":1,\"1671\":1}}],[\"tactoron2\",{\"1\":{\"822\":1,\"2088\":1}}],[\"tacotron`\",{\"1\":{\"2079\":1}}],[\"tacotron\",{\"0\":{\"222\":1,\"2000\":1,\"2078\":1,\"2081\":1,\"2083\":1,\"2092\":1,\"2093\":1,\"2095\":3},\"1\":{\"217\":2,\"461\":1,\"701\":1,\"758\":1,\"1983\":1,\"1986\":1,\"1994\":1,\"1999\":1,\"2000\":2,\"2078\":3,\"2081\":3,\"2083\":4,\"2092\":1,\"2093\":1,\"2095\":7,\"2134\":1,\"2364\":2,\"2507\":4,\"2510\":15,\"2513\":4,\"2654\":2,\"2658\":2}}],[\"tacotron2loss\",{\"0\":{\"822\":1},\"1\":{\"822\":1}}],[\"tacotron2\",{\"0\":{\"210\":1,\"701\":1,\"702\":1,\"762\":1,\"764\":1,\"802\":1,\"803\":1,\"821\":2,\"822\":1,\"837\":1,\"871\":1,\"879\":1,\"2263\":3},\"1\":{\"197\":1,\"198\":1,\"210\":5,\"212\":1,\"221\":1,\"222\":7,\"235\":1,\"240\":16,\"241\":4,\"242\":3,\"295\":10,\"701\":1,\"702\":1,\"762\":1,\"764\":1,\"802\":1,\"803\":1,\"821\":5,\"822\":2,\"837\":1,\"871\":1,\"879\":1,\"1017\":3,\"2002\":1,\"2088\":1,\"2263\":9,\"2363\":6,\"2506\":2,\"2510\":4,\"2512\":3,\"2653\":6,\"2657\":3}}],[\"taps=10\",{\"1\":{\"962\":1,\"1758\":1}}],[\"taps\",{\"1\":{\"251\":2,\"730\":1,\"756\":1,\"1158\":1,\"1239\":1,\"1525\":1,\"1611\":1,\"1701\":6,\"1703\":4,\"1737\":1,\"1739\":1,\"1758\":2,\"1759\":3,\"1778\":1,\"1791\":1,\"1852\":1,\"1860\":3,\"1883\":4}}],[\"tables\",{\"1\":{\"295\":1}}],[\"table\",{\"0\":{\"2598\":1},\"1\":{\"237\":2,\"950\":1,\"968\":1}}],[\"tarfile\",{\"1\":{\"1969\":2}}],[\"tarinfo\",{\"1\":{\"1961\":1}}],[\"tar\",{\"1\":{\"167\":4,\"178\":4,\"196\":3,\"200\":1,\"210\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"216\":1,\"222\":2,\"223\":2,\"229\":2,\"230\":2,\"234\":3,\"2567\":2}}],[\"target1\",{\"1\":{\"987\":1}}],[\"target=false\",{\"1\":{\"987\":1}}],[\"target=none\",{\"1\":{\"834\":1}}],[\"targetspeakerextractiontask\",{\"0\":{\"2117\":1},\"1\":{\"2117\":1}}],[\"targets\",{\"1\":{\"617\":1,\"738\":2,\"739\":1,\"759\":2,\"925\":2,\"1211\":1,\"1224\":1,\"1269\":1,\"1287\":1,\"1336\":1,\"1348\":1,\"1655\":1}}],[\"target\",{\"0\":{\"924\":1,\"1924\":1},\"1\":{\"28\":1,\"30\":1,\"56\":1,\"57\":1,\"59\":3,\"109\":1,\"119\":1,\"265\":2,\"269\":2,\"274\":1,\"294\":2,\"429\":2,\"560\":1,\"610\":2,\"626\":2,\"627\":1,\"652\":2,\"659\":2,\"660\":2,\"661\":4,\"662\":2,\"731\":3,\"734\":1,\"735\":1,\"742\":1,\"745\":4,\"746\":4,\"750\":1,\"754\":4,\"755\":2,\"759\":5,\"767\":1,\"768\":3,\"817\":1,\"821\":4,\"822\":2,\"825\":27,\"826\":4,\"828\":1,\"834\":3,\"852\":1,\"905\":2,\"920\":1,\"924\":2,\"925\":1,\"975\":2,\"979\":1,\"987\":2,\"1042\":2,\"1048\":1,\"1059\":11,\"1099\":2,\"1115\":4,\"1142\":3,\"1145\":1,\"1154\":2,\"1155\":1,\"1173\":11,\"1181\":2,\"1186\":2,\"1210\":2,\"1211\":1,\"1224\":1,\"1247\":1,\"1248\":1,\"1298\":3,\"1299\":3,\"1301\":3,\"1302\":3,\"1303\":3,\"1304\":3,\"1327\":4,\"1334\":2,\"1336\":1,\"1337\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1462\":3,\"1463\":3,\"1464\":2,\"1551\":1,\"1554\":1,\"1560\":3,\"1604\":2,\"1654\":1,\"1659\":2,\"1660\":3,\"1661\":3,\"1662\":2,\"1719\":6,\"1778\":2,\"1804\":4,\"1805\":2,\"1808\":5,\"1809\":1,\"1841\":1,\"1879\":2,\"1924\":3,\"1931\":1,\"1932\":2,\"1933\":2,\"1934\":2,\"1944\":1,\"1957\":1,\"1960\":1,\"2000\":2,\"2001\":2,\"2002\":2,\"2004\":2,\"2046\":1,\"2078\":1,\"2086\":3,\"2087\":3,\"2088\":2,\"2090\":2,\"2091\":4,\"2095\":2,\"2152\":1,\"2156\":1,\"2197\":3,\"2200\":1,\"2243\":2,\"2244\":2,\"2245\":4,\"2255\":2,\"2256\":4,\"2257\":1,\"2259\":2,\"2261\":1,\"2263\":2,\"2264\":2,\"2280\":4,\"2311\":1,\"2341\":2,\"2419\":1,\"2456\":1,\"2460\":1,\"2461\":1,\"2518\":1,\"2521\":1,\"2573\":1,\"2584\":1,\"2585\":1}}],[\"tao\",{\"1\":{\"130\":1}}],[\"tanaka21\",{\"1\":{\"1529\":1,\"1568\":1}}],[\"tanaka\",{\"1\":{\"1529\":1,\"1568\":1}}],[\"tan\",{\"1\":{\"130\":1,\"1523\":1}}],[\"tanh\",{\"1\":{\"117\":1,\"725\":1,\"825\":1,\"1064\":1,\"1067\":1,\"1082\":1,\"1505\":1,\"1515\":2,\"1528\":2,\"1529\":2,\"1534\":1,\"1539\":1,\"1626\":1,\"1654\":1,\"1658\":1,\"1659\":2,\"1669\":1,\"1807\":1}}],[\"takamichi\",{\"1\":{\"130\":1}}],[\"takaaki\",{\"1\":{\"130\":2}}],[\"taking\",{\"1\":{\"102\":1,\"104\":1}}],[\"taken\",{\"1\":{\"1211\":1,\"1224\":1,\"1336\":1,\"1348\":1,\"1622\":1,\"2324\":1}}],[\"takenori\",{\"1\":{\"130\":2,\"195\":1}}],[\"takeda\",{\"1\":{\"130\":1}}],[\"take\",{\"1\":{\"29\":1,\"113\":1,\"144\":1,\"167\":1,\"178\":1,\"234\":2,\"652\":1,\"656\":1,\"684\":1,\"685\":1,\"745\":1,\"746\":1,\"997\":1,\"998\":1,\"1430\":1,\"1670\":1,\"2372\":1,\"2375\":2,\"2394\":1,\"2397\":1,\"2409\":1,\"2429\":1,\"2467\":1,\"2499\":1,\"2530\":1,\"2533\":1,\"2552\":1,\"2559\":1,\"2584\":2,\"2617\":1,\"2635\":1,\"2638\":1}}],[\"takes\",{\"1\":{\"28\":1,\"84\":1,\"196\":1,\"200\":1,\"204\":1,\"240\":1,\"740\":1,\"741\":1,\"753\":1,\"757\":1,\"761\":1,\"775\":1,\"776\":1,\"779\":1,\"832\":1,\"875\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":2,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1511\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1638\":1,\"1642\":1,\"1644\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2355\":1,\"2358\":1,\"2362\":1,\"2372\":3,\"2383\":1,\"2384\":1,\"2387\":1,\"2393\":1,\"2394\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2409\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2466\":1,\"2520\":1,\"2529\":1,\"2530\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2550\":1,\"2551\":1,\"2552\":1,\"2558\":1,\"2571\":1,\"2579\":1,\"2584\":4,\"2585\":1}}],[\"tag=\",{\"1\":{\"2589\":1,\"2590\":1}}],[\"tag=str\",{\"1\":{\"2364\":2,\"2507\":1,\"2510\":4,\"2513\":1,\"2654\":2,\"2658\":2}}],[\"tag=medium\",{\"1\":{\"98\":1}}],[\"tag\",{\"1\":{\"97\":1,\"98\":4,\"307\":2,\"315\":2,\"321\":2,\"333\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":2,\"437\":2,\"443\":2,\"464\":4,\"470\":4,\"478\":2,\"485\":2,\"1441\":1,\"1659\":4,\"2357\":5,\"2358\":1,\"2363\":6,\"2364\":2,\"2371\":2,\"2372\":1,\"2506\":2,\"2507\":1,\"2510\":7,\"2512\":2,\"2513\":1,\"2514\":1,\"2520\":2,\"2568\":1,\"2578\":5,\"2579\":1,\"2584\":2,\"2585\":2,\"2592\":1,\"2599\":2,\"2600\":11,\"2612\":2,\"2614\":1,\"2615\":1,\"2630\":2,\"2632\":1,\"2633\":1,\"2653\":6,\"2654\":2,\"2657\":2,\"2658\":2,\"2659\":1}}],[\"tags=none\",{\"1\":{\"2315\":1}}],[\"tags\",{\"0\":{\"7\":1},\"1\":{\"207\":1}}],[\"tails\",{\"1\":{\"2126\":1}}],[\"tails=\",{\"1\":{\"1888\":1}}],[\"tails=none\",{\"1\":{\"1886\":1}}],[\"tail\",{\"1\":{\"17\":1,\"87\":1,\"186\":1,\"929\":1,\"1833\":3,\"1886\":1,\"1888\":1,\"2559\":1}}],[\"tenemos\",{\"1\":{\"2457\":1}}],[\"tensor2\",{\"1\":{\"2259\":2}}],[\"tensor1\",{\"1\":{\"2259\":2}}],[\"tensororint\",{\"1\":{\"1698\":1,\"1704\":1,\"1705\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1713\":1,\"1715\":1}}],[\"tensororcomplextensor\",{\"1\":{\"1377\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1659\":2,\"1669\":1,\"1671\":1}}],[\"tensordot\",{\"1\":{\"1686\":1}}],[\"tensor=false\",{\"1\":{\"826\":1}}],[\"tensor=\",{\"1\":{\"751\":1,\"1060\":2,\"1176\":1}}],[\"tensors\",{\"1\":{\"60\":1,\"697\":1,\"713\":2,\"749\":1,\"786\":2,\"802\":1,\"804\":2,\"906\":2,\"925\":2,\"1071\":1,\"1141\":1,\"1170\":1,\"1187\":5,\"1202\":5,\"1256\":2,\"1286\":2,\"1287\":2,\"1553\":1,\"1660\":2,\"1661\":2,\"1662\":1,\"1704\":1,\"1719\":1,\"1739\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1845\":1,\"1846\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1856\":1,\"1858\":1,\"2078\":2}}],[\"tensor\",{\"0\":{\"927\":1,\"1318\":1,\"1330\":1,\"1724\":1},\"1\":{\"26\":1,\"60\":12,\"80\":1,\"174\":3,\"617\":2,\"629\":2,\"676\":8,\"677\":5,\"678\":3,\"679\":3,\"681\":5,\"682\":6,\"684\":5,\"685\":3,\"686\":4,\"687\":4,\"688\":4,\"689\":4,\"690\":3,\"691\":30,\"692\":10,\"693\":4,\"694\":13,\"695\":8,\"696\":8,\"697\":39,\"698\":3,\"699\":3,\"700\":7,\"701\":4,\"702\":4,\"705\":3,\"706\":19,\"708\":4,\"710\":9,\"711\":28,\"712\":6,\"713\":2,\"714\":6,\"715\":6,\"716\":6,\"717\":1,\"718\":6,\"719\":6,\"720\":6,\"721\":6,\"722\":6,\"723\":4,\"725\":6,\"726\":2,\"729\":2,\"734\":13,\"735\":6,\"737\":3,\"738\":2,\"740\":6,\"741\":6,\"742\":2,\"749\":14,\"750\":4,\"751\":7,\"754\":12,\"755\":7,\"758\":6,\"759\":4,\"760\":1,\"762\":2,\"763\":2,\"764\":2,\"765\":6,\"766\":2,\"767\":5,\"770\":5,\"771\":16,\"772\":6,\"773\":8,\"774\":3,\"775\":6,\"776\":6,\"777\":1,\"778\":1,\"781\":6,\"782\":1,\"785\":26,\"786\":2,\"793\":3,\"794\":4,\"796\":8,\"797\":17,\"798\":10,\"802\":3,\"804\":2,\"806\":12,\"809\":16,\"810\":5,\"812\":1,\"813\":4,\"815\":8,\"817\":10,\"818\":5,\"820\":5,\"821\":13,\"822\":7,\"824\":12,\"825\":26,\"826\":14,\"827\":3,\"828\":17,\"829\":6,\"830\":4,\"831\":1,\"835\":7,\"838\":6,\"852\":5,\"857\":2,\"865\":1,\"884\":1,\"885\":2,\"887\":2,\"899\":6,\"900\":6,\"901\":7,\"902\":6,\"903\":5,\"904\":2,\"905\":5,\"906\":2,\"907\":4,\"909\":2,\"913\":1,\"915\":2,\"917\":2,\"920\":2,\"921\":2,\"924\":2,\"925\":1,\"926\":5,\"927\":4,\"928\":6,\"929\":4,\"932\":1,\"997\":1,\"998\":1,\"1010\":3,\"1011\":3,\"1019\":2,\"1025\":3,\"1036\":1,\"1037\":2,\"1039\":2,\"1040\":2,\"1046\":14,\"1047\":1,\"1048\":6,\"1049\":8,\"1050\":8,\"1051\":3,\"1052\":10,\"1053\":2,\"1054\":3,\"1055\":3,\"1056\":8,\"1057\":10,\"1058\":5,\"1059\":6,\"1060\":4,\"1061\":1,\"1062\":2,\"1063\":4,\"1064\":2,\"1065\":10,\"1066\":7,\"1067\":1,\"1068\":8,\"1069\":5,\"1070\":1,\"1071\":4,\"1072\":1,\"1073\":10,\"1074\":2,\"1075\":6,\"1076\":37,\"1077\":2,\"1079\":1,\"1080\":1,\"1081\":11,\"1082\":1,\"1083\":3,\"1084\":1,\"1086\":10,\"1099\":2,\"1101\":1,\"1102\":1,\"1106\":6,\"1107\":5,\"1108\":6,\"1111\":2,\"1113\":7,\"1114\":4,\"1116\":10,\"1117\":4,\"1119\":3,\"1121\":2,\"1123\":2,\"1125\":2,\"1127\":2,\"1132\":3,\"1133\":23,\"1138\":8,\"1139\":7,\"1140\":11,\"1141\":11,\"1142\":28,\"1145\":13,\"1148\":11,\"1149\":18,\"1150\":18,\"1153\":3,\"1154\":1,\"1155\":5,\"1158\":2,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1169\":11,\"1170\":11,\"1171\":18,\"1172\":6,\"1173\":6,\"1176\":7,\"1177\":1,\"1178\":5,\"1179\":7,\"1180\":5,\"1181\":7,\"1182\":1,\"1184\":2,\"1186\":28,\"1187\":3,\"1190\":14,\"1191\":2,\"1192\":2,\"1193\":8,\"1194\":1,\"1195\":2,\"1198\":2,\"1200\":6,\"1201\":2,\"1202\":3,\"1203\":11,\"1204\":5,\"1205\":1,\"1206\":12,\"1207\":2,\"1209\":26,\"1210\":21,\"1211\":4,\"1214\":14,\"1215\":3,\"1216\":5,\"1217\":2,\"1218\":8,\"1219\":1,\"1221\":5,\"1222\":3,\"1224\":4,\"1226\":1,\"1227\":2,\"1230\":1,\"1239\":2,\"1244\":14,\"1248\":2,\"1252\":4,\"1253\":5,\"1254\":4,\"1255\":5,\"1256\":1,\"1269\":7,\"1270\":12,\"1272\":6,\"1273\":20,\"1279\":1,\"1282\":3,\"1284\":2,\"1285\":4,\"1286\":3,\"1287\":7,\"1298\":17,\"1299\":17,\"1300\":2,\"1301\":20,\"1302\":18,\"1303\":18,\"1304\":21,\"1306\":2,\"1318\":2,\"1326\":1,\"1330\":2,\"1334\":9,\"1336\":4,\"1337\":8,\"1344\":1,\"1346\":1,\"1348\":4,\"1349\":8,\"1350\":8,\"1352\":5,\"1360\":3,\"1362\":2,\"1364\":2,\"1365\":2,\"1371\":12,\"1373\":2,\"1374\":4,\"1375\":9,\"1376\":6,\"1377\":5,\"1427\":4,\"1430\":2,\"1431\":2,\"1432\":3,\"1433\":3,\"1435\":2,\"1436\":2,\"1439\":2,\"1440\":2,\"1441\":4,\"1445\":2,\"1446\":1,\"1451\":4,\"1454\":11,\"1455\":4,\"1462\":4,\"1463\":8,\"1464\":4,\"1466\":2,\"1470\":2,\"1471\":2,\"1484\":2,\"1505\":8,\"1510\":7,\"1511\":7,\"1514\":4,\"1515\":8,\"1516\":18,\"1517\":4,\"1522\":2,\"1523\":8,\"1524\":13,\"1525\":11,\"1528\":8,\"1529\":5,\"1530\":3,\"1531\":5,\"1534\":8,\"1539\":8,\"1543\":1,\"1545\":2,\"1551\":4,\"1552\":19,\"1553\":17,\"1554\":13,\"1557\":4,\"1558\":9,\"1563\":3,\"1564\":1,\"1572\":2,\"1576\":2,\"1577\":2,\"1585\":4,\"1594\":4,\"1595\":2,\"1600\":5,\"1601\":2,\"1602\":4,\"1603\":4,\"1604\":2,\"1611\":8,\"1612\":4,\"1615\":4,\"1616\":4,\"1617\":4,\"1622\":3,\"1623\":4,\"1626\":9,\"1637\":4,\"1638\":1,\"1639\":4,\"1640\":4,\"1641\":2,\"1643\":3,\"1644\":7,\"1645\":9,\"1646\":2,\"1654\":9,\"1655\":4,\"1658\":9,\"1659\":10,\"1660\":8,\"1661\":8,\"1662\":7,\"1669\":8,\"1670\":2,\"1671\":9,\"1678\":2,\"1681\":1,\"1685\":1,\"1688\":3,\"1693\":2,\"1695\":2,\"1696\":4,\"1697\":4,\"1698\":4,\"1701\":2,\"1702\":2,\"1703\":2,\"1704\":3,\"1705\":3,\"1706\":2,\"1707\":4,\"1708\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1716\":1,\"1719\":16,\"1721\":1,\"1722\":1,\"1724\":1,\"1727\":2,\"1733\":3,\"1735\":3,\"1736\":2,\"1737\":2,\"1739\":5,\"1740\":1,\"1741\":2,\"1742\":2,\"1744\":1,\"1751\":1,\"1755\":2,\"1756\":3,\"1758\":1,\"1759\":2,\"1761\":2,\"1763\":2,\"1765\":7,\"1766\":4,\"1767\":6,\"1768\":5,\"1771\":9,\"1772\":8,\"1773\":183,\"1774\":2,\"1776\":8,\"1777\":6,\"1778\":50,\"1781\":20,\"1785\":3,\"1786\":3,\"1787\":7,\"1788\":10,\"1791\":2,\"1797\":5,\"1798\":18,\"1800\":13,\"1803\":9,\"1804\":65,\"1805\":59,\"1808\":27,\"1810\":1,\"1811\":5,\"1829\":9,\"1830\":2,\"1831\":2,\"1832\":2,\"1833\":13,\"1834\":5,\"1835\":11,\"1836\":14,\"1837\":83,\"1838\":10,\"1839\":9,\"1840\":7,\"1841\":7,\"1842\":7,\"1843\":7,\"1844\":16,\"1845\":2,\"1846\":3,\"1847\":3,\"1848\":3,\"1849\":3,\"1850\":48,\"1851\":70,\"1852\":30,\"1853\":12,\"1854\":8,\"1855\":10,\"1856\":3,\"1857\":10,\"1858\":3,\"1859\":10,\"1860\":10,\"1861\":4,\"1862\":12,\"1863\":18,\"1864\":11,\"1865\":13,\"1866\":5,\"1867\":5,\"1868\":15,\"1869\":5,\"1870\":3,\"1871\":13,\"1872\":10,\"1873\":10,\"1874\":14,\"1875\":1,\"1876\":4,\"1877\":56,\"1878\":74,\"1879\":14,\"1880\":11,\"1881\":7,\"1884\":10,\"1885\":8,\"1889\":8,\"1892\":12,\"1893\":12,\"1902\":2,\"1904\":2,\"1906\":4,\"1909\":2,\"1910\":2,\"1911\":2,\"1912\":2,\"1914\":2,\"1915\":2,\"1916\":2,\"1917\":6,\"1918\":5,\"1919\":2,\"1920\":2,\"1921\":3,\"1922\":3,\"1925\":2,\"1927\":2,\"1928\":2,\"1929\":2,\"1935\":2,\"1936\":4,\"1938\":3,\"1939\":3,\"1940\":2,\"1941\":2,\"1943\":2,\"1946\":3,\"1947\":2,\"1948\":1,\"1949\":2,\"1951\":2,\"1953\":6,\"1954\":2,\"1955\":6,\"1956\":2,\"1957\":13,\"1958\":8,\"1959\":5,\"1960\":13,\"1963\":2,\"1964\":2,\"1970\":10,\"1971\":4,\"1975\":18,\"1980\":6,\"1981\":2,\"1984\":17,\"1985\":7,\"1987\":2,\"1989\":2,\"1991\":2,\"1993\":14,\"1996\":2,\"1999\":4,\"2000\":13,\"2001\":20,\"2002\":24,\"2004\":11,\"2012\":1,\"2026\":2,\"2027\":16,\"2029\":6,\"2034\":1,\"2036\":1,\"2038\":1,\"2044\":1,\"2046\":11,\"2049\":5,\"2050\":1,\"2052\":1,\"2054\":5,\"2055\":1,\"2068\":1,\"2076\":14,\"2077\":6,\"2078\":7,\"2079\":5,\"2081\":2,\"2082\":196,\"2083\":4,\"2084\":8,\"2086\":39,\"2087\":39,\"2088\":5,\"2089\":12,\"2090\":40,\"2091\":22,\"2095\":41,\"2152\":2,\"2153\":3,\"2161\":1,\"2162\":1,\"2163\":1,\"2168\":2,\"2170\":6,\"2175\":2,\"2185\":2,\"2198\":2,\"2199\":3,\"2201\":3,\"2203\":2,\"2205\":6,\"2208\":1,\"2230\":2,\"2233\":2,\"2235\":6,\"2237\":5,\"2239\":3,\"2240\":109,\"2241\":5,\"2243\":28,\"2244\":41,\"2245\":22,\"2246\":2,\"2248\":2,\"2250\":2,\"2252\":5,\"2254\":5,\"2255\":41,\"2256\":22,\"2257\":3,\"2258\":11,\"2259\":10,\"2260\":36,\"2261\":3,\"2262\":3,\"2263\":26,\"2264\":26,\"2265\":4,\"2266\":5,\"2267\":5,\"2269\":1,\"2270\":1,\"2271\":1,\"2273\":1,\"2274\":1,\"2275\":2,\"2277\":6,\"2278\":121,\"2279\":41,\"2280\":22,\"2281\":2,\"2283\":2,\"2285\":2,\"2287\":4,\"2290\":2,\"2292\":5,\"2294\":14,\"2295\":4,\"2296\":4,\"2301\":2,\"2302\":2,\"2303\":1,\"2304\":2,\"2305\":2,\"2474\":1,\"2498\":4,\"2521\":4,\"2522\":4,\"2523\":4,\"2616\":4,\"2634\":4,\"2649\":1}}],[\"tensorflow\",{\"1\":{\"17\":1,\"1688\":1,\"1735\":3,\"1756\":1}}],[\"tensorboardlogger\",{\"0\":{\"996\":1},\"1\":{\"996\":1}}],[\"tensorboard\",{\"0\":{\"89\":1,\"188\":1,\"996\":1},\"1\":{\"17\":8,\"89\":2,\"188\":1,\"240\":5,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"429\":4,\"629\":1,\"799\":1,\"996\":3,\"2186\":2,\"2193\":1,\"2199\":1,\"2202\":4,\"2204\":2,\"2401\":5,\"2440\":5,\"2537\":5,\"2558\":5,\"2571\":5}}],[\"tesnor\",{\"1\":{\"2239\":1}}],[\"tests\",{\"0\":{\"2645\":1},\"1\":{\"2638\":1,\"2642\":1,\"2645\":1}}],[\"test|1925|10725|98\",{\"1\":{\"2572\":1}}],[\"test|1925|6325|98\",{\"1\":{\"2572\":1}}],[\"test|130|773|85\",{\"1\":{\"2564\":1}}],[\"test|130|773|63\",{\"1\":{\"2441\":1}}],[\"test|130|773|95\",{\"1\":{\"2440\":1}}],[\"test|130|2695|93\",{\"1\":{\"2564\":1}}],[\"test|130|2695|98\",{\"1\":{\"2440\":1}}],[\"test|130|2695|81\",{\"1\":{\"2441\":1}}],[\"test|130|2565|93\",{\"1\":{\"2564\":1}}],[\"test|130|2565|98\",{\"1\":{\"2440\":1}}],[\"test|130|2565|80\",{\"1\":{\"2441\":1}}],[\"test=false\",{\"1\":{\"1466\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1604\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1666\":1,\"1667\":1,\"1668\":1}}],[\"testing\",{\"0\":{\"2372\":1,\"2496\":1,\"2614\":1,\"2632\":1},\"1\":{\"1245\":1,\"1251\":1,\"1418\":1,\"1423\":1,\"2372\":1,\"2385\":1,\"2429\":1,\"2554\":1,\"2565\":1}}],[\"tested\",{\"1\":{\"41\":1,\"84\":1}}],[\"test\",{\"0\":{\"1745\":1},\"1\":{\"5\":2,\"11\":3,\"15\":4,\"98\":4,\"110\":3,\"175\":3,\"185\":3,\"191\":1,\"193\":3,\"194\":1,\"201\":4,\"203\":5,\"237\":1,\"238\":1,\"239\":1,\"253\":2,\"1003\":2,\"1004\":2,\"1438\":1,\"1551\":2,\"1553\":2,\"1570\":1,\"1667\":1,\"1745\":2,\"2373\":12,\"2375\":9,\"2377\":4,\"2385\":1,\"2386\":1,\"2387\":1,\"2397\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":2,\"2405\":1,\"2411\":1,\"2430\":9,\"2431\":2,\"2432\":10,\"2433\":6,\"2436\":4,\"2440\":2,\"2444\":1,\"2445\":1,\"2446\":1,\"2533\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":2,\"2541\":1,\"2543\":1,\"2555\":13,\"2558\":4,\"2559\":5,\"2562\":4,\"2564\":2,\"2568\":3,\"2569\":2,\"2584\":8,\"2585\":4,\"2638\":1,\"2642\":1,\"2645\":3}}],[\"ter\",{\"0\":{\"2446\":1},\"1\":{\"2440\":1,\"2441\":1,\"2564\":1,\"2572\":1}}],[\"terhardt\",{\"1\":{\"1904\":1}}],[\"terminology\",{\"1\":{\"2573\":1}}],[\"terminal\",{\"1\":{\"85\":1}}],[\"term\",{\"1\":{\"74\":1,\"700\":1,\"740\":1,\"741\":1,\"775\":1,\"776\":1,\"1132\":3,\"1269\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1715\":1}}],[\"terms\",{\"1\":{\"37\":1,\"1890\":1,\"2363\":1,\"2506\":1,\"2512\":1,\"2653\":1,\"2657\":1}}],[\"temb=none\",{\"1\":{\"1631\":1,\"1633\":1,\"1635\":1}}],[\"temb\",{\"1\":{\"1517\":3,\"1518\":2,\"1520\":2,\"1631\":1,\"1633\":1,\"1635\":1}}],[\"tempo\",{\"1\":{\"952\":3,\"1389\":1,\"1414\":1,\"1778\":1,\"2081\":1}}],[\"temporalconvnetinformed\",{\"0\":{\"1665\":1},\"1\":{\"1665\":1}}],[\"temporalconvnet\",{\"0\":{\"1379\":1,\"1664\":1},\"1\":{\"1379\":1,\"1664\":2,\"1665\":1}}],[\"temporalblock\",{\"0\":{\"1378\":1,\"1663\":1},\"1\":{\"1378\":1,\"1663\":2}}],[\"temporal\",{\"1\":{\"796\":1,\"1377\":1,\"1658\":1,\"1696\":1,\"1697\":1,\"1702\":1}}],[\"temporary\",{\"1\":{\"286\":1,\"296\":1,\"2520\":1}}],[\"temp\",{\"1\":{\"698\":1,\"1115\":4}}],[\"temperature\",{\"1\":{\"98\":1,\"249\":2,\"700\":2,\"1842\":1,\"2294\":3}}],[\"template\",{\"1\":{\"46\":1,\"2361\":1,\"2366\":1,\"2379\":1,\"2382\":1,\"2384\":1,\"2391\":1,\"2394\":1,\"2397\":1,\"2430\":1,\"2461\":1,\"2527\":1,\"2530\":1,\"2533\":1,\"2543\":1,\"2555\":1,\"2564\":1,\"2566\":1,\"2576\":1,\"2601\":1,\"2618\":1,\"2637\":2,\"2640\":1,\"2646\":1,\"2650\":1}}],[\"team\",{\"1\":{\"1454\":2,\"1695\":1}}],[\"teacher\",{\"1\":{\"399\":2,\"455\":2,\"464\":2,\"470\":2,\"735\":2,\"754\":5,\"1778\":3,\"1804\":3,\"1805\":3,\"1850\":3,\"1851\":3,\"1877\":3,\"1878\":4,\"1985\":1,\"2002\":3,\"2079\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":3,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2279\":3}}],[\"techqnique\",{\"1\":{\"295\":1}}],[\"technologies\",{\"1\":{\"2468\":1}}],[\"technology\",{\"1\":{\"130\":1}}],[\"technique\",{\"1\":{\"76\":1,\"80\":1,\"1302\":1,\"1303\":1,\"1304\":1}}],[\"tee\",{\"1\":{\"201\":1,\"202\":1}}],[\"text1\",{\"1\":{\"2474\":3,\"2649\":3}}],[\"text1line\",{\"1\":{\"1398\":1}}],[\"textcleaner\",{\"0\":{\"2133\":1},\"1\":{\"2133\":2,\"2134\":1}}],[\"textiowrapper\",{\"1\":{\"2099\":1}}],[\"textencoder\",{\"0\":{\"1798\":1,\"1874\":1},\"1\":{\"1798\":2,\"1874\":2}}],[\"text3line\",{\"1\":{\"1398\":1}}],[\"text=text\",{\"1\":{\"2478\":1}}],[\"text=\",{\"1\":{\"2472\":1,\"2474\":1,\"2476\":1,\"2648\":1,\"2649\":1}}],[\"text=false\",{\"1\":{\"936\":1}}],[\"text=dump\",{\"1\":{\"98\":1}}],[\"text2speech\",{\"1\":{\"2364\":3,\"2365\":2,\"2507\":3,\"2508\":2,\"2510\":8,\"2513\":3,\"2514\":3,\"2515\":2,\"2654\":3,\"2655\":2,\"2658\":3,\"2659\":3,\"2660\":2}}],[\"text2wav\",{\"0\":{\"1852\":1},\"1\":{\"1852\":1,\"2363\":3,\"2506\":3,\"2653\":3}}],[\"text2mel\",{\"1\":{\"1778\":6,\"1852\":10,\"2363\":2,\"2506\":2,\"2653\":2}}],[\"text2line\",{\"1\":{\"1398\":1}}],[\"text2vocabulary\",{\"0\":{\"582\":1},\"1\":{\"582\":2}}],[\"text2tokens\",{\"1\":{\"2119\":1,\"2120\":1,\"2124\":1,\"2129\":1,\"2130\":2,\"2132\":1,\"2136\":1,\"2474\":1,\"2649\":1}}],[\"text2token\",{\"0\":{\"579\":1},\"1\":{\"579\":2}}],[\"texts\",{\"1\":{\"461\":1}}],[\"textdata\",{\"1\":{\"182\":1}}],[\"text\",{\"0\":{\"158\":1,\"184\":1,\"201\":1,\"202\":1,\"233\":1,\"304\":1,\"313\":1,\"320\":1,\"326\":1,\"332\":1,\"338\":1,\"390\":1,\"396\":1,\"407\":1,\"414\":1,\"428\":1,\"448\":1,\"454\":1,\"461\":1,\"484\":1,\"490\":1,\"1398\":1,\"1417\":2,\"1419\":1,\"1420\":2,\"1422\":1,\"1424\":2,\"1798\":1,\"1874\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1,\"2124\":1,\"2125\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2133\":1,\"2135\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2453\":1,\"2460\":1,\"2502\":1,\"2708\":1},\"1\":{\"47\":1,\"57\":6,\"58\":1,\"59\":1,\"60\":5,\"84\":2,\"98\":4,\"130\":1,\"144\":1,\"171\":1,\"175\":2,\"182\":1,\"184\":1,\"185\":2,\"193\":2,\"194\":1,\"198\":1,\"201\":2,\"202\":2,\"217\":15,\"218\":2,\"224\":8,\"225\":2,\"226\":1,\"231\":10,\"232\":2,\"235\":1,\"237\":2,\"239\":2,\"241\":1,\"245\":3,\"249\":1,\"257\":1,\"261\":1,\"263\":1,\"265\":1,\"274\":1,\"295\":10,\"301\":6,\"327\":2,\"449\":2,\"461\":2,\"538\":1,\"549\":1,\"564\":2,\"579\":3,\"582\":3,\"601\":1,\"619\":1,\"692\":1,\"736\":1,\"750\":2,\"754\":1,\"760\":1,\"762\":1,\"774\":1,\"786\":1,\"826\":2,\"936\":1,\"1057\":8,\"1171\":6,\"1172\":12,\"1206\":4,\"1214\":1,\"1391\":1,\"1398\":14,\"1406\":1,\"1417\":3,\"1418\":2,\"1419\":2,\"1420\":3,\"1421\":1,\"1422\":3,\"1423\":1,\"1424\":3,\"1425\":1,\"1552\":9,\"1773\":19,\"1778\":10,\"1798\":11,\"1804\":58,\"1805\":34,\"1829\":9,\"1837\":15,\"1841\":1,\"1842\":3,\"1850\":14,\"1851\":26,\"1852\":13,\"1853\":2,\"1854\":2,\"1863\":1,\"1864\":1,\"1868\":5,\"1874\":9,\"1877\":31,\"1878\":69,\"1879\":6,\"1881\":5,\"1889\":5,\"1892\":6,\"1893\":6,\"1953\":8,\"1954\":4,\"1955\":8,\"1956\":4,\"1970\":16,\"1971\":1,\"1975\":18,\"1984\":4,\"2012\":1,\"2027\":6,\"2076\":12,\"2077\":3,\"2082\":19,\"2086\":6,\"2087\":6,\"2090\":10,\"2091\":6,\"2095\":8,\"2104\":1,\"2119\":2,\"2120\":2,\"2121\":2,\"2122\":2,\"2123\":1,\"2124\":1,\"2125\":2,\"2126\":2,\"2127\":4,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":2,\"2132\":1,\"2133\":3,\"2135\":1,\"2136\":2,\"2137\":1,\"2138\":2,\"2139\":2,\"2140\":2,\"2141\":2,\"2142\":4,\"2143\":1,\"2144\":2,\"2145\":2,\"2146\":2,\"2147\":2,\"2178\":3,\"2179\":3,\"2183\":1,\"2190\":1,\"2191\":3,\"2194\":8,\"2195\":3,\"2196\":3,\"2235\":3,\"2239\":3,\"2240\":19,\"2243\":14,\"2244\":22,\"2245\":6,\"2255\":21,\"2256\":6,\"2263\":9,\"2264\":9,\"2265\":1,\"2277\":3,\"2278\":19,\"2279\":20,\"2280\":6,\"2292\":1,\"2294\":4,\"2347\":4,\"2349\":2,\"2354\":1,\"2358\":5,\"2359\":6,\"2360\":5,\"2373\":4,\"2385\":6,\"2387\":1,\"2395\":2,\"2398\":1,\"2412\":3,\"2414\":1,\"2415\":1,\"2418\":1,\"2419\":1,\"2420\":1,\"2430\":2,\"2433\":2,\"2451\":3,\"2452\":2,\"2456\":8,\"2458\":2,\"2460\":8,\"2461\":1,\"2462\":1,\"2467\":1,\"2472\":5,\"2474\":4,\"2476\":9,\"2478\":2,\"2492\":2,\"2499\":2,\"2500\":31,\"2502\":1,\"2518\":1,\"2520\":5,\"2521\":14,\"2523\":2,\"2531\":2,\"2534\":1,\"2555\":4,\"2568\":5,\"2569\":4,\"2579\":5,\"2580\":6,\"2581\":3,\"2582\":5,\"2592\":8,\"2596\":5,\"2600\":9,\"2617\":33,\"2628\":2,\"2635\":33,\"2648\":5,\"2649\":4}}],[\"tell\",{\"1\":{\"40\":1}}],[\"tedlium3\",{\"1\":{\"286\":1}}],[\"tedlium2\",{\"0\":{\"2590\":1},\"1\":{\"197\":3,\"286\":6}}],[\"ted\",{\"1\":{\"16\":1}}],[\"thu\",{\"1\":{\"2442\":1}}],[\"thus\",{\"1\":{\"24\":1,\"28\":1,\"49\":1,\"74\":1,\"85\":1,\"112\":1,\"595\":1,\"2019\":1,\"2212\":1,\"2437\":1,\"2563\":1,\"2584\":1}}],[\"thousand\",{\"1\":{\"2154\":1}}],[\"thousands\",{\"1\":{\"2154\":1}}],[\"though\",{\"1\":{\"59\":1,\"1187\":1,\"1202\":1}}],[\"thoreshold\",{\"1\":{\"1797\":1}}],[\"thorough\",{\"1\":{\"1655\":1,\"1719\":1}}],[\"those\",{\"1\":{\"737\":1,\"1337\":1,\"2373\":1,\"2384\":1,\"2420\":1,\"2433\":1,\"2437\":1,\"2555\":1,\"2563\":1,\"2584\":1}}],[\"thin\",{\"1\":{\"1972\":1}}],[\"think\",{\"1\":{\"1245\":1,\"2387\":1,\"2441\":1}}],[\"things\",{\"1\":{\"112\":1,\"1241\":1,\"2568\":1}}],[\"third\",{\"1\":{\"1142\":1,\"1154\":1}}],[\"this\",{\"0\":{\"2374\":1,\"2434\":1,\"2557\":1},\"1\":{\"1\":1,\"5\":2,\"11\":2,\"14\":2,\"16\":1,\"17\":3,\"19\":1,\"25\":2,\"26\":6,\"33\":1,\"38\":1,\"40\":1,\"56\":1,\"57\":2,\"59\":3,\"60\":3,\"62\":1,\"63\":1,\"74\":4,\"75\":3,\"76\":5,\"77\":2,\"78\":1,\"79\":2,\"80\":1,\"85\":3,\"92\":1,\"95\":1,\"98\":1,\"99\":1,\"102\":1,\"108\":1,\"112\":1,\"113\":1,\"115\":2,\"132\":1,\"135\":2,\"137\":1,\"144\":3,\"148\":2,\"150\":1,\"166\":1,\"167\":1,\"177\":1,\"178\":1,\"195\":1,\"198\":1,\"201\":2,\"206\":1,\"216\":1,\"233\":2,\"235\":3,\"236\":1,\"237\":2,\"238\":2,\"239\":1,\"240\":3,\"241\":1,\"242\":1,\"295\":4,\"594\":1,\"595\":1,\"607\":4,\"608\":2,\"610\":1,\"612\":2,\"626\":1,\"627\":2,\"628\":2,\"672\":1,\"676\":1,\"679\":1,\"684\":1,\"685\":1,\"686\":1,\"688\":1,\"689\":1,\"692\":2,\"693\":2,\"701\":1,\"706\":2,\"727\":3,\"728\":3,\"734\":2,\"736\":1,\"740\":3,\"741\":3,\"743\":2,\"745\":9,\"746\":8,\"753\":2,\"754\":1,\"757\":2,\"760\":1,\"761\":2,\"762\":1,\"764\":1,\"767\":1,\"774\":1,\"775\":3,\"776\":3,\"778\":1,\"779\":2,\"781\":1,\"785\":1,\"786\":1,\"791\":1,\"802\":1,\"803\":1,\"804\":1,\"812\":1,\"816\":1,\"821\":1,\"826\":1,\"831\":1,\"832\":2,\"834\":2,\"835\":1,\"836\":3,\"837\":2,\"875\":2,\"899\":1,\"901\":1,\"944\":1,\"952\":1,\"999\":1,\"1011\":1,\"1013\":2,\"1015\":2,\"1037\":1,\"1067\":1,\"1096\":1,\"1110\":2,\"1112\":2,\"1118\":2,\"1120\":2,\"1122\":2,\"1124\":2,\"1126\":2,\"1128\":2,\"1131\":2,\"1132\":2,\"1135\":2,\"1137\":2,\"1138\":1,\"1143\":1,\"1149\":1,\"1150\":1,\"1152\":2,\"1157\":2,\"1159\":2,\"1160\":4,\"1161\":4,\"1162\":1,\"1163\":1,\"1164\":4,\"1165\":3,\"1171\":4,\"1172\":1,\"1175\":2,\"1177\":3,\"1185\":2,\"1186\":2,\"1187\":3,\"1189\":2,\"1198\":2,\"1202\":3,\"1206\":3,\"1208\":2,\"1209\":3,\"1210\":2,\"1213\":2,\"1216\":2,\"1221\":2,\"1223\":2,\"1230\":2,\"1232\":2,\"1234\":2,\"1236\":2,\"1238\":2,\"1240\":2,\"1241\":1,\"1245\":3,\"1246\":2,\"1248\":3,\"1250\":2,\"1252\":4,\"1253\":7,\"1254\":5,\"1255\":1,\"1258\":2,\"1260\":2,\"1262\":2,\"1264\":2,\"1266\":2,\"1268\":2,\"1274\":1,\"1275\":2,\"1277\":2,\"1279\":2,\"1281\":2,\"1283\":2,\"1285\":2,\"1286\":2,\"1287\":1,\"1301\":2,\"1304\":2,\"1361\":2,\"1363\":2,\"1365\":2,\"1367\":2,\"1375\":3,\"1377\":3,\"1379\":1,\"1390\":1,\"1400\":1,\"1406\":1,\"1427\":1,\"1429\":1,\"1432\":3,\"1434\":2,\"1436\":4,\"1438\":2,\"1440\":2,\"1442\":2,\"1444\":2,\"1446\":2,\"1448\":2,\"1450\":2,\"1453\":2,\"1457\":2,\"1459\":2,\"1461\":2,\"1463\":1,\"1467\":2,\"1469\":2,\"1475\":2,\"1476\":1,\"1477\":2,\"1479\":2,\"1481\":2,\"1483\":2,\"1486\":2,\"1488\":2,\"1490\":2,\"1492\":2,\"1494\":2,\"1496\":2,\"1498\":2,\"1500\":2,\"1502\":2,\"1504\":2,\"1505\":1,\"1507\":2,\"1509\":2,\"1510\":1,\"1511\":2,\"1513\":2,\"1516\":1,\"1519\":2,\"1521\":2,\"1529\":1,\"1530\":1,\"1533\":2,\"1534\":1,\"1536\":2,\"1538\":2,\"1539\":1,\"1541\":2,\"1544\":2,\"1548\":2,\"1550\":2,\"1551\":3,\"1552\":5,\"1553\":3,\"1556\":2,\"1558\":2,\"1562\":2,\"1565\":2,\"1574\":2,\"1582\":2,\"1584\":2,\"1587\":2,\"1589\":2,\"1591\":2,\"1593\":2,\"1597\":2,\"1598\":1,\"1599\":2,\"1600\":1,\"1603\":2,\"1606\":2,\"1608\":2,\"1610\":2,\"1611\":1,\"1614\":2,\"1621\":2,\"1622\":4,\"1625\":2,\"1626\":1,\"1628\":2,\"1630\":2,\"1632\":2,\"1634\":2,\"1636\":2,\"1639\":4,\"1642\":2,\"1643\":1,\"1644\":2,\"1645\":1,\"1647\":2,\"1649\":2,\"1651\":2,\"1653\":2,\"1654\":1,\"1655\":1,\"1657\":2,\"1658\":1,\"1659\":1,\"1660\":3,\"1661\":3,\"1662\":4,\"1664\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1673\":2,\"1675\":2,\"1677\":2,\"1697\":1,\"1713\":1,\"1719\":4,\"1762\":2,\"1764\":2,\"1765\":1,\"1770\":2,\"1775\":2,\"1780\":2,\"1783\":2,\"1785\":1,\"1790\":2,\"1792\":2,\"1794\":2,\"1796\":2,\"1797\":1,\"1798\":1,\"1800\":1,\"1802\":2,\"1803\":1,\"1804\":1,\"1805\":1,\"1807\":2,\"1844\":1,\"1850\":1,\"1857\":1,\"1858\":1,\"1860\":1,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1883\":1,\"1891\":2,\"1892\":1,\"1893\":1,\"1895\":3,\"1897\":1,\"1900\":3,\"1903\":2,\"1905\":3,\"1906\":2,\"1908\":2,\"1910\":1,\"1912\":1,\"1913\":2,\"1914\":1,\"1915\":1,\"1917\":2,\"1918\":1,\"1919\":1,\"1920\":1,\"1926\":1,\"1927\":1,\"1940\":2,\"1941\":1,\"1946\":1,\"1947\":1,\"1951\":2,\"1952\":2,\"1953\":2,\"1954\":3,\"1955\":2,\"1956\":3,\"1959\":2,\"1964\":1,\"1970\":1,\"1975\":1,\"1977\":2,\"1979\":2,\"1982\":2,\"1984\":1,\"1985\":2,\"1986\":1,\"1988\":2,\"1990\":2,\"1992\":2,\"1995\":2,\"1998\":2,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2011\":2,\"2019\":2,\"2020\":1,\"2021\":2,\"2022\":2,\"2023\":2,\"2025\":2,\"2027\":1,\"2031\":2,\"2033\":2,\"2035\":2,\"2037\":2,\"2039\":2,\"2041\":2,\"2043\":2,\"2044\":1,\"2045\":2,\"2048\":2,\"2051\":2,\"2053\":2,\"2056\":2,\"2058\":2,\"2060\":2,\"2062\":2,\"2065\":2,\"2067\":2,\"2068\":1,\"2069\":2,\"2071\":2,\"2073\":2,\"2076\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2081\":2,\"2083\":2,\"2084\":1,\"2086\":1,\"2087\":1,\"2089\":1,\"2090\":1,\"2095\":1,\"2096\":2,\"2098\":2,\"2099\":6,\"2100\":2,\"2101\":2,\"2102\":4,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2121\":1,\"2122\":1,\"2131\":1,\"2148\":1,\"2150\":2,\"2168\":1,\"2169\":2,\"2170\":2,\"2185\":1,\"2199\":2,\"2201\":2,\"2203\":1,\"2212\":1,\"2216\":1,\"2217\":1,\"2234\":2,\"2236\":1,\"2237\":1,\"2238\":2,\"2242\":2,\"2243\":1,\"2244\":1,\"2247\":2,\"2249\":2,\"2251\":2,\"2255\":1,\"2257\":1,\"2259\":2,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2267\":2,\"2268\":1,\"2270\":2,\"2272\":2,\"2276\":2,\"2279\":1,\"2282\":2,\"2284\":2,\"2286\":2,\"2289\":2,\"2291\":2,\"2293\":2,\"2298\":2,\"2300\":2,\"2309\":2,\"2354\":1,\"2355\":1,\"2357\":1,\"2361\":1,\"2368\":1,\"2371\":1,\"2372\":4,\"2373\":6,\"2374\":1,\"2380\":1,\"2381\":1,\"2384\":2,\"2385\":4,\"2387\":2,\"2389\":2,\"2391\":1,\"2397\":1,\"2399\":1,\"2400\":1,\"2401\":2,\"2406\":1,\"2407\":1,\"2408\":1,\"2410\":1,\"2411\":1,\"2412\":1,\"2415\":1,\"2421\":1,\"2422\":2,\"2423\":1,\"2424\":1,\"2426\":1,\"2429\":4,\"2430\":6,\"2431\":2,\"2433\":1,\"2434\":1,\"2435\":1,\"2437\":2,\"2440\":4,\"2441\":1,\"2447\":1,\"2448\":1,\"2449\":1,\"2450\":3,\"2451\":1,\"2452\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":2,\"2473\":1,\"2480\":2,\"2481\":1,\"2486\":1,\"2490\":1,\"2494\":1,\"2499\":1,\"2502\":2,\"2503\":1,\"2506\":1,\"2516\":1,\"2518\":1,\"2525\":2,\"2527\":1,\"2533\":1,\"2535\":1,\"2536\":1,\"2537\":2,\"2545\":2,\"2546\":1,\"2547\":1,\"2549\":1,\"2552\":2,\"2554\":2,\"2555\":7,\"2557\":1,\"2558\":4,\"2561\":1,\"2563\":2,\"2564\":2,\"2565\":1,\"2566\":1,\"2567\":1,\"2568\":1,\"2569\":1,\"2572\":1,\"2574\":1,\"2576\":1,\"2578\":1,\"2583\":1,\"2584\":8,\"2585\":5,\"2586\":1,\"2597\":1,\"2598\":2,\"2600\":3,\"2601\":1,\"2605\":1,\"2609\":1,\"2612\":1,\"2617\":1,\"2618\":2,\"2622\":1,\"2626\":1,\"2630\":1,\"2635\":1,\"2646\":1,\"2650\":1}}],[\"th\",{\"0\":{\"925\":1},\"1\":{\"52\":2,\"135\":2,\"185\":1,\"193\":1,\"743\":1,\"760\":1,\"875\":3,\"925\":1,\"1069\":4,\"1145\":4,\"1427\":1,\"2394\":1,\"2409\":1,\"2429\":1,\"2530\":1,\"2552\":1}}],[\"thank\",{\"1\":{\"2388\":1,\"2524\":1}}],[\"thanks\",{\"1\":{\"133\":1}}],[\"than\",{\"1\":{\"26\":3,\"44\":1,\"48\":1,\"72\":1,\"80\":2,\"109\":1,\"203\":1,\"217\":2,\"272\":2,\"623\":1,\"691\":1,\"693\":1,\"697\":1,\"797\":1,\"940\":1,\"1138\":1,\"1143\":1,\"1255\":1,\"1429\":1,\"1464\":1,\"1688\":1,\"1697\":1,\"1735\":1,\"1756\":1,\"1927\":1,\"2154\":1,\"2309\":1,\"2387\":1,\"2398\":1,\"2400\":1,\"2420\":1,\"2441\":1,\"2450\":1,\"2482\":1,\"2515\":1,\"2534\":1,\"2536\":1,\"2542\":1,\"2564\":1,\"2585\":2}}],[\"thatn\",{\"1\":{\"2585\":1}}],[\"that\",{\"1\":{\"3\":2,\"5\":3,\"11\":4,\"14\":1,\"17\":2,\"19\":1,\"24\":1,\"26\":2,\"33\":1,\"35\":1,\"36\":1,\"45\":1,\"46\":2,\"47\":1,\"48\":1,\"49\":3,\"56\":1,\"57\":1,\"59\":2,\"62\":2,\"69\":2,\"72\":2,\"73\":1,\"75\":1,\"76\":1,\"79\":2,\"80\":1,\"82\":1,\"85\":3,\"91\":1,\"92\":1,\"94\":1,\"95\":1,\"112\":2,\"126\":1,\"135\":1,\"137\":1,\"144\":2,\"148\":3,\"150\":1,\"172\":1,\"182\":1,\"237\":1,\"595\":2,\"607\":2,\"610\":2,\"613\":1,\"619\":2,\"621\":1,\"623\":1,\"650\":1,\"691\":4,\"692\":2,\"695\":2,\"696\":1,\"697\":4,\"706\":4,\"727\":2,\"728\":2,\"734\":2,\"742\":1,\"743\":1,\"745\":5,\"746\":5,\"773\":2,\"792\":1,\"796\":2,\"797\":2,\"815\":2,\"817\":1,\"823\":1,\"828\":2,\"929\":1,\"1001\":1,\"1011\":2,\"1028\":1,\"1085\":1,\"1133\":1,\"1138\":1,\"1142\":2,\"1154\":1,\"1156\":1,\"1171\":1,\"1172\":1,\"1180\":1,\"1186\":5,\"1187\":2,\"1190\":3,\"1194\":1,\"1198\":3,\"1202\":2,\"1210\":6,\"1214\":1,\"1217\":1,\"1221\":2,\"1243\":1,\"1244\":3,\"1253\":1,\"1273\":1,\"1286\":1,\"1287\":1,\"1298\":2,\"1299\":2,\"1301\":1,\"1302\":2,\"1303\":2,\"1304\":1,\"1339\":1,\"1342\":1,\"1343\":1,\"1377\":1,\"1391\":1,\"1406\":2,\"1427\":1,\"1432\":1,\"1436\":1,\"1452\":2,\"1463\":1,\"1466\":1,\"1510\":1,\"1511\":1,\"1551\":4,\"1552\":2,\"1553\":4,\"1561\":1,\"1564\":1,\"1612\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1638\":1,\"1643\":1,\"1644\":1,\"1693\":2,\"1719\":1,\"1755\":2,\"1758\":1,\"1759\":1,\"1804\":3,\"1810\":1,\"1851\":3,\"1878\":3,\"1892\":1,\"1893\":1,\"1905\":3,\"1928\":1,\"1957\":2,\"1958\":1,\"1959\":1,\"1960\":2,\"1970\":1,\"1972\":1,\"1975\":1,\"1984\":1,\"2001\":4,\"2002\":3,\"2004\":3,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2027\":1,\"2076\":1,\"2086\":3,\"2087\":3,\"2090\":3,\"2095\":3,\"2099\":2,\"2102\":2,\"2170\":1,\"2209\":2,\"2212\":2,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2279\":3,\"2309\":1,\"2355\":1,\"2363\":1,\"2373\":2,\"2375\":1,\"2384\":1,\"2385\":3,\"2387\":3,\"2389\":1,\"2393\":1,\"2394\":5,\"2396\":1,\"2398\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2408\":1,\"2410\":3,\"2412\":1,\"2419\":1,\"2420\":1,\"2422\":1,\"2430\":3,\"2431\":1,\"2449\":1,\"2450\":1,\"2451\":1,\"2465\":1,\"2468\":1,\"2473\":1,\"2481\":1,\"2499\":1,\"2500\":1,\"2503\":1,\"2506\":1,\"2512\":1,\"2518\":1,\"2523\":1,\"2525\":1,\"2529\":1,\"2530\":5,\"2532\":1,\"2534\":1,\"2536\":1,\"2537\":1,\"2539\":1,\"2543\":2,\"2545\":1,\"2555\":3,\"2565\":1,\"2570\":1,\"2574\":1,\"2583\":2,\"2584\":4,\"2585\":2,\"2591\":1,\"2593\":1,\"2600\":1,\"2617\":2,\"2635\":2,\"2640\":1,\"2653\":1,\"2657\":1}}],[\"thres\",{\"1\":{\"1524\":1,\"1525\":1,\"1611\":2,\"2178\":1,\"2179\":1,\"2184\":1,\"2191\":1,\"2194\":1,\"2195\":1,\"2197\":2,\"2200\":1}}],[\"thresh\",{\"1\":{\"429\":2}}],[\"thresholds\",{\"1\":{\"2311\":1}}],[\"threshold=0\",{\"1\":{\"1797\":1,\"2022\":1,\"2079\":1,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"thresholding\",{\"1\":{\"585\":1}}],[\"threshold`\",{\"1\":{\"121\":1}}],[\"threshold\",{\"1\":{\"62\":2,\"115\":4,\"121\":3,\"217\":1,\"224\":1,\"231\":1,\"249\":2,\"251\":2,\"255\":2,\"259\":2,\"263\":2,\"267\":2,\"297\":3,\"307\":2,\"321\":2,\"399\":2,\"408\":2,\"464\":2,\"470\":2,\"585\":2,\"627\":2,\"821\":2,\"826\":2,\"1061\":6,\"1067\":3,\"1068\":1,\"1093\":4,\"1096\":5,\"1205\":1,\"1219\":1,\"1528\":3,\"1797\":2,\"1985\":1,\"2002\":3,\"2022\":1,\"2079\":2,\"2095\":3,\"2197\":1,\"2210\":1,\"2263\":3,\"2264\":3}}],[\"thread\",{\"1\":{\"1144\":1,\"2125\":1}}],[\"threads\",{\"0\":{\"1353\":1},\"1\":{\"144\":1,\"1142\":3,\"1144\":1,\"1186\":3,\"1210\":3,\"1228\":1,\"1337\":3,\"1345\":1,\"1347\":1,\"1349\":3,\"1350\":3,\"1353\":1}}],[\"threads=1\",{\"1\":{\"144\":1}}],[\"threads=\",{\"1\":{\"144\":1}}],[\"threading\",{\"1\":{\"32\":2,\"34\":1}}],[\"threed\",{\"1\":{\"1230\":1}}],[\"three\",{\"1\":{\"26\":1,\"38\":1,\"57\":1,\"99\":1,\"114\":2,\"115\":1,\"117\":1,\"119\":1,\"124\":1,\"170\":1,\"209\":1,\"632\":1,\"1429\":1,\"1905\":1,\"2046\":2,\"2151\":1,\"2154\":1,\"2168\":1,\"2387\":3,\"2431\":1,\"2568\":1}}],[\"throughout\",{\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"through\",{\"1\":{\"5\":1,\"21\":3,\"24\":1,\"113\":1,\"116\":1,\"117\":2,\"121\":1,\"130\":1,\"169\":1,\"181\":1,\"233\":1,\"235\":2,\"237\":1,\"240\":1,\"718\":1,\"1246\":2,\"1452\":1,\"1695\":1,\"1766\":1,\"1768\":1,\"1772\":1,\"1781\":1,\"2046\":2,\"2384\":1,\"2431\":2,\"2585\":1}}],[\"theory\",{\"1\":{\"1705\":1}}],[\"theta\",{\"1\":{\"1618\":2,\"2040\":1}}],[\"theta=1\",{\"1\":{\"1618\":1}}],[\"they\",{\"1\":{\"27\":1,\"28\":1,\"37\":2,\"48\":1,\"60\":1,\"64\":1,\"85\":1,\"124\":1,\"126\":1,\"239\":1,\"1187\":2,\"1202\":2,\"1693\":1,\"1755\":1,\"2209\":1,\"2357\":1,\"2372\":1,\"2385\":1,\"2429\":2,\"2467\":1,\"2468\":1,\"2552\":1,\"2554\":1,\"2638\":1,\"2642\":1}}],[\"thereby\",{\"1\":{\"691\":2,\"697\":3}}],[\"there\",{\"1\":{\"24\":1,\"38\":1,\"57\":1,\"62\":1,\"73\":1,\"80\":1,\"82\":1,\"83\":1,\"91\":1,\"141\":1,\"238\":1,\"595\":1,\"1187\":1,\"1202\":1,\"1255\":1,\"1286\":1,\"1287\":1,\"1810\":1,\"1956\":1,\"2212\":1,\"2372\":1,\"2373\":2,\"2384\":1,\"2385\":2,\"2389\":1,\"2394\":1,\"2400\":1,\"2401\":1,\"2408\":1,\"2422\":2,\"2428\":1,\"2449\":1,\"2465\":1,\"2468\":1,\"2481\":1,\"2503\":1,\"2525\":2,\"2530\":1,\"2536\":1,\"2537\":1,\"2543\":1,\"2545\":2,\"2551\":1,\"2555\":1,\"2573\":2,\"2585\":1}}],[\"therefore\",{\"1\":{\"17\":1,\"33\":1,\"40\":1,\"60\":1,\"69\":1,\"72\":1,\"79\":1,\"95\":1,\"143\":1,\"148\":1,\"237\":1,\"1011\":1,\"2237\":1,\"2398\":1,\"2403\":1,\"2412\":1,\"2534\":1,\"2539\":1}}],[\"their\",{\"1\":{\"22\":1,\"118\":1,\"691\":1,\"697\":1,\"797\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1,\"1643\":1,\"1644\":1,\"2355\":1,\"2430\":1,\"2467\":1,\"2555\":1}}],[\"then\",{\"1\":{\"5\":1,\"16\":1,\"18\":1,\"47\":1,\"51\":1,\"54\":1,\"90\":1,\"99\":1,\"110\":1,\"127\":1,\"135\":1,\"217\":1,\"224\":1,\"231\":1,\"242\":1,\"295\":1,\"612\":1,\"691\":2,\"697\":3,\"727\":2,\"728\":2,\"797\":1,\"1171\":1,\"1187\":1,\"1198\":1,\"1202\":1,\"1206\":1,\"1211\":1,\"1224\":1,\"1336\":1,\"1348\":1,\"1429\":1,\"1531\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1719\":1,\"1953\":1,\"1955\":1,\"2096\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2201\":1,\"2383\":1,\"2387\":1,\"2393\":1,\"2399\":1,\"2418\":1,\"2427\":1,\"2430\":1,\"2467\":1,\"2529\":1,\"2535\":1,\"2542\":1,\"2550\":1,\"2555\":1,\"2567\":1,\"2568\":4,\"2573\":1,\"2585\":1,\"2644\":1}}],[\"them\",{\"1\":{\"3\":1,\"48\":1,\"74\":1,\"85\":1,\"91\":1,\"124\":1,\"126\":2,\"132\":1,\"136\":1,\"143\":1,\"295\":1,\"745\":1,\"746\":1,\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1375\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1773\":2,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1837\":1,\"1891\":1,\"1903\":1,\"1905\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2082\":2,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2240\":2,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2278\":2,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2385\":1,\"2429\":1,\"2430\":1,\"2451\":1,\"2479\":1,\"2542\":1,\"2543\":1,\"2552\":1,\"2555\":1,\"2558\":1,\"2572\":1,\"2573\":1,\"2585\":2,\"2639\":1}}],[\"these\",{\"1\":{\"1\":1,\"3\":1,\"5\":1,\"6\":1,\"38\":2,\"112\":1,\"121\":1,\"136\":1,\"142\":1,\"148\":1,\"167\":1,\"170\":1,\"238\":1,\"1245\":1,\"2168\":1,\"2355\":1,\"2373\":1,\"2384\":1,\"2389\":1,\"2408\":1,\"2422\":1,\"2430\":1,\"2440\":1,\"2449\":1,\"2465\":1,\"2467\":2,\"2473\":1,\"2481\":1,\"2503\":1,\"2508\":1,\"2515\":1,\"2525\":1,\"2545\":1,\"2555\":1,\"2558\":2,\"2573\":1,\"2574\":1,\"2600\":1}}],[\"the\",{\"0\":{\"25\":1,\"45\":1,\"49\":1,\"61\":1,\"64\":1,\"69\":1,\"72\":1,\"87\":1,\"88\":1,\"92\":1,\"93\":1,\"124\":1,\"128\":1,\"149\":2,\"235\":1,\"310\":1,\"318\":1,\"324\":1,\"330\":1,\"336\":1,\"342\":1,\"346\":1,\"354\":1,\"360\":1,\"372\":1,\"383\":1,\"387\":1,\"394\":1,\"402\":1,\"425\":1,\"432\":1,\"436\":1,\"440\":1,\"446\":1,\"452\":1,\"458\":1,\"467\":1,\"473\":1,\"481\":1,\"488\":1,\"2367\":1,\"2368\":1,\"2369\":1,\"2372\":1,\"2376\":1,\"2432\":1,\"2435\":1,\"2452\":1,\"2468\":1,\"2484\":1,\"2486\":1,\"2487\":1,\"2489\":1,\"2491\":1,\"2496\":1,\"2561\":1,\"2568\":1,\"2569\":1,\"2570\":1,\"2571\":1,\"2572\":1,\"2574\":1,\"2593\":1,\"2594\":1,\"2604\":1,\"2605\":1,\"2606\":1,\"2609\":1,\"2610\":1,\"2614\":1,\"2621\":1,\"2622\":1,\"2623\":1,\"2626\":1,\"2627\":1,\"2632\":1,\"2643\":1,\"2645\":1},\"1\":{\"1\":13,\"2\":5,\"3\":11,\"4\":4,\"5\":18,\"6\":1,\"11\":2,\"14\":3,\"15\":4,\"16\":6,\"17\":8,\"18\":2,\"19\":4,\"20\":2,\"21\":30,\"22\":19,\"23\":7,\"24\":4,\"25\":6,\"26\":13,\"27\":4,\"28\":6,\"29\":2,\"30\":4,\"33\":5,\"34\":1,\"35\":1,\"36\":1,\"37\":3,\"38\":1,\"40\":3,\"45\":5,\"46\":12,\"47\":8,\"48\":14,\"49\":21,\"51\":3,\"54\":3,\"56\":5,\"57\":13,\"58\":4,\"59\":8,\"60\":15,\"62\":10,\"63\":4,\"64\":3,\"65\":7,\"66\":2,\"68\":4,\"69\":13,\"70\":1,\"71\":1,\"72\":8,\"73\":4,\"74\":14,\"75\":17,\"76\":12,\"77\":4,\"78\":4,\"79\":4,\"80\":8,\"82\":1,\"83\":1,\"84\":8,\"85\":19,\"90\":3,\"91\":3,\"92\":7,\"93\":2,\"94\":1,\"95\":5,\"96\":1,\"97\":2,\"99\":9,\"100\":1,\"102\":19,\"103\":1,\"104\":3,\"105\":1,\"106\":2,\"107\":6,\"109\":3,\"110\":2,\"111\":2,\"112\":15,\"113\":19,\"114\":4,\"115\":56,\"116\":20,\"117\":10,\"118\":12,\"119\":14,\"120\":5,\"121\":10,\"122\":5,\"124\":11,\"126\":2,\"127\":1,\"128\":1,\"130\":3,\"132\":2,\"133\":3,\"134\":2,\"135\":9,\"136\":2,\"137\":2,\"141\":2,\"143\":8,\"144\":9,\"148\":13,\"149\":4,\"150\":19,\"161\":3,\"168\":2,\"169\":1,\"171\":6,\"174\":1,\"175\":1,\"179\":1,\"180\":1,\"185\":3,\"186\":1,\"188\":1,\"192\":1,\"193\":1,\"195\":2,\"196\":2,\"197\":4,\"198\":3,\"201\":3,\"202\":1,\"203\":4,\"204\":2,\"206\":1,\"209\":1,\"213\":1,\"221\":1,\"226\":2,\"233\":3,\"234\":2,\"235\":16,\"236\":1,\"237\":6,\"238\":8,\"239\":6,\"240\":16,\"241\":2,\"242\":2,\"243\":1,\"276\":1,\"277\":1,\"279\":1,\"281\":1,\"283\":1,\"284\":1,\"295\":9,\"564\":1,\"595\":3,\"602\":2,\"603\":2,\"604\":2,\"605\":7,\"606\":2,\"607\":12,\"608\":1,\"610\":2,\"612\":4,\"613\":3,\"614\":5,\"616\":3,\"617\":3,\"619\":4,\"621\":2,\"622\":2,\"623\":2,\"624\":1,\"625\":1,\"626\":7,\"627\":13,\"629\":1,\"632\":7,\"635\":2,\"636\":1,\"643\":2,\"645\":1,\"648\":3,\"649\":2,\"650\":2,\"652\":5,\"656\":1,\"657\":2,\"668\":1,\"676\":2,\"678\":1,\"681\":2,\"682\":2,\"685\":3,\"690\":1,\"691\":21,\"692\":3,\"693\":13,\"695\":1,\"696\":2,\"697\":29,\"700\":1,\"701\":14,\"702\":1,\"703\":1,\"704\":1,\"705\":1,\"706\":8,\"708\":2,\"709\":3,\"710\":17,\"711\":9,\"712\":4,\"723\":1,\"724\":1,\"727\":12,\"728\":12,\"729\":1,\"730\":2,\"731\":4,\"734\":5,\"735\":2,\"736\":2,\"737\":2,\"738\":1,\"740\":2,\"741\":2,\"742\":6,\"743\":3,\"745\":30,\"746\":27,\"747\":2,\"749\":4,\"752\":1,\"753\":5,\"754\":10,\"755\":2,\"756\":1,\"757\":5,\"758\":2,\"760\":12,\"761\":5,\"762\":2,\"764\":2,\"767\":3,\"768\":1,\"770\":5,\"771\":3,\"773\":2,\"774\":2,\"775\":2,\"776\":2,\"778\":9,\"779\":5,\"781\":2,\"782\":3,\"784\":5,\"785\":6,\"793\":2,\"794\":7,\"795\":2,\"796\":1,\"797\":19,\"799\":1,\"801\":2,\"802\":10,\"803\":2,\"804\":5,\"805\":1,\"807\":5,\"808\":2,\"809\":4,\"810\":1,\"812\":1,\"814\":2,\"815\":5,\"817\":5,\"818\":1,\"820\":8,\"821\":36,\"822\":2,\"823\":4,\"825\":1,\"826\":13,\"828\":5,\"829\":2,\"830\":2,\"831\":3,\"832\":5,\"834\":14,\"835\":5,\"836\":6,\"855\":3,\"856\":5,\"857\":5,\"864\":1,\"867\":5,\"869\":2,\"870\":6,\"875\":2,\"877\":2,\"878\":1,\"885\":1,\"889\":3,\"890\":2,\"892\":1,\"897\":1,\"898\":1,\"899\":4,\"900\":2,\"901\":4,\"902\":2,\"905\":2,\"906\":1,\"917\":1,\"919\":1,\"923\":4,\"926\":3,\"929\":3,\"933\":1,\"934\":1,\"935\":1,\"936\":1,\"937\":2,\"938\":2,\"940\":6,\"943\":1,\"944\":1,\"950\":7,\"952\":3,\"955\":1,\"956\":2,\"958\":1,\"965\":1,\"968\":7,\"972\":1,\"973\":2,\"975\":1,\"976\":2,\"987\":6,\"988\":2,\"989\":1,\"996\":5,\"997\":7,\"998\":5,\"999\":1,\"1001\":6,\"1003\":1,\"1004\":1,\"1007\":4,\"1010\":1,\"1011\":16,\"1013\":3,\"1015\":3,\"1025\":5,\"1031\":3,\"1032\":2,\"1033\":2,\"1034\":5,\"1037\":1,\"1042\":1,\"1043\":2,\"1047\":1,\"1048\":2,\"1049\":2,\"1050\":2,\"1051\":2,\"1052\":6,\"1054\":2,\"1055\":2,\"1056\":2,\"1057\":2,\"1058\":2,\"1060\":1,\"1062\":2,\"1064\":1,\"1065\":2,\"1066\":3,\"1068\":5,\"1071\":4,\"1072\":3,\"1074\":3,\"1075\":2,\"1077\":2,\"1080\":1,\"1081\":2,\"1085\":4,\"1093\":4,\"1095\":1,\"1096\":2,\"1097\":2,\"1098\":3,\"1101\":4,\"1102\":1,\"1109\":1,\"1110\":5,\"1111\":1,\"1112\":5,\"1113\":1,\"1116\":1,\"1117\":1,\"1118\":5,\"1119\":1,\"1120\":5,\"1121\":1,\"1122\":5,\"1123\":1,\"1124\":5,\"1125\":1,\"1126\":5,\"1127\":3,\"1128\":5,\"1130\":2,\"1131\":5,\"1132\":11,\"1133\":8,\"1134\":1,\"1135\":5,\"1136\":1,\"1137\":6,\"1138\":5,\"1139\":1,\"1141\":4,\"1142\":23,\"1143\":5,\"1145\":2,\"1148\":9,\"1149\":5,\"1150\":5,\"1151\":1,\"1152\":5,\"1154\":5,\"1155\":7,\"1156\":2,\"1157\":5,\"1158\":1,\"1159\":5,\"1160\":7,\"1161\":7,\"1164\":7,\"1165\":7,\"1170\":3,\"1171\":2,\"1172\":1,\"1174\":1,\"1175\":5,\"1177\":3,\"1178\":2,\"1180\":3,\"1181\":3,\"1184\":1,\"1185\":5,\"1186\":30,\"1187\":17,\"1188\":1,\"1189\":5,\"1190\":2,\"1191\":2,\"1192\":2,\"1194\":1,\"1195\":2,\"1198\":18,\"1200\":2,\"1201\":2,\"1202\":17,\"1203\":9,\"1204\":1,\"1206\":1,\"1207\":1,\"1208\":5,\"1209\":6,\"1210\":28,\"1211\":11,\"1212\":1,\"1213\":5,\"1214\":2,\"1215\":1,\"1216\":6,\"1218\":2,\"1220\":1,\"1221\":7,\"1222\":5,\"1223\":5,\"1224\":10,\"1226\":1,\"1227\":4,\"1228\":1,\"1229\":1,\"1230\":5,\"1231\":1,\"1232\":5,\"1233\":1,\"1234\":5,\"1235\":1,\"1236\":5,\"1237\":1,\"1238\":5,\"1239\":1,\"1240\":5,\"1241\":4,\"1242\":2,\"1243\":2,\"1244\":3,\"1245\":16,\"1246\":7,\"1248\":3,\"1249\":1,\"1250\":5,\"1252\":8,\"1253\":16,\"1254\":11,\"1255\":2,\"1257\":1,\"1258\":5,\"1259\":1,\"1260\":5,\"1261\":1,\"1262\":5,\"1263\":1,\"1264\":5,\"1265\":1,\"1266\":5,\"1267\":1,\"1268\":5,\"1269\":20,\"1272\":4,\"1273\":2,\"1274\":1,\"1275\":5,\"1276\":2,\"1277\":5,\"1279\":4,\"1280\":1,\"1281\":5,\"1282\":5,\"1283\":5,\"1284\":1,\"1285\":5,\"1286\":14,\"1287\":16,\"1298\":28,\"1299\":28,\"1301\":28,\"1302\":27,\"1303\":27,\"1304\":29,\"1327\":5,\"1334\":11,\"1336\":10,\"1337\":11,\"1339\":2,\"1341\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":9,\"1349\":6,\"1350\":6,\"1352\":7,\"1360\":1,\"1361\":5,\"1362\":1,\"1363\":5,\"1364\":1,\"1365\":5,\"1366\":1,\"1367\":5,\"1369\":3,\"1371\":5,\"1375\":3,\"1377\":4,\"1381\":1,\"1391\":1,\"1392\":3,\"1398\":2,\"1400\":2,\"1406\":4,\"1407\":3,\"1409\":4,\"1427\":6,\"1430\":9,\"1431\":1,\"1432\":9,\"1433\":1,\"1434\":5,\"1435\":1,\"1436\":8,\"1437\":1,\"1438\":5,\"1439\":1,\"1440\":5,\"1441\":1,\"1442\":5,\"1443\":1,\"1444\":5,\"1445\":1,\"1446\":5,\"1447\":1,\"1448\":5,\"1449\":1,\"1450\":5,\"1451\":6,\"1452\":3,\"1453\":5,\"1454\":4,\"1456\":1,\"1457\":5,\"1458\":1,\"1459\":5,\"1460\":1,\"1461\":5,\"1462\":5,\"1463\":2,\"1464\":4,\"1466\":1,\"1467\":5,\"1468\":1,\"1469\":5,\"1470\":2,\"1471\":2,\"1473\":3,\"1474\":1,\"1475\":5,\"1476\":3,\"1477\":5,\"1478\":1,\"1479\":5,\"1480\":1,\"1481\":5,\"1482\":1,\"1483\":5,\"1484\":3,\"1485\":1,\"1486\":5,\"1487\":1,\"1488\":5,\"1489\":1,\"1490\":5,\"1491\":1,\"1492\":5,\"1493\":1,\"1494\":5,\"1495\":1,\"1496\":5,\"1497\":1,\"1498\":5,\"1499\":1,\"1500\":5,\"1501\":1,\"1502\":5,\"1503\":1,\"1504\":5,\"1505\":6,\"1506\":1,\"1507\":5,\"1508\":1,\"1509\":5,\"1510\":4,\"1511\":3,\"1512\":1,\"1513\":5,\"1514\":6,\"1515\":4,\"1516\":6,\"1517\":4,\"1518\":1,\"1519\":5,\"1520\":1,\"1521\":5,\"1522\":18,\"1523\":20,\"1524\":1,\"1525\":1,\"1528\":9,\"1529\":4,\"1530\":1,\"1531\":14,\"1532\":6,\"1533\":5,\"1534\":4,\"1535\":6,\"1536\":5,\"1537\":6,\"1538\":5,\"1539\":5,\"1540\":1,\"1541\":5,\"1543\":4,\"1544\":5,\"1545\":8,\"1547\":1,\"1548\":5,\"1549\":1,\"1550\":5,\"1551\":19,\"1552\":5,\"1553\":21,\"1554\":3,\"1555\":1,\"1556\":5,\"1557\":5,\"1558\":1,\"1559\":1,\"1560\":3,\"1561\":1,\"1562\":5,\"1563\":1,\"1564\":2,\"1565\":5,\"1572\":1,\"1573\":1,\"1574\":5,\"1576\":1,\"1577\":2,\"1581\":5,\"1582\":5,\"1583\":1,\"1584\":5,\"1585\":5,\"1586\":1,\"1587\":5,\"1588\":1,\"1589\":5,\"1590\":1,\"1591\":5,\"1592\":1,\"1593\":5,\"1595\":2,\"1596\":1,\"1597\":5,\"1598\":8,\"1599\":5,\"1600\":1,\"1601\":3,\"1602\":9,\"1603\":9,\"1604\":4,\"1605\":1,\"1606\":5,\"1607\":1,\"1608\":5,\"1609\":1,\"1610\":5,\"1612\":5,\"1613\":1,\"1614\":5,\"1615\":5,\"1616\":3,\"1618\":8,\"1619\":9,\"1620\":1,\"1621\":5,\"1622\":6,\"1623\":6,\"1624\":1,\"1625\":5,\"1626\":4,\"1627\":1,\"1628\":5,\"1629\":1,\"1630\":5,\"1631\":1,\"1632\":5,\"1633\":2,\"1634\":5,\"1635\":1,\"1636\":5,\"1637\":5,\"1638\":12,\"1639\":15,\"1640\":3,\"1641\":1,\"1642\":5,\"1643\":4,\"1644\":3,\"1645\":7,\"1646\":7,\"1647\":5,\"1648\":6,\"1649\":5,\"1650\":5,\"1651\":5,\"1652\":9,\"1653\":5,\"1654\":7,\"1655\":11,\"1656\":2,\"1657\":5,\"1658\":4,\"1659\":6,\"1660\":4,\"1661\":5,\"1662\":7,\"1664\":1,\"1665\":3,\"1669\":6,\"1670\":22,\"1671\":22,\"1672\":1,\"1673\":5,\"1674\":1,\"1675\":5,\"1676\":1,\"1677\":5,\"1679\":1,\"1683\":1,\"1688\":9,\"1692\":1,\"1693\":13,\"1695\":1,\"1696\":6,\"1697\":4,\"1698\":3,\"1699\":1,\"1701\":1,\"1702\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":1,\"1712\":3,\"1713\":2,\"1714\":3,\"1715\":4,\"1718\":1,\"1719\":22,\"1735\":2,\"1739\":2,\"1741\":4,\"1742\":1,\"1752\":4,\"1755\":13,\"1756\":9,\"1761\":1,\"1762\":5,\"1763\":1,\"1764\":5,\"1765\":4,\"1766\":12,\"1767\":2,\"1768\":2,\"1769\":1,\"1770\":5,\"1771\":24,\"1772\":2,\"1773\":1,\"1774\":1,\"1775\":5,\"1776\":6,\"1777\":4,\"1778\":12,\"1779\":1,\"1780\":5,\"1781\":2,\"1782\":1,\"1783\":5,\"1785\":1,\"1786\":2,\"1787\":20,\"1788\":7,\"1789\":1,\"1790\":5,\"1791\":4,\"1792\":5,\"1793\":1,\"1794\":5,\"1795\":1,\"1796\":5,\"1797\":1,\"1798\":2,\"1800\":4,\"1801\":8,\"1802\":5,\"1803\":4,\"1804\":20,\"1805\":10,\"1806\":1,\"1807\":5,\"1808\":18,\"1810\":5,\"1833\":1,\"1834\":3,\"1836\":2,\"1837\":1,\"1838\":1,\"1839\":5,\"1840\":1,\"1841\":2,\"1843\":2,\"1844\":4,\"1845\":1,\"1846\":6,\"1847\":7,\"1848\":5,\"1849\":11,\"1850\":7,\"1851\":16,\"1852\":9,\"1855\":1,\"1856\":7,\"1857\":4,\"1858\":9,\"1860\":3,\"1861\":4,\"1862\":3,\"1864\":1,\"1865\":1,\"1868\":1,\"1870\":2,\"1871\":4,\"1874\":2,\"1876\":2,\"1877\":8,\"1878\":9,\"1879\":1,\"1880\":7,\"1881\":1,\"1883\":2,\"1890\":1,\"1891\":5,\"1892\":1,\"1893\":1,\"1895\":5,\"1897\":4,\"1900\":5,\"1902\":1,\"1903\":5,\"1904\":1,\"1905\":15,\"1906\":2,\"1907\":1,\"1908\":5,\"1910\":2,\"1911\":1,\"1912\":5,\"1913\":5,\"1914\":2,\"1915\":2,\"1916\":1,\"1917\":11,\"1918\":2,\"1919\":2,\"1920\":2,\"1921\":1,\"1922\":1,\"1923\":2,\"1924\":2,\"1925\":4,\"1926\":2,\"1927\":5,\"1928\":3,\"1929\":6,\"1930\":2,\"1932\":1,\"1933\":1,\"1934\":1,\"1935\":1,\"1936\":2,\"1937\":2,\"1938\":1,\"1939\":1,\"1940\":2,\"1941\":7,\"1943\":1,\"1944\":2,\"1946\":3,\"1947\":5,\"1951\":4,\"1952\":5,\"1953\":2,\"1954\":5,\"1955\":2,\"1956\":5,\"1957\":1,\"1958\":2,\"1959\":5,\"1960\":1,\"1962\":4,\"1963\":2,\"1964\":1,\"1968\":1,\"1970\":1,\"1971\":2,\"1972\":4,\"1975\":1,\"1976\":1,\"1977\":5,\"1978\":1,\"1979\":5,\"1980\":1,\"1981\":1,\"1982\":5,\"1984\":1,\"1985\":5,\"1986\":1,\"1987\":1,\"1988\":6,\"1989\":1,\"1990\":6,\"1991\":1,\"1992\":6,\"1994\":1,\"1995\":5,\"1997\":1,\"1998\":5,\"2000\":2,\"2001\":13,\"2002\":14,\"2003\":1,\"2004\":12,\"2012\":2,\"2018\":1,\"2019\":5,\"2020\":2,\"2021\":2,\"2022\":3,\"2023\":3,\"2024\":1,\"2025\":5,\"2026\":1,\"2027\":1,\"2028\":2,\"2029\":4,\"2030\":3,\"2031\":5,\"2032\":2,\"2033\":5,\"2034\":1,\"2035\":5,\"2036\":1,\"2037\":5,\"2038\":1,\"2039\":5,\"2040\":1,\"2041\":5,\"2042\":1,\"2043\":5,\"2044\":4,\"2045\":5,\"2046\":6,\"2047\":1,\"2048\":5,\"2049\":2,\"2050\":2,\"2051\":5,\"2052\":2,\"2053\":5,\"2054\":9,\"2055\":4,\"2056\":5,\"2057\":1,\"2058\":5,\"2059\":1,\"2060\":5,\"2061\":1,\"2062\":5,\"2064\":3,\"2065\":5,\"2066\":1,\"2067\":5,\"2068\":4,\"2069\":5,\"2070\":2,\"2071\":5,\"2072\":1,\"2073\":5,\"2076\":1,\"2077\":1,\"2078\":15,\"2079\":6,\"2081\":4,\"2082\":2,\"2083\":6,\"2084\":2,\"2086\":18,\"2087\":18,\"2088\":1,\"2089\":2,\"2090\":25,\"2091\":2,\"2095\":18,\"2096\":2,\"2098\":2,\"2099\":21,\"2100\":2,\"2101\":2,\"2102\":14,\"2103\":2,\"2104\":2,\"2105\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2121\":1,\"2122\":1,\"2125\":1,\"2142\":1,\"2148\":3,\"2149\":1,\"2150\":5,\"2151\":7,\"2153\":4,\"2154\":1,\"2155\":1,\"2156\":2,\"2157\":1,\"2166\":1,\"2168\":7,\"2169\":5,\"2170\":15,\"2176\":1,\"2185\":1,\"2193\":2,\"2197\":9,\"2199\":4,\"2201\":2,\"2203\":1,\"2209\":2,\"2212\":3,\"2216\":3,\"2217\":3,\"2233\":1,\"2234\":5,\"2235\":1,\"2237\":2,\"2238\":5,\"2240\":2,\"2241\":1,\"2242\":5,\"2243\":20,\"2244\":20,\"2245\":2,\"2246\":1,\"2247\":6,\"2248\":1,\"2249\":6,\"2250\":1,\"2251\":6,\"2254\":1,\"2255\":20,\"2256\":2,\"2257\":11,\"2259\":3,\"2260\":4,\"2261\":13,\"2262\":3,\"2263\":15,\"2264\":16,\"2266\":1,\"2267\":5,\"2275\":2,\"2276\":5,\"2277\":1,\"2278\":2,\"2279\":14,\"2280\":2,\"2281\":2,\"2282\":5,\"2283\":1,\"2284\":5,\"2285\":1,\"2286\":5,\"2288\":1,\"2289\":5,\"2290\":1,\"2291\":5,\"2292\":1,\"2293\":5,\"2294\":1,\"2297\":1,\"2298\":5,\"2299\":1,\"2300\":5,\"2317\":3,\"2322\":2,\"2325\":2,\"2330\":2,\"2343\":1,\"2344\":3,\"2347\":2,\"2349\":1,\"2354\":1,\"2355\":1,\"2357\":1,\"2360\":1,\"2361\":1,\"2363\":2,\"2365\":1,\"2368\":3,\"2369\":1,\"2371\":1,\"2372\":15,\"2373\":20,\"2374\":2,\"2375\":11,\"2377\":1,\"2378\":3,\"2380\":2,\"2381\":1,\"2383\":1,\"2384\":10,\"2385\":38,\"2386\":1,\"2387\":35,\"2388\":2,\"2389\":8,\"2392\":1,\"2393\":3,\"2394\":16,\"2395\":10,\"2396\":3,\"2397\":2,\"2398\":13,\"2399\":4,\"2400\":8,\"2401\":14,\"2403\":7,\"2405\":3,\"2406\":2,\"2407\":1,\"2408\":6,\"2409\":2,\"2410\":4,\"2411\":7,\"2412\":8,\"2413\":1,\"2414\":2,\"2415\":7,\"2416\":2,\"2418\":3,\"2419\":3,\"2420\":3,\"2421\":1,\"2422\":10,\"2423\":1,\"2425\":1,\"2426\":1,\"2427\":2,\"2428\":1,\"2429\":13,\"2430\":23,\"2431\":13,\"2432\":5,\"2433\":3,\"2434\":2,\"2435\":3,\"2436\":2,\"2437\":5,\"2439\":2,\"2440\":9,\"2441\":8,\"2446\":1,\"2449\":6,\"2450\":4,\"2451\":2,\"2452\":5,\"2457\":7,\"2458\":1,\"2459\":5,\"2461\":5,\"2462\":2,\"2463\":1,\"2465\":6,\"2466\":2,\"2467\":11,\"2468\":5,\"2472\":1,\"2473\":5,\"2474\":1,\"2480\":1,\"2481\":10,\"2482\":3,\"2486\":3,\"2487\":3,\"2490\":4,\"2491\":2,\"2492\":3,\"2494\":3,\"2497\":2,\"2499\":5,\"2500\":5,\"2501\":2,\"2502\":1,\"2503\":6,\"2506\":3,\"2508\":2,\"2510\":3,\"2512\":2,\"2514\":6,\"2515\":2,\"2516\":1,\"2518\":5,\"2521\":1,\"2522\":1,\"2523\":2,\"2524\":2,\"2525\":9,\"2528\":1,\"2529\":3,\"2530\":16,\"2531\":10,\"2532\":3,\"2533\":2,\"2534\":13,\"2535\":4,\"2536\":8,\"2537\":14,\"2539\":7,\"2541\":3,\"2542\":5,\"2543\":13,\"2545\":10,\"2546\":1,\"2548\":1,\"2549\":1,\"2550\":2,\"2551\":1,\"2552\":1,\"2554\":11,\"2555\":29,\"2556\":1,\"2557\":2,\"2558\":21,\"2559\":8,\"2560\":2,\"2561\":3,\"2562\":2,\"2563\":5,\"2564\":12,\"2565\":2,\"2566\":1,\"2567\":3,\"2568\":13,\"2569\":6,\"2570\":4,\"2571\":3,\"2572\":7,\"2573\":7,\"2574\":2,\"2575\":1,\"2576\":1,\"2581\":1,\"2582\":1,\"2584\":28,\"2585\":19,\"2587\":1,\"2588\":2,\"2591\":1,\"2592\":2,\"2593\":4,\"2596\":2,\"2598\":2,\"2599\":4,\"2600\":22,\"2601\":1,\"2605\":3,\"2606\":1,\"2609\":4,\"2612\":1,\"2617\":8,\"2618\":3,\"2622\":3,\"2623\":1,\"2626\":4,\"2628\":3,\"2630\":2,\"2635\":11,\"2637\":6,\"2638\":6,\"2639\":2,\"2640\":1,\"2641\":5,\"2642\":3,\"2643\":1,\"2644\":1,\"2646\":1,\"2648\":1,\"2649\":1,\"2650\":1,\"2653\":2,\"2655\":1,\"2657\":2,\"2659\":6,\"2660\":1}}],[\"toy\",{\"1\":{\"2584\":1}}],[\"toward\",{\"1\":{\"1670\":1,\"1671\":1}}],[\"towards\",{\"1\":{\"701\":1,\"1454\":1,\"1604\":1,\"1655\":1,\"1719\":1}}],[\"tolerance=0\",{\"1\":{\"1248\":1}}],[\"together\",{\"1\":{\"1198\":1}}],[\"toggleableshufflingserialiterator\",{\"0\":{\"998\":1},\"1\":{\"998\":1}}],[\"toggleableshufflingmultiprocessiterator\",{\"0\":{\"997\":1},\"1\":{\"997\":1}}],[\"top=5\",{\"1\":{\"2040\":1}}],[\"top=1\",{\"1\":{\"1887\":1}}],[\"top=false\",{\"1\":{\"648\":1}}],[\"topo\",{\"1\":{\"1429\":2}}],[\"topk\",{\"1\":{\"691\":3,\"697\":3,\"797\":2,\"917\":4,\"1048\":4,\"2040\":1}}],[\"top\",{\"1\":{\"648\":1,\"1662\":1,\"2040\":1}}],[\"topics\",{\"1\":{\"2363\":1,\"2506\":1,\"2653\":1}}],[\"topic\",{\"1\":{\"44\":1,\"952\":1,\"2467\":1}}],[\"tone3\",{\"1\":{\"231\":1}}],[\"tomwujec\",{\"1\":{\"197\":1}}],[\"tomoki\",{\"1\":{\"130\":7,\"206\":1,\"233\":1,\"2361\":1,\"2650\":1}}],[\"tokyo\",{\"1\":{\"176\":1}}],[\"tokenizing\",{\"1\":{\"1429\":1}}],[\"tokenizer\",{\"0\":{\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2124\":1,\"2125\":1,\"2126\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2136\":1,\"2137\":2,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2348\":1},\"1\":{\"1429\":2,\"2119\":2,\"2120\":2,\"2121\":2,\"2122\":2,\"2124\":1,\"2125\":2,\"2126\":2,\"2129\":1,\"2130\":1,\"2131\":2,\"2132\":1,\"2136\":2,\"2137\":3,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2143\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2191\":1,\"2348\":2,\"2414\":1,\"2474\":1,\"2649\":1}}],[\"tokenized\",{\"1\":{\"579\":1}}],[\"tokenize\",{\"0\":{\"461\":1,\"2348\":1},\"1\":{\"301\":1,\"461\":3,\"2348\":2}}],[\"tokenization\",{\"1\":{\"84\":1}}],[\"tokenidconverter\",{\"0\":{\"2135\":1},\"1\":{\"2135\":1}}],[\"tokenid=\",{\"1\":{\"987\":1}}],[\"tokenid\",{\"1\":{\"174\":1,\"239\":1,\"601\":1}}],[\"tokens2ids\",{\"1\":{\"2123\":1,\"2128\":1,\"2135\":1,\"2474\":1,\"2649\":1}}],[\"tokens2text\",{\"1\":{\"2119\":1,\"2120\":1,\"2124\":1,\"2129\":1,\"2130\":1,\"2132\":1,\"2136\":1}}],[\"tokens\",{\"0\":{\"618\":1,\"621\":1},\"1\":{\"113\":2,\"174\":3,\"272\":2,\"618\":4,\"619\":4,\"621\":3,\"691\":8,\"692\":1,\"693\":1,\"695\":5,\"696\":2,\"697\":10,\"706\":6,\"710\":1,\"734\":5,\"773\":5,\"795\":2,\"796\":4,\"797\":6,\"814\":1,\"815\":4,\"817\":2,\"824\":3,\"828\":4,\"857\":1,\"905\":1,\"920\":1,\"1057\":2,\"1133\":2,\"1173\":1,\"1190\":4,\"1214\":2,\"1221\":2,\"1244\":4,\"1273\":2,\"1429\":1,\"1670\":4,\"1671\":3,\"1850\":1,\"1851\":2,\"1852\":1,\"1957\":4,\"1958\":2,\"1959\":2,\"1960\":4,\"1971\":1,\"2001\":2,\"2044\":1,\"2046\":3,\"2052\":1,\"2068\":1,\"2095\":2,\"2119\":1,\"2120\":1,\"2123\":1,\"2124\":1,\"2128\":2,\"2129\":2,\"2130\":1,\"2132\":1,\"2135\":1,\"2136\":1,\"2243\":2,\"2244\":2,\"2255\":2,\"2257\":1,\"2261\":3,\"2262\":3,\"2263\":2,\"2264\":3,\"2373\":1,\"2474\":2,\"2556\":1,\"2585\":2,\"2649\":2}}],[\"token\",{\"0\":{\"905\":1,\"2123\":1,\"2128\":1,\"2135\":1},\"1\":{\"57\":2,\"175\":6,\"194\":5,\"239\":6,\"301\":1,\"307\":7,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"384\":1,\"391\":1,\"399\":1,\"408\":1,\"422\":1,\"443\":2,\"449\":1,\"461\":1,\"478\":1,\"485\":3,\"491\":2,\"551\":1,\"557\":1,\"601\":1,\"612\":1,\"618\":4,\"619\":2,\"621\":1,\"691\":8,\"693\":2,\"695\":2,\"696\":2,\"697\":14,\"698\":1,\"699\":1,\"706\":6,\"731\":4,\"734\":4,\"758\":3,\"759\":2,\"773\":4,\"796\":3,\"797\":5,\"815\":2,\"817\":2,\"821\":1,\"822\":2,\"824\":2,\"828\":4,\"857\":2,\"905\":3,\"1057\":2,\"1059\":3,\"1115\":2,\"1133\":11,\"1138\":4,\"1139\":1,\"1142\":2,\"1155\":1,\"1171\":2,\"1172\":2,\"1173\":2,\"1186\":2,\"1190\":6,\"1191\":1,\"1192\":1,\"1204\":3,\"1206\":1,\"1210\":2,\"1214\":9,\"1219\":2,\"1221\":2,\"1228\":1,\"1244\":7,\"1269\":1,\"1273\":8,\"1298\":3,\"1299\":3,\"1301\":3,\"1302\":2,\"1303\":2,\"1304\":3,\"1334\":1,\"1337\":1,\"1345\":1,\"1347\":1,\"1349\":1,\"1350\":1,\"1429\":4,\"1670\":2,\"1671\":1,\"1842\":1,\"1851\":3,\"1879\":2,\"1881\":3,\"1889\":1,\"1892\":1,\"1893\":1,\"1955\":1,\"1957\":4,\"1958\":2,\"1959\":2,\"1960\":4,\"1970\":2,\"1975\":1,\"1984\":3,\"1996\":1,\"2000\":1,\"2001\":5,\"2002\":1,\"2024\":1,\"2027\":2,\"2028\":1,\"2076\":3,\"2078\":3,\"2079\":3,\"2081\":2,\"2086\":1,\"2087\":1,\"2095\":3,\"2123\":1,\"2128\":1,\"2131\":1,\"2135\":2,\"2137\":1,\"2178\":2,\"2179\":2,\"2183\":1,\"2190\":1,\"2191\":2,\"2194\":2,\"2195\":3,\"2196\":2,\"2236\":1,\"2241\":1,\"2243\":1,\"2244\":7,\"2245\":2,\"2255\":6,\"2256\":2,\"2261\":4,\"2262\":7,\"2263\":3,\"2264\":1,\"2266\":1,\"2275\":1,\"2278\":2,\"2279\":7,\"2280\":2,\"2294\":4,\"2301\":2,\"2373\":2,\"2398\":1,\"2433\":1,\"2474\":1,\"2534\":1,\"2555\":1,\"2556\":2,\"2569\":1,\"2592\":5,\"2596\":4,\"2649\":1}}],[\"todos\",{\"1\":{\"2403\":1,\"2539\":1,\"2543\":1}}],[\"todo\",{\"1\":{\"167\":1,\"173\":1,\"240\":1,\"1906\":1,\"2002\":2,\"2400\":1,\"2536\":1,\"2542\":1,\"2543\":1,\"2568\":3,\"2569\":1,\"2570\":1}}],[\"toda\",{\"1\":{\"130\":1}}],[\"today\",{\"1\":{\"58\":1,\"2384\":1,\"2410\":1}}],[\"torque\",{\"1\":{\"141\":1,\"142\":1,\"235\":1}}],[\"torchvision==0\",{\"1\":{\"2482\":1}}],[\"torchstft\",{\"0\":{\"1799\":1},\"1\":{\"1799\":1}}],[\"torchaudio==0\",{\"1\":{\"2482\":1}}],[\"torchaudiohubert\",{\"1\":{\"2431\":3}}],[\"torchaudiohubertpretrainmodel\",{\"0\":{\"1893\":1},\"1\":{\"1893\":1}}],[\"torchaudiohubertpretrainencoder\",{\"0\":{\"1269\":1},\"1\":{\"1269\":1}}],[\"torchaudio\",{\"1\":{\"1207\":1,\"1269\":2,\"1524\":1,\"1611\":1,\"1893\":1,\"1926\":1,\"1927\":3}}],[\"torch=false\",{\"1\":{\"1526\":1}}],[\"torch==1\",{\"1\":{\"200\":1,\"2482\":1}}],[\"torch=1\",{\"1\":{\"65\":1,\"2372\":1,\"2553\":1}}],[\"torch\",{\"0\":{\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"868\":1,\"927\":1,\"1207\":1,\"1292\":1,\"1724\":1,\"1750\":1,\"2148\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":1,\"2166\":1,\"2709\":1},\"1\":{\"60\":2,\"65\":1,\"80\":1,\"174\":9,\"194\":2,\"217\":5,\"218\":1,\"224\":5,\"225\":1,\"231\":5,\"232\":1,\"363\":2,\"617\":2,\"624\":1,\"625\":1,\"626\":2,\"627\":3,\"629\":1,\"632\":1,\"639\":2,\"645\":1,\"646\":1,\"653\":5,\"654\":2,\"655\":5,\"656\":2,\"676\":8,\"677\":5,\"678\":3,\"679\":3,\"681\":5,\"682\":6,\"684\":5,\"685\":3,\"686\":4,\"687\":4,\"688\":4,\"689\":4,\"690\":3,\"691\":17,\"692\":5,\"693\":2,\"695\":6,\"696\":5,\"697\":21,\"698\":1,\"699\":1,\"705\":3,\"706\":14,\"710\":10,\"711\":22,\"713\":2,\"714\":5,\"715\":5,\"716\":5,\"718\":5,\"719\":5,\"720\":5,\"721\":5,\"722\":4,\"723\":2,\"727\":1,\"729\":2,\"734\":14,\"740\":5,\"741\":5,\"749\":10,\"750\":4,\"758\":6,\"759\":4,\"767\":5,\"770\":2,\"771\":8,\"772\":3,\"773\":9,\"775\":5,\"776\":5,\"781\":7,\"782\":1,\"785\":15,\"786\":2,\"787\":1,\"794\":5,\"796\":5,\"797\":11,\"809\":8,\"810\":2,\"812\":3,\"813\":2,\"815\":5,\"817\":11,\"818\":2,\"828\":14,\"829\":2,\"838\":3,\"852\":3,\"856\":1,\"857\":1,\"868\":1,\"885\":1,\"887\":1,\"893\":2,\"900\":3,\"902\":3,\"904\":1,\"905\":3,\"907\":3,\"909\":2,\"920\":3,\"921\":3,\"924\":3,\"926\":5,\"927\":2,\"928\":4,\"936\":1,\"975\":2,\"976\":3,\"981\":1,\"997\":1,\"998\":1,\"1019\":1,\"1037\":1,\"1039\":1,\"1040\":1,\"1042\":2,\"1043\":3,\"1049\":1,\"1050\":1,\"1054\":1,\"1056\":1,\"1065\":1,\"1068\":1,\"1070\":2,\"1074\":1,\"1106\":3,\"1107\":2,\"1108\":3,\"1114\":2,\"1133\":4,\"1140\":6,\"1141\":5,\"1145\":4,\"1148\":7,\"1153\":3,\"1169\":6,\"1170\":5,\"1182\":1,\"1190\":8,\"1198\":1,\"1203\":7,\"1207\":1,\"1209\":15,\"1214\":4,\"1218\":4,\"1221\":5,\"1244\":8,\"1257\":1,\"1269\":1,\"1273\":4,\"1374\":2,\"1375\":6,\"1376\":3,\"1377\":4,\"1427\":4,\"1430\":2,\"1452\":1,\"1454\":7,\"1455\":4,\"1462\":2,\"1463\":7,\"1464\":2,\"1470\":2,\"1471\":2,\"1505\":8,\"1506\":1,\"1510\":2,\"1511\":3,\"1515\":7,\"1516\":12,\"1522\":2,\"1523\":7,\"1524\":20,\"1525\":11,\"1528\":7,\"1529\":4,\"1530\":3,\"1531\":3,\"1534\":7,\"1539\":7,\"1543\":2,\"1545\":2,\"1552\":2,\"1553\":3,\"1558\":7,\"1563\":3,\"1564\":1,\"1572\":2,\"1576\":2,\"1577\":2,\"1594\":3,\"1595\":1,\"1600\":3,\"1602\":2,\"1603\":4,\"1611\":9,\"1616\":2,\"1617\":2,\"1622\":3,\"1626\":7,\"1629\":1,\"1638\":2,\"1643\":1,\"1644\":4,\"1645\":7,\"1646\":2,\"1654\":7,\"1655\":4,\"1656\":2,\"1658\":7,\"1659\":8,\"1660\":6,\"1661\":6,\"1662\":6,\"1669\":7,\"1670\":2,\"1671\":7,\"1680\":3,\"1696\":4,\"1697\":4,\"1698\":5,\"1703\":1,\"1704\":5,\"1705\":4,\"1706\":4,\"1707\":5,\"1708\":4,\"1711\":3,\"1712\":4,\"1713\":4,\"1715\":4,\"1719\":11,\"1724\":1,\"1736\":1,\"1739\":5,\"1741\":1,\"1746\":2,\"1750\":1,\"1766\":2,\"1808\":16,\"1844\":1,\"1846\":1,\"1857\":1,\"1875\":1,\"1904\":2,\"1916\":2,\"1917\":5,\"1919\":1,\"1921\":2,\"1922\":2,\"1925\":2,\"1927\":2,\"1928\":2,\"1929\":2,\"1930\":1,\"1932\":1,\"1935\":2,\"1936\":2,\"1938\":2,\"1939\":2,\"1941\":2,\"1943\":2,\"1946\":3,\"1947\":2,\"1948\":1,\"1957\":11,\"1958\":4,\"1959\":2,\"1960\":11,\"1972\":1,\"1993\":5,\"2001\":4,\"2049\":2,\"2054\":3,\"2078\":2,\"2096\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2148\":2,\"2149\":3,\"2151\":1,\"2152\":1,\"2153\":4,\"2154\":1,\"2155\":7,\"2156\":2,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":2,\"2166\":2,\"2168\":1,\"2170\":1,\"2175\":1,\"2180\":1,\"2205\":3,\"2208\":1,\"2216\":1,\"2217\":1,\"2252\":2,\"2258\":5,\"2259\":4,\"2260\":14,\"2267\":3,\"2270\":1,\"2273\":1,\"2358\":1,\"2365\":2,\"2372\":5,\"2401\":2,\"2429\":1,\"2455\":1,\"2456\":1,\"2460\":2,\"2474\":2,\"2482\":1,\"2498\":10,\"2508\":2,\"2510\":2,\"2514\":2,\"2515\":2,\"2520\":2,\"2521\":2,\"2522\":1,\"2523\":1,\"2537\":2,\"2552\":1,\"2553\":4,\"2579\":1,\"2598\":2,\"2616\":12,\"2634\":12,\"2649\":2,\"2655\":2,\"2659\":2,\"2660\":2}}],[\"totonac\",{\"0\":{\"2387\":1},\"1\":{\"2387\":9}}],[\"tot\",{\"1\":{\"118\":1}}],[\"totally\",{\"1\":{\"74\":1,\"2372\":1,\"2429\":1,\"2554\":1}}],[\"total\",{\"1\":{\"17\":1,\"21\":1,\"22\":1,\"33\":1,\"77\":1,\"78\":1,\"95\":1,\"102\":1,\"110\":2,\"118\":1,\"148\":1,\"174\":1,\"203\":1,\"668\":2,\"676\":2,\"709\":2,\"711\":3,\"742\":2,\"1063\":1,\"1097\":1,\"1248\":1,\"1522\":1,\"1523\":1,\"1545\":1,\"1572\":1,\"1586\":1,\"1925\":2,\"2199\":2,\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2507\":1,\"2513\":1,\"2525\":1,\"2545\":1,\"2564\":1}}],[\"touched\",{\"1\":{\"2395\":1,\"2531\":1}}],[\"touch\",{\"1\":{\"60\":1,\"85\":1,\"135\":1,\"167\":1,\"178\":1,\"196\":1,\"200\":1,\"234\":1,\"235\":1,\"2568\":2,\"2569\":1,\"2570\":1}}],[\"tooshortutterror\",{\"0\":{\"823\":1,\"1085\":1},\"1\":{\"823\":2,\"1085\":2}}],[\"toolkits\",{\"1\":{\"2415\":1,\"2450\":1,\"2452\":1}}],[\"toolkit\",{\"1\":{\"113\":1,\"130\":7,\"167\":1,\"178\":1,\"1917\":1,\"2354\":1,\"2372\":1,\"2373\":1,\"2388\":1,\"2400\":1,\"2421\":1,\"2433\":1,\"2452\":3,\"2468\":2,\"2512\":1,\"2524\":1,\"2536\":1,\"2544\":1,\"2555\":1,\"2657\":1}}],[\"tool\",{\"0\":{\"136\":1},\"1\":{\"111\":1,\"142\":1,\"1850\":3,\"1877\":3,\"2641\":1}}],[\"tools\",{\"0\":{\"205\":1,\"2663\":1},\"1\":{\"56\":1,\"62\":1,\"85\":1,\"92\":2,\"101\":2,\"111\":1,\"126\":1,\"127\":4,\"128\":2,\"134\":5,\"135\":14,\"136\":4,\"137\":1,\"167\":13,\"173\":1,\"178\":12,\"196\":13,\"200\":5,\"234\":12,\"235\":1,\"528\":2,\"564\":1,\"2155\":1,\"2372\":9,\"2384\":1,\"2386\":1,\"2387\":1,\"2394\":2,\"2409\":2,\"2428\":1,\"2429\":5,\"2431\":1,\"2432\":1,\"2530\":2,\"2551\":1,\"2552\":4,\"2554\":2,\"2584\":2}}],[\"too\",{\"1\":{\"56\":1,\"69\":1,\"204\":1,\"235\":1,\"795\":1,\"823\":1,\"864\":1,\"1085\":1,\"1095\":1,\"1138\":1,\"2373\":2,\"2389\":1,\"2408\":1,\"2422\":1,\"2433\":2,\"2449\":1,\"2465\":1,\"2481\":1,\"2499\":1,\"2500\":1,\"2503\":1,\"2525\":1,\"2545\":1,\"2555\":2,\"2558\":1,\"2584\":1,\"2617\":2,\"2635\":2}}],[\"to\",{\"0\":{\"26\":1,\"27\":1,\"35\":1,\"53\":1,\"90\":1,\"124\":2,\"126\":1,\"128\":1,\"158\":1,\"202\":1,\"233\":1,\"281\":1,\"519\":1,\"533\":1,\"600\":1,\"633\":1,\"856\":1,\"919\":1,\"926\":1,\"927\":1,\"1355\":1,\"1356\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1809\":1,\"2094\":1,\"2165\":1,\"2166\":1,\"2230\":1,\"2306\":1,\"2375\":1,\"2376\":1,\"2379\":1,\"2386\":1,\"2392\":1,\"2425\":1,\"2435\":1,\"2453\":1,\"2460\":1,\"2502\":1,\"2518\":1,\"2528\":1,\"2548\":1,\"2558\":1,\"2561\":1,\"2575\":1,\"2643\":1},\"1\":{\"1\":4,\"2\":6,\"3\":3,\"4\":2,\"5\":4,\"6\":1,\"7\":2,\"10\":1,\"11\":3,\"14\":2,\"15\":1,\"16\":2,\"17\":4,\"18\":1,\"19\":3,\"20\":2,\"21\":16,\"22\":4,\"23\":4,\"24\":3,\"25\":4,\"26\":2,\"27\":3,\"28\":2,\"30\":2,\"33\":2,\"34\":1,\"38\":4,\"44\":4,\"45\":3,\"46\":5,\"47\":3,\"48\":3,\"49\":5,\"54\":2,\"56\":6,\"57\":3,\"58\":1,\"59\":1,\"60\":5,\"62\":3,\"63\":1,\"64\":1,\"66\":2,\"70\":2,\"71\":1,\"72\":2,\"73\":2,\"74\":5,\"76\":3,\"77\":2,\"78\":2,\"79\":7,\"80\":3,\"82\":1,\"83\":3,\"84\":4,\"85\":10,\"87\":1,\"90\":1,\"91\":3,\"92\":5,\"93\":2,\"94\":2,\"95\":2,\"96\":3,\"97\":1,\"98\":6,\"99\":8,\"100\":3,\"102\":7,\"103\":1,\"104\":3,\"105\":2,\"106\":2,\"107\":1,\"108\":1,\"109\":2,\"111\":1,\"112\":2,\"113\":10,\"114\":1,\"115\":14,\"116\":3,\"117\":1,\"118\":3,\"119\":4,\"120\":2,\"121\":6,\"122\":4,\"124\":10,\"126\":1,\"127\":3,\"128\":1,\"130\":5,\"133\":2,\"134\":1,\"135\":7,\"136\":6,\"141\":2,\"142\":1,\"143\":3,\"144\":8,\"148\":5,\"149\":1,\"150\":4,\"155\":1,\"159\":1,\"161\":4,\"162\":2,\"164\":3,\"168\":1,\"170\":1,\"172\":1,\"175\":2,\"179\":1,\"182\":1,\"185\":1,\"186\":1,\"195\":1,\"197\":1,\"198\":3,\"204\":1,\"217\":10,\"224\":10,\"231\":9,\"233\":1,\"234\":1,\"235\":8,\"237\":4,\"239\":1,\"240\":2,\"241\":2,\"242\":3,\"243\":1,\"245\":1,\"265\":1,\"271\":2,\"272\":3,\"274\":1,\"275\":3,\"276\":2,\"277\":2,\"279\":3,\"280\":1,\"281\":4,\"282\":3,\"283\":3,\"284\":3,\"286\":1,\"294\":1,\"295\":7,\"296\":1,\"297\":1,\"363\":2,\"397\":1,\"493\":1,\"496\":1,\"519\":3,\"533\":3,\"536\":1,\"546\":1,\"549\":1,\"551\":1,\"554\":1,\"557\":1,\"570\":1,\"574\":1,\"579\":1,\"588\":1,\"590\":1,\"594\":1,\"600\":3,\"605\":1,\"606\":2,\"607\":4,\"610\":1,\"612\":2,\"614\":1,\"615\":1,\"616\":1,\"617\":4,\"619\":1,\"620\":1,\"621\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":3,\"627\":6,\"628\":1,\"629\":5,\"630\":1,\"631\":1,\"632\":3,\"633\":3,\"634\":1,\"637\":1,\"639\":2,\"642\":1,\"643\":1,\"645\":1,\"648\":6,\"651\":1,\"652\":4,\"653\":1,\"655\":1,\"656\":1,\"658\":1,\"676\":1,\"677\":1,\"678\":2,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"691\":10,\"692\":4,\"693\":5,\"695\":1,\"697\":14,\"698\":2,\"699\":2,\"700\":1,\"701\":3,\"702\":1,\"704\":2,\"705\":3,\"706\":4,\"710\":3,\"711\":2,\"712\":5,\"713\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"725\":3,\"727\":7,\"728\":7,\"729\":1,\"731\":2,\"734\":4,\"736\":1,\"737\":1,\"738\":2,\"742\":1,\"743\":1,\"745\":7,\"746\":7,\"747\":2,\"749\":2,\"753\":1,\"754\":15,\"755\":2,\"757\":1,\"758\":2,\"760\":3,\"761\":1,\"762\":5,\"763\":6,\"765\":1,\"767\":2,\"769\":1,\"770\":1,\"771\":1,\"774\":3,\"778\":3,\"779\":1,\"781\":1,\"784\":2,\"785\":2,\"786\":2,\"791\":1,\"793\":1,\"794\":1,\"795\":2,\"796\":1,\"797\":10,\"798\":1,\"799\":1,\"801\":2,\"802\":2,\"803\":2,\"806\":3,\"807\":1,\"808\":1,\"809\":1,\"812\":1,\"815\":3,\"817\":2,\"820\":3,\"821\":9,\"822\":2,\"824\":2,\"826\":22,\"828\":2,\"831\":1,\"832\":1,\"834\":4,\"835\":1,\"836\":1,\"838\":1,\"856\":2,\"857\":4,\"859\":1,\"860\":1,\"862\":1,\"867\":1,\"869\":2,\"870\":2,\"877\":2,\"880\":1,\"892\":1,\"903\":2,\"909\":6,\"914\":1,\"915\":2,\"917\":2,\"918\":1,\"919\":1,\"920\":1,\"926\":1,\"927\":2,\"928\":3,\"936\":1,\"950\":1,\"952\":2,\"956\":1,\"958\":1,\"968\":1,\"973\":1,\"975\":1,\"976\":1,\"987\":2,\"988\":1,\"989\":1,\"996\":2,\"997\":6,\"998\":3,\"999\":1,\"1001\":2,\"1003\":5,\"1004\":5,\"1005\":7,\"1008\":1,\"1011\":4,\"1013\":1,\"1015\":1,\"1016\":1,\"1025\":1,\"1028\":8,\"1029\":1,\"1031\":1,\"1042\":1,\"1043\":1,\"1046\":2,\"1047\":1,\"1048\":4,\"1049\":1,\"1050\":1,\"1051\":2,\"1052\":8,\"1053\":1,\"1054\":3,\"1055\":3,\"1056\":1,\"1057\":4,\"1059\":4,\"1066\":1,\"1067\":2,\"1068\":1,\"1071\":4,\"1072\":1,\"1073\":2,\"1075\":2,\"1076\":4,\"1080\":1,\"1082\":2,\"1083\":2,\"1084\":1,\"1085\":1,\"1093\":2,\"1096\":1,\"1098\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1130\":1,\"1131\":1,\"1132\":3,\"1133\":6,\"1135\":1,\"1137\":1,\"1138\":2,\"1139\":1,\"1140\":2,\"1142\":5,\"1143\":1,\"1144\":2,\"1145\":1,\"1148\":9,\"1149\":10,\"1150\":10,\"1152\":1,\"1157\":1,\"1159\":1,\"1160\":5,\"1161\":5,\"1162\":3,\"1163\":3,\"1164\":5,\"1165\":2,\"1169\":2,\"1171\":2,\"1173\":4,\"1175\":1,\"1177\":4,\"1178\":6,\"1179\":1,\"1180\":6,\"1181\":3,\"1182\":1,\"1185\":1,\"1186\":7,\"1187\":9,\"1189\":1,\"1198\":4,\"1200\":2,\"1202\":9,\"1203\":9,\"1206\":2,\"1208\":1,\"1209\":1,\"1210\":7,\"1211\":6,\"1213\":1,\"1214\":1,\"1216\":3,\"1218\":2,\"1221\":1,\"1223\":1,\"1224\":5,\"1227\":2,\"1228\":1,\"1230\":2,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1241\":4,\"1243\":2,\"1245\":11,\"1246\":1,\"1248\":3,\"1250\":1,\"1252\":8,\"1253\":14,\"1254\":10,\"1255\":1,\"1256\":1,\"1257\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1269\":19,\"1270\":3,\"1272\":4,\"1273\":2,\"1274\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1278\":1,\"1279\":4,\"1281\":1,\"1283\":1,\"1285\":1,\"1286\":7,\"1287\":6,\"1298\":1,\"1299\":1,\"1301\":6,\"1302\":1,\"1303\":1,\"1304\":6,\"1327\":5,\"1336\":3,\"1337\":4,\"1339\":1,\"1343\":1,\"1344\":2,\"1345\":1,\"1346\":2,\"1347\":1,\"1348\":2,\"1349\":3,\"1350\":3,\"1351\":1,\"1352\":1,\"1355\":4,\"1356\":3,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1369\":1,\"1371\":1,\"1375\":4,\"1377\":2,\"1382\":1,\"1392\":6,\"1398\":1,\"1400\":1,\"1406\":1,\"1409\":2,\"1427\":6,\"1430\":1,\"1432\":4,\"1434\":1,\"1436\":3,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":2,\"1453\":1,\"1454\":4,\"1457\":1,\"1459\":1,\"1461\":1,\"1462\":5,\"1463\":2,\"1464\":3,\"1467\":1,\"1469\":1,\"1473\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":4,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1505\":5,\"1507\":1,\"1509\":1,\"1510\":3,\"1511\":2,\"1513\":1,\"1516\":9,\"1517\":1,\"1519\":1,\"1521\":1,\"1522\":5,\"1523\":3,\"1524\":1,\"1526\":2,\"1528\":3,\"1531\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1538\":1,\"1539\":2,\"1541\":1,\"1543\":1,\"1544\":1,\"1545\":2,\"1548\":1,\"1550\":1,\"1551\":16,\"1552\":4,\"1553\":16,\"1556\":1,\"1558\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1564\":1,\"1565\":1,\"1572\":2,\"1574\":1,\"1577\":1,\"1581\":2,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1601\":4,\"1603\":1,\"1604\":2,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1618\":2,\"1619\":2,\"1621\":1,\"1622\":2,\"1625\":1,\"1626\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1638\":3,\"1639\":5,\"1640\":1,\"1642\":1,\"1643\":3,\"1644\":2,\"1645\":1,\"1646\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1655\":3,\"1656\":1,\"1657\":1,\"1658\":2,\"1659\":3,\"1660\":3,\"1661\":3,\"1662\":4,\"1664\":1,\"1665\":1,\"1669\":3,\"1670\":3,\"1671\":4,\"1673\":1,\"1675\":1,\"1677\":1,\"1688\":1,\"1693\":2,\"1696\":2,\"1697\":2,\"1698\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1712\":5,\"1715\":4,\"1717\":2,\"1719\":6,\"1735\":1,\"1741\":2,\"1747\":1,\"1748\":2,\"1749\":2,\"1752\":5,\"1755\":2,\"1756\":1,\"1762\":1,\"1764\":1,\"1765\":5,\"1766\":4,\"1770\":1,\"1771\":11,\"1773\":3,\"1775\":1,\"1776\":1,\"1777\":2,\"1778\":13,\"1780\":1,\"1781\":4,\"1783\":1,\"1785\":1,\"1786\":1,\"1787\":3,\"1788\":18,\"1790\":1,\"1791\":2,\"1792\":1,\"1794\":1,\"1796\":1,\"1798\":6,\"1800\":7,\"1801\":4,\"1802\":1,\"1803\":6,\"1804\":18,\"1805\":12,\"1807\":1,\"1808\":3,\"1809\":1,\"1810\":3,\"1811\":2,\"1829\":1,\"1833\":1,\"1836\":1,\"1837\":4,\"1838\":1,\"1839\":3,\"1840\":1,\"1842\":2,\"1843\":1,\"1844\":5,\"1846\":1,\"1847\":1,\"1848\":8,\"1849\":7,\"1850\":11,\"1851\":27,\"1852\":11,\"1855\":1,\"1856\":1,\"1857\":4,\"1858\":2,\"1859\":3,\"1861\":3,\"1862\":5,\"1863\":4,\"1864\":6,\"1865\":4,\"1866\":2,\"1867\":1,\"1868\":3,\"1870\":3,\"1871\":4,\"1872\":1,\"1873\":1,\"1874\":5,\"1877\":12,\"1878\":14,\"1879\":2,\"1880\":8,\"1881\":3,\"1891\":1,\"1895\":2,\"1897\":1,\"1900\":2,\"1903\":1,\"1904\":2,\"1905\":8,\"1906\":1,\"1908\":1,\"1910\":1,\"1912\":3,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":2,\"1917\":5,\"1918\":1,\"1919\":1,\"1920\":1,\"1923\":1,\"1925\":3,\"1926\":2,\"1927\":1,\"1928\":2,\"1929\":3,\"1930\":1,\"1932\":5,\"1941\":3,\"1947\":2,\"1951\":2,\"1952\":1,\"1953\":2,\"1954\":1,\"1955\":2,\"1956\":2,\"1959\":1,\"1962\":3,\"1972\":1,\"1976\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2001\":9,\"2002\":13,\"2003\":2,\"2004\":9,\"2012\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1,\"2028\":1,\"2029\":3,\"2031\":1,\"2032\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2046\":1,\"2048\":1,\"2049\":1,\"2050\":2,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":5,\"2055\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2064\":1,\"2065\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2078\":4,\"2079\":5,\"2081\":1,\"2082\":2,\"2083\":2,\"2084\":1,\"2086\":10,\"2087\":11,\"2088\":2,\"2089\":1,\"2090\":19,\"2091\":2,\"2094\":1,\"2095\":20,\"2096\":1,\"2098\":1,\"2099\":6,\"2100\":1,\"2101\":1,\"2102\":4,\"2103\":1,\"2104\":2,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2121\":1,\"2122\":2,\"2125\":1,\"2137\":1,\"2142\":2,\"2148\":1,\"2150\":1,\"2151\":3,\"2153\":4,\"2154\":1,\"2155\":1,\"2157\":1,\"2165\":2,\"2166\":2,\"2168\":2,\"2169\":1,\"2170\":5,\"2176\":1,\"2185\":1,\"2197\":8,\"2201\":1,\"2203\":1,\"2208\":2,\"2216\":1,\"2217\":1,\"2230\":2,\"2234\":1,\"2238\":1,\"2239\":1,\"2240\":3,\"2242\":1,\"2243\":25,\"2244\":29,\"2245\":2,\"2247\":1,\"2249\":1,\"2251\":1,\"2254\":1,\"2255\":27,\"2256\":2,\"2257\":1,\"2259\":4,\"2260\":12,\"2261\":1,\"2262\":1,\"2263\":18,\"2264\":24,\"2265\":2,\"2267\":3,\"2268\":1,\"2270\":1,\"2272\":1,\"2274\":3,\"2276\":1,\"2278\":3,\"2279\":24,\"2280\":2,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2306\":2,\"2313\":2,\"2317\":3,\"2318\":1,\"2330\":3,\"2347\":1,\"2349\":6,\"2354\":6,\"2355\":1,\"2358\":1,\"2359\":1,\"2360\":1,\"2362\":2,\"2363\":2,\"2365\":1,\"2371\":1,\"2372\":3,\"2373\":4,\"2374\":1,\"2375\":2,\"2380\":1,\"2381\":1,\"2383\":2,\"2384\":5,\"2385\":7,\"2386\":1,\"2387\":7,\"2388\":3,\"2389\":4,\"2391\":4,\"2392\":1,\"2393\":7,\"2394\":9,\"2395\":4,\"2397\":1,\"2398\":4,\"2399\":3,\"2400\":6,\"2401\":8,\"2403\":4,\"2406\":1,\"2407\":1,\"2408\":4,\"2410\":2,\"2411\":3,\"2412\":4,\"2414\":2,\"2415\":3,\"2416\":1,\"2417\":1,\"2418\":1,\"2421\":2,\"2422\":5,\"2423\":5,\"2424\":1,\"2425\":1,\"2427\":4,\"2429\":5,\"2430\":7,\"2431\":1,\"2432\":2,\"2433\":2,\"2434\":1,\"2435\":1,\"2437\":2,\"2439\":3,\"2440\":4,\"2441\":4,\"2446\":3,\"2448\":1,\"2449\":4,\"2451\":3,\"2452\":4,\"2457\":1,\"2458\":1,\"2461\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":4,\"2467\":6,\"2468\":3,\"2472\":1,\"2473\":5,\"2479\":1,\"2481\":6,\"2487\":1,\"2491\":1,\"2492\":2,\"2497\":1,\"2499\":1,\"2500\":5,\"2501\":1,\"2502\":1,\"2503\":4,\"2504\":2,\"2506\":2,\"2508\":1,\"2510\":1,\"2514\":4,\"2515\":1,\"2516\":1,\"2518\":3,\"2520\":1,\"2521\":2,\"2523\":2,\"2524\":3,\"2525\":4,\"2527\":4,\"2528\":1,\"2529\":7,\"2530\":9,\"2531\":4,\"2533\":1,\"2534\":4,\"2535\":3,\"2536\":6,\"2537\":8,\"2539\":4,\"2542\":2,\"2543\":2,\"2544\":2,\"2545\":5,\"2546\":5,\"2547\":1,\"2548\":1,\"2550\":4,\"2552\":3,\"2554\":3,\"2555\":7,\"2557\":1,\"2558\":9,\"2559\":1,\"2560\":1,\"2561\":1,\"2563\":2,\"2564\":4,\"2565\":2,\"2566\":1,\"2567\":1,\"2568\":5,\"2569\":2,\"2571\":2,\"2572\":1,\"2573\":1,\"2574\":1,\"2575\":2,\"2576\":2,\"2579\":1,\"2580\":1,\"2582\":1,\"2583\":1,\"2584\":18,\"2585\":14,\"2592\":1,\"2596\":1,\"2597\":1,\"2598\":5,\"2599\":2,\"2600\":5,\"2612\":1,\"2617\":5,\"2618\":1,\"2628\":2,\"2630\":1,\"2635\":9,\"2637\":1,\"2638\":5,\"2639\":2,\"2640\":1,\"2642\":2,\"2643\":4,\"2644\":3,\"2645\":1,\"2646\":1,\"2648\":1,\"2651\":2,\"2653\":2,\"2655\":1,\"2659\":4,\"2660\":1}}],[\"a0\",{\"1\":{\"2500\":9,\"2617\":9,\"2635\":9}}],[\"aisshell\",{\"1\":{\"2593\":1}}],[\"aishell\",{\"0\":{\"2589\":1},\"1\":{\"2357\":2,\"2578\":2,\"2587\":1,\"2589\":1}}],[\"aims\",{\"1\":{\"2395\":1,\"2531\":1}}],[\"aiming\",{\"1\":{\"778\":1}}],[\"aim\",{\"1\":{\"57\":1,\"2467\":1,\"2473\":1,\"2585\":1}}],[\"a=3\",{\"1\":{\"2314\":1}}],[\"a=a\",{\"1\":{\"2176\":1}}],[\"a=1\",{\"1\":{\"944\":1}}],[\"aaa\",{\"1\":{\"2340\":2}}],[\"aaaa\",{\"1\":{\"2176\":1}}],[\"aamsoftmax\",{\"0\":{\"2030\":2,\"2040\":1},\"1\":{\"2030\":6,\"2040\":1}}],[\"aa\",{\"1\":{\"1397\":1,\"1404\":1,\"1408\":7,\"1412\":1,\"1416\":1}}],[\"aand\",{\"1\":{\"47\":1}}],[\"a3\",{\"1\":{\"1391\":1}}],[\"a^i\",{\"1\":{\"1341\":1}}],[\"a^l\",{\"1\":{\"1341\":1}}],[\"a^dt\",{\"1\":{\"1248\":1}}],[\"aes\",{\"1\":{\"1048\":1}}],[\"ax\",{\"1\":{\"1742\":1}}],[\"axes=\",{\"1\":{\"940\":1}}],[\"axis=0\",{\"1\":{\"1390\":1,\"1391\":1,\"1392\":1,\"1757\":1}}],[\"axis=1\",{\"1\":{\"942\":1,\"1405\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1757\":1}}],[\"axis\",{\"0\":{\"1914\":1,\"1915\":1,\"1940\":2},\"1\":{\"60\":2,\"648\":2,\"940\":1,\"1005\":1,\"1028\":3,\"1216\":1,\"1243\":1,\"1251\":1,\"1274\":1,\"1276\":1,\"1392\":2,\"1426\":1,\"1543\":5,\"1564\":1,\"1684\":1,\"1710\":2,\"1834\":3,\"1869\":2,\"1876\":3,\"1914\":1,\"1915\":2,\"1940\":2,\"2401\":1,\"2537\":1}}],[\"aggregation\",{\"0\":{\"1910\":1},\"1\":{\"1910\":1,\"2044\":1,\"2049\":1,\"2054\":1}}],[\"aggregator\",{\"1\":{\"1371\":1,\"1373\":1}}],[\"aggregates\",{\"1\":{\"2044\":1,\"2068\":1}}],[\"aggregate\",{\"0\":{\"299\":1,\"2206\":1},\"1\":{\"299\":3,\"2046\":3,\"2084\":2,\"2206\":2}}],[\"agument\",{\"1\":{\"943\":1,\"950\":1,\"955\":1,\"965\":1,\"968\":1,\"972\":1}}],[\"agent\",{\"1\":{\"682\":1,\"2461\":2}}],[\"against\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"again\",{\"1\":{\"59\":1,\"127\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"2433\":1,\"2571\":1,\"2584\":1}}],[\"a8fava5ogczicjnw4blqyjfg4oa9vet\",{\"1\":{\"215\":1}}],[\"april\",{\"1\":{\"2583\":1}}],[\"ap\",{\"1\":{\"2461\":1,\"2462\":1}}],[\"apart\",{\"1\":{\"2387\":1}}],[\"apex\",{\"1\":{\"627\":2}}],[\"apex=false\",{\"1\":{\"627\":1}}],[\"api\",{\"1\":{\"150\":2,\"249\":1,\"257\":1,\"261\":1,\"286\":5,\"296\":3,\"1375\":1,\"1379\":1,\"1524\":1,\"1611\":1,\"1664\":1,\"2431\":2,\"2481\":2,\"2618\":2}}],[\"apt\",{\"1\":{\"132\":3,\"134\":1,\"167\":1,\"178\":1,\"196\":1,\"200\":1,\"234\":1,\"2372\":1}}],[\"appear\",{\"1\":{\"2373\":1,\"2555\":1}}],[\"appendix\",{\"1\":{\"772\":1,\"810\":1}}],[\"append\",{\"1\":{\"174\":6,\"175\":1,\"194\":1,\"217\":1,\"224\":1,\"231\":1,\"697\":3,\"1659\":1,\"2313\":1,\"2431\":3,\"2500\":12,\"2592\":1,\"2617\":12,\"2635\":12}}],[\"appended\",{\"1\":{\"60\":1}}],[\"approaches\",{\"1\":{\"2452\":1,\"2467\":1,\"2473\":2}}],[\"approach\",{\"1\":{\"1883\":1,\"2403\":1,\"2430\":1,\"2437\":1,\"2467\":1,\"2539\":1,\"2555\":1,\"2563\":1,\"2574\":1}}],[\"approximation\",{\"1\":{\"1712\":2,\"1715\":2}}],[\"approximate\",{\"1\":{\"1660\":1,\"1661\":1}}],[\"approx\",{\"1\":{\"1578\":1,\"1579\":1,\"1660\":2,\"1661\":2,\"1712\":3,\"1715\":3}}],[\"appropriate\",{\"1\":{\"44\":2,\"133\":1,\"377\":1,\"2467\":1}}],[\"appropriately\",{\"1\":{\"19\":1,\"150\":1,\"1245\":1,\"2373\":1,\"2430\":1,\"2431\":1,\"2555\":1}}],[\"applicable\",{\"1\":{\"2441\":1}}],[\"application\",{\"1\":{\"1436\":1,\"1511\":1,\"1644\":1,\"1712\":1,\"1715\":1}}],[\"applications\",{\"1\":{\"1432\":1,\"1510\":1,\"1643\":1,\"2236\":1,\"2355\":1,\"2467\":1,\"2574\":1}}],[\"applies\",{\"1\":{\"804\":1,\"1564\":1,\"1911\":1}}],[\"applied\",{\"1\":{\"76\":1,\"84\":1,\"711\":2,\"749\":2,\"797\":1,\"826\":3,\"1061\":1,\"1133\":2,\"1145\":3,\"1148\":2,\"1149\":2,\"1150\":2,\"1169\":1,\"1203\":2,\"1211\":1,\"1224\":1,\"1244\":1,\"1252\":1,\"1255\":1,\"1269\":5,\"1272\":2,\"1336\":1,\"1348\":1,\"1430\":1,\"1505\":2,\"1535\":1,\"1537\":1,\"1539\":1,\"1581\":1,\"1669\":2,\"1670\":1,\"1765\":1,\"1777\":1,\"1800\":1,\"1803\":1,\"1810\":1,\"1844\":1,\"1848\":2,\"1849\":2,\"1851\":1,\"1857\":1,\"1861\":1,\"1862\":2,\"1871\":1,\"1880\":2,\"1905\":4,\"1917\":2,\"1929\":1,\"1941\":1,\"1947\":1,\"2001\":2,\"2004\":2,\"2029\":2,\"2148\":1,\"2264\":6,\"2468\":1}}],[\"applying\",{\"1\":{\"677\":1,\"678\":1,\"679\":1,\"681\":1,\"682\":1,\"684\":1,\"685\":1,\"688\":1,\"758\":1,\"1244\":1,\"1252\":1,\"1452\":1,\"1905\":1,\"2197\":3}}],[\"apply\",{\"0\":{\"496\":1,\"854\":1,\"1001\":1,\"1678\":1},\"1\":{\"113\":1,\"148\":1,\"242\":2,\"247\":2,\"251\":2,\"259\":2,\"276\":1,\"281\":1,\"496\":3,\"752\":1,\"754\":2,\"755\":1,\"760\":1,\"770\":2,\"822\":2,\"826\":5,\"854\":1,\"932\":1,\"950\":1,\"952\":1,\"958\":1,\"968\":1,\"1001\":2,\"1011\":1,\"1132\":1,\"1158\":1,\"1180\":1,\"1198\":1,\"1211\":1,\"1224\":1,\"1255\":1,\"1256\":1,\"1257\":3,\"1336\":1,\"1348\":1,\"1352\":2,\"1463\":1,\"1484\":2,\"1516\":2,\"1522\":1,\"1523\":1,\"1524\":1,\"1531\":2,\"1572\":1,\"1601\":2,\"1645\":1,\"1678\":1,\"1765\":2,\"1787\":1,\"1798\":1,\"1800\":2,\"1803\":2,\"1804\":4,\"1810\":2,\"1844\":2,\"1848\":4,\"1849\":4,\"1851\":4,\"1857\":2,\"1858\":2,\"1861\":2,\"1862\":2,\"1863\":1,\"1870\":4,\"1871\":2,\"1874\":1,\"1878\":4,\"1879\":1,\"1880\":2,\"1905\":3,\"1906\":3,\"1923\":1,\"1925\":1,\"1926\":1,\"1928\":1,\"1932\":1,\"1940\":1,\"1949\":1,\"1971\":1,\"2002\":1,\"2079\":2,\"2086\":1,\"2087\":1,\"2088\":2,\"2090\":4,\"2091\":1,\"2095\":3,\"2178\":2,\"2179\":2,\"2184\":2,\"2191\":2,\"2194\":4,\"2195\":2,\"2197\":4,\"2200\":2,\"2243\":4,\"2244\":4,\"2245\":1,\"2255\":4,\"2256\":1,\"2263\":2,\"2264\":7,\"2279\":4,\"2280\":1,\"2584\":1,\"2600\":1}}],[\"ahead=16\",{\"1\":{\"692\":1}}],[\"aheads=4\",{\"1\":{\"689\":1}}],[\"aheads\",{\"1\":{\"686\":3,\"687\":3,\"688\":4,\"689\":5,\"754\":1,\"826\":1,\"892\":2,\"1220\":1,\"1289\":1,\"1505\":2,\"1669\":2,\"1778\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"2002\":1,\"2003\":1,\"2090\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2264\":2,\"2279\":1}}],[\"ahead\",{\"1\":{\"104\":2,\"693\":5,\"1149\":3,\"1150\":3,\"2387\":1,\"2600\":2}}],[\"able\",{\"1\":{\"2389\":1,\"2398\":1,\"2408\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2534\":1}}],[\"abbreviates\",{\"1\":{\"2154\":1}}],[\"abel\",{\"1\":{\"1773\":1,\"2082\":1}}],[\"abc\",{\"1\":{\"1046\":1,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1514\":1,\"1570\":1,\"1623\":1,\"1638\":1,\"1667\":1,\"1760\":1,\"1828\":1,\"1894\":1,\"1902\":1,\"1909\":1,\"1951\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"2005\":1,\"2016\":1,\"2024\":1,\"2036\":1,\"2038\":1,\"2077\":1,\"2099\":1,\"2119\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2171\":1,\"2233\":1,\"2235\":1,\"2275\":1,\"2277\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2335\":4,\"2337\":2}}],[\"above\",{\"0\":{\"1822\":1},\"1\":{\"106\":1,\"115\":1,\"137\":1,\"143\":1,\"150\":1,\"201\":1,\"203\":1,\"204\":1,\"237\":1,\"238\":1,\"792\":1,\"899\":1,\"901\":1,\"1067\":1,\"1096\":1,\"1211\":1,\"1286\":1,\"1406\":1,\"1452\":1,\"1804\":1,\"1822\":1,\"1846\":1,\"1847\":1,\"1858\":1,\"1871\":1,\"1878\":1,\"1905\":2,\"2154\":1,\"2387\":2,\"2492\":1,\"2628\":1}}],[\"about\",{\"0\":{\"38\":1},\"1\":{\"47\":2,\"70\":1,\"71\":1,\"85\":1,\"92\":1,\"97\":1,\"106\":1,\"150\":1,\"235\":1,\"1371\":1,\"1603\":1,\"1622\":1,\"2382\":1,\"2384\":1,\"2430\":1,\"2479\":1,\"2555\":1,\"2573\":2,\"2584\":3,\"2585\":3,\"2635\":1}}],[\"absuasrloss\",{\"0\":{\"2288\":1},\"1\":{\"2288\":2,\"2294\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1}}],[\"absgenerator\",{\"0\":{\"2285\":1},\"1\":{\"2285\":1,\"2292\":1,\"2294\":1}}],[\"absgantts\",{\"0\":{\"1828\":1},\"1\":{\"1828\":2,\"1837\":1,\"1850\":1,\"1852\":1,\"1877\":1}}],[\"absganespnetmodel\",{\"0\":{\"2170\":1},\"1\":{\"1773\":1,\"1837\":1,\"2170\":2,\"2185\":1,\"2203\":1}}],[\"absgansvs\",{\"0\":{\"1760\":1},\"1\":{\"1760\":2,\"1773\":1,\"1778\":1,\"1805\":1}}],[\"absvalepochstepscheduler\",{\"0\":{\"2017\":1},\"1\":{\"2017\":1,\"2022\":1}}],[\"absbatchstepscheduler\",{\"0\":{\"2014\":1},\"1\":{\"2014\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1}}],[\"absloss\",{\"0\":{\"2034\":1},\"1\":{\"2030\":1,\"2034\":2,\"2040\":1,\"2046\":1}}],[\"abslosswrapper\",{\"0\":{\"1443\":1},\"1\":{\"1218\":1,\"1443\":1,\"1530\":1,\"1553\":1,\"1554\":1,\"1563\":1,\"1600\":1,\"1603\":1,\"1622\":1}}],[\"abslm\",{\"0\":{\"1951\":1},\"1\":{\"1951\":4,\"1953\":1,\"1955\":1,\"1957\":1,\"1958\":1,\"1960\":1}}],[\"absiterfactory\",{\"0\":{\"1894\":1},\"1\":{\"1894\":1,\"1895\":1,\"1896\":1,\"1898\":2,\"1900\":1,\"2201\":3,\"2345\":1}}],[\"absfeatsextractdiscrete\",{\"0\":{\"2275\":1},\"1\":{\"2275\":1,\"2278\":1,\"2281\":1}}],[\"absfeatsextract\",{\"0\":{\"2233\":1},\"1\":{\"1773\":8,\"1837\":3,\"1981\":1,\"2082\":8,\"2084\":1,\"2089\":1,\"2233\":1,\"2236\":1,\"2240\":3,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2278\":2}}],[\"absfrontend\",{\"0\":{\"1121\":1},\"1\":{\"1057\":1,\"1113\":1,\"1121\":1,\"1132\":1,\"1158\":1,\"1171\":1,\"1172\":1,\"1184\":1,\"1206\":1,\"1207\":1,\"1239\":1,\"1255\":1,\"1284\":1,\"1371\":1,\"1773\":1,\"1774\":1,\"1791\":1,\"1892\":1,\"1893\":1,\"1970\":1,\"1971\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2046\":1,\"2076\":1,\"2294\":1}}],[\"abssegmenter\",{\"0\":{\"2287\":1},\"1\":{\"2287\":1,\"2294\":1,\"2295\":1,\"2296\":1}}],[\"absseprator\",{\"1\":{\"1454\":1}}],[\"absseparator\",{\"0\":{\"1445\":1},\"1\":{\"1377\":1,\"1445\":1,\"1454\":1,\"1463\":1,\"1505\":1,\"1515\":1,\"1516\":1,\"1523\":1,\"1528\":1,\"1529\":1,\"1534\":1,\"1539\":1,\"1553\":1,\"1558\":2,\"1611\":1,\"1626\":1,\"1645\":1,\"1654\":1,\"1658\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1669\":1,\"1671\":1,\"1719\":1}}],[\"absscheduler\",{\"0\":{\"2016\":1},\"1\":{\"2014\":1,\"2015\":1,\"2016\":1,\"2185\":1,\"2201\":3,\"2203\":1}}],[\"abssynthesizer\",{\"0\":{\"1980\":1},\"1\":{\"1980\":1,\"1984\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1}}],[\"abss2stloss\",{\"0\":{\"1978\":1},\"1\":{\"1978\":2,\"1984\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1}}],[\"abss2stauxattention\",{\"0\":{\"1976\":1},\"1\":{\"1976\":1,\"1984\":1,\"1993\":1}}],[\"abssampler\",{\"0\":{\"2005\":1},\"1\":{\"1895\":1,\"1896\":1,\"1899\":1,\"1900\":1,\"2005\":2,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1}}],[\"abssvs\",{\"0\":{\"2077\":1},\"1\":{\"1760\":1,\"2077\":2,\"2082\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2095\":1}}],[\"absspecaug\",{\"0\":{\"1127\":1},\"1\":{\"1057\":1,\"1113\":1,\"1127\":2,\"1171\":1,\"1172\":1,\"1206\":1,\"1257\":1,\"1371\":1,\"1892\":1,\"1893\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2046\":1,\"2076\":1}}],[\"absmask\",{\"0\":{\"1366\":1},\"1\":{\"1366\":2,\"1375\":1,\"1553\":1}}],[\"absdataset\",{\"0\":{\"2167\":1},\"1\":{\"2099\":1,\"2167\":2,\"2182\":1,\"2346\":1}}],[\"absdiscriminator\",{\"0\":{\"2283\":1},\"1\":{\"2283\":1,\"2290\":1,\"2294\":1,\"2302\":1}}],[\"absdiffusion\",{\"0\":{\"1433\":1},\"1\":{\"1433\":1,\"1551\":1,\"1646\":1}}],[\"absdiarization\",{\"0\":{\"1364\":1},\"1\":{\"1364\":2}}],[\"absdecoder\",{\"0\":{\"1046\":1,\"1111\":1,\"1117\":1,\"1362\":1,\"1431\":1},\"1\":{\"1046\":1,\"1048\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1073\":1,\"1075\":1,\"1083\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1117\":2,\"1133\":1,\"1138\":1,\"1139\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1190\":1,\"1204\":1,\"1214\":1,\"1219\":1,\"1220\":1,\"1244\":1,\"1270\":1,\"1362\":1,\"1371\":1,\"1374\":1,\"1431\":2,\"1510\":1,\"1551\":1,\"1553\":1,\"1554\":1,\"1616\":1,\"1643\":1,\"1970\":1,\"1975\":1,\"1984\":2,\"2027\":1,\"2076\":3}}],[\"absattractor\",{\"0\":{\"1360\":1},\"1\":{\"1360\":1,\"1371\":1,\"1376\":1}}],[\"absasvspoofloss\",{\"0\":{\"1109\":1},\"1\":{\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1113\":1}}],[\"absorb\",{\"1\":{\"1278\":1}}],[\"absolute\",{\"1\":{\"697\":2,\"797\":1,\"1400\":1,\"1566\":1,\"2567\":1}}],[\"abspreprocessor\",{\"0\":{\"2171\":1},\"1\":{\"2171\":2,\"2178\":1,\"2181\":1,\"2196\":1}}],[\"abspreencoder\",{\"0\":{\"1125\":1},\"1\":{\"1113\":1,\"1125\":1,\"1171\":1,\"1172\":1,\"1198\":1,\"1201\":1,\"1206\":1,\"1892\":1,\"1893\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2027\":1,\"2076\":1}}],[\"absprojector\",{\"0\":{\"2038\":1},\"1\":{\"2038\":1,\"2046\":1,\"2057\":1,\"2066\":1,\"2072\":1}}],[\"abspooling\",{\"0\":{\"2036\":1},\"1\":{\"2036\":2,\"2044\":1,\"2046\":1,\"2052\":1,\"2068\":1}}],[\"abspostdecoder\",{\"0\":{\"2024\":1},\"1\":{\"2024\":1,\"2027\":1,\"2028\":1}}],[\"abspostencoder\",{\"0\":{\"1123\":1},\"1\":{\"1123\":1,\"1171\":1,\"1172\":1,\"1192\":1,\"1195\":1,\"1206\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2026\":1,\"2027\":2,\"2029\":1,\"2076\":1}}],[\"absepochstepscheduler\",{\"0\":{\"2015\":1},\"1\":{\"2015\":1,\"2017\":1}}],[\"absextractor\",{\"0\":{\"1441\":1},\"1\":{\"1441\":1,\"1554\":1,\"1659\":1}}],[\"absenhancement\",{\"0\":{\"1439\":1},\"1\":{\"1439\":2}}],[\"absenhloss\",{\"0\":{\"1437\":1},\"1\":{\"1437\":1,\"1530\":1,\"1563\":1,\"1570\":1,\"1600\":3,\"1603\":3,\"1622\":3,\"1667\":1}}],[\"absencoder\",{\"0\":{\"1119\":1,\"1435\":1},\"1\":{\"1113\":1,\"1119\":2,\"1140\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1169\":1,\"1171\":1,\"1172\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1191\":1,\"1200\":1,\"1206\":1,\"1215\":1,\"1222\":1,\"1269\":1,\"1272\":1,\"1282\":1,\"1371\":1,\"1435\":2,\"1511\":1,\"1551\":1,\"1553\":1,\"1554\":1,\"1617\":1,\"1644\":1,\"1892\":1,\"1893\":1,\"1970\":1,\"1975\":1,\"1984\":2,\"2027\":1,\"2046\":1,\"2049\":1,\"2050\":1,\"2054\":1,\"2055\":1,\"2064\":1,\"2070\":1,\"2076\":4}}],[\"absespnetmodel\",{\"0\":{\"2168\":1},\"1\":{\"56\":1,\"1057\":1,\"1113\":1,\"1171\":1,\"1371\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1892\":1,\"1893\":1,\"1953\":1,\"1955\":1,\"1963\":1,\"1964\":1,\"1970\":1,\"1975\":1,\"1984\":1,\"2046\":1,\"2076\":1,\"2082\":1,\"2096\":4,\"2098\":4,\"2099\":4,\"2100\":4,\"2101\":4,\"2102\":4,\"2103\":4,\"2104\":4,\"2105\":4,\"2107\":4,\"2108\":4,\"2109\":4,\"2110\":4,\"2111\":4,\"2112\":4,\"2113\":4,\"2114\":4,\"2115\":4,\"2116\":4,\"2117\":4,\"2118\":4,\"2168\":2,\"2170\":1,\"2201\":1,\"2240\":1,\"2278\":1,\"2294\":1}}],[\"absnormalize\",{\"0\":{\"1902\":1},\"1\":{\"1057\":1,\"1113\":1,\"1171\":1,\"1206\":1,\"1371\":1,\"1892\":1,\"1893\":1,\"1902\":2,\"1906\":1,\"1920\":1,\"1975\":1,\"1984\":2,\"2027\":1,\"2046\":1,\"2076\":1}}],[\"abstokenizer\",{\"0\":{\"2119\":1},\"1\":{\"2119\":2,\"2120\":1,\"2124\":1,\"2129\":1,\"2130\":1,\"2132\":1,\"2136\":1}}],[\"abstgtfeatsextract\",{\"0\":{\"1981\":1},\"1\":{\"1981\":1,\"1984\":1,\"1987\":1,\"1989\":1,\"1991\":1}}],[\"abstts2\",{\"0\":{\"2277\":1},\"1\":{\"2277\":2,\"2278\":1,\"2279\":1}}],[\"abstts\",{\"0\":{\"2235\":1},\"1\":{\"1828\":1,\"2235\":2,\"2240\":1,\"2243\":1,\"2244\":1,\"2255\":1,\"2263\":1,\"2264\":1}}],[\"abstract\",{\"0\":{\"177\":1},\"1\":{\"1046\":8,\"1109\":1,\"1110\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1253\":1,\"1360\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1431\":1,\"1433\":2,\"1435\":1,\"1436\":1,\"1437\":1,\"1439\":1,\"1440\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1446\":1,\"1514\":2,\"1559\":1,\"1560\":1,\"1570\":2,\"1623\":2,\"1638\":7,\"1718\":1,\"1760\":2,\"1828\":2,\"1894\":1,\"1902\":1,\"1909\":1,\"1951\":2,\"1976\":1,\"1978\":1,\"1980\":3,\"1981\":1,\"1982\":3,\"2014\":3,\"2015\":3,\"2016\":3,\"2017\":3,\"2024\":2,\"2025\":1,\"2034\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2039\":1,\"2077\":3,\"2099\":6,\"2119\":2,\"2167\":2,\"2168\":3,\"2170\":3,\"2233\":1,\"2234\":2,\"2235\":3,\"2275\":1,\"2277\":3,\"2283\":1,\"2285\":1,\"2286\":1,\"2287\":2,\"2288\":1,\"2394\":2,\"2530\":2}}],[\"abstask\",{\"0\":{\"2099\":1},\"1\":{\"56\":3,\"59\":1,\"60\":1,\"1972\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":3,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2168\":2,\"2170\":2}}],[\"abs\",{\"0\":{\"1046\":1,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1760\":1,\"1828\":1,\"1894\":1,\"1902\":1,\"1951\":1,\"1976\":1,\"1978\":1,\"1980\":1,\"1981\":1,\"2005\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2024\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2077\":1,\"2099\":1,\"2106\":1,\"2119\":1,\"2168\":1,\"2170\":1,\"2233\":1,\"2235\":1,\"2275\":1,\"2277\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1},\"1\":{\"56\":2,\"678\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"692\":1,\"693\":1,\"706\":2,\"725\":1,\"726\":1,\"729\":1,\"758\":1,\"771\":1,\"772\":1,\"809\":1,\"810\":1,\"813\":1,\"858\":1,\"1046\":1,\"1048\":1,\"1061\":1,\"1067\":1,\"1084\":3,\"1109\":1,\"1111\":1,\"1117\":2,\"1119\":2,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":2,\"1138\":1,\"1150\":1,\"1198\":1,\"1203\":1,\"1360\":1,\"1362\":1,\"1364\":2,\"1366\":2,\"1431\":2,\"1433\":1,\"1435\":2,\"1437\":1,\"1439\":2,\"1441\":1,\"1443\":1,\"1445\":1,\"1466\":1,\"1551\":1,\"1553\":1,\"1566\":1,\"1643\":1,\"1644\":1,\"1645\":1,\"1715\":1,\"1760\":2,\"1767\":1,\"1828\":2,\"1829\":1,\"1842\":1,\"1850\":1,\"1894\":1,\"1896\":1,\"1902\":2,\"1911\":1,\"1917\":1,\"1951\":3,\"1976\":2,\"1978\":2,\"1980\":1,\"1981\":1,\"1986\":1,\"2005\":2,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2024\":1,\"2034\":2,\"2036\":2,\"2038\":1,\"2077\":2,\"2081\":1,\"2083\":1,\"2096\":2,\"2098\":2,\"2099\":4,\"2100\":2,\"2101\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2106\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2119\":2,\"2168\":2,\"2170\":2,\"2185\":1,\"2203\":1,\"2209\":1,\"2233\":1,\"2235\":2,\"2275\":1,\"2277\":2,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":2,\"2394\":2,\"2467\":2,\"2498\":3,\"2530\":2,\"2586\":1,\"2616\":3,\"2634\":3,\"2646\":1}}],[\"a2\",{\"1\":{\"54\":1,\"1391\":1,\"1406\":2,\"1425\":2}}],[\"a1\",{\"1\":{\"54\":1,\"1391\":1,\"1425\":2,\"2500\":9,\"2617\":9,\"2635\":9}}],[\"afms\",{\"0\":{\"2032\":1},\"1\":{\"2032\":2}}],[\"affect\",{\"1\":{\"1245\":2,\"1269\":1,\"2467\":1}}],[\"affine=true\",{\"1\":{\"1476\":1}}],[\"affine\",{\"0\":{\"1130\":1},\"1\":{\"1130\":2,\"1233\":1,\"1465\":2,\"1838\":1,\"1864\":2,\"1865\":1}}],[\"af\",{\"1\":{\"52\":3}}],[\"afterwards\",{\"1\":{\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1}}],[\"after=false\",{\"1\":{\"711\":1,\"749\":1}}],[\"after\",{\"1\":{\"5\":1,\"21\":4,\"40\":1,\"69\":2,\"107\":1,\"113\":1,\"115\":4,\"122\":1,\"144\":1,\"150\":1,\"203\":1,\"238\":1,\"711\":1,\"712\":2,\"734\":1,\"749\":2,\"754\":4,\"755\":3,\"822\":3,\"826\":4,\"836\":1,\"1052\":2,\"1058\":2,\"1093\":4,\"1133\":1,\"1140\":1,\"1148\":3,\"1149\":3,\"1150\":3,\"1151\":1,\"1153\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1196\":1,\"1197\":1,\"1203\":3,\"1204\":1,\"1242\":1,\"1246\":1,\"1269\":2,\"1271\":1,\"1272\":3,\"1273\":1,\"1337\":1,\"1454\":1,\"1505\":3,\"1511\":1,\"1522\":1,\"1523\":1,\"1531\":1,\"1537\":1,\"1539\":1,\"1543\":1,\"1551\":1,\"1553\":1,\"1572\":1,\"1602\":2,\"1669\":3,\"1670\":1,\"1671\":3,\"1719\":1,\"1771\":1,\"1778\":2,\"1850\":2,\"1851\":6,\"1852\":2,\"1861\":1,\"1929\":1,\"1941\":1,\"1947\":1,\"1971\":1,\"2000\":3,\"2001\":2,\"2002\":1,\"2004\":2,\"2026\":1,\"2029\":3,\"2054\":1,\"2078\":1,\"2088\":3,\"2090\":6,\"2091\":3,\"2243\":6,\"2244\":6,\"2245\":3,\"2255\":5,\"2256\":3,\"2264\":6,\"2279\":6,\"2280\":3,\"2290\":1,\"2373\":1,\"2381\":1,\"2385\":1,\"2391\":1,\"2398\":1,\"2401\":1,\"2407\":1,\"2412\":1,\"2415\":1,\"2422\":2,\"2423\":1,\"2430\":2,\"2440\":1,\"2448\":1,\"2464\":1,\"2525\":2,\"2527\":1,\"2534\":1,\"2537\":1,\"2545\":2,\"2546\":1,\"2555\":2,\"2564\":1,\"2572\":1,\"2584\":1}}],[\"await\",{\"1\":{\"2360\":3,\"2458\":3,\"2523\":3,\"2582\":3}}],[\"aware\",{\"1\":{\"14\":1,\"60\":1,\"679\":2,\"684\":2,\"685\":2,\"688\":1,\"689\":1,\"1529\":1,\"1568\":1,\"1581\":1}}],[\"awin\",{\"1\":{\"892\":2,\"1220\":1,\"1289\":1}}],[\"aws\",{\"0\":{\"126\":1},\"1\":{\"44\":1}}],[\"america\",{\"1\":{\"2392\":1,\"2425\":1,\"2528\":1,\"2548\":1}}],[\"amfs\",{\"1\":{\"2032\":1}}],[\"amb\",{\"1\":{\"1927\":1}}],[\"amr\",{\"1\":{\"1927\":1}}],[\"amplitudes\",{\"1\":{\"1817\":1,\"1822\":1}}],[\"amplitude\",{\"1\":{\"1797\":1,\"1859\":1,\"1987\":2,\"1989\":1,\"1991\":1,\"2246\":2,\"2248\":1,\"2250\":1}}],[\"amp=0\",{\"1\":{\"1797\":1}}],[\"amp\",{\"0\":{\"1809\":1},\"1\":{\"65\":1,\"81\":1,\"429\":2,\"1181\":2,\"1797\":2,\"1809\":2,\"2186\":1,\"2202\":2,\"2204\":1,\"2440\":1,\"2558\":1}}],[\"amsgrad\",{\"1\":{\"62\":1}}],[\"among\",{\"1\":{\"56\":1,\"1113\":1,\"1171\":1,\"1172\":1,\"1371\":1,\"1551\":1,\"1553\":1,\"1554\":1,\"1773\":1,\"1837\":1,\"1892\":1,\"1893\":1,\"1951\":1,\"1970\":1,\"1975\":1,\"2027\":1,\"2076\":1,\"2082\":1,\"2168\":1,\"2170\":1,\"2240\":1,\"2278\":1}}],[\"amounts\",{\"1\":{\"2574\":1}}],[\"amount\",{\"1\":{\"49\":1,\"1406\":1,\"1776\":1,\"1928\":5}}],[\"am\",{\"0\":{\"1106\":1,\"1427\":1,\"1428\":1},\"1\":{\"24\":1,\"113\":2,\"1106\":1,\"1427\":4,\"1428\":3,\"2394\":1,\"2530\":1,\"2543\":2}}],[\"administration\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"administrator\",{\"1\":{\"141\":1}}],[\"advantages\",{\"1\":{\"2479\":1}}],[\"advancements\",{\"1\":{\"2448\":1,\"2464\":1}}],[\"advanced\",{\"1\":{\"2389\":1,\"2525\":1,\"2572\":1}}],[\"advance\",{\"1\":{\"1897\":1}}],[\"advances\",{\"1\":{\"940\":1}}],[\"advancing\",{\"1\":{\"130\":1}}],[\"adversely\",{\"1\":{\"2467\":1}}],[\"adversarial\",{\"1\":{\"1778\":3,\"1798\":1,\"1805\":4,\"1836\":2,\"1843\":3,\"1850\":3,\"1852\":3,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":4,\"1878\":1}}],[\"adv\",{\"1\":{\"1778\":6,\"1805\":6,\"1850\":6,\"1852\":6,\"1877\":6}}],[\"adjacent\",{\"1\":{\"1652\":1,\"1654\":1}}],[\"adjustment\",{\"1\":{\"2389\":1,\"2408\":1,\"2422\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2545\":1}}],[\"adjusted\",{\"1\":{\"1245\":1}}],[\"adjust\",{\"1\":{\"834\":1,\"1245\":1,\"1501\":1,\"1512\":1,\"1629\":1}}],[\"adjusts\",{\"1\":{\"79\":1}}],[\"adhere\",{\"1\":{\"1253\":1}}],[\"adim=512\",{\"1\":{\"1017\":1}}],[\"adim\",{\"1\":{\"754\":1,\"821\":1,\"826\":1,\"892\":2,\"1220\":1,\"1289\":1,\"1505\":2,\"1669\":2,\"1778\":1,\"1829\":3,\"1842\":2,\"1850\":1,\"1851\":1,\"1852\":1,\"2002\":2,\"2003\":1,\"2090\":1,\"2095\":2,\"2243\":1,\"2244\":1,\"2255\":1,\"2258\":2,\"2260\":2,\"2263\":2,\"2264\":2,\"2279\":1}}],[\"adithya\",{\"1\":{\"130\":1}}],[\"adopt\",{\"1\":{\"63\":1,\"73\":1,\"1462\":1,\"2415\":1}}],[\"adopts\",{\"1\":{\"49\":1,\"2641\":1}}],[\"adaptaion\",{\"1\":{\"1932\":1}}],[\"adaptation\",{\"1\":{\"1484\":2,\"1601\":2,\"1659\":2,\"1665\":4,\"1932\":1}}],[\"adapterforsoundscpreader\",{\"0\":{\"2174\":1},\"1\":{\"2174\":1}}],[\"adapterforsingingscorescpreader\",{\"0\":{\"2173\":1},\"1\":{\"2173\":1}}],[\"adapterforlabelscpreader\",{\"0\":{\"2172\":1},\"1\":{\"2172\":1}}],[\"adapter\",{\"0\":{\"1907\":2,\"1924\":1,\"1930\":2,\"1931\":2,\"1932\":2,\"1933\":1,\"1934\":1,\"1937\":1,\"1944\":1},\"1\":{\"1907\":2,\"1924\":1,\"1930\":9,\"1931\":2,\"1932\":3,\"1933\":2,\"1934\":1,\"1937\":1,\"1944\":1,\"2186\":2,\"2202\":4,\"2204\":2}}],[\"adapted\",{\"1\":{\"1452\":1,\"1484\":1,\"1601\":1,\"1605\":1,\"1635\":1,\"1917\":1,\"1930\":1,\"1932\":2,\"2480\":1,\"2502\":1}}],[\"adapt\",{\"0\":{\"1484\":1,\"1601\":1,\"1720\":1,\"1721\":1,\"1725\":2},\"1\":{\"1484\":1,\"1601\":1,\"1659\":9,\"1665\":10,\"1720\":1,\"1721\":2,\"1725\":2,\"2384\":1}}],[\"adaptor\",{\"0\":{\"1195\":1,\"2148\":1},\"1\":{\"661\":1,\"1192\":1,\"1195\":3,\"2148\":1}}],[\"adaptive\",{\"1\":{\"23\":2,\"113\":1,\"119\":2,\"429\":2,\"700\":2,\"1005\":2,\"1028\":2,\"1048\":2,\"1138\":2,\"1139\":2}}],[\"adadeltafactory\",{\"0\":{\"659\":1},\"1\":{\"659\":2}}],[\"adadelta\",{\"0\":{\"630\":1,\"663\":1},\"1\":{\"251\":1,\"255\":1,\"259\":1,\"630\":3,\"659\":1,\"663\":2}}],[\"adamfactory\",{\"0\":{\"660\":1},\"1\":{\"660\":2}}],[\"adam\",{\"0\":{\"631\":1,\"664\":1},\"1\":{\"62\":3,\"174\":1,\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"631\":3,\"660\":1,\"664\":2,\"2440\":1,\"2558\":1,\"2584\":1}}],[\"addfile\",{\"1\":{\"1961\":1}}],[\"add=none\",{\"1\":{\"1153\":1}}],[\"adddeltas\",{\"0\":{\"939\":1},\"1\":{\"939\":2,\"1012\":1}}],[\"adds\",{\"1\":{\"632\":1,\"1735\":1,\"2151\":1}}],[\"addjson\",{\"0\":{\"493\":1},\"1\":{\"493\":2}}],[\"addr=\",{\"1\":{\"38\":1,\"596\":1}}],[\"addr\",{\"0\":{\"2214\":1},\"1\":{\"36\":2,\"38\":1,\"39\":3,\"377\":2,\"429\":2,\"2180\":2,\"2214\":1}}],[\"address\",{\"1\":{\"17\":1,\"130\":1,\"1015\":1}}],[\"added\",{\"1\":{\"11\":1,\"105\":1,\"119\":1,\"124\":1,\"238\":1,\"600\":1,\"633\":1,\"1047\":1,\"1072\":1,\"1080\":1,\"1098\":1,\"1577\":1,\"1639\":1,\"1735\":1,\"1776\":1,\"1791\":1,\"1962\":1,\"2032\":1,\"2128\":1,\"2129\":1,\"2433\":1,\"2439\":1,\"2567\":1}}],[\"additive\",{\"1\":{\"677\":1,\"686\":2,\"2030\":2}}],[\"addition=true\",{\"1\":{\"1601\":1}}],[\"additionaly\",{\"1\":{\"2600\":1}}],[\"additional\",{\"0\":{\"24\":1},\"1\":{\"23\":1,\"119\":1,\"124\":2,\"700\":1,\"711\":3,\"742\":1,\"749\":2,\"1048\":1,\"1133\":2,\"1138\":1,\"1139\":1,\"1148\":2,\"1149\":2,\"1150\":2,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1174\":1,\"1177\":1,\"1203\":2,\"1209\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1272\":2,\"1279\":1,\"1327\":1,\"1441\":1,\"1445\":1,\"1451\":1,\"1454\":2,\"1463\":2,\"1505\":4,\"1514\":1,\"1515\":2,\"1516\":2,\"1523\":1,\"1528\":1,\"1529\":2,\"1534\":2,\"1539\":2,\"1553\":1,\"1554\":1,\"1557\":1,\"1558\":2,\"1577\":1,\"1585\":1,\"1611\":2,\"1612\":1,\"1615\":1,\"1623\":1,\"1626\":2,\"1637\":1,\"1645\":2,\"1654\":2,\"1658\":2,\"1659\":3,\"1660\":3,\"1661\":3,\"1662\":3,\"1669\":4,\"1671\":3,\"1719\":3,\"1763\":1,\"1765\":3,\"1778\":1,\"1798\":1,\"1800\":3,\"1803\":3,\"1844\":3,\"1848\":2,\"1850\":1,\"1851\":3,\"1852\":1,\"1866\":3,\"1874\":1,\"1878\":1,\"1890\":1,\"2001\":2,\"2004\":2,\"2029\":2,\"2185\":1,\"2203\":1,\"2394\":1,\"2530\":1,\"2585\":1}}],[\"additionally\",{\"1\":{\"22\":1,\"28\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1177\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1279\":1}}],[\"addition\",{\"1\":{\"1\":1,\"17\":1,\"22\":1,\"112\":1,\"115\":1,\"119\":1,\"136\":1,\"742\":1,\"2387\":1,\"2574\":1}}],[\"adding\",{\"1\":{\"5\":1,\"27\":1,\"124\":1,\"749\":1,\"770\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1203\":1,\"1272\":1,\"1505\":1,\"1669\":1,\"1929\":1,\"1971\":1,\"2029\":1,\"2054\":1,\"2309\":1,\"2373\":1,\"2394\":3,\"2400\":1,\"2430\":1,\"2530\":3,\"2536\":1,\"2555\":1,\"2618\":1}}],[\"add\",{\"0\":{\"124\":1,\"600\":1,\"632\":1,\"633\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":2,\"853\":1,\"905\":1,\"939\":1,\"963\":2,\"964\":1,\"1288\":1,\"1735\":1,\"1973\":1,\"2151\":2,\"2643\":1},\"1\":{\"1\":1,\"2\":2,\"4\":1,\"11\":1,\"21\":3,\"56\":2,\"115\":1,\"124\":4,\"144\":3,\"162\":1,\"168\":1,\"173\":1,\"180\":1,\"217\":1,\"224\":1,\"231\":1,\"375\":2,\"461\":4,\"491\":2,\"493\":1,\"600\":3,\"606\":3,\"616\":1,\"628\":1,\"629\":1,\"632\":2,\"633\":3,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"666\":1,\"672\":2,\"673\":1,\"676\":2,\"712\":1,\"734\":3,\"742\":2,\"754\":3,\"767\":2,\"770\":1,\"781\":2,\"797\":4,\"799\":1,\"810\":1,\"813\":1,\"817\":3,\"818\":1,\"820\":3,\"821\":3,\"826\":3,\"828\":3,\"839\":2,\"840\":1,\"841\":1,\"842\":1,\"843\":2,\"844\":2,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":3,\"853\":1,\"905\":2,\"939\":2,\"944\":1,\"948\":1,\"957\":1,\"963\":4,\"964\":2,\"1012\":1,\"1016\":3,\"1017\":1,\"1052\":1,\"1137\":1,\"1153\":1,\"1190\":1,\"1226\":1,\"1227\":1,\"1228\":1,\"1243\":1,\"1288\":1,\"1345\":1,\"1347\":1,\"1388\":1,\"1427\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":1,\"1715\":1,\"1719\":1,\"1735\":1,\"1765\":1,\"1778\":1,\"1800\":1,\"1803\":1,\"1807\":1,\"1844\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":2,\"1852\":1,\"1856\":1,\"1857\":1,\"1858\":1,\"1866\":1,\"1867\":1,\"1871\":1,\"1961\":1,\"1973\":1,\"2002\":2,\"2003\":1,\"2086\":3,\"2087\":3,\"2090\":3,\"2096\":1,\"2097\":2,\"2098\":1,\"2099\":2,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2151\":2,\"2176\":2,\"2185\":2,\"2193\":1,\"2199\":1,\"2201\":2,\"2203\":2,\"2243\":1,\"2244\":1,\"2255\":1,\"2264\":1,\"2279\":1,\"2314\":1,\"2320\":1,\"2328\":1,\"2335\":1,\"2340\":1,\"2391\":2,\"2394\":2,\"2400\":1,\"2527\":2,\"2530\":2,\"2536\":1,\"2559\":1,\"2584\":4,\"2640\":1,\"2643\":4,\"2644\":2}}],[\"aug\",{\"1\":{\"2178\":3,\"2179\":3,\"2184\":3,\"2191\":3,\"2195\":3,\"2200\":3}}],[\"augment\",{\"0\":{\"943\":1,\"950\":1,\"955\":1,\"956\":1,\"965\":1,\"968\":2,\"972\":1,\"973\":1,\"1001\":1,\"1009\":1,\"1010\":1,\"1011\":1,\"1018\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1025\":1,\"1026\":1,\"1031\":1,\"1035\":1,\"1036\":1,\"1037\":1,\"1039\":1,\"1040\":1},\"1\":{\"943\":2,\"950\":2,\"955\":2,\"956\":3,\"965\":2,\"968\":4,\"972\":2,\"973\":3,\"1001\":1,\"1009\":1,\"1010\":1,\"1011\":2,\"1018\":1,\"1019\":2,\"1021\":1,\"1022\":1,\"1025\":1,\"1026\":1,\"1031\":2,\"1035\":1,\"1036\":1,\"1037\":3,\"1039\":2,\"1040\":2}}],[\"augmentation\",{\"0\":{\"1905\":1,\"1921\":1,\"1922\":1,\"1923\":1,\"1925\":1,\"1926\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1945\":1,\"1946\":1,\"1947\":1,\"1950\":1},\"1\":{\"515\":1,\"1037\":1,\"1127\":1,\"1179\":1,\"1198\":1,\"1257\":1,\"1905\":3,\"1921\":1,\"1922\":1,\"1923\":1,\"1925\":2,\"1926\":2,\"1928\":2,\"1929\":2,\"1935\":2,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":2,\"1942\":1,\"1943\":2,\"1945\":2,\"1946\":2,\"1947\":2,\"1950\":1,\"2373\":1,\"2430\":2,\"2468\":1,\"2555\":2,\"2564\":1}}],[\"augmented\",{\"1\":{\"22\":1,\"2430\":1,\"2542\":1,\"2555\":1}}],[\"auth\",{\"1\":{\"363\":2,\"1527\":1}}],[\"authors\",{\"1\":{\"1619\":1,\"2516\":1}}],[\"author\",{\"1\":{\"130\":1,\"166\":1,\"176\":1,\"195\":1,\"199\":1,\"206\":1,\"226\":1,\"233\":1,\"2354\":1,\"2356\":1,\"2361\":1,\"2366\":1,\"2380\":1,\"2388\":1,\"2406\":1,\"2421\":1,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":1,\"2544\":1,\"2576\":1,\"2583\":1,\"2586\":1,\"2597\":1,\"2601\":1,\"2646\":1,\"2650\":1}}],[\"author=\",{\"1\":{\"130\":9}}],[\"autotokenizer\",{\"1\":{\"2474\":1,\"2649\":1}}],[\"automodel\",{\"1\":{\"2474\":1,\"2649\":1}}],[\"automatic\",{\"0\":{\"81\":1},\"1\":{\"251\":1,\"1037\":1,\"1181\":1,\"1187\":1,\"1202\":1,\"1257\":1,\"1286\":1,\"1287\":1,\"2054\":1,\"2440\":1,\"2467\":1,\"2558\":1,\"2583\":1}}],[\"automatically\",{\"1\":{\"13\":1,\"17\":1,\"79\":1,\"107\":1,\"111\":1,\"237\":1,\"692\":1,\"693\":1,\"697\":1,\"797\":1,\"857\":1,\"2095\":1,\"2309\":1,\"2363\":1,\"2506\":1,\"2566\":1,\"2653\":1}}],[\"autoencoder\",{\"1\":{\"1375\":1,\"1379\":1,\"1664\":1,\"1665\":1,\"1798\":1,\"1805\":1,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1}}],[\"autoregressive\",{\"1\":{\"785\":2,\"797\":3,\"798\":1,\"1205\":1,\"1209\":1,\"1219\":1}}],[\"auto\",{\"1\":{\"25\":1,\"251\":1,\"255\":1,\"259\":1,\"265\":1,\"269\":1,\"301\":1,\"735\":1,\"754\":3,\"803\":1,\"1028\":1,\"2080\":1,\"2243\":2,\"2357\":5,\"2363\":3,\"2371\":1,\"2384\":1,\"2506\":1,\"2510\":1,\"2512\":1,\"2578\":5,\"2612\":1,\"2630\":1,\"2653\":3,\"2657\":1}}],[\"audios\",{\"1\":{\"1390\":1,\"1392\":2,\"2492\":1,\"2628\":1}}],[\"audio\",{\"0\":{\"46\":1,\"48\":1,\"49\":1,\"52\":1,\"1071\":1,\"2470\":1,\"2588\":1,\"2593\":1,\"2647\":1},\"1\":{\"46\":5,\"47\":7,\"48\":7,\"49\":9,\"51\":1,\"54\":6,\"109\":1,\"110\":1,\"122\":1,\"194\":2,\"197\":1,\"201\":3,\"202\":2,\"218\":2,\"225\":2,\"232\":2,\"237\":1,\"245\":1,\"286\":1,\"296\":1,\"297\":1,\"301\":1,\"836\":1,\"869\":2,\"877\":2,\"1071\":3,\"1115\":10,\"1116\":4,\"1132\":1,\"1179\":4,\"1198\":2,\"1216\":2,\"1255\":3,\"1269\":2,\"1285\":1,\"1407\":6,\"1432\":3,\"1436\":5,\"1510\":3,\"1511\":5,\"1526\":2,\"1551\":2,\"1554\":2,\"1558\":1,\"1643\":3,\"1644\":5,\"1660\":3,\"1661\":3,\"1662\":3,\"1719\":3,\"1776\":2,\"1785\":1,\"1791\":1,\"1804\":1,\"1810\":1,\"1921\":1,\"1922\":1,\"1925\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1935\":1,\"1936\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1947\":1,\"2188\":1,\"2267\":2,\"2270\":1,\"2272\":1,\"2359\":2,\"2360\":9,\"2365\":2,\"2367\":2,\"2369\":2,\"2372\":4,\"2373\":2,\"2385\":7,\"2386\":3,\"2430\":3,\"2433\":1,\"2456\":2,\"2458\":9,\"2460\":2,\"2467\":1,\"2470\":4,\"2471\":1,\"2473\":1,\"2474\":1,\"2476\":5,\"2478\":5,\"2481\":1,\"2485\":2,\"2487\":2,\"2491\":2,\"2497\":4,\"2501\":3,\"2508\":2,\"2510\":2,\"2515\":2,\"2521\":3,\"2522\":3,\"2523\":10,\"2555\":4,\"2568\":6,\"2574\":1,\"2580\":2,\"2581\":2,\"2582\":9,\"2586\":1,\"2593\":2,\"2600\":1,\"2604\":2,\"2606\":2,\"2607\":3,\"2610\":2,\"2614\":4,\"2615\":4,\"2621\":2,\"2623\":2,\"2624\":3,\"2627\":2,\"2632\":4,\"2633\":4,\"2647\":5,\"2655\":2,\"2660\":2}}],[\"aux=28\",{\"1\":{\"835\":1}}],[\"auxlm\",{\"1\":{\"118\":2}}],[\"auxctc\",{\"1\":{\"118\":2}}],[\"auxirialy\",{\"1\":{\"1872\":1,\"1873\":2}}],[\"auxially\",{\"1\":{\"59\":2}}],[\"auxiliary\",{\"0\":{\"844\":1},\"1\":{\"22\":7,\"118\":7,\"726\":3,\"766\":1,\"825\":10,\"835\":2,\"844\":2,\"933\":3,\"1057\":11,\"1659\":2,\"1862\":4,\"1871\":2,\"1872\":1,\"1873\":1,\"1976\":1,\"2304\":1}}],[\"aux\",{\"0\":{\"933\":1,\"1976\":2,\"1993\":1},\"1\":{\"22\":6,\"726\":4,\"766\":2,\"825\":27,\"835\":4,\"933\":3,\"1171\":1,\"1441\":2,\"1551\":2,\"1554\":2,\"1659\":4,\"1778\":1,\"1804\":2,\"1805\":1,\"1834\":4,\"1850\":2,\"1851\":2,\"1852\":1,\"1862\":4,\"1871\":2,\"1872\":5,\"1873\":3,\"1877\":2,\"1878\":4,\"1880\":3,\"1976\":2,\"1984\":1,\"1993\":1,\"2178\":1,\"2179\":1}}],[\"avanzar\",{\"1\":{\"2457\":1}}],[\"available\",{\"1\":{\"21\":2,\"23\":1,\"24\":1,\"49\":1,\"80\":1,\"112\":1,\"113\":3,\"116\":1,\"119\":1,\"122\":2,\"130\":1,\"168\":1,\"179\":1,\"197\":1,\"198\":1,\"235\":1,\"236\":1,\"286\":2,\"295\":2,\"296\":1,\"816\":1,\"836\":1,\"1067\":1,\"1084\":1,\"2131\":1,\"2583\":1}}],[\"avocodo=false\",{\"1\":{\"1800\":1,\"1804\":1}}],[\"avocodogenerator\",{\"0\":{\"1765\":1},\"1\":{\"1765\":2}}],[\"avocododiscriminatorplus\",{\"0\":{\"1763\":1},\"1\":{\"1763\":1}}],[\"avocododiscriminator\",{\"0\":{\"1761\":1},\"1\":{\"1761\":1}}],[\"avocodo\",{\"0\":{\"1761\":2,\"1763\":2,\"1765\":2,\"1767\":2,\"1768\":2,\"1782\":2,\"1784\":2,\"1793\":2,\"1795\":2,\"1815\":2},\"1\":{\"1761\":3,\"1763\":3,\"1765\":3,\"1767\":4,\"1768\":4,\"1782\":4,\"1784\":4,\"1793\":4,\"1795\":4,\"1804\":2,\"1805\":1,\"1815\":2}}],[\"avoid\",{\"1\":{\"11\":1,\"38\":1,\"49\":2,\"62\":1,\"737\":1,\"738\":1,\"920\":1,\"1138\":1,\"1171\":2,\"1206\":2,\"1427\":1,\"1522\":1,\"1552\":2,\"1905\":1,\"1953\":2,\"1955\":2,\"2184\":1,\"2200\":1,\"2393\":1,\"2427\":1,\"2467\":1,\"2529\":1,\"2550\":1,\"2598\":1}}],[\"avhubertmodel\",{\"0\":{\"1116\":1},\"1\":{\"1116\":1}}],[\"avhubertconfig\",{\"0\":{\"1115\":1},\"1\":{\"1115\":1,\"1116\":2}}],[\"avhubert\",{\"0\":{\"1115\":1,\"1116\":1,\"1134\":1,\"1179\":1,\"1187\":1,\"1229\":1,\"1231\":1,\"1249\":1,\"1263\":1,\"1305\":1,\"1308\":2,\"1312\":1,\"1313\":1,\"1326\":1,\"1330\":1,\"1354\":1},\"1\":{\"1115\":2,\"1116\":3,\"1134\":1,\"1179\":9,\"1187\":1,\"1229\":1,\"1231\":1,\"1249\":1,\"1263\":1,\"1305\":1,\"1308\":2,\"1312\":1,\"1313\":1,\"1326\":1,\"1330\":1,\"1354\":1}}],[\"avgpool1d\",{\"1\":{\"1778\":1,\"1801\":1,\"1805\":1,\"1846\":1,\"1847\":1,\"1850\":1,\"1852\":1,\"1858\":1,\"1877\":1}}],[\"avgpool\",{\"1\":{\"1198\":2}}],[\"avgpool=false\",{\"1\":{\"1198\":1}}],[\"avg\",{\"1\":{\"210\":1,\"211\":1,\"212\":1,\"222\":1,\"223\":1,\"229\":1,\"230\":1,\"285\":1,\"1231\":1,\"2244\":2,\"2255\":2,\"2279\":2}}],[\"averaging\",{\"1\":{\"1430\":1,\"1670\":1,\"2186\":1,\"2202\":2,\"2204\":1}}],[\"averaged\",{\"1\":{\"1851\":4,\"1879\":2,\"1881\":2,\"1962\":3,\"2236\":1,\"2241\":1,\"2244\":3,\"2245\":2,\"2255\":2,\"2256\":2,\"2266\":1,\"2279\":2,\"2280\":2}}],[\"average\",{\"0\":{\"499\":1,\"1881\":1,\"1962\":2,\"2161\":1,\"2175\":1},\"1\":{\"104\":2,\"107\":2,\"429\":4,\"499\":3,\"1059\":2,\"1065\":1,\"1069\":1,\"1149\":3,\"1150\":3,\"1173\":2,\"1198\":1,\"1430\":1,\"1471\":1,\"1670\":1,\"1688\":1,\"1693\":1,\"1778\":4,\"1805\":4,\"1836\":3,\"1839\":6,\"1843\":3,\"1850\":4,\"1852\":4,\"1877\":4,\"1881\":2,\"1962\":2,\"2052\":1,\"2161\":1,\"2175\":2,\"2198\":1,\"2440\":1,\"2462\":1,\"2558\":1}}],[\"ave\",{\"1\":{\"110\":4,\"1141\":4,\"2357\":9,\"2371\":4,\"2375\":2,\"2431\":1,\"2440\":8,\"2441\":6,\"2444\":2,\"2445\":2,\"2446\":2,\"2454\":1,\"2455\":1,\"2460\":2,\"2461\":1,\"2472\":2,\"2474\":1,\"2476\":2,\"2478\":1,\"2494\":2,\"2500\":3,\"2507\":1,\"2513\":1,\"2559\":2,\"2564\":8,\"2572\":4,\"2578\":9,\"2584\":1,\"2589\":1,\"2590\":1,\"2599\":1,\"2600\":4,\"2612\":4,\"2617\":1,\"2630\":5,\"2635\":1,\"2648\":2,\"2649\":1}}],[\"aka\",{\"1\":{\"20\":1,\"112\":1}}],[\"acabo\",{\"1\":{\"2457\":1}}],[\"acm\",{\"1\":{\"1715\":1}}],[\"ac\",{\"1\":{\"940\":1,\"2040\":1,\"2618\":1}}],[\"acl\",{\"1\":{\"130\":1}}],[\"aclweb\",{\"1\":{\"130\":1}}],[\"aconv\",{\"1\":{\"679\":4,\"681\":4,\"682\":4,\"683\":4,\"684\":4,\"685\":4,\"688\":4,\"689\":6,\"758\":4,\"821\":2,\"892\":4,\"1017\":2,\"1220\":2,\"1289\":2,\"2002\":4,\"2095\":4,\"2263\":4}}],[\"acoustic\",{\"1\":{\"681\":1,\"682\":1,\"821\":5,\"1142\":3,\"1155\":1,\"1186\":2,\"1210\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":2,\"1337\":1,\"1349\":1,\"1350\":1,\"1704\":1,\"1778\":2,\"1804\":2,\"1805\":2,\"1829\":1,\"1850\":2,\"1852\":2,\"1877\":2,\"1878\":2,\"2083\":3,\"2142\":1,\"2467\":1,\"2473\":1}}],[\"acoustics\",{\"1\":{\"130\":2}}],[\"acouctic\",{\"1\":{\"676\":1,\"781\":1,\"812\":1}}],[\"acodec\",{\"1\":{\"52\":2}}],[\"across\",{\"1\":{\"115\":1,\"116\":1,\"124\":1,\"148\":1,\"1166\":1,\"1242\":1,\"1244\":1,\"1245\":1,\"1252\":1,\"1254\":1,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1}}],[\"action=nesteddictaction\",{\"1\":{\"2314\":1}}],[\"action\",{\"0\":{\"2313\":1},\"1\":{\"2313\":3,\"2467\":1,\"2472\":2,\"2474\":2,\"2476\":1,\"2648\":2,\"2649\":2}}],[\"actively\",{\"1\":{\"2585\":1}}],[\"active\",{\"1\":{\"2450\":1}}],[\"activity\",{\"1\":{\"2210\":1}}],[\"activatio\",{\"1\":{\"1142\":1}}],[\"activation=<class\",{\"1\":{\"1506\":1,\"1543\":1,\"1564\":1,\"1655\":1,\"1656\":1}}],[\"activation=\",{\"1\":{\"1177\":1,\"1241\":1,\"1430\":1,\"1470\":1,\"1518\":2,\"1520\":2,\"1537\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1670\":1,\"1719\":1,\"1800\":1}}],[\"activation=none\",{\"1\":{\"1129\":1,\"1162\":1,\"1199\":1,\"1279\":1,\"1322\":1}}],[\"activation=relu\",{\"1\":{\"723\":1}}],[\"activations\",{\"1\":{\"703\":1,\"837\":1,\"1145\":3,\"1181\":1,\"1484\":1,\"1601\":1}}],[\"activation\",{\"0\":{\"883\":1,\"1061\":1,\"1067\":1,\"1082\":1,\"1084\":1,\"1096\":2,\"1129\":1,\"1700\":1},\"1\":{\"21\":2,\"115\":14,\"116\":4,\"117\":3,\"712\":1,\"725\":6,\"726\":6,\"766\":3,\"819\":1,\"821\":2,\"825\":3,\"858\":6,\"859\":6,\"862\":3,\"883\":2,\"1051\":3,\"1052\":1,\"1061\":5,\"1064\":5,\"1065\":3,\"1066\":6,\"1067\":5,\"1069\":3,\"1070\":4,\"1082\":5,\"1084\":4,\"1093\":6,\"1096\":13,\"1115\":6,\"1129\":1,\"1140\":1,\"1142\":1,\"1144\":1,\"1148\":3,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":1,\"1169\":2,\"1179\":1,\"1180\":3,\"1181\":1,\"1186\":4,\"1198\":3,\"1199\":1,\"1203\":3,\"1210\":2,\"1228\":2,\"1242\":3,\"1243\":1,\"1298\":4,\"1299\":4,\"1301\":4,\"1302\":4,\"1303\":4,\"1304\":4,\"1334\":3,\"1337\":1,\"1345\":2,\"1347\":2,\"1349\":1,\"1350\":1,\"1430\":2,\"1470\":2,\"1505\":3,\"1506\":2,\"1517\":2,\"1537\":2,\"1539\":3,\"1543\":4,\"1564\":1,\"1581\":2,\"1655\":4,\"1656\":4,\"1660\":3,\"1661\":3,\"1662\":3,\"1670\":2,\"1671\":3,\"1699\":1,\"1700\":2,\"1719\":3,\"1765\":6,\"1771\":3,\"1778\":7,\"1787\":3,\"1788\":3,\"1798\":3,\"1800\":5,\"1801\":4,\"1803\":6,\"1804\":3,\"1805\":5,\"1834\":6,\"1844\":6,\"1845\":2,\"1846\":2,\"1847\":4,\"1848\":6,\"1849\":6,\"1850\":7,\"1851\":9,\"1852\":7,\"1856\":6,\"1857\":9,\"1858\":6,\"1861\":4,\"1866\":6,\"1867\":6,\"1870\":2,\"1871\":6,\"1874\":3,\"1876\":6,\"1877\":5,\"1878\":3,\"1911\":2,\"1917\":1,\"2002\":3,\"2003\":1,\"2026\":1,\"2054\":3,\"2078\":3,\"2090\":1,\"2095\":3,\"2243\":3,\"2244\":3,\"2252\":2,\"2255\":3,\"2263\":3,\"2279\":3,\"2429\":1,\"2440\":1,\"2552\":1,\"2564\":1}}],[\"activate=false\",{\"1\":{\"1199\":1}}],[\"activated\",{\"1\":{\"997\":1,\"998\":1}}],[\"activate\",{\"1\":{\"127\":1,\"128\":1,\"135\":4,\"136\":4,\"137\":1,\"167\":1,\"178\":1,\"196\":2,\"200\":2,\"234\":2,\"2372\":1,\"2387\":1,\"2409\":3,\"2429\":2,\"2552\":2,\"2554\":1}}],[\"act=elu\",{\"1\":{\"1501\":1,\"1629\":1}}],[\"act=relu\",{\"1\":{\"1468\":1,\"1485\":1,\"1489\":1,\"1491\":1,\"1624\":1,\"1627\":1}}],[\"act=none\",{\"1\":{\"1241\":1}}],[\"acts\",{\"1\":{\"1186\":8,\"1202\":1,\"1210\":5,\"1211\":1,\"1224\":1,\"1227\":1,\"1228\":1,\"1286\":1,\"1287\":1,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":2,\"1336\":2,\"1337\":2,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":2,\"1349\":2,\"1350\":2}}],[\"act\",{\"0\":{\"1699\":1},\"1\":{\"115\":4,\"117\":1,\"883\":1,\"1001\":1,\"1064\":1,\"1093\":4,\"1211\":2,\"1224\":2,\"1242\":1,\"1286\":1,\"1287\":2,\"1336\":2,\"1348\":2,\"1631\":1,\"1633\":1,\"1635\":1,\"1699\":1,\"2290\":1}}],[\"actual\",{\"1\":{\"72\":2,\"121\":1,\"627\":1,\"727\":1,\"728\":1,\"823\":2,\"1085\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1778\":1,\"1805\":1,\"1850\":1,\"1852\":1,\"1877\":1}}],[\"actually\",{\"1\":{\"46\":1,\"2414\":1}}],[\"achieved\",{\"1\":{\"83\":1,\"124\":1,\"1132\":1,\"1662\":1}}],[\"achieve\",{\"1\":{\"19\":1,\"104\":1,\"164\":1,\"2414\":1,\"2542\":1,\"2564\":1}}],[\"acc=0\",{\"1\":{\"87\":2}}],[\"accurate\",{\"1\":{\"2095\":1}}],[\"accuracy\",{\"0\":{\"925\":1},\"1\":{\"17\":1,\"24\":1,\"88\":1,\"150\":1,\"710\":1,\"742\":1,\"925\":3,\"1639\":1,\"2375\":1,\"2418\":1,\"2558\":1}}],[\"accum\",{\"1\":{\"80\":6,\"110\":3,\"251\":2,\"253\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"604\":2,\"627\":1,\"727\":2,\"728\":2,\"742\":2,\"976\":1,\"1043\":1,\"2186\":1,\"2202\":2,\"2204\":1,\"2440\":1,\"2558\":1,\"2584\":1}}],[\"accumulation\",{\"1\":{\"727\":1,\"728\":1,\"745\":1,\"746\":1,\"2440\":1,\"2558\":1}}],[\"accumulating\",{\"0\":{\"80\":1},\"1\":{\"80\":2}}],[\"accumulates\",{\"1\":{\"607\":1}}],[\"accumulated\",{\"1\":{\"80\":1,\"691\":1,\"693\":1,\"697\":1,\"797\":1,\"2090\":1}}],[\"accumulate\",{\"1\":{\"80\":1,\"745\":1,\"746\":1}}],[\"accomplished\",{\"1\":{\"2564\":1}}],[\"accordingly\",{\"1\":{\"92\":1,\"102\":1,\"2543\":1}}],[\"according\",{\"1\":{\"24\":1,\"58\":1,\"74\":1,\"96\":1,\"99\":1,\"141\":1,\"295\":1,\"639\":1,\"705\":1,\"903\":1,\"917\":1,\"1048\":1,\"1218\":1,\"1227\":1,\"1375\":1,\"1560\":1,\"1842\":1,\"1881\":2,\"1895\":1,\"1900\":1,\"2154\":1}}],[\"account\",{\"1\":{\"1\":1,\"684\":1,\"685\":1,\"1138\":1}}],[\"acc\",{\"1\":{\"17\":2,\"88\":1,\"110\":2,\"174\":22,\"251\":1,\"255\":1,\"259\":1,\"286\":1,\"296\":1,\"499\":1,\"811\":1,\"1962\":1,\"2193\":1,\"2357\":11,\"2375\":3,\"2440\":10,\"2441\":6,\"2444\":2,\"2445\":2,\"2446\":2,\"2454\":1,\"2455\":1,\"2460\":2,\"2461\":1,\"2472\":2,\"2474\":1,\"2476\":2,\"2478\":1,\"2500\":3,\"2558\":2,\"2559\":2,\"2564\":8,\"2572\":5,\"2578\":11,\"2584\":2,\"2589\":1,\"2590\":1,\"2599\":1,\"2600\":4,\"2617\":1,\"2635\":1,\"2648\":2,\"2649\":1}}],[\"accent\",{\"0\":{\"2139\":1,\"2140\":1},\"1\":{\"461\":2,\"2139\":1,\"2140\":1,\"2363\":4,\"2653\":4}}],[\"accelerate\",{\"1\":{\"93\":1}}],[\"accessing\",{\"1\":{\"1337\":1,\"1349\":1,\"1350\":1}}],[\"accessed\",{\"1\":{\"1015\":1}}],[\"access\",{\"1\":{\"1\":1,\"4\":1,\"121\":1,\"148\":1,\"909\":4,\"1003\":2,\"1004\":2,\"1005\":4,\"1028\":2,\"1398\":1,\"2373\":1,\"2430\":1,\"2431\":1,\"2468\":1,\"2555\":1}}],[\"accepts\",{\"1\":{\"1243\":1,\"1693\":1,\"1755\":1}}],[\"acceptable\",{\"1\":{\"760\":1,\"778\":1,\"831\":1,\"1476\":1,\"1598\":1,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"2084\":1,\"2089\":1}}],[\"accept\",{\"1\":{\"1\":1,\"816\":1,\"836\":1,\"1187\":2,\"1202\":2,\"1278\":1,\"1286\":1,\"1287\":1,\"2170\":1}}],[\"alarm\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"alaw\",{\"1\":{\"1927\":1}}],[\"alguien\",{\"1\":{\"2457\":2}}],[\"algorithm\",{\"1\":{\"23\":3,\"113\":1,\"119\":2,\"242\":2,\"519\":1,\"698\":1,\"699\":1,\"700\":1,\"704\":1,\"705\":1,\"835\":2,\"1048\":1,\"1138\":1,\"1139\":1,\"1245\":1,\"1514\":1,\"1623\":1,\"1713\":1,\"2142\":1,\"2236\":2,\"2415\":1,\"2510\":1}}],[\"algorithms\",{\"1\":{\"23\":2,\"119\":4,\"122\":1,\"1063\":1,\"1193\":1,\"1712\":1,\"1715\":1,\"2574\":1}}],[\"ald\",{\"1\":{\"1646\":2}}],[\"alway\",{\"1\":{\"804\":1}}],[\"always=true\",{\"1\":{\"762\":1,\"763\":1}}],[\"always\",{\"1\":{\"57\":1,\"60\":1,\"62\":1,\"137\":1,\"150\":1,\"169\":1,\"181\":1,\"399\":2,\"464\":2,\"470\":2,\"762\":2,\"763\":4,\"1241\":1,\"1390\":1,\"1405\":1,\"1426\":1,\"1551\":3,\"1553\":4,\"1581\":1,\"1797\":1,\"2099\":1,\"2102\":1}}],[\"although\",{\"1\":{\"753\":1,\"757\":1,\"761\":1,\"779\":1,\"832\":1,\"1110\":1,\"1112\":1,\"1118\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1128\":1,\"1131\":1,\"1135\":1,\"1137\":1,\"1152\":1,\"1157\":1,\"1159\":1,\"1175\":1,\"1185\":1,\"1189\":1,\"1208\":1,\"1213\":1,\"1216\":1,\"1221\":1,\"1223\":1,\"1230\":1,\"1232\":1,\"1234\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1246\":1,\"1250\":1,\"1258\":1,\"1260\":1,\"1262\":1,\"1264\":1,\"1266\":1,\"1268\":1,\"1275\":1,\"1277\":1,\"1281\":1,\"1283\":1,\"1285\":1,\"1361\":1,\"1363\":1,\"1365\":1,\"1367\":1,\"1432\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1453\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1467\":1,\"1469\":1,\"1475\":1,\"1477\":1,\"1479\":1,\"1481\":1,\"1483\":1,\"1486\":1,\"1488\":1,\"1490\":1,\"1492\":1,\"1494\":1,\"1496\":1,\"1498\":1,\"1500\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1509\":1,\"1513\":1,\"1519\":1,\"1521\":1,\"1533\":1,\"1536\":1,\"1538\":1,\"1541\":1,\"1544\":1,\"1548\":1,\"1550\":1,\"1556\":1,\"1562\":1,\"1565\":1,\"1574\":1,\"1582\":1,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1593\":1,\"1597\":1,\"1599\":1,\"1606\":1,\"1608\":1,\"1610\":1,\"1614\":1,\"1621\":1,\"1625\":1,\"1628\":1,\"1630\":1,\"1632\":1,\"1634\":1,\"1636\":1,\"1642\":1,\"1647\":1,\"1649\":1,\"1651\":1,\"1653\":1,\"1657\":1,\"1673\":1,\"1675\":1,\"1677\":1,\"1762\":1,\"1764\":1,\"1770\":1,\"1775\":1,\"1780\":1,\"1783\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1796\":1,\"1802\":1,\"1807\":1,\"1891\":1,\"1903\":1,\"1908\":1,\"1913\":1,\"1952\":1,\"1954\":1,\"1956\":1,\"1959\":1,\"1977\":1,\"1979\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1990\":1,\"1992\":1,\"1995\":1,\"1998\":1,\"2025\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2037\":1,\"2039\":1,\"2041\":1,\"2043\":1,\"2045\":1,\"2048\":1,\"2051\":1,\"2053\":1,\"2056\":1,\"2058\":1,\"2060\":1,\"2062\":1,\"2065\":1,\"2067\":1,\"2069\":1,\"2071\":1,\"2073\":1,\"2150\":1,\"2169\":1,\"2234\":1,\"2238\":1,\"2242\":1,\"2247\":1,\"2249\":1,\"2251\":1,\"2267\":1,\"2276\":1,\"2282\":1,\"2284\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2293\":1,\"2298\":1,\"2300\":1,\"2429\":1,\"2552\":1}}],[\"alternative\",{\"1\":{\"745\":1,\"746\":1}}],[\"altenative\",{\"1\":{\"91\":1}}],[\"aliases\",{\"1\":{\"2315\":1}}],[\"alias=\",{\"1\":{\"1012\":1}}],[\"alias\",{\"0\":{\"2315\":1,\"2342\":2},\"1\":{\"665\":1,\"668\":1,\"669\":1,\"670\":1,\"672\":1,\"674\":1,\"694\":6,\"765\":4,\"798\":5,\"872\":1,\"873\":1,\"874\":1,\"1012\":1,\"1187\":1,\"1202\":1,\"1286\":1,\"1287\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2315\":1,\"2342\":3}}],[\"align\",{\"0\":{\"245\":1,\"273\":1,\"301\":1,\"1882\":1},\"1\":{\"245\":3,\"301\":2,\"700\":1,\"1048\":1,\"1138\":1,\"1139\":1,\"1184\":1,\"1774\":1,\"1850\":2,\"2600\":2}}],[\"alignement\",{\"1\":{\"23\":1,\"119\":1}}],[\"alignmentmodule\",{\"0\":{\"1829\":1},\"1\":{\"1829\":2}}],[\"alignments\",{\"0\":{\"1829\":1,\"1881\":1,\"1889\":1},\"1\":{\"1829\":1,\"1881\":1,\"1889\":1}}],[\"alignment\",{\"1\":{\"23\":1,\"119\":1,\"700\":1,\"1048\":1,\"1138\":1,\"1139\":1,\"1429\":1,\"1829\":2,\"1850\":1,\"1851\":1,\"2095\":1}}],[\"alone\",{\"1\":{\"107\":1}}],[\"along\",{\"0\":{\"1914\":1,\"1915\":1,\"1940\":2},\"1\":{\"79\":1,\"940\":1,\"1406\":1,\"1914\":1,\"1915\":2,\"1935\":1,\"1940\":3,\"1943\":1,\"2125\":1,\"2501\":1}}],[\"alongside\",{\"1\":{\"22\":1}}],[\"almost\",{\"1\":{\"58\":1,\"80\":1,\"85\":1,\"135\":1,\"143\":1,\"169\":1,\"181\":1,\"2021\":1,\"2387\":1,\"2394\":1,\"2530\":1}}],[\"alphas\",{\"0\":{\"1298\":1,\"1302\":1},\"1\":{\"1142\":5,\"1298\":4,\"1301\":2,\"1302\":4,\"1304\":2}}],[\"alphabet\",{\"1\":{\"1142\":2,\"1144\":1,\"1154\":2,\"1186\":2,\"1210\":2,\"1298\":2,\"1299\":2,\"1301\":2,\"1302\":2,\"1303\":2,\"1304\":2,\"1334\":2}}],[\"alpha=1\",{\"1\":{\"762\":1,\"763\":1,\"774\":1,\"1233\":1,\"1501\":1,\"1629\":1,\"2364\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2654\":1,\"2658\":1}}],[\"alpha\",{\"1\":{\"23\":2,\"115\":2,\"217\":1,\"249\":2,\"263\":2,\"464\":2,\"470\":2,\"562\":2,\"700\":2,\"762\":1,\"763\":2,\"774\":2,\"1082\":7,\"1096\":3,\"1108\":1,\"1138\":2,\"1139\":2,\"1142\":5,\"1298\":1,\"1301\":1,\"1302\":1,\"1304\":1,\"1528\":2,\"1778\":5,\"1804\":3,\"1805\":3,\"1850\":2,\"1851\":6,\"1852\":2,\"1877\":3,\"1878\":3,\"1930\":1,\"1932\":2,\"1934\":1,\"1999\":1,\"2003\":2,\"2032\":1,\"2090\":8,\"2243\":9,\"2244\":9,\"2255\":9,\"2264\":6,\"2279\":9}}],[\"alsd\",{\"1\":{\"23\":2,\"119\":2,\"122\":1,\"249\":1,\"700\":1,\"1048\":1,\"1138\":2,\"1139\":2}}],[\"also\",{\"1\":{\"5\":2,\"19\":1,\"22\":1,\"23\":1,\"25\":2,\"28\":1,\"30\":1,\"37\":1,\"47\":1,\"48\":1,\"57\":2,\"60\":1,\"64\":1,\"69\":1,\"74\":1,\"76\":1,\"79\":1,\"84\":3,\"85\":1,\"90\":1,\"91\":1,\"93\":1,\"107\":1,\"113\":2,\"117\":1,\"118\":1,\"119\":1,\"126\":2,\"133\":1,\"134\":1,\"135\":2,\"136\":1,\"144\":2,\"166\":1,\"173\":1,\"177\":2,\"189\":1,\"195\":1,\"199\":1,\"233\":1,\"238\":1,\"240\":1,\"280\":1,\"295\":3,\"684\":1,\"705\":1,\"710\":1,\"727\":2,\"728\":2,\"734\":1,\"745\":1,\"746\":1,\"754\":1,\"817\":1,\"820\":1,\"821\":1,\"826\":1,\"1187\":1,\"1198\":2,\"1202\":1,\"1241\":2,\"1245\":2,\"1253\":1,\"1286\":1,\"1287\":1,\"1603\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":2,\"1946\":1,\"1958\":1,\"2354\":1,\"2355\":1,\"2372\":3,\"2373\":3,\"2375\":3,\"2377\":1,\"2378\":1,\"2384\":3,\"2385\":2,\"2387\":2,\"2388\":1,\"2393\":1,\"2401\":1,\"2414\":1,\"2415\":1,\"2416\":1,\"2419\":1,\"2421\":1,\"2422\":1,\"2429\":3,\"2430\":4,\"2431\":1,\"2436\":1,\"2437\":1,\"2450\":1,\"2452\":1,\"2468\":2,\"2481\":1,\"2501\":1,\"2508\":1,\"2510\":1,\"2524\":1,\"2525\":1,\"2529\":1,\"2537\":1,\"2542\":1,\"2543\":1,\"2544\":1,\"2545\":1,\"2552\":1,\"2554\":2,\"2555\":4,\"2558\":3,\"2559\":1,\"2562\":1,\"2563\":1,\"2574\":1,\"2585\":1,\"2593\":1,\"2597\":1,\"2600\":2,\"2638\":2}}],[\"al\",{\"1\":{\"23\":4,\"24\":1,\"113\":2,\"119\":4,\"120\":1,\"692\":1,\"693\":1,\"704\":1,\"705\":2,\"729\":1,\"880\":1,\"885\":1,\"1150\":1,\"1198\":1,\"1257\":1,\"1466\":1,\"1515\":1,\"1523\":1,\"1524\":1,\"1528\":1,\"1529\":2,\"1547\":1,\"1566\":1,\"1568\":2,\"1572\":1,\"1581\":1,\"1645\":1,\"1706\":1,\"1707\":1,\"1712\":1,\"1715\":2,\"1917\":1,\"2030\":1,\"2044\":1,\"2054\":1,\"2055\":1,\"2064\":1,\"2068\":1,\"2070\":1,\"2438\":1,\"2457\":1,\"2461\":1,\"2462\":3,\"2564\":1}}],[\"already\",{\"1\":{\"5\":1,\"26\":2,\"85\":1,\"96\":1,\"124\":1,\"126\":1,\"135\":1,\"922\":1,\"1058\":1,\"1116\":1,\"1462\":1,\"1464\":1,\"1616\":1,\"2050\":1,\"2400\":2,\"2418\":1,\"2468\":1,\"2536\":2,\"2583\":1,\"2598\":1}}],[\"all=c\",{\"1\":{\"2638\":1}}],[\"allzero\",{\"0\":{\"2207\":1},\"1\":{\"2184\":1,\"2200\":1,\"2207\":2}}],[\"allheadprelulayernormalization4dcf\",{\"0\":{\"1449\":1},\"1\":{\"1449\":1}}],[\"allheadprelulayernormalization4dc\",{\"0\":{\"1447\":1},\"1\":{\"1447\":1}}],[\"allenai\",{\"1\":{\"1203\":1}}],[\"allocated\",{\"1\":{\"1142\":1,\"1186\":1,\"1210\":2,\"2426\":1,\"2549\":1}}],[\"allowing\",{\"1\":{\"1452\":1}}],[\"allows\",{\"1\":{\"745\":1,\"746\":1,\"1217\":1,\"1253\":2,\"1466\":1}}],[\"allow\",{\"1\":{\"59\":2,\"173\":1,\"307\":2,\"315\":2,\"321\":2,\"327\":2,\"333\":2,\"339\":2,\"343\":2,\"350\":2,\"357\":2,\"368\":2,\"380\":2,\"384\":2,\"391\":2,\"399\":2,\"408\":2,\"416\":2,\"422\":2,\"429\":4,\"443\":2,\"449\":2,\"455\":2,\"464\":2,\"470\":2,\"476\":2,\"478\":2,\"485\":2,\"564\":2,\"998\":1,\"1186\":1,\"1269\":2,\"1551\":1,\"1553\":1,\"2099\":2,\"2106\":2,\"2174\":1,\"2182\":1,\"2222\":1,\"2225\":1,\"2229\":1,\"2231\":1,\"2315\":1}}],[\"allowed\",{\"1\":{\"21\":1,\"23\":1,\"59\":1,\"119\":1,\"658\":1,\"700\":1,\"917\":1,\"1048\":1,\"1138\":1,\"1139\":1,\"1639\":1,\"1928\":1}}],[\"allreduce\",{\"1\":{\"628\":1}}],[\"all\",{\"0\":{\"1963\":2,\"2164\":2},\"1\":{\"1\":3,\"3\":1,\"24\":1,\"28\":1,\"38\":1,\"44\":1,\"49\":1,\"57\":1,\"58\":1,\"62\":1,\"66\":3,\"79\":1,\"85\":2,\"90\":1,\"102\":1,\"107\":1,\"115\":1,\"121\":1,\"122\":1,\"130\":1,\"135\":1,\"182\":1,\"204\":1,\"235\":2,\"237\":1,\"239\":1,\"240\":2,\"265\":2,\"269\":2,\"294\":1,\"295\":1,\"608\":1,\"628\":1,\"629\":1,\"676\":2,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"691\":1,\"710\":1,\"742\":1,\"752\":1,\"754\":2,\"756\":1,\"760\":1,\"778\":2,\"781\":1,\"785\":1,\"794\":1,\"795\":1,\"797\":1,\"814\":1,\"820\":1,\"821\":2,\"826\":2,\"831\":1,\"836\":1,\"858\":1,\"875\":1,\"981\":1,\"987\":1,\"1005\":1,\"1028\":1,\"1087\":1,\"1109\":2,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1133\":3,\"1134\":1,\"1136\":2,\"1148\":3,\"1151\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1174\":1,\"1180\":1,\"1184\":1,\"1187\":2,\"1188\":1,\"1202\":2,\"1203\":3,\"1207\":1,\"1211\":1,\"1212\":1,\"1215\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1243\":1,\"1245\":3,\"1249\":1,\"1253\":3,\"1254\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1272\":3,\"1274\":1,\"1276\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":2,\"1336\":1,\"1348\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1392\":1,\"1430\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":2,\"1439\":1,\"1441\":1,\"1443\":2,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":1,\"1518\":1,\"1520\":1,\"1522\":10,\"1523\":10,\"1529\":1,\"1530\":1,\"1532\":1,\"1535\":1,\"1537\":1,\"1540\":1,\"1543\":1,\"1545\":2,\"1547\":1,\"1549\":1,\"1551\":1,\"1553\":1,\"1555\":1,\"1561\":1,\"1564\":1,\"1570\":1,\"1572\":1,\"1573\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1596\":1,\"1598\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1613\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1656\":1,\"1662\":1,\"1667\":1,\"1670\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1679\":1,\"1735\":1,\"1739\":4,\"1761\":1,\"1763\":1,\"1765\":3,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1800\":3,\"1801\":1,\"1803\":3,\"1806\":1,\"1844\":3,\"1848\":4,\"1849\":6,\"1851\":1,\"1857\":3,\"1858\":2,\"1861\":3,\"1862\":3,\"1870\":1,\"1871\":3,\"1880\":3,\"1890\":1,\"1902\":1,\"1907\":1,\"1912\":1,\"1917\":1,\"1932\":4,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1963\":3,\"1968\":1,\"1976\":2,\"1978\":2,\"1981\":1,\"1984\":2,\"1987\":1,\"1989\":1,\"1991\":1,\"1994\":1,\"1997\":1,\"2001\":1,\"2004\":1,\"2019\":1,\"2024\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2050\":1,\"2052\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2090\":1,\"2149\":1,\"2164\":2,\"2168\":1,\"2193\":1,\"2199\":1,\"2200\":1,\"2233\":1,\"2237\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2275\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2288\":2,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1,\"2377\":1,\"2385\":2,\"2387\":1,\"2389\":1,\"2401\":1,\"2408\":1,\"2411\":1,\"2422\":1,\"2436\":1,\"2449\":1,\"2465\":1,\"2481\":1,\"2503\":1,\"2525\":1,\"2537\":1,\"2545\":1,\"2558\":1,\"2560\":1,\"2562\":1,\"2568\":2,\"2584\":1,\"2641\":1,\"2644\":1}}],[\"ant\",{\"1\":{\"2584\":1}}],[\"anti\",{\"1\":{\"2543\":2}}],[\"anthology\",{\"1\":{\"130\":1}}],[\"an311\",{\"1\":{\"2432\":1}}],[\"answer\",{\"1\":{\"2414\":2,\"2418\":2,\"2419\":2,\"2420\":2,\"2461\":2,\"2462\":2,\"2479\":1,\"2501\":2}}],[\"answers\",{\"1\":{\"2387\":3}}],[\"anchor\",{\"1\":{\"2342\":1}}],[\"angular\",{\"1\":{\"2030\":2}}],[\"angles\",{\"1\":{\"1529\":1,\"1568\":1}}],[\"analyze\",{\"1\":{\"2317\":2,\"2330\":2}}],[\"analysis\",{\"0\":{\"2477\":1},\"1\":{\"1566\":1,\"1776\":1,\"1860\":2,\"2415\":1,\"2478\":1}}],[\"analytic\",{\"0\":{\"1680\":1},\"1\":{\"1132\":1,\"1680\":2}}],[\"anaconda\",{\"0\":{\"2384\":1,\"2394\":1,\"2428\":1,\"2530\":1,\"2551\":1},\"1\":{\"132\":1,\"135\":4,\"136\":1,\"2372\":4,\"2384\":3,\"2394\":2,\"2409\":2,\"2428\":3,\"2431\":1,\"2432\":1,\"2530\":2,\"2551\":3,\"2584\":2}}],[\"anomaly\",{\"1\":{\"429\":2}}],[\"another\",{\"1\":{\"62\":1,\"121\":1,\"1391\":5,\"2201\":1,\"2393\":1,\"2398\":1,\"2401\":1,\"2430\":1,\"2451\":1,\"2529\":1,\"2534\":1,\"2537\":1,\"2555\":1,\"2565\":1,\"2585\":1}}],[\"anneal\",{\"0\":{\"2018\":1},\"1\":{\"2018\":1}}],[\"annealed\",{\"1\":{\"1451\":1}}],[\"annealedlangevindynamics\",{\"0\":{\"1451\":1},\"1\":{\"1451\":1}}],[\"annealing\",{\"1\":{\"668\":2,\"2018\":1}}],[\"annual\",{\"1\":{\"130\":1}}],[\"announcement\",{\"1\":{\"84\":1}}],[\"an\",{\"1\":{\"5\":2,\"14\":1,\"16\":2,\"20\":1,\"21\":1,\"24\":2,\"46\":1,\"57\":3,\"59\":3,\"60\":1,\"62\":1,\"63\":1,\"69\":2,\"85\":1,\"98\":1,\"102\":1,\"104\":1,\"112\":1,\"117\":1,\"127\":1,\"130\":2,\"132\":1,\"135\":1,\"144\":2,\"161\":1,\"197\":1,\"198\":1,\"202\":1,\"216\":1,\"233\":1,\"235\":1,\"251\":1,\"493\":1,\"593\":1,\"594\":1,\"604\":1,\"607\":2,\"619\":2,\"627\":1,\"628\":1,\"630\":1,\"631\":1,\"651\":1,\"652\":1,\"656\":1,\"676\":1,\"684\":1,\"685\":1,\"691\":1,\"696\":1,\"697\":1,\"704\":1,\"706\":2,\"710\":1,\"711\":1,\"714\":1,\"715\":1,\"716\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"726\":1,\"727\":2,\"728\":2,\"742\":1,\"743\":1,\"745\":2,\"746\":2,\"749\":1,\"750\":1,\"770\":1,\"771\":1,\"781\":1,\"785\":1,\"792\":1,\"797\":1,\"809\":1,\"810\":1,\"815\":1,\"817\":1,\"818\":1,\"819\":1,\"827\":1,\"829\":2,\"834\":1,\"855\":1,\"875\":1,\"931\":1,\"988\":2,\"1007\":1,\"1011\":1,\"1051\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1076\":1,\"1095\":1,\"1142\":1,\"1160\":3,\"1161\":3,\"1162\":1,\"1163\":1,\"1164\":3,\"1165\":2,\"1173\":1,\"1177\":3,\"1186\":2,\"1187\":2,\"1194\":1,\"1198\":2,\"1202\":2,\"1209\":3,\"1210\":3,\"1221\":1,\"1225\":1,\"1248\":1,\"1252\":3,\"1253\":5,\"1254\":3,\"1279\":1,\"1286\":2,\"1287\":2,\"1356\":1,\"1400\":1,\"1427\":2,\"1429\":1,\"1522\":1,\"1563\":1,\"1600\":1,\"1603\":1,\"1612\":1,\"1615\":1,\"1618\":2,\"1619\":2,\"1622\":1,\"1638\":1,\"1639\":1,\"1645\":1,\"1735\":1,\"1851\":1,\"1889\":1,\"1917\":1,\"2086\":1,\"2087\":1,\"2090\":1,\"2154\":1,\"2170\":1,\"2208\":1,\"2259\":1,\"2309\":1,\"2354\":1,\"2373\":1,\"2380\":1,\"2384\":1,\"2385\":2,\"2387\":1,\"2389\":1,\"2399\":1,\"2400\":1,\"2408\":1,\"2410\":1,\"2422\":1,\"2423\":1,\"2430\":1,\"2438\":1,\"2441\":1,\"2449\":1,\"2450\":1,\"2465\":1,\"2467\":2,\"2468\":5,\"2473\":1,\"2481\":1,\"2492\":2,\"2503\":1,\"2518\":1,\"2525\":1,\"2535\":1,\"2536\":1,\"2545\":1,\"2546\":1,\"2555\":1,\"2564\":1,\"2584\":3,\"2585\":2,\"2598\":1,\"2628\":2,\"2641\":1}}],[\"an4\",{\"0\":{\"2385\":1},\"1\":{\"2\":1,\"4\":1,\"15\":2,\"16\":2,\"17\":1,\"85\":6,\"168\":1,\"171\":1,\"175\":1,\"180\":1,\"181\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":2,\"187\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"193\":1,\"194\":1,\"233\":1,\"235\":4,\"236\":1,\"2372\":4,\"2375\":5,\"2377\":3,\"2384\":1,\"2385\":10,\"2387\":5,\"2429\":4,\"2432\":2,\"2436\":3,\"2438\":1,\"2440\":1,\"2554\":4,\"2558\":6,\"2559\":2,\"2562\":3,\"2564\":2,\"2569\":1,\"2570\":1,\"2584\":4,\"2585\":2}}],[\"anymore\",{\"1\":{\"2387\":1}}],[\"anything\",{\"1\":{\"85\":1,\"144\":2,\"2387\":1}}],[\"any\",{\"0\":{\"2207\":1},\"1\":{\"1\":1,\"3\":1,\"20\":1,\"25\":1,\"36\":2,\"38\":3,\"39\":3,\"44\":1,\"47\":1,\"48\":1,\"74\":1,\"85\":1,\"102\":1,\"112\":1,\"134\":1,\"135\":1,\"141\":1,\"143\":1,\"237\":1,\"608\":1,\"629\":1,\"633\":3,\"641\":1,\"647\":1,\"691\":4,\"692\":1,\"695\":3,\"696\":3,\"697\":4,\"698\":5,\"699\":5,\"700\":1,\"706\":2,\"707\":4,\"718\":1,\"725\":2,\"734\":4,\"742\":1,\"745\":1,\"746\":1,\"751\":2,\"754\":1,\"765\":2,\"773\":4,\"796\":2,\"797\":1,\"798\":2,\"806\":2,\"815\":4,\"817\":1,\"824\":2,\"828\":5,\"858\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"866\":2,\"910\":1,\"911\":1,\"918\":2,\"934\":1,\"1003\":2,\"1004\":2,\"1005\":1,\"1015\":1,\"1028\":2,\"1046\":1,\"1058\":3,\"1060\":2,\"1063\":4,\"1083\":2,\"1087\":2,\"1088\":2,\"1089\":2,\"1090\":1,\"1091\":2,\"1094\":1,\"1103\":2,\"1104\":1,\"1105\":2,\"1133\":4,\"1138\":1,\"1139\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1176\":2,\"1177\":1,\"1187\":1,\"1190\":4,\"1193\":4,\"1202\":1,\"1214\":3,\"1221\":1,\"1244\":4,\"1252\":1,\"1253\":1,\"1254\":1,\"1270\":2,\"1273\":3,\"1279\":1,\"1543\":1,\"1655\":1,\"1656\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1719\":1,\"1761\":3,\"1763\":4,\"1765\":2,\"1773\":1,\"1778\":17,\"1801\":8,\"1803\":2,\"1805\":13,\"1834\":2,\"1837\":1,\"1844\":2,\"1845\":2,\"1846\":4,\"1847\":3,\"1848\":2,\"1849\":2,\"1850\":13,\"1851\":2,\"1852\":17,\"1856\":4,\"1857\":4,\"1858\":6,\"1861\":2,\"1862\":2,\"1866\":2,\"1867\":4,\"1870\":2,\"1871\":2,\"1876\":2,\"1877\":13,\"1895\":1,\"1896\":1,\"1900\":1,\"1932\":1,\"1957\":5,\"1958\":2,\"1960\":5,\"2001\":3,\"2011\":1,\"2170\":1,\"2207\":2,\"2243\":1,\"2309\":1,\"2394\":1,\"2429\":1,\"2514\":1,\"2530\":1,\"2552\":1,\"2584\":4,\"2585\":1,\"2641\":1,\"2659\":1}}],[\"andrew\",{\"1\":{\"2380\":1,\"2388\":1,\"2406\":1,\"2421\":2,\"2447\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2524\":1,\"2544\":1,\"2583\":1}}],[\"and\",{\"0\":{\"31\":1,\"36\":1,\"37\":1,\"39\":1,\"40\":1,\"42\":1,\"55\":1,\"59\":1,\"72\":1,\"82\":1,\"91\":1,\"94\":1,\"95\":1,\"99\":1,\"107\":1,\"133\":1,\"150\":1,\"189\":1,\"239\":1,\"886\":1,\"1427\":1,\"1429\":1,\"1735\":1,\"1965\":1,\"2368\":1,\"2392\":1,\"2425\":1,\"2472\":1,\"2486\":1,\"2489\":1,\"2528\":1,\"2548\":1,\"2588\":1,\"2605\":1,\"2609\":1,\"2622\":1,\"2626\":1,\"2648\":1,\"2649\":1},\"1\":{\"1\":3,\"3\":5,\"5\":3,\"6\":1,\"7\":1,\"15\":1,\"16\":3,\"17\":3,\"18\":1,\"19\":1,\"21\":9,\"22\":2,\"23\":9,\"24\":2,\"25\":7,\"26\":3,\"27\":1,\"28\":4,\"29\":1,\"30\":4,\"34\":1,\"37\":4,\"38\":1,\"44\":1,\"46\":2,\"47\":1,\"48\":2,\"49\":2,\"51\":1,\"52\":1,\"54\":2,\"56\":3,\"57\":18,\"58\":1,\"59\":15,\"60\":9,\"62\":1,\"64\":1,\"65\":1,\"66\":2,\"69\":3,\"70\":2,\"72\":3,\"74\":13,\"75\":12,\"76\":18,\"77\":8,\"78\":8,\"79\":4,\"80\":3,\"82\":1,\"84\":2,\"85\":5,\"87\":1,\"91\":5,\"92\":1,\"96\":1,\"98\":1,\"99\":1,\"102\":3,\"107\":5,\"108\":4,\"109\":1,\"110\":1,\"112\":3,\"113\":3,\"114\":2,\"115\":7,\"116\":1,\"117\":1,\"118\":2,\"119\":4,\"120\":2,\"121\":4,\"124\":4,\"130\":99,\"133\":1,\"134\":1,\"135\":3,\"138\":1,\"141\":1,\"142\":1,\"143\":2,\"144\":1,\"148\":1,\"149\":2,\"150\":10,\"161\":1,\"166\":1,\"167\":1,\"168\":1,\"171\":2,\"173\":1,\"177\":1,\"178\":1,\"179\":1,\"192\":1,\"195\":1,\"197\":1,\"198\":1,\"200\":2,\"203\":1,\"204\":1,\"206\":1,\"217\":1,\"224\":1,\"231\":1,\"235\":7,\"237\":4,\"238\":9,\"239\":3,\"240\":1,\"242\":1,\"275\":1,\"279\":1,\"280\":1,\"282\":1,\"283\":1,\"284\":1,\"295\":2,\"307\":4,\"315\":4,\"321\":4,\"327\":4,\"333\":4,\"339\":4,\"343\":4,\"350\":4,\"357\":4,\"368\":4,\"380\":4,\"384\":4,\"391\":4,\"399\":4,\"408\":4,\"416\":4,\"422\":4,\"429\":4,\"437\":4,\"443\":4,\"449\":4,\"455\":4,\"464\":4,\"470\":4,\"476\":4,\"478\":4,\"485\":4,\"506\":1,\"560\":1,\"564\":1,\"585\":1,\"595\":2,\"596\":1,\"605\":1,\"607\":2,\"610\":1,\"612\":2,\"616\":1,\"618\":2,\"619\":3,\"627\":1,\"628\":1,\"629\":1,\"636\":1,\"650\":1,\"668\":1,\"677\":2,\"678\":2,\"679\":3,\"681\":1,\"682\":1,\"684\":2,\"685\":4,\"686\":3,\"687\":3,\"688\":3,\"689\":3,\"690\":1,\"691\":10,\"692\":5,\"693\":2,\"695\":1,\"696\":1,\"697\":12,\"700\":2,\"706\":2,\"707\":1,\"708\":1,\"710\":6,\"711\":1,\"717\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"734\":3,\"736\":1,\"737\":1,\"740\":1,\"741\":1,\"742\":1,\"745\":2,\"746\":1,\"749\":3,\"750\":1,\"751\":1,\"752\":1,\"754\":6,\"756\":1,\"758\":3,\"760\":5,\"766\":1,\"767\":1,\"773\":2,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":2,\"782\":1,\"785\":1,\"786\":1,\"792\":1,\"793\":1,\"795\":1,\"796\":1,\"797\":9,\"812\":1,\"815\":1,\"817\":2,\"819\":1,\"820\":2,\"821\":2,\"825\":5,\"826\":5,\"828\":3,\"830\":1,\"831\":2,\"834\":1,\"835\":1,\"836\":2,\"852\":1,\"863\":1,\"865\":1,\"868\":1,\"875\":1,\"886\":2,\"889\":1,\"900\":1,\"902\":1,\"905\":2,\"917\":1,\"927\":1,\"932\":1,\"934\":1,\"950\":1,\"952\":2,\"968\":1,\"981\":1,\"999\":1,\"1001\":1,\"1007\":1,\"1010\":1,\"1011\":3,\"1012\":1,\"1013\":1,\"1015\":1,\"1025\":1,\"1028\":2,\"1046\":1,\"1048\":2,\"1049\":1,\"1050\":1,\"1056\":1,\"1057\":4,\"1059\":1,\"1060\":1,\"1061\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1068\":2,\"1076\":1,\"1082\":2,\"1084\":1,\"1097\":1,\"1098\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"1116\":3,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1132\":1,\"1133\":5,\"1134\":1,\"1136\":1,\"1138\":2,\"1139\":2,\"1140\":1,\"1141\":2,\"1142\":5,\"1143\":2,\"1145\":2,\"1148\":2,\"1149\":7,\"1150\":7,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1160\":2,\"1161\":2,\"1162\":2,\"1163\":2,\"1164\":2,\"1165\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":3,\"1172\":1,\"1173\":1,\"1174\":1,\"1176\":1,\"1177\":2,\"1178\":2,\"1179\":2,\"1180\":3,\"1181\":2,\"1182\":2,\"1183\":1,\"1184\":1,\"1186\":5,\"1187\":2,\"1188\":1,\"1190\":3,\"1196\":1,\"1197\":1,\"1198\":4,\"1199\":1,\"1200\":2,\"1202\":2,\"1203\":2,\"1204\":1,\"1206\":3,\"1207\":1,\"1209\":2,\"1210\":5,\"1211\":2,\"1212\":1,\"1214\":3,\"1215\":1,\"1217\":2,\"1218\":2,\"1220\":1,\"1221\":1,\"1222\":1,\"1224\":2,\"1227\":1,\"1229\":1,\"1231\":1,\"1233\":2,\"1235\":1,\"1237\":1,\"1239\":1,\"1242\":1,\"1243\":2,\"1244\":4,\"1245\":5,\"1247\":1,\"1248\":2,\"1249\":1,\"1252\":3,\"1253\":3,\"1254\":2,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":4,\"1270\":1,\"1271\":1,\"1272\":3,\"1273\":3,\"1274\":2,\"1276\":1,\"1279\":2,\"1280\":1,\"1282\":1,\"1284\":1,\"1286\":2,\"1287\":2,\"1327\":1,\"1334\":1,\"1336\":1,\"1337\":1,\"1341\":1,\"1348\":1,\"1356\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":2,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1378\":1,\"1381\":1,\"1398\":5,\"1406\":3,\"1427\":3,\"1429\":2,\"1430\":1,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":2,\"1454\":1,\"1455\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":3,\"1463\":2,\"1464\":1,\"1465\":1,\"1466\":1,\"1468\":1,\"1470\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1476\":2,\"1482\":1,\"1484\":3,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1505\":1,\"1506\":1,\"1508\":2,\"1510\":2,\"1511\":2,\"1512\":1,\"1517\":2,\"1518\":1,\"1520\":1,\"1522\":3,\"1524\":1,\"1525\":1,\"1529\":1,\"1530\":1,\"1531\":2,\"1532\":2,\"1534\":1,\"1535\":2,\"1537\":2,\"1539\":1,\"1540\":1,\"1542\":1,\"1543\":1,\"1546\":1,\"1547\":1,\"1549\":1,\"1551\":5,\"1552\":4,\"1553\":5,\"1554\":1,\"1555\":1,\"1558\":1,\"1559\":1,\"1560\":2,\"1561\":1,\"1563\":1,\"1564\":1,\"1566\":1,\"1567\":1,\"1568\":2,\"1569\":1,\"1570\":1,\"1571\":1,\"1573\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":2,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1598\":3,\"1600\":1,\"1601\":3,\"1602\":1,\"1603\":3,\"1604\":2,\"1605\":2,\"1607\":1,\"1609\":1,\"1611\":1,\"1613\":1,\"1616\":1,\"1617\":1,\"1620\":1,\"1622\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1638\":3,\"1639\":2,\"1640\":1,\"1641\":1,\"1643\":3,\"1644\":3,\"1645\":2,\"1646\":2,\"1648\":1,\"1650\":2,\"1652\":2,\"1654\":2,\"1655\":1,\"1656\":1,\"1660\":9,\"1661\":11,\"1662\":10,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":4,\"1671\":6,\"1672\":1,\"1674\":1,\"1676\":1,\"1683\":1,\"1688\":1,\"1693\":2,\"1695\":1,\"1696\":2,\"1698\":2,\"1704\":1,\"1705\":1,\"1712\":1,\"1715\":1,\"1718\":1,\"1719\":8,\"1735\":2,\"1739\":1,\"1755\":2,\"1756\":1,\"1760\":1,\"1761\":1,\"1763\":1,\"1765\":1,\"1766\":1,\"1767\":4,\"1768\":2,\"1769\":1,\"1773\":3,\"1774\":1,\"1776\":2,\"1778\":2,\"1779\":1,\"1782\":1,\"1785\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1797\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":2,\"1805\":1,\"1806\":1,\"1828\":1,\"1837\":2,\"1840\":1,\"1842\":1,\"1844\":1,\"1846\":1,\"1847\":2,\"1848\":1,\"1849\":2,\"1850\":2,\"1851\":6,\"1852\":2,\"1853\":1,\"1854\":1,\"1855\":1,\"1856\":3,\"1857\":1,\"1858\":2,\"1860\":1,\"1861\":1,\"1877\":1,\"1878\":2,\"1890\":1,\"1892\":1,\"1893\":1,\"1895\":1,\"1897\":1,\"1900\":1,\"1902\":1,\"1904\":1,\"1905\":1,\"1906\":3,\"1907\":1,\"1910\":2,\"1912\":2,\"1914\":2,\"1915\":2,\"1917\":3,\"1918\":2,\"1919\":2,\"1920\":2,\"1927\":2,\"1935\":1,\"1943\":1,\"1947\":1,\"1949\":1,\"1951\":1,\"1953\":3,\"1955\":3,\"1957\":3,\"1958\":2,\"1959\":1,\"1960\":3,\"1964\":1,\"1965\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1980\":2,\"1981\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2001\":5,\"2002\":2,\"2003\":1,\"2004\":3,\"2012\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2026\":1,\"2027\":1,\"2029\":3,\"2030\":2,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":3,\"2042\":1,\"2044\":2,\"2046\":4,\"2047\":1,\"2049\":2,\"2050\":1,\"2052\":1,\"2054\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2063\":1,\"2064\":2,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":2,\"2079\":2,\"2081\":1,\"2082\":3,\"2083\":1,\"2084\":2,\"2086\":3,\"2087\":3,\"2089\":2,\"2090\":7,\"2095\":3,\"2096\":1,\"2098\":1,\"2099\":5,\"2100\":1,\"2101\":1,\"2102\":3,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":4,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2121\":1,\"2122\":1,\"2134\":2,\"2148\":1,\"2149\":1,\"2154\":1,\"2157\":1,\"2168\":6,\"2170\":8,\"2176\":1,\"2185\":1,\"2193\":3,\"2199\":1,\"2201\":2,\"2203\":1,\"2208\":1,\"2212\":2,\"2216\":1,\"2217\":1,\"2233\":1,\"2235\":2,\"2237\":1,\"2239\":1,\"2240\":3,\"2241\":1,\"2243\":7,\"2244\":8,\"2246\":1,\"2248\":1,\"2250\":1,\"2252\":1,\"2255\":6,\"2257\":1,\"2261\":1,\"2262\":1,\"2263\":2,\"2264\":5,\"2265\":1,\"2266\":1,\"2267\":1,\"2275\":1,\"2277\":2,\"2278\":3,\"2279\":6,\"2281\":1,\"2283\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2299\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2305\":1,\"2349\":1,\"2354\":5,\"2355\":2,\"2357\":1,\"2358\":2,\"2361\":1,\"2363\":2,\"2371\":1,\"2372\":5,\"2373\":9,\"2375\":2,\"2377\":2,\"2380\":2,\"2383\":1,\"2384\":1,\"2385\":7,\"2387\":8,\"2388\":7,\"2391\":1,\"2392\":4,\"2393\":1,\"2394\":2,\"2395\":1,\"2397\":1,\"2398\":3,\"2399\":2,\"2400\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2411\":2,\"2412\":1,\"2415\":1,\"2419\":1,\"2421\":4,\"2423\":2,\"2424\":1,\"2425\":4,\"2427\":1,\"2429\":4,\"2430\":11,\"2431\":3,\"2432\":1,\"2433\":2,\"2438\":1,\"2440\":7,\"2441\":5,\"2446\":1,\"2451\":1,\"2452\":3,\"2459\":1,\"2467\":7,\"2468\":4,\"2473\":1,\"2481\":2,\"2487\":3,\"2491\":3,\"2494\":1,\"2497\":3,\"2498\":2,\"2499\":1,\"2500\":4,\"2501\":3,\"2506\":2,\"2508\":3,\"2510\":2,\"2514\":2,\"2515\":1,\"2518\":1,\"2520\":2,\"2523\":1,\"2524\":6,\"2527\":1,\"2528\":4,\"2529\":1,\"2530\":2,\"2531\":1,\"2533\":1,\"2534\":2,\"2535\":2,\"2536\":1,\"2537\":1,\"2539\":1,\"2541\":1,\"2542\":2,\"2543\":4,\"2544\":4,\"2546\":2,\"2547\":1,\"2548\":4,\"2550\":1,\"2552\":2,\"2553\":1,\"2554\":2,\"2555\":14,\"2556\":1,\"2558\":6,\"2559\":1,\"2560\":2,\"2564\":5,\"2565\":2,\"2566\":1,\"2567\":1,\"2568\":1,\"2569\":2,\"2570\":1,\"2572\":2,\"2573\":3,\"2574\":1,\"2575\":1,\"2579\":2,\"2584\":4,\"2585\":3,\"2592\":4,\"2593\":1,\"2596\":3,\"2599\":1,\"2600\":1,\"2601\":1,\"2612\":1,\"2617\":3,\"2618\":3,\"2630\":2,\"2635\":3,\"2638\":3,\"2639\":1,\"2640\":2,\"2641\":6,\"2642\":3,\"2644\":1,\"2650\":1,\"2653\":2,\"2659\":2}}],[\"arcmarginproduct\",{\"0\":{\"2040\":1},\"1\":{\"2040\":1}}],[\"arcface\",{\"1\":{\"2030\":1,\"2040\":1}}],[\"arcname=none\",{\"1\":{\"1961\":1}}],[\"arcs\",{\"1\":{\"1427\":1}}],[\"arc\",{\"1\":{\"1136\":2,\"2040\":1}}],[\"arch=\",{\"1\":{\"144\":2}}],[\"arch\",{\"1\":{\"21\":4,\"725\":3,\"726\":3,\"889\":3}}],[\"architecure\",{\"1\":{\"2468\":1}}],[\"architectures\",{\"1\":{\"21\":1,\"80\":1,\"1352\":1,\"2452\":1,\"2467\":1,\"2473\":1}}],[\"architecture\",{\"0\":{\"21\":1,\"114\":1,\"1103\":1},\"1\":{\"21\":10,\"114\":1,\"115\":3,\"704\":1,\"705\":1,\"725\":1,\"726\":1,\"847\":1,\"861\":1,\"880\":1,\"1057\":1,\"1062\":2,\"1068\":4,\"1074\":1,\"1081\":2,\"1087\":1,\"1103\":2,\"1104\":1,\"1517\":1,\"1662\":1,\"1798\":1,\"1862\":1,\"1874\":1,\"1878\":1,\"2046\":1,\"2049\":1,\"2055\":1,\"2064\":1,\"2090\":1,\"2279\":1,\"2641\":1}}],[\"archiver\",{\"0\":{\"1961\":1},\"1\":{\"1961\":2}}],[\"archive\",{\"1\":{\"5\":1,\"130\":1,\"397\":1,\"1462\":1,\"1463\":1,\"1528\":1,\"1529\":1,\"1568\":1,\"1968\":2,\"2032\":1,\"2499\":1,\"2635\":1}}],[\"arange\",{\"1\":{\"904\":1}}],[\"arabic\",{\"1\":{\"461\":1}}],[\"arr\",{\"1\":{\"629\":1,\"923\":2}}],[\"arrays\",{\"1\":{\"536\":1,\"683\":1,\"727\":1,\"728\":1,\"745\":3,\"746\":3,\"940\":1,\"1391\":2,\"1392\":4}}],[\"array\",{\"1\":{\"143\":1,\"238\":2,\"605\":2,\"703\":2,\"731\":4,\"744\":5,\"745\":3,\"746\":3,\"747\":1,\"750\":2,\"760\":2,\"784\":2,\"808\":1,\"856\":2,\"870\":2,\"876\":6,\"878\":2,\"923\":2,\"950\":1,\"968\":1,\"981\":1,\"984\":1,\"989\":3,\"992\":1,\"995\":1,\"1013\":1,\"1014\":4,\"1015\":2,\"1216\":2,\"1384\":1,\"1385\":4,\"1386\":1,\"1387\":4,\"1392\":1,\"1395\":1,\"1397\":2,\"1406\":3,\"1408\":4,\"1410\":1,\"1418\":2,\"1423\":1,\"1655\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1705\":1,\"1719\":1,\"2183\":2,\"2190\":2,\"2208\":1,\"2386\":1,\"2432\":2,\"2474\":1,\"2514\":1,\"2649\":1,\"2659\":1}}],[\"ar\",{\"0\":{\"314\":1,\"415\":1,\"765\":1,\"797\":1,\"798\":1,\"1219\":1},\"1\":{\"307\":2,\"408\":2,\"765\":1,\"797\":1,\"798\":1,\"1133\":2,\"1219\":2}}],[\"around\",{\"1\":{\"196\":1,\"200\":1,\"234\":1,\"1011\":1,\"1245\":1,\"1619\":1,\"2387\":1,\"2397\":1,\"2401\":1,\"2409\":1,\"2466\":1,\"2533\":1,\"2537\":1,\"2565\":1}}],[\"arora\",{\"1\":{\"130\":2,\"2421\":1,\"2463\":1,\"2480\":1,\"2502\":1,\"2646\":1}}],[\"arora2021espnet\",{\"1\":{\"130\":1}}],[\"artifacts\",{\"1\":{\"167\":1,\"178\":1,\"196\":1,\"234\":1}}],[\"article\",{\"1\":{\"130\":3,\"1001\":1,\"2032\":1}}],[\"art\",{\"1\":{\"161\":3,\"2452\":1}}],[\"arxiv\",{\"1\":{\"130\":6,\"668\":1,\"678\":1,\"681\":1,\"682\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"692\":1,\"693\":1,\"700\":3,\"706\":2,\"729\":1,\"758\":1,\"770\":2,\"771\":1,\"772\":1,\"809\":1,\"810\":1,\"813\":1,\"950\":1,\"968\":1,\"1037\":1,\"1048\":2,\"1049\":1,\"1056\":1,\"1061\":1,\"1066\":1,\"1067\":1,\"1072\":1,\"1075\":1,\"1080\":1,\"1084\":3,\"1138\":4,\"1139\":3,\"1150\":1,\"1198\":1,\"1210\":1,\"1211\":1,\"1286\":2,\"1302\":1,\"1303\":1,\"1304\":1,\"1336\":2,\"1337\":2,\"1371\":3,\"1466\":1,\"1566\":1,\"1645\":1,\"1660\":4,\"1767\":1,\"1782\":1,\"1793\":1,\"1829\":1,\"1842\":1,\"1850\":1,\"1917\":1,\"1932\":1,\"1976\":1,\"1986\":1,\"2003\":1,\"2019\":1,\"2040\":1,\"2078\":2,\"2081\":1,\"2083\":1,\"2095\":1,\"2260\":1,\"2467\":2,\"2586\":1,\"2646\":1}}],[\"arbitrary\",{\"1\":{\"59\":1,\"1187\":1,\"1202\":1,\"1352\":1,\"1688\":1,\"1756\":1}}],[\"arbitrarily\",{\"1\":{\"47\":1}}],[\"ark\",{\"1\":{\"57\":1,\"58\":3,\"169\":1,\"181\":1,\"238\":14,\"241\":1,\"285\":1,\"286\":2,\"296\":2,\"959\":1,\"984\":1,\"987\":1,\"992\":1,\"995\":1,\"1013\":3,\"1014\":4,\"1015\":10,\"1024\":1,\"1030\":4,\"2431\":1,\"2514\":4,\"2659\":4}}],[\"argv\",{\"1\":{\"2568\":3}}],[\"arguements\",{\"1\":{\"2086\":1,\"2087\":1}}],[\"argumentgroup\",{\"1\":{\"844\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"853\":1}}],[\"argumentparser\",{\"0\":{\"2309\":1},\"1\":{\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"672\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":2,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2176\":1,\"2185\":1,\"2201\":1,\"2203\":1,\"2309\":3,\"2314\":1,\"2320\":1,\"2328\":1,\"2335\":1,\"2340\":1}}],[\"argument\",{\"0\":{\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"935\":1,\"2351\":1,\"2378\":1,\"2437\":1,\"2563\":1},\"1\":{\"28\":1,\"29\":1,\"57\":1,\"59\":1,\"60\":2,\"63\":1,\"105\":1,\"144\":1,\"606\":1,\"672\":1,\"673\":1,\"676\":1,\"711\":4,\"734\":1,\"742\":1,\"767\":1,\"781\":1,\"812\":1,\"817\":1,\"828\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"889\":1,\"935\":1,\"1187\":3,\"1202\":3,\"1242\":1,\"1269\":2,\"1286\":2,\"1287\":2,\"1552\":1,\"1603\":1,\"1618\":1,\"1619\":1,\"1622\":1,\"2170\":2,\"2309\":1,\"2314\":1,\"2320\":1,\"2328\":1,\"2335\":1,\"2340\":1,\"2351\":2,\"2373\":1,\"2430\":1,\"2440\":1,\"2555\":1,\"2584\":2,\"2640\":1}}],[\"arguments\",{\"0\":{\"90\":1,\"246\":1,\"248\":1,\"250\":1,\"252\":1,\"254\":1,\"256\":1,\"258\":1,\"260\":1,\"262\":1,\"264\":1,\"266\":1,\"268\":1,\"270\":1,\"300\":1,\"302\":1,\"306\":1,\"308\":1,\"316\":1,\"322\":1,\"328\":1,\"334\":1,\"340\":1,\"344\":1,\"351\":1,\"358\":1,\"364\":1,\"369\":1,\"376\":1,\"378\":1,\"379\":1,\"381\":1,\"385\":1,\"392\":1,\"400\":1,\"409\":1,\"417\":1,\"423\":1,\"430\":1,\"438\":1,\"442\":1,\"444\":1,\"450\":1,\"456\":1,\"462\":1,\"465\":1,\"471\":1,\"477\":1,\"479\":1,\"486\":1,\"492\":1,\"494\":1,\"495\":1,\"497\":1,\"498\":1,\"500\":1,\"502\":1,\"504\":1,\"505\":1,\"507\":1,\"508\":1,\"510\":1,\"511\":1,\"513\":1,\"514\":1,\"516\":1,\"518\":1,\"520\":1,\"521\":1,\"523\":1,\"524\":1,\"526\":1,\"527\":1,\"529\":1,\"531\":1,\"532\":1,\"534\":1,\"535\":1,\"537\":1,\"539\":1,\"540\":1,\"542\":1,\"543\":1,\"545\":1,\"547\":1,\"548\":1,\"550\":1,\"552\":1,\"553\":1,\"555\":1,\"556\":1,\"558\":1,\"559\":1,\"561\":1,\"563\":1,\"565\":1,\"567\":1,\"569\":1,\"571\":1,\"573\":1,\"575\":1,\"577\":1,\"578\":1,\"580\":1,\"581\":1,\"583\":1,\"584\":1,\"586\":1,\"587\":1,\"589\":1,\"591\":1,\"592\":1,\"839\":1,\"840\":1,\"841\":1,\"842\":1,\"843\":1,\"844\":2,\"845\":2,\"846\":2,\"847\":2,\"848\":2,\"849\":2,\"850\":2,\"851\":2,\"853\":2,\"934\":1,\"1104\":1,\"1243\":1},\"1\":{\"1\":5,\"2\":1,\"3\":2,\"18\":1,\"24\":2,\"25\":2,\"56\":2,\"57\":1,\"59\":1,\"62\":1,\"90\":1,\"92\":1,\"102\":1,\"108\":1,\"116\":2,\"124\":1,\"143\":1,\"173\":1,\"271\":1,\"272\":1,\"398\":11,\"596\":1,\"602\":1,\"603\":1,\"606\":3,\"622\":1,\"636\":1,\"645\":1,\"646\":1,\"649\":1,\"650\":1,\"657\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"666\":1,\"672\":2,\"676\":2,\"734\":3,\"742\":3,\"754\":2,\"767\":2,\"778\":1,\"781\":2,\"816\":1,\"817\":3,\"820\":1,\"821\":2,\"826\":2,\"828\":3,\"836\":1,\"839\":2,\"840\":2,\"841\":2,\"842\":2,\"843\":2,\"844\":3,\"845\":3,\"846\":3,\"847\":3,\"848\":3,\"849\":3,\"850\":3,\"851\":3,\"853\":3,\"855\":2,\"867\":1,\"870\":1,\"875\":1,\"911\":1,\"934\":2,\"935\":2,\"937\":1,\"938\":1,\"944\":1,\"957\":1,\"987\":1,\"1008\":1,\"1016\":5,\"1017\":1,\"1032\":2,\"1033\":2,\"1034\":2,\"1049\":1,\"1050\":1,\"1051\":1,\"1054\":1,\"1056\":1,\"1057\":1,\"1060\":1,\"1066\":2,\"1068\":1,\"1074\":1,\"1075\":1,\"1098\":2,\"1104\":2,\"1187\":1,\"1202\":1,\"1327\":1,\"1451\":1,\"1514\":1,\"1557\":1,\"1585\":1,\"1612\":1,\"1615\":1,\"1623\":1,\"1637\":1,\"1752\":4,\"1834\":1,\"1876\":1,\"1905\":1,\"1912\":1,\"1972\":2,\"2096\":1,\"2097\":6,\"2098\":1,\"2099\":2,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2176\":2,\"2185\":2,\"2201\":2,\"2203\":2,\"2378\":1,\"2431\":3,\"2437\":3,\"2563\":3,\"2568\":1,\"2569\":1,\"2638\":1}}],[\"argmin\",{\"1\":{\"2500\":1,\"2617\":1,\"2635\":1}}],[\"argmax\",{\"1\":{\"835\":1,\"1145\":3}}],[\"argments\",{\"1\":{\"820\":1}}],[\"argment\",{\"1\":{\"676\":1,\"781\":1,\"812\":1}}],[\"arg2\",{\"1\":{\"92\":1}}],[\"argparse\",{\"0\":{\"2309\":1},\"1\":{\"56\":1,\"60\":1,\"173\":1,\"194\":2,\"217\":1,\"224\":1,\"231\":1,\"606\":1,\"659\":2,\"660\":2,\"661\":2,\"662\":2,\"734\":1,\"817\":1,\"828\":1,\"1017\":1,\"2099\":3,\"2168\":1,\"2170\":1,\"2176\":2,\"2309\":1,\"2314\":1,\"2320\":2,\"2328\":2,\"2335\":2,\"2340\":2,\"2591\":1}}],[\"argscomplexmultiplicationwrapper\",{\"0\":{\"1452\":1},\"1\":{\"1452\":1}}],[\"args=\",{\"1\":{\"1245\":1}}],[\"args=none\",{\"1\":{\"754\":1,\"821\":1,\"826\":1,\"987\":1,\"2309\":1}}],[\"args\",{\"0\":{\"638\":1,\"1016\":2,\"1020\":1,\"1726\":1,\"1752\":1},\"1\":{\"56\":3,\"60\":3,\"92\":3,\"96\":2,\"113\":2,\"116\":2,\"173\":1,\"175\":3,\"194\":3,\"217\":4,\"218\":1,\"224\":4,\"225\":1,\"231\":4,\"232\":1,\"377\":2,\"596\":1,\"602\":3,\"603\":3,\"606\":1,\"622\":3,\"636\":2,\"638\":11,\"644\":1,\"645\":1,\"646\":2,\"649\":3,\"650\":2,\"657\":3,\"659\":5,\"660\":5,\"661\":6,\"662\":5,\"668\":1,\"669\":1,\"670\":1,\"672\":2,\"676\":5,\"692\":1,\"709\":3,\"710\":4,\"731\":1,\"733\":1,\"734\":1,\"742\":7,\"747\":1,\"754\":6,\"759\":1,\"767\":1,\"781\":5,\"787\":2,\"797\":1,\"799\":1,\"812\":4,\"816\":2,\"817\":2,\"820\":3,\"821\":6,\"825\":1,\"826\":6,\"828\":2,\"836\":2,\"855\":2,\"867\":3,\"870\":3,\"878\":1,\"881\":1,\"882\":1,\"889\":3,\"894\":2,\"935\":3,\"937\":3,\"938\":3,\"977\":1,\"978\":1,\"981\":1,\"987\":2,\"1010\":1,\"1011\":1,\"1016\":5,\"1017\":3,\"1020\":1,\"1031\":1,\"1032\":2,\"1033\":2,\"1034\":2,\"1044\":1,\"1045\":1,\"1049\":2,\"1050\":2,\"1051\":2,\"1054\":2,\"1056\":2,\"1057\":2,\"1066\":4,\"1068\":2,\"1074\":2,\"1075\":2,\"1086\":1,\"1130\":1,\"1156\":1,\"1177\":1,\"1180\":1,\"1183\":1,\"1187\":1,\"1188\":1,\"1202\":1,\"1209\":1,\"1218\":1,\"1219\":1,\"1241\":1,\"1243\":1,\"1245\":2,\"1251\":1,\"1252\":1,\"1254\":1,\"1269\":1,\"1286\":1,\"1287\":1,\"1297\":1,\"1327\":2,\"1351\":1,\"1451\":2,\"1452\":4,\"1454\":1,\"1514\":2,\"1557\":2,\"1559\":1,\"1585\":2,\"1612\":3,\"1615\":3,\"1616\":1,\"1620\":1,\"1623\":3,\"1637\":2,\"1638\":4,\"1681\":1,\"1718\":1,\"1726\":3,\"1735\":1,\"1744\":1,\"1752\":1,\"1760\":1,\"1828\":1,\"1830\":1,\"1832\":1,\"1840\":1,\"1895\":1,\"1996\":1,\"1999\":1,\"2096\":4,\"2097\":6,\"2098\":4,\"2099\":18,\"2100\":4,\"2101\":4,\"2102\":5,\"2103\":5,\"2104\":5,\"2105\":4,\"2107\":4,\"2108\":4,\"2109\":4,\"2110\":4,\"2111\":2,\"2112\":4,\"2113\":4,\"2114\":4,\"2115\":4,\"2116\":4,\"2117\":4,\"2118\":5,\"2149\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2176\":6,\"2185\":1,\"2201\":1,\"2203\":1,\"2227\":1,\"2294\":1,\"2304\":1,\"2309\":2,\"2314\":4,\"2318\":2,\"2320\":4,\"2328\":4,\"2335\":1,\"2340\":4,\"2378\":2,\"2437\":2,\"2440\":1,\"2474\":1,\"2563\":2,\"2569\":2,\"2644\":1,\"2649\":1}}],[\"arg\",{\"1\":{\"3\":4,\"92\":1,\"173\":1,\"503\":1,\"672\":1}}],[\"area=default\",{\"1\":{\"2414\":1}}],[\"area\",{\"1\":{\"778\":1,\"1810\":1,\"2573\":1}}],[\"are\",{\"1\":{\"1\":2,\"3\":1,\"4\":1,\"5\":3,\"17\":1,\"21\":2,\"22\":1,\"23\":4,\"24\":2,\"25\":2,\"30\":1,\"35\":1,\"38\":2,\"44\":1,\"45\":1,\"47\":1,\"48\":3,\"49\":1,\"54\":1,\"58\":1,\"59\":1,\"60\":2,\"62\":1,\"63\":1,\"64\":1,\"69\":1,\"72\":2,\"73\":1,\"75\":1,\"80\":2,\"82\":1,\"83\":1,\"85\":1,\"91\":2,\"102\":1,\"109\":1,\"111\":1,\"112\":1,\"113\":1,\"115\":1,\"116\":3,\"118\":1,\"119\":4,\"122\":2,\"124\":1,\"126\":2,\"132\":1,\"135\":2,\"136\":1,\"141\":1,\"143\":1,\"150\":1,\"169\":1,\"197\":1,\"198\":1,\"237\":1,\"238\":3,\"239\":1,\"240\":4,\"241\":1,\"242\":1,\"295\":1,\"595\":1,\"607\":1,\"612\":1,\"629\":1,\"635\":2,\"691\":3,\"697\":5,\"710\":1,\"727\":1,\"728\":1,\"734\":1,\"737\":2,\"745\":2,\"746\":2,\"754\":1,\"760\":3,\"764\":1,\"767\":1,\"778\":1,\"785\":1,\"797\":1,\"817\":1,\"821\":1,\"826\":1,\"828\":1,\"831\":1,\"858\":1,\"917\":1,\"929\":1,\"933\":1,\"934\":1,\"952\":1,\"1008\":2,\"1016\":1,\"1025\":1,\"1048\":1,\"1132\":1,\"1138\":1,\"1187\":2,\"1202\":2,\"1243\":2,\"1245\":4,\"1255\":1,\"1269\":2,\"1406\":3,\"1409\":1,\"1430\":1,\"1452\":1,\"1476\":1,\"1484\":1,\"1515\":1,\"1517\":2,\"1528\":1,\"1529\":1,\"1531\":1,\"1532\":1,\"1534\":1,\"1535\":1,\"1537\":1,\"1539\":1,\"1576\":1,\"1577\":1,\"1598\":2,\"1601\":1,\"1602\":1,\"1626\":1,\"1638\":1,\"1639\":1,\"1645\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1670\":2,\"1671\":2,\"1693\":2,\"1739\":1,\"1755\":2,\"1847\":1,\"1849\":1,\"1860\":1,\"1897\":1,\"1905\":3,\"1906\":1,\"1910\":1,\"1912\":1,\"1914\":1,\"1915\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1927\":2,\"2012\":1,\"2084\":1,\"2086\":1,\"2087\":1,\"2089\":1,\"2099\":3,\"2102\":3,\"2156\":1,\"2168\":1,\"2209\":2,\"2212\":1,\"2357\":1,\"2363\":1,\"2372\":4,\"2373\":5,\"2377\":1,\"2381\":1,\"2384\":1,\"2385\":6,\"2387\":2,\"2389\":3,\"2391\":1,\"2393\":1,\"2394\":7,\"2400\":2,\"2401\":1,\"2407\":1,\"2408\":3,\"2422\":3,\"2423\":1,\"2428\":1,\"2429\":4,\"2430\":1,\"2433\":2,\"2436\":1,\"2440\":2,\"2448\":1,\"2449\":3,\"2464\":1,\"2465\":3,\"2468\":1,\"2481\":3,\"2499\":2,\"2500\":1,\"2503\":3,\"2525\":3,\"2527\":1,\"2529\":1,\"2530\":7,\"2536\":2,\"2537\":1,\"2542\":1,\"2545\":3,\"2546\":1,\"2551\":1,\"2552\":3,\"2553\":1,\"2554\":1,\"2555\":4,\"2558\":2,\"2562\":1,\"2564\":1,\"2568\":1,\"2569\":1,\"2571\":1,\"2573\":2,\"2574\":2,\"2583\":1,\"2584\":4,\"2585\":3,\"2617\":3,\"2635\":3,\"2637\":1,\"2641\":1,\"2653\":1}}],[\"atfblock\",{\"0\":{\"1430\":1},\"1\":{\"1430\":2}}],[\"atype=\",{\"1\":{\"1017\":1}}],[\"atype\",{\"1\":{\"892\":2,\"1220\":1,\"1289\":1,\"2002\":2,\"2095\":1,\"2263\":1}}],[\"atlas\",{\"1\":{\"134\":2}}],[\"attacks\",{\"1\":{\"2543\":1}}],[\"attack\",{\"1\":{\"2499\":1,\"2500\":1,\"2617\":2,\"2635\":2}}],[\"attadd\",{\"0\":{\"677\":1},\"1\":{\"677\":2}}],[\"attnblockpp\",{\"0\":{\"1458\":1},\"1\":{\"1458\":1}}],[\"attnblock\",{\"0\":{\"1456\":1},\"1\":{\"1456\":1}}],[\"attn\",{\"0\":{\"2044\":1},\"1\":{\"691\":6,\"697\":9,\"711\":2,\"726\":2,\"754\":3,\"821\":3,\"826\":9,\"858\":2,\"859\":2,\"886\":3,\"909\":2,\"1065\":5,\"1076\":1,\"1140\":2,\"1141\":5,\"1148\":1,\"1170\":2,\"1182\":3,\"1203\":4,\"1273\":1,\"1505\":2,\"1581\":1,\"1605\":1,\"1660\":4,\"1661\":4,\"1662\":5,\"1778\":3,\"1841\":2,\"1850\":3,\"1851\":6,\"1852\":3,\"1889\":3,\"1932\":1,\"2003\":1,\"2044\":1,\"2054\":1,\"2090\":5,\"2095\":6,\"2243\":6,\"2244\":6,\"2255\":5,\"2263\":6,\"2264\":19,\"2279\":6,\"2440\":2,\"2564\":2}}],[\"attmultiheadmultiresloc\",{\"0\":{\"689\":1},\"1\":{\"689\":2}}],[\"attmultiheadloc\",{\"0\":{\"688\":1},\"1\":{\"688\":2}}],[\"attmultiheaddot\",{\"0\":{\"687\":1},\"1\":{\"687\":2}}],[\"attmultiheadadd\",{\"0\":{\"686\":1},\"1\":{\"686\":2}}],[\"attlocrec\",{\"0\":{\"685\":1},\"1\":{\"685\":2}}],[\"attloc2d\",{\"0\":{\"684\":1},\"1\":{\"684\":2}}],[\"attloc\",{\"0\":{\"683\":1},\"1\":{\"683\":1,\"758\":1}}],[\"attetion\",{\"1\":{\"681\":1,\"682\":1,\"758\":1}}],[\"attenuate\",{\"1\":{\"2090\":1}}],[\"attenuation\",{\"1\":{\"1936\":1}}],[\"attentive\",{\"1\":{\"1430\":1,\"1983\":1,\"1994\":1}}],[\"attentional\",{\"1\":{\"707\":1}}],[\"attentionreference\",{\"0\":{\"690\":1,\"1455\":1},\"1\":{\"690\":1,\"1455\":1}}],[\"attentions\",{\"0\":{\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"758\":1,\"791\":1,\"855\":1,\"856\":1,\"892\":1,\"1963\":2},\"1\":{\"629\":2,\"676\":1,\"677\":1,\"678\":1,\"679\":2,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"688\":1,\"689\":1,\"742\":1,\"754\":1,\"758\":1,\"781\":1,\"791\":1,\"799\":1,\"803\":1,\"820\":1,\"821\":1,\"826\":1,\"855\":1,\"856\":1,\"870\":1,\"892\":1,\"909\":1,\"1963\":2}}],[\"attention\",{\"0\":{\"150\":2,\"771\":1,\"780\":1,\"784\":1,\"785\":1,\"809\":1,\"840\":1,\"909\":1,\"1076\":1,\"1081\":1,\"1086\":1,\"1100\":1,\"1209\":1,\"1289\":1,\"1976\":2,\"1993\":1,\"1996\":1,\"1999\":1},\"1\":{\"21\":4,\"31\":2,\"88\":1,\"112\":1,\"115\":7,\"116\":7,\"121\":2,\"122\":1,\"150\":13,\"217\":1,\"240\":2,\"251\":2,\"255\":2,\"259\":2,\"265\":2,\"269\":2,\"629\":13,\"676\":4,\"677\":5,\"678\":6,\"679\":9,\"680\":2,\"681\":9,\"682\":9,\"683\":2,\"684\":12,\"685\":11,\"686\":11,\"687\":8,\"688\":14,\"689\":14,\"691\":1,\"692\":1,\"704\":1,\"705\":2,\"711\":2,\"726\":1,\"731\":4,\"732\":1,\"740\":2,\"741\":2,\"742\":4,\"747\":4,\"748\":1,\"749\":10,\"754\":13,\"758\":11,\"762\":8,\"763\":7,\"771\":4,\"775\":2,\"776\":2,\"781\":4,\"784\":6,\"785\":8,\"791\":3,\"799\":3,\"809\":4,\"820\":5,\"821\":10,\"826\":20,\"827\":3,\"836\":2,\"840\":2,\"855\":2,\"856\":5,\"859\":1,\"870\":1,\"880\":1,\"886\":3,\"892\":9,\"909\":2,\"920\":1,\"924\":1,\"1049\":4,\"1050\":4,\"1052\":2,\"1056\":4,\"1058\":2,\"1065\":12,\"1066\":5,\"1068\":2,\"1069\":1,\"1074\":3,\"1075\":3,\"1076\":18,\"1077\":2,\"1081\":8,\"1086\":1,\"1093\":2,\"1100\":1,\"1101\":1,\"1115\":8,\"1133\":6,\"1140\":3,\"1141\":2,\"1148\":10,\"1149\":8,\"1150\":8,\"1167\":3,\"1168\":3,\"1169\":3,\"1170\":2,\"1171\":1,\"1178\":1,\"1179\":2,\"1180\":4,\"1181\":7,\"1182\":2,\"1190\":1,\"1196\":3,\"1197\":3,\"1200\":1,\"1203\":18,\"1204\":3,\"1209\":7,\"1269\":5,\"1271\":3,\"1272\":8,\"1273\":3,\"1289\":1,\"1430\":2,\"1455\":1,\"1456\":1,\"1458\":1,\"1470\":4,\"1505\":7,\"1537\":1,\"1539\":1,\"1581\":1,\"1660\":1,\"1661\":2,\"1662\":1,\"1669\":6,\"1670\":2,\"1671\":2,\"1771\":13,\"1787\":12,\"1788\":13,\"1798\":16,\"1804\":10,\"1805\":3,\"1829\":2,\"1841\":1,\"1851\":10,\"1874\":16,\"1877\":4,\"1878\":11,\"1889\":2,\"1960\":1,\"1963\":1,\"1975\":1,\"1976\":3,\"1984\":1,\"1986\":1,\"1993\":2,\"1996\":2,\"1999\":2,\"2001\":9,\"2002\":9,\"2004\":9,\"2019\":1,\"2024\":1,\"2026\":2,\"2027\":1,\"2028\":1,\"2029\":8,\"2040\":1,\"2044\":1,\"2049\":1,\"2054\":9,\"2063\":1,\"2064\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2078\":3,\"2079\":4,\"2083\":1,\"2090\":6,\"2095\":13,\"2185\":1,\"2201\":3,\"2203\":1,\"2239\":2,\"2243\":9,\"2244\":9,\"2253\":2,\"2255\":9,\"2261\":1,\"2262\":2,\"2263\":12,\"2264\":15,\"2279\":8,\"2440\":8,\"2452\":2,\"2558\":7,\"2564\":3,\"2584\":2}}],[\"attending\",{\"1\":{\"691\":2,\"697\":3,\"920\":1}}],[\"attended\",{\"1\":{\"681\":3,\"682\":3,\"758\":3,\"1986\":1}}],[\"attforwardta\",{\"0\":{\"682\":1},\"1\":{\"682\":2}}],[\"attforward\",{\"0\":{\"681\":1},\"1\":{\"681\":2}}],[\"attdot\",{\"0\":{\"680\":1},\"1\":{\"680\":1}}],[\"attcovloc\",{\"0\":{\"679\":1},\"1\":{\"679\":2}}],[\"attcov\",{\"0\":{\"678\":1},\"1\":{\"678\":2}}],[\"attractor\",{\"0\":{\"1360\":2,\"1376\":2},\"1\":{\"1360\":2,\"1371\":5,\"1376\":5,\"1515\":2}}],[\"attrs\",{\"0\":{\"1317\":1},\"1\":{\"1317\":2,\"1552\":2}}],[\"attr\",{\"1\":{\"544\":1,\"834\":2}}],[\"attributes\",{\"1\":{\"503\":1,\"1245\":1,\"1253\":1,\"1552\":1}}],[\"attribute\",{\"1\":{\"106\":1,\"544\":1,\"710\":1,\"834\":8,\"1160\":1,\"1161\":1,\"1164\":1,\"1165\":1,\"1187\":1,\"1202\":1,\"1253\":2,\"1254\":1,\"1279\":1,\"1286\":1,\"1287\":1,\"1427\":1,\"1515\":1,\"1973\":1}}],[\"att=28\",{\"1\":{\"87\":1}}],[\"att=35\",{\"1\":{\"87\":1}}],[\"att\",{\"0\":{\"855\":1,\"856\":1,\"886\":1,\"892\":1},\"1\":{\"17\":2,\"21\":3,\"88\":1,\"115\":5,\"116\":2,\"240\":3,\"263\":2,\"267\":2,\"399\":2,\"464\":2,\"470\":2,\"629\":5,\"677\":4,\"678\":4,\"679\":4,\"680\":2,\"681\":4,\"682\":4,\"683\":2,\"684\":8,\"685\":8,\"686\":6,\"687\":6,\"688\":6,\"689\":6,\"690\":1,\"711\":2,\"712\":4,\"749\":2,\"758\":4,\"762\":2,\"763\":2,\"799\":2,\"811\":1,\"821\":1,\"855\":1,\"856\":5,\"870\":2,\"886\":1,\"892\":1,\"996\":2,\"1049\":2,\"1050\":2,\"1052\":1,\"1056\":2,\"1065\":2,\"1066\":2,\"1068\":1,\"1074\":4,\"1075\":12,\"1081\":8,\"1086\":10,\"1093\":2,\"1133\":2,\"1148\":2,\"1149\":2,\"1150\":2,\"1203\":2,\"1220\":2,\"1272\":2,\"1289\":1,\"1371\":1,\"1376\":1,\"1430\":6,\"1455\":1,\"1470\":4,\"1505\":2,\"1537\":3,\"1539\":2,\"1581\":2,\"1669\":2,\"1670\":7,\"1671\":7,\"1877\":1,\"1960\":1,\"1985\":1,\"1999\":1,\"2001\":2,\"2002\":6,\"2004\":2,\"2029\":2,\"2078\":4,\"2079\":2,\"2095\":5,\"2239\":2,\"2263\":5,\"2264\":1,\"2364\":1,\"2440\":1,\"2507\":1,\"2510\":2,\"2513\":1,\"2558\":1,\"2654\":1,\"2658\":1}}],[\"at\",{\"0\":{\"91\":1,\"149\":1},\"1\":{\"1\":1,\"5\":1,\"23\":4,\"25\":1,\"47\":1,\"57\":1,\"65\":1,\"69\":3,\"84\":1,\"85\":3,\"109\":1,\"113\":1,\"119\":3,\"132\":1,\"135\":1,\"141\":1,\"144\":1,\"149\":2,\"197\":1,\"198\":1,\"295\":2,\"627\":1,\"652\":1,\"672\":1,\"700\":2,\"742\":1,\"745\":1,\"746\":1,\"752\":1,\"756\":1,\"760\":1,\"778\":1,\"811\":1,\"831\":1,\"834\":2,\"870\":1,\"940\":1,\"1001\":3,\"1011\":2,\"1048\":2,\"1057\":1,\"1059\":1,\"1109\":1,\"1111\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1130\":1,\"1134\":1,\"1136\":1,\"1138\":2,\"1139\":2,\"1143\":1,\"1144\":1,\"1151\":1,\"1156\":1,\"1158\":1,\"1174\":1,\"1184\":1,\"1188\":1,\"1198\":1,\"1207\":1,\"1212\":1,\"1215\":1,\"1220\":1,\"1222\":1,\"1228\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1245\":1,\"1249\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1274\":1,\"1276\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1345\":1,\"1347\":1,\"1360\":1,\"1362\":1,\"1364\":1,\"1366\":1,\"1398\":6,\"1431\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1452\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1474\":1,\"1476\":1,\"1478\":1,\"1480\":1,\"1482\":1,\"1484\":2,\"1485\":1,\"1487\":1,\"1489\":1,\"1491\":1,\"1493\":1,\"1495\":1,\"1497\":1,\"1499\":1,\"1501\":1,\"1503\":1,\"1506\":1,\"1508\":1,\"1512\":1,\"1518\":1,\"1520\":1,\"1532\":1,\"1535\":1,\"1537\":2,\"1539\":1,\"1540\":1,\"1543\":1,\"1547\":1,\"1549\":1,\"1551\":1,\"1553\":1,\"1555\":1,\"1561\":1,\"1564\":1,\"1573\":1,\"1581\":2,\"1583\":1,\"1586\":1,\"1588\":1,\"1590\":1,\"1592\":1,\"1596\":1,\"1598\":1,\"1601\":2,\"1605\":1,\"1607\":1,\"1609\":1,\"1613\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1624\":1,\"1627\":1,\"1629\":1,\"1631\":1,\"1633\":1,\"1635\":1,\"1641\":1,\"1646\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1656\":1,\"1672\":1,\"1674\":1,\"1676\":1,\"1688\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1735\":1,\"1741\":1,\"1756\":1,\"1761\":1,\"1763\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1774\":1,\"1779\":1,\"1782\":1,\"1789\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1801\":1,\"1806\":1,\"1841\":1,\"1890\":1,\"1902\":1,\"1904\":1,\"1905\":1,\"1907\":1,\"1912\":1,\"1936\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1976\":1,\"1978\":1,\"1981\":1,\"1984\":1,\"1987\":1,\"1989\":1,\"1991\":1,\"1994\":1,\"1997\":1,\"2019\":1,\"2024\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2038\":1,\"2040\":1,\"2042\":1,\"2044\":1,\"2047\":1,\"2049\":1,\"2050\":1,\"2052\":1,\"2055\":1,\"2057\":1,\"2059\":1,\"2061\":1,\"2064\":1,\"2066\":1,\"2068\":1,\"2070\":1,\"2072\":1,\"2099\":1,\"2102\":1,\"2149\":1,\"2168\":1,\"2201\":1,\"2233\":1,\"2237\":1,\"2241\":1,\"2246\":1,\"2248\":1,\"2250\":1,\"2266\":1,\"2275\":1,\"2281\":1,\"2283\":1,\"2285\":1,\"2288\":1,\"2290\":1,\"2292\":1,\"2297\":1,\"2299\":1,\"2302\":1,\"2401\":1,\"2414\":1,\"2415\":1,\"2441\":1,\"2537\":1,\"2583\":1,\"2584\":1,\"2600\":1,\"2638\":1}}],[\"asapp\",{\"1\":{\"2520\":1}}],[\"async\",{\"1\":{\"2360\":2,\"2458\":2,\"2523\":2,\"2582\":2}}],[\"asteroidmodel\",{\"0\":{\"1454\":1},\"1\":{\"1454\":1}}],[\"asteroidfrontend\",{\"0\":{\"1132\":1},\"1\":{\"1132\":1}}],[\"asteroid\",{\"0\":{\"1132\":1,\"1454\":1},\"1\":{\"1132\":3,\"1452\":1,\"1454\":11,\"1695\":3}}],[\"astype\",{\"1\":{\"744\":2,\"876\":2,\"959\":1,\"2592\":1,\"2596\":1}}],[\"asv\",{\"1\":{\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1113\":2}}],[\"asvspoof2019\",{\"1\":{\"2396\":1,\"2532\":1}}],[\"asvspoof11\",{\"1\":{\"2394\":1,\"2530\":1}}],[\"asvspoof1\",{\"1\":{\"2394\":1,\"2396\":1,\"2398\":1,\"2401\":1,\"2530\":1,\"2532\":1,\"2537\":1,\"2542\":1}}],[\"asvspooftask\",{\"0\":{\"2098\":1},\"1\":{\"2098\":2}}],[\"asvspoofocsoftmaxloss\",{\"0\":{\"1108\":1},\"1\":{\"1108\":1}}],[\"asvspoofbinaryloss\",{\"0\":{\"1107\":1},\"1\":{\"1107\":1}}],[\"asvspoofamsoftmaxloss\",{\"0\":{\"1106\":1},\"1\":{\"1106\":1}}],[\"asvspoof\",{\"0\":{\"339\":1,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"2098\":1,\"2395\":1,\"2398\":1,\"2399\":1,\"2531\":1,\"2534\":1,\"2535\":1,\"2684\":1},\"1\":{\"339\":7,\"1106\":1,\"1107\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1114\":1,\"2098\":2,\"2393\":1,\"2394\":19,\"2395\":1,\"2396\":1,\"2397\":1,\"2398\":3,\"2399\":2,\"2400\":3,\"2401\":4,\"2403\":4,\"2405\":3,\"2529\":1,\"2530\":19,\"2531\":1,\"2532\":1,\"2533\":1,\"2534\":3,\"2535\":2,\"2536\":3,\"2537\":4,\"2539\":4,\"2541\":3,\"2542\":1,\"2543\":2}}],[\"asdict\",{\"1\":{\"765\":1,\"798\":1}}],[\"ascending\",{\"1\":{\"429\":2,\"1695\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":2,\"2012\":1}}],[\"ask\",{\"1\":{\"141\":1}}],[\"aswin\",{\"1\":{\"130\":1}}],[\"assistants\",{\"1\":{\"2467\":1}}],[\"assignemnt3\",{\"1\":{\"161\":1}}],[\"assigned\",{\"1\":{\"72\":2}}],[\"assign\",{\"1\":{\"143\":1}}],[\"assignments\",{\"1\":{\"2413\":1}}],[\"assignment\",{\"1\":{\"161\":1,\"2387\":1,\"2450\":3,\"2482\":1}}],[\"assignment1\",{\"1\":{\"140\":1,\"161\":1,\"244\":1}}],[\"assignment0\",{\"1\":{\"140\":1,\"161\":1,\"244\":1}}],[\"assignment8\",{\"1\":{\"140\":1,\"161\":1,\"244\":1}}],[\"assignment6\",{\"1\":{\"140\":1,\"161\":1,\"244\":1}}],[\"assignment3\",{\"1\":{\"140\":1,\"244\":1}}],[\"assignment4\",{\"1\":{\"140\":1,\"161\":1,\"244\":1}}],[\"assignment7\",{\"1\":{\"140\":1,\"161\":1,\"244\":1}}],[\"assignment5\",{\"1\":{\"140\":1,\"161\":1,\"244\":1}}],[\"assigns\",{\"1\":{\"52\":1}}],[\"associated\",{\"1\":{\"1145\":1,\"2467\":1}}],[\"association\",{\"1\":{\"130\":2}}],[\"assemble\",{\"1\":{\"692\":2}}],[\"asserterror\",{\"1\":{\"1522\":1}}],[\"assert\",{\"0\":{\"1002\":1},\"1\":{\"172\":1,\"928\":1,\"1002\":1,\"1385\":2,\"1387\":2,\"1391\":1,\"1418\":1,\"1423\":1,\"2210\":2,\"2359\":1,\"2456\":1,\"2460\":1,\"2501\":1,\"2521\":1,\"2522\":1,\"2580\":1,\"2581\":1,\"2607\":1,\"2615\":1,\"2624\":1,\"2633\":1}}],[\"assertion\",{\"1\":{\"59\":2}}],[\"assuming\",{\"1\":{\"1810\":1}}],[\"assumption\",{\"1\":{\"607\":1}}],[\"assumed\",{\"1\":{\"932\":1,\"1462\":1,\"1464\":1,\"1564\":1,\"1693\":1,\"1755\":1,\"1949\":1}}],[\"assume\",{\"1\":{\"47\":1,\"48\":1,\"237\":1,\"1116\":1,\"1138\":1,\"1804\":3,\"1851\":3,\"1878\":3,\"2001\":3,\"2002\":3,\"2004\":3,\"2086\":3,\"2087\":3,\"2090\":3,\"2095\":3,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2279\":3,\"2637\":1}}],[\"assumes\",{\"1\":{\"36\":1,\"59\":1,\"126\":1,\"607\":1,\"1274\":1,\"1276\":1}}],[\"as\",{\"0\":{\"100\":1,\"126\":1,\"170\":1,\"191\":1,\"2429\":1,\"2569\":1,\"2574\":1},\"1\":{\"1\":2,\"3\":1,\"14\":1,\"17\":1,\"18\":1,\"19\":1,\"21\":1,\"22\":3,\"25\":1,\"26\":1,\"28\":1,\"29\":2,\"30\":1,\"34\":1,\"45\":1,\"47\":1,\"48\":1,\"49\":2,\"52\":1,\"53\":1,\"57\":4,\"58\":1,\"64\":1,\"65\":1,\"72\":1,\"74\":1,\"76\":3,\"77\":2,\"78\":2,\"80\":1,\"84\":1,\"85\":5,\"90\":1,\"99\":1,\"102\":2,\"104\":2,\"107\":1,\"110\":1,\"112\":1,\"118\":1,\"119\":1,\"120\":1,\"142\":1,\"143\":1,\"149\":1,\"166\":1,\"170\":1,\"171\":3,\"173\":1,\"174\":1,\"175\":3,\"177\":2,\"185\":2,\"192\":1,\"193\":2,\"194\":2,\"197\":1,\"201\":1,\"202\":1,\"203\":2,\"204\":3,\"217\":1,\"224\":1,\"231\":1,\"235\":1,\"238\":4,\"239\":2,\"240\":1,\"241\":1,\"286\":1,\"295\":1,\"296\":1,\"564\":1,\"613\":1,\"621\":1,\"692\":1,\"693\":1,\"697\":2,\"706\":2,\"710\":1,\"711\":4,\"743\":1,\"745\":3,\"746\":3,\"754\":1,\"760\":2,\"764\":1,\"778\":1,\"797\":2,\"820\":1,\"821\":1,\"826\":1,\"834\":1,\"899\":1,\"901\":1,\"926\":1,\"984\":1,\"992\":1,\"995\":1,\"1001\":1,\"1008\":2,\"1011\":1,\"1013\":2,\"1015\":3,\"1025\":1,\"1028\":6,\"1058\":1,\"1063\":1,\"1142\":3,\"1145\":1,\"1149\":1,\"1150\":1,\"1160\":1,\"1161\":1,\"1162\":2,\"1163\":1,\"1164\":1,\"1177\":1,\"1186\":4,\"1187\":8,\"1198\":8,\"1202\":8,\"1210\":3,\"1228\":1,\"1241\":1,\"1243\":2,\"1248\":1,\"1252\":1,\"1253\":2,\"1254\":1,\"1269\":1,\"1279\":2,\"1286\":7,\"1287\":7,\"1298\":1,\"1299\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1337\":2,\"1339\":1,\"1343\":1,\"1345\":1,\"1347\":1,\"1349\":2,\"1350\":2,\"1369\":1,\"1377\":1,\"1383\":1,\"1407\":1,\"1420\":1,\"1424\":1,\"1427\":1,\"1429\":1,\"1436\":1,\"1473\":1,\"1511\":1,\"1543\":1,\"1547\":1,\"1552\":1,\"1576\":1,\"1577\":1,\"1618\":1,\"1619\":1,\"1644\":1,\"1656\":1,\"1659\":1,\"1660\":2,\"1661\":2,\"1662\":2,\"1671\":1,\"1688\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1712\":1,\"1715\":1,\"1719\":2,\"1731\":1,\"1732\":1,\"1756\":1,\"1758\":1,\"1759\":1,\"1773\":2,\"1798\":1,\"1800\":1,\"1804\":3,\"1837\":1,\"1842\":1,\"1851\":3,\"1864\":1,\"1874\":1,\"1878\":4,\"1905\":1,\"1912\":1,\"1917\":4,\"1929\":1,\"1941\":1,\"1946\":1,\"1947\":2,\"1963\":1,\"1968\":1,\"1980\":1,\"2001\":4,\"2002\":3,\"2004\":3,\"2012\":1,\"2019\":1,\"2021\":1,\"2046\":2,\"2077\":1,\"2082\":2,\"2086\":3,\"2087\":3,\"2090\":3,\"2095\":3,\"2096\":4,\"2098\":4,\"2099\":7,\"2100\":4,\"2101\":4,\"2102\":6,\"2103\":4,\"2104\":4,\"2105\":4,\"2107\":4,\"2108\":4,\"2109\":4,\"2110\":4,\"2111\":4,\"2112\":4,\"2113\":4,\"2114\":4,\"2115\":4,\"2116\":4,\"2117\":4,\"2118\":4,\"2121\":1,\"2122\":1,\"2125\":1,\"2142\":1,\"2156\":1,\"2168\":1,\"2170\":1,\"2193\":1,\"2198\":1,\"2209\":1,\"2216\":1,\"2217\":1,\"2235\":1,\"2240\":2,\"2243\":3,\"2244\":3,\"2255\":3,\"2263\":3,\"2264\":3,\"2275\":2,\"2277\":1,\"2278\":2,\"2279\":5,\"2281\":1,\"2309\":1,\"2354\":1,\"2359\":2,\"2360\":2,\"2372\":1,\"2373\":2,\"2380\":1,\"2384\":1,\"2385\":7,\"2386\":6,\"2387\":5,\"2388\":1,\"2389\":2,\"2394\":1,\"2395\":3,\"2397\":2,\"2398\":1,\"2400\":4,\"2401\":2,\"2403\":1,\"2408\":2,\"2409\":1,\"2414\":1,\"2419\":1,\"2421\":1,\"2422\":2,\"2430\":4,\"2440\":2,\"2449\":2,\"2452\":1,\"2456\":3,\"2458\":2,\"2459\":2,\"2460\":3,\"2465\":2,\"2466\":1,\"2467\":1,\"2468\":2,\"2474\":1,\"2481\":2,\"2498\":4,\"2500\":1,\"2503\":2,\"2514\":3,\"2520\":1,\"2521\":3,\"2522\":1,\"2523\":2,\"2524\":1,\"2525\":2,\"2530\":1,\"2531\":3,\"2533\":2,\"2534\":1,\"2536\":4,\"2537\":2,\"2539\":1,\"2544\":1,\"2545\":2,\"2555\":4,\"2558\":1,\"2564\":1,\"2567\":1,\"2568\":3,\"2569\":2,\"2574\":1,\"2580\":2,\"2581\":1,\"2582\":2,\"2584\":4,\"2585\":3,\"2591\":1,\"2592\":1,\"2600\":5,\"2616\":4,\"2617\":1,\"2634\":4,\"2635\":1,\"2638\":2,\"2644\":1,\"2649\":1,\"2659\":3}}],[\"asrinit\",{\"1\":{\"2454\":1,\"2455\":2,\"2460\":3,\"2461\":2}}],[\"asrinterface|mtinterface|ttsinterface\",{\"1\":{\"646\":1}}],[\"asrinterface\",{\"0\":{\"676\":1},\"1\":{\"646\":1,\"676\":4,\"709\":1,\"781\":2,\"812\":2}}],[\"asru\",{\"1\":{\"1670\":1,\"1671\":1}}],[\"asrtask\",{\"0\":{\"2096\":1},\"1\":{\"112\":1,\"2096\":2,\"2111\":1}}],[\"asrtransducertask\",{\"0\":{\"2097\":1},\"1\":{\"112\":1,\"2097\":7}}],[\"asr|tts|vc\",{\"1\":{\"27\":1}}],[\"asr1\",{\"1\":{\"1\":1,\"2\":1,\"3\":3,\"4\":1,\"15\":1,\"16\":1,\"28\":1,\"85\":7,\"99\":1,\"110\":2,\"168\":2,\"169\":1,\"171\":1,\"175\":1,\"179\":1,\"180\":1,\"181\":2,\"183\":1,\"184\":1,\"185\":1,\"186\":2,\"187\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"193\":1,\"194\":1,\"197\":2,\"2372\":2,\"2375\":5,\"2377\":6,\"2385\":3,\"2387\":2,\"2429\":2,\"2432\":2,\"2436\":5,\"2440\":1,\"2554\":2,\"2558\":6,\"2559\":2,\"2562\":5,\"2566\":3,\"2567\":3,\"2571\":1,\"2574\":1,\"2576\":1,\"2584\":2,\"2585\":3,\"2587\":1}}],[\"asr\",{\"0\":{\"103\":1,\"112\":1,\"155\":1,\"191\":1,\"245\":1,\"247\":1,\"249\":1,\"251\":1,\"273\":1,\"301\":1,\"307\":1,\"315\":1,\"321\":1,\"327\":1,\"333\":1,\"623\":2,\"624\":2,\"625\":2,\"626\":2,\"627\":2,\"628\":2,\"629\":2,\"630\":2,\"631\":2,\"632\":2,\"633\":2,\"634\":2,\"635\":2,\"636\":2,\"637\":2,\"638\":2,\"639\":2,\"640\":2,\"641\":2,\"642\":2,\"643\":2,\"644\":2,\"645\":2,\"646\":2,\"647\":2,\"648\":2,\"649\":2,\"650\":1,\"651\":2,\"652\":2,\"653\":2,\"654\":2,\"655\":2,\"656\":2,\"657\":2,\"658\":2,\"676\":1,\"709\":1,\"742\":1,\"794\":1,\"811\":1,\"872\":2,\"880\":1,\"890\":1,\"896\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1086\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1115\":1,\"1116\":1,\"1117\":1,\"1119\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":1,\"1129\":1,\"1130\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":1,\"1142\":1,\"1143\":1,\"1145\":1,\"1146\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1153\":1,\"1154\":1,\"1155\":1,\"1156\":1,\"1158\":1,\"1160\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":2,\"1173\":1,\"1174\":1,\"1176\":1,\"1177\":1,\"1178\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1186\":1,\"1187\":1,\"1188\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1204\":1,\"1205\":1,\"1206\":1,\"1207\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1227\":1,\"1229\":1,\"1231\":1,\"1233\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1244\":1,\"1245\":1,\"1247\":1,\"1248\":1,\"1249\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":1,\"1288\":1,\"1289\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1346\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"2096\":1,\"2097\":1,\"2375\":1,\"2443\":2,\"2499\":1,\"2558\":1,\"2576\":1,\"2577\":1,\"2583\":1,\"2617\":1,\"2635\":1,\"2673\":1,\"2682\":1,\"2685\":1,\"2727\":1},\"1\":{\"1\":1,\"11\":6,\"15\":1,\"16\":3,\"19\":2,\"20\":1,\"24\":1,\"25\":2,\"34\":2,\"35\":1,\"36\":2,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"48\":1,\"56\":1,\"57\":1,\"58\":2,\"59\":1,\"62\":5,\"63\":2,\"64\":2,\"65\":1,\"66\":6,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":2,\"72\":4,\"74\":1,\"75\":1,\"76\":2,\"77\":1,\"78\":1,\"80\":1,\"81\":1,\"82\":1,\"85\":1,\"92\":5,\"96\":5,\"98\":1,\"99\":1,\"102\":1,\"103\":1,\"104\":1,\"108\":3,\"110\":10,\"111\":3,\"112\":5,\"113\":4,\"115\":1,\"124\":1,\"130\":2,\"134\":1,\"136\":1,\"139\":2,\"140\":3,\"155\":6,\"161\":1,\"164\":2,\"165\":2,\"166\":1,\"167\":2,\"168\":1,\"170\":1,\"173\":3,\"175\":1,\"177\":1,\"178\":2,\"179\":1,\"194\":2,\"196\":1,\"204\":1,\"217\":4,\"224\":4,\"231\":4,\"234\":1,\"244\":3,\"245\":2,\"247\":2,\"249\":4,\"251\":3,\"259\":2,\"301\":7,\"307\":11,\"315\":7,\"321\":7,\"327\":7,\"333\":9,\"397\":1,\"398\":6,\"422\":3,\"443\":16,\"491\":2,\"501\":1,\"549\":1,\"623\":4,\"624\":2,\"625\":2,\"626\":2,\"627\":2,\"628\":2,\"629\":5,\"630\":4,\"631\":4,\"632\":4,\"633\":4,\"634\":4,\"635\":2,\"636\":4,\"637\":2,\"638\":4,\"639\":2,\"640\":3,\"641\":4,\"642\":2,\"643\":2,\"644\":2,\"645\":2,\"646\":4,\"647\":4,\"648\":4,\"649\":4,\"650\":5,\"651\":4,\"652\":4,\"653\":4,\"654\":4,\"655\":4,\"656\":4,\"657\":4,\"658\":2,\"676\":3,\"692\":1,\"693\":4,\"709\":2,\"727\":2,\"728\":2,\"742\":2,\"794\":1,\"811\":1,\"812\":1,\"816\":1,\"836\":1,\"872\":5,\"880\":2,\"889\":1,\"890\":2,\"896\":1,\"987\":2,\"1003\":2,\"1004\":2,\"1005\":4,\"1028\":3,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1050\":1,\"1051\":1,\"1052\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1061\":2,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1067\":2,\"1068\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":2,\"1083\":1,\"1084\":2,\"1085\":1,\"1086\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":2,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1115\":1,\"1116\":1,\"1117\":2,\"1119\":2,\"1121\":1,\"1123\":1,\"1125\":1,\"1127\":2,\"1129\":1,\"1130\":2,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":2,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":1,\"1142\":2,\"1143\":1,\"1145\":2,\"1148\":1,\"1149\":1,\"1150\":2,\"1151\":1,\"1153\":1,\"1154\":1,\"1155\":1,\"1156\":1,\"1158\":2,\"1160\":2,\"1161\":1,\"1162\":2,\"1163\":2,\"1164\":2,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":3,\"1172\":2,\"1173\":1,\"1174\":1,\"1176\":1,\"1177\":2,\"1178\":1,\"1179\":1,\"1180\":2,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":2,\"1186\":2,\"1187\":1,\"1188\":2,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1204\":2,\"1205\":3,\"1206\":2,\"1207\":1,\"1209\":1,\"1210\":2,\"1211\":2,\"1212\":1,\"1214\":1,\"1215\":1,\"1217\":2,\"1218\":1,\"1219\":1,\"1220\":2,\"1222\":2,\"1224\":2,\"1225\":1,\"1226\":1,\"1227\":1,\"1229\":1,\"1231\":1,\"1233\":2,\"1235\":1,\"1237\":1,\"1239\":3,\"1241\":2,\"1244\":2,\"1245\":2,\"1247\":2,\"1248\":2,\"1249\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":2,\"1257\":2,\"1259\":1,\"1261\":1,\"1263\":1,\"1265\":1,\"1267\":1,\"1269\":3,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1276\":1,\"1278\":1,\"1279\":2,\"1280\":2,\"1282\":1,\"1284\":1,\"1286\":1,\"1287\":2,\"1288\":1,\"1289\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":2,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":2,\"1302\":1,\"1303\":1,\"1304\":2,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":2,\"1312\":1,\"1313\":1,\"1314\":2,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":2,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":3,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":2,\"1328\":2,\"1329\":2,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":2,\"1338\":1,\"1339\":2,\"1340\":1,\"1341\":2,\"1342\":1,\"1343\":2,\"1344\":1,\"1346\":1,\"1348\":1,\"1349\":2,\"1350\":2,\"1351\":2,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":2,\"1356\":2,\"1357\":2,\"1358\":2,\"1359\":1,\"1398\":1,\"1552\":2,\"1892\":1,\"1893\":1,\"1917\":1,\"1969\":1,\"1984\":2,\"1991\":1,\"2027\":1,\"2046\":1,\"2076\":2,\"2096\":2,\"2097\":3,\"2137\":1,\"2250\":1,\"2294\":1,\"2357\":26,\"2358\":1,\"2359\":5,\"2360\":2,\"2372\":3,\"2373\":6,\"2375\":15,\"2377\":13,\"2378\":2,\"2380\":1,\"2382\":1,\"2384\":2,\"2385\":3,\"2387\":1,\"2394\":1,\"2395\":1,\"2398\":1,\"2400\":1,\"2406\":1,\"2407\":1,\"2410\":2,\"2418\":1,\"2419\":1,\"2420\":1,\"2429\":11,\"2430\":4,\"2431\":1,\"2433\":3,\"2436\":12,\"2437\":2,\"2440\":35,\"2441\":17,\"2444\":4,\"2445\":4,\"2446\":4,\"2455\":2,\"2467\":4,\"2468\":3,\"2472\":10,\"2473\":1,\"2474\":4,\"2476\":10,\"2478\":9,\"2481\":1,\"2492\":1,\"2498\":2,\"2499\":1,\"2500\":17,\"2520\":6,\"2521\":4,\"2530\":1,\"2531\":1,\"2534\":1,\"2536\":1,\"2554\":3,\"2555\":6,\"2558\":16,\"2559\":25,\"2560\":2,\"2562\":12,\"2563\":2,\"2564\":24,\"2566\":1,\"2568\":1,\"2569\":3,\"2570\":2,\"2572\":10,\"2573\":3,\"2576\":3,\"2578\":26,\"2579\":1,\"2580\":5,\"2581\":2,\"2582\":2,\"2583\":2,\"2584\":22,\"2585\":13,\"2586\":2,\"2589\":2,\"2590\":2,\"2591\":1,\"2597\":1,\"2599\":2,\"2600\":12,\"2616\":2,\"2617\":3,\"2618\":1,\"2628\":1,\"2634\":2,\"2635\":4,\"2648\":10,\"2649\":4}}],[\"a\",{\"0\":{\"54\":1,\"88\":1,\"91\":2,\"124\":1,\"126\":1,\"128\":1,\"170\":1,\"210\":1,\"214\":1,\"222\":1,\"229\":1,\"2379\":1,\"2566\":1,\"2569\":1,\"2573\":1,\"2585\":1,\"2587\":1,\"2636\":1,\"2641\":1},\"1\":{\"1\":3,\"4\":3,\"5\":7,\"14\":1,\"16\":3,\"17\":1,\"19\":1,\"21\":7,\"23\":2,\"24\":1,\"25\":7,\"28\":6,\"29\":3,\"30\":2,\"38\":1,\"45\":1,\"46\":1,\"47\":4,\"48\":4,\"49\":1,\"51\":6,\"52\":2,\"53\":2,\"54\":3,\"56\":5,\"57\":3,\"58\":12,\"59\":1,\"60\":7,\"62\":2,\"64\":2,\"69\":1,\"72\":1,\"79\":1,\"80\":4,\"83\":1,\"84\":4,\"85\":2,\"92\":2,\"94\":1,\"96\":1,\"98\":1,\"99\":2,\"102\":2,\"107\":1,\"109\":1,\"112\":2,\"113\":1,\"115\":7,\"119\":2,\"120\":1,\"121\":1,\"124\":3,\"127\":1,\"134\":1,\"135\":2,\"136\":1,\"142\":1,\"143\":2,\"144\":3,\"148\":2,\"161\":2,\"162\":2,\"163\":1,\"164\":1,\"166\":2,\"167\":1,\"170\":2,\"173\":1,\"177\":3,\"178\":1,\"182\":1,\"187\":2,\"192\":1,\"196\":1,\"197\":1,\"198\":3,\"200\":1,\"202\":1,\"206\":1,\"234\":2,\"235\":3,\"237\":1,\"241\":1,\"243\":2,\"245\":1,\"249\":1,\"253\":1,\"255\":1,\"257\":1,\"259\":1,\"261\":1,\"263\":1,\"265\":1,\"267\":1,\"269\":1,\"295\":3,\"301\":1,\"503\":2,\"525\":1,\"538\":1,\"544\":2,\"551\":3,\"557\":3,\"568\":1,\"576\":1,\"582\":1,\"594\":1,\"596\":1,\"599\":1,\"604\":1,\"605\":3,\"607\":4,\"610\":2,\"612\":3,\"614\":1,\"619\":1,\"620\":1,\"621\":1,\"624\":1,\"626\":2,\"632\":1,\"635\":2,\"641\":1,\"652\":3,\"672\":1,\"676\":1,\"679\":1,\"692\":1,\"693\":1,\"695\":2,\"697\":3,\"698\":2,\"699\":2,\"701\":1,\"703\":1,\"706\":6,\"710\":2,\"712\":3,\"713\":1,\"725\":1,\"727\":2,\"728\":2,\"732\":2,\"736\":2,\"743\":4,\"745\":1,\"746\":1,\"747\":2,\"748\":2,\"750\":1,\"754\":4,\"762\":1,\"763\":2,\"764\":1,\"767\":1,\"774\":1,\"778\":1,\"781\":1,\"782\":1,\"786\":1,\"791\":1,\"794\":2,\"796\":2,\"797\":7,\"802\":1,\"803\":1,\"807\":1,\"811\":1,\"815\":3,\"816\":1,\"820\":1,\"821\":2,\"823\":1,\"826\":2,\"835\":1,\"836\":1,\"837\":1,\"856\":1,\"857\":1,\"866\":1,\"875\":4,\"892\":1,\"895\":2,\"905\":1,\"917\":2,\"918\":1,\"919\":1,\"936\":1,\"944\":2,\"981\":1,\"987\":3,\"989\":3,\"996\":1,\"997\":1,\"998\":1,\"1003\":1,\"1004\":3,\"1011\":5,\"1013\":1,\"1025\":5,\"1028\":7,\"1034\":1,\"1037\":1,\"1046\":1,\"1047\":1,\"1048\":3,\"1049\":1,\"1050\":1,\"1052\":3,\"1053\":2,\"1054\":1,\"1055\":1,\"1056\":1,\"1062\":1,\"1064\":1,\"1065\":1,\"1066\":2,\"1067\":1,\"1068\":1,\"1069\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":2,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1096\":1,\"1113\":1,\"1116\":1,\"1132\":2,\"1136\":1,\"1143\":2,\"1145\":1,\"1154\":2,\"1160\":4,\"1161\":4,\"1162\":6,\"1163\":4,\"1164\":4,\"1177\":4,\"1186\":9,\"1187\":7,\"1190\":1,\"1198\":3,\"1199\":1,\"1202\":7,\"1210\":7,\"1217\":2,\"1221\":1,\"1222\":1,\"1226\":1,\"1241\":4,\"1242\":1,\"1243\":1,\"1244\":1,\"1245\":10,\"1246\":1,\"1247\":1,\"1248\":8,\"1252\":9,\"1253\":12,\"1254\":9,\"1255\":4,\"1257\":1,\"1269\":1,\"1274\":1,\"1278\":1,\"1279\":6,\"1282\":1,\"1286\":6,\"1287\":6,\"1301\":3,\"1304\":3,\"1327\":1,\"1332\":3,\"1333\":1,\"1337\":3,\"1339\":2,\"1341\":2,\"1342\":1,\"1343\":2,\"1349\":2,\"1350\":2,\"1352\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1383\":2,\"1389\":1,\"1391\":1,\"1394\":1,\"1395\":1,\"1396\":1,\"1397\":1,\"1400\":2,\"1406\":6,\"1409\":1,\"1414\":1,\"1416\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1421\":2,\"1422\":1,\"1424\":1,\"1427\":3,\"1429\":1,\"1430\":1,\"1436\":2,\"1451\":4,\"1452\":7,\"1466\":1,\"1482\":2,\"1511\":2,\"1514\":5,\"1528\":1,\"1529\":1,\"1530\":1,\"1537\":1,\"1551\":4,\"1553\":4,\"1557\":4,\"1561\":1,\"1585\":4,\"1595\":1,\"1603\":1,\"1612\":4,\"1615\":4,\"1623\":5,\"1637\":4,\"1638\":4,\"1639\":1,\"1640\":1,\"1643\":1,\"1644\":3,\"1650\":1,\"1670\":2,\"1671\":3,\"1679\":2,\"1693\":3,\"1695\":7,\"1696\":2,\"1697\":1,\"1698\":2,\"1701\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1712\":3,\"1715\":4,\"1716\":1,\"1719\":1,\"1727\":1,\"1735\":5,\"1739\":5,\"1740\":1,\"1742\":1,\"1751\":1,\"1752\":1,\"1755\":3,\"1758\":1,\"1759\":1,\"1768\":1,\"1773\":2,\"1785\":4,\"1791\":1,\"1797\":1,\"1798\":1,\"1803\":1,\"1805\":1,\"1807\":1,\"1810\":6,\"1837\":1,\"1850\":1,\"1857\":1,\"1863\":1,\"1864\":1,\"1868\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1883\":1,\"1896\":1,\"1905\":6,\"1915\":1,\"1917\":4,\"1921\":2,\"1923\":1,\"1927\":1,\"1928\":1,\"1932\":1,\"1933\":1,\"1934\":1,\"1935\":1,\"1941\":1,\"1943\":1,\"1962\":1,\"1963\":2,\"1968\":1,\"1971\":1,\"1980\":1,\"2001\":2,\"2002\":2,\"2003\":1,\"2004\":1,\"2012\":1,\"2019\":1,\"2022\":2,\"2046\":2,\"2050\":1,\"2052\":1,\"2077\":1,\"2078\":1,\"2081\":2,\"2082\":2,\"2083\":2,\"2090\":4,\"2095\":1,\"2096\":1,\"2098\":1,\"2099\":2,\"2100\":1,\"2101\":1,\"2102\":2,\"2103\":1,\"2104\":1,\"2105\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2121\":1,\"2122\":1,\"2125\":2,\"2137\":1,\"2143\":1,\"2149\":2,\"2151\":1,\"2153\":1,\"2154\":2,\"2156\":1,\"2157\":1,\"2162\":1,\"2168\":3,\"2170\":3,\"2176\":5,\"2197\":1,\"2201\":1,\"2235\":1,\"2236\":1,\"2240\":2,\"2243\":1,\"2244\":1,\"2252\":1,\"2255\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2275\":1,\"2277\":1,\"2278\":2,\"2279\":1,\"2314\":4,\"2321\":1,\"2323\":1,\"2343\":2,\"2344\":2,\"2354\":2,\"2355\":2,\"2358\":1,\"2361\":1,\"2368\":1,\"2372\":5,\"2373\":4,\"2377\":1,\"2383\":2,\"2384\":4,\"2385\":5,\"2386\":1,\"2387\":4,\"2388\":3,\"2391\":2,\"2392\":1,\"2393\":2,\"2394\":6,\"2395\":3,\"2396\":2,\"2398\":2,\"2399\":1,\"2400\":3,\"2401\":2,\"2403\":1,\"2410\":4,\"2411\":2,\"2412\":1,\"2414\":2,\"2421\":3,\"2422\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2427\":2,\"2429\":3,\"2430\":3,\"2432\":1,\"2433\":2,\"2437\":2,\"2438\":1,\"2439\":1,\"2440\":1,\"2441\":2,\"2446\":1,\"2450\":3,\"2451\":2,\"2452\":3,\"2457\":1,\"2467\":6,\"2468\":2,\"2473\":2,\"2482\":2,\"2486\":1,\"2490\":1,\"2492\":1,\"2494\":1,\"2499\":2,\"2500\":2,\"2501\":1,\"2516\":1,\"2518\":2,\"2520\":2,\"2524\":3,\"2527\":2,\"2528\":1,\"2529\":3,\"2530\":6,\"2531\":3,\"2532\":2,\"2534\":2,\"2535\":1,\"2536\":3,\"2537\":2,\"2539\":1,\"2542\":2,\"2543\":3,\"2544\":3,\"2545\":1,\"2546\":1,\"2547\":1,\"2548\":1,\"2550\":3,\"2554\":3,\"2555\":4,\"2558\":3,\"2559\":1,\"2563\":2,\"2564\":6,\"2566\":1,\"2568\":4,\"2569\":1,\"2570\":1,\"2571\":1,\"2573\":3,\"2575\":1,\"2576\":1,\"2579\":1,\"2583\":3,\"2584\":13,\"2585\":7,\"2586\":3,\"2587\":1,\"2597\":1,\"2599\":1,\"2600\":1,\"2601\":1,\"2605\":1,\"2609\":1,\"2617\":4,\"2618\":3,\"2622\":1,\"2626\":1,\"2628\":1,\"2635\":4,\"2639\":1,\"2640\":1,\"2642\":1,\"2644\":1,\"2646\":1,\"2650\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n,id:o}})=>{const u=bt[s];e==="suggest"?self.postMessage([e,o,tt(t,u,n)]):e==="search"?self.postMessage([e,o,Z(t,u,n)]):self.postMessage({suggestions:[e,o,tt(t,u,n)],results:[e,o,Z(t,u,n)]})};
//# sourceMappingURL=index.js.map

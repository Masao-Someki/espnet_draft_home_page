import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,c as a,f as t,b as e,d as s,e as p,w as i,a as c,o as d}from"./app-KOUU_Wij.js";const l={},h=e("h1",{id:"espnet2-asr-encoder-whisper-encoder-openaiwhisperencoder",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-asr-encoder-whisper-encoder-openaiwhisperencoder"},[e("span",null,"espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder")])],-1),m=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),s(" espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder"),e("span",{class:"small-bracket"},"(input_size: int = 1, dropout_rate: float = 0.0, whisper_model: str = 'small', download_dir: str | None = None, use_specaug: bool = False, specaug_conf: dict | None = None, do_pad_trim: bool = False)")])],-1),_=e("code",null,"AbsEncoder",-1),u=c('<p>Transformer-based Speech Encoder from OpenAIâ€™s Whisper Model:</p><p>URL: <a href="https://github.com/openai/whisper" target="_blank" rel="noopener noreferrer">https://github.com/openai/whisper</a></p><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None)</span></p></div><p>Defines the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><h5 id="note" tabindex="-1"><a class="header-anchor" href="#note"><span>NOTE</span></a></h5><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p><div class="custom-h4"><p>log_mel_spectrogram<span class="small-bracket">(audio: Tensor, ilens: Tensor | None = None)</span></p></div><p>Use log-mel spectrogram computation native to Whisper training</p><div class="custom-h4"><p>output_size()</p></div><div class="custom-h4"><p>pad_or_trim<span class="small-bracket">(array: Tensor, length: int, axis: int = -1)</span></p></div><p>Pad or trim the audio array to N_SAMPLES.</p><p>Used in zero-shot inference cases.</p><div class="custom-h4"><p>training <em>: bool</em></p></div><div class="custom-h4"><p>whisper_encode<span class="small-bracket">(input: Tensor, ilens: Tensor | None = None)</span></p></div>',16);function f(b,w){const n=r("RouteLink");return d(),a("div",null,[t(" _espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder "),h,m,e("p",null,[s("Bases: "),p(n,{to:"/guide/espnet2/asr/AbsEncoder.html#espnet2.asr.encoder.abs_encoder.AbsEncoder"},{default:i(()=>[_]),_:1})]),u])}const N=o(l,[["render",f],["__file","OpenAIWhisperEncoder.html.vue"]]),E=JSON.parse(`{"path":"/guide/espnet2/asr/OpenAIWhisperEncoder.html","title":"espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.81,"words":243},"filePathRelative":"guide/espnet2/asr/OpenAIWhisperEncoder.md","excerpt":"<!-- _espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder -->\\n<h1>espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder<span class=\\"small-bracket\\">(input_size: int = 1, dropout_rate: float = 0.0, whisper_model: str = 'small', download_dir: str | None = None, use_specaug: bool = False, specaug_conf: dict | None = None, do_pad_trim: bool = False)</span></p></div>"}`);export{N as comp,E as data};

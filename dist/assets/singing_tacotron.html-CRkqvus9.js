import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,c as l,f as n,b as e,d as t,e as a,w as m,a as o,o as g}from"./app-KOUU_Wij.js";const c={},d=e("h1",{id:"espnet2-svs-singing-tacotron-singing-tacotron-singing-tacotron",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-svs-singing-tacotron-singing-tacotron-singing-tacotron"},[e("span",null,"espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron")])],-1),_=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron"),e("span",{class:"small-bracket"},"(idim: int, odim: int, midi_dim: int = 129, duration_dim: int = 500, embed_dim: int = 512, elayers: int = 1, eunits: int = 512, econv_layers: int = 3, econv_chans: int = 512, econv_filts: int = 5, atype: str = 'GDCA', adim: int = 512, aconv_chans: int = 32, aconv_filts: int = 15, cumulate_att_w: bool = True, dlayers: int = 2, dunits: int = 1024, prenet_layers: int = 2, prenet_units: int = 256, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, output_activation: str | None = None, use_batch_norm: bool = True, use_concate: bool = True, use_residual: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128)"),t(", gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, dropout_rate: float = 0.5, zoneout_rate: float = 0.1, use_masking: bool = True, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1', use_guided_attn_loss: bool = True, guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)")])],-1),u=e("code",null,"AbsSVS",-1),p=o('<p>singing_Tacotron module for end-to-end singing-voice-synthesis.</p><p>This is a module of Spectrogram prediction network in Singing Tacotron described in</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>`Singing-Tacotron: Global Duration Control Attention and</span></span>\n<span class="line"><span>Dynamic Filter for End-to-end Singing Voice Synthesis`_</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>, which learn accurate alignment information automatically.</p>',4),h=o('<p>Filter for End-to-end Singing Voice Synthesis`: : <a href="https://arxiv.org/pdf/2202.07907v1.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2202.07907v1.pdf</a></p><p>Initialize Singing Tacotron module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>idim</strong> (<em>int</em>) – Dimension of the label inputs.</li><li><strong>odim</strong> – (int) Dimension of the outputs.</li><li><strong>embed_dim</strong> (<em>int</em>) – Dimension of the token embedding.</li><li><strong>elayers</strong> (<em>int</em>) – Number of encoder blstm layers.</li><li><strong>eunits</strong> (<em>int</em>) – Number of encoder blstm units.</li><li><strong>econv_layers</strong> (<em>int</em>) – Number of encoder conv layers.</li><li><strong>econv_filts</strong> (<em>int</em>) – Number of encoder conv filter size.</li><li><strong>econv_chans</strong> (<em>int</em>) – Number of encoder conv filter channels.</li><li><strong>dlayers</strong> (<em>int</em>) – Number of decoder lstm layers.</li><li><strong>dunits</strong> (<em>int</em>) – Number of decoder lstm units.</li><li><strong>prenet_layers</strong> (<em>int</em>) – Number of prenet layers.</li><li><strong>prenet_units</strong> (<em>int</em>) – Number of prenet units.</li><li><strong>postnet_layers</strong> (<em>int</em>) – Number of postnet layers.</li><li><strong>postnet_filts</strong> (<em>int</em>) – Number of postnet filter size.</li><li><strong>postnet_chans</strong> (<em>int</em>) – Number of postnet filter channels.</li><li><strong>output_activation</strong> (<em>str</em>) – Name of activation function for outputs.</li><li><strong>adim</strong> (<em>int</em>) – Number of dimension of mlp in attention.</li><li><strong>aconv_chans</strong> (<em>int</em>) – Number of attention conv filter channels.</li><li><strong>aconv_filts</strong> (<em>int</em>) – Number of attention conv filter size.</li><li><strong>cumulate_att_w</strong> (<em>bool</em>) – Whether to cumulate previous attention weight.</li><li><strong>use_batch_norm</strong> (<em>bool</em>) – Whether to use batch normalization.</li><li><strong>use_concate</strong> (<em>bool</em>) – Whether to concat enc outputs w/ dec lstm outputs.</li><li><strong>reduction_factor</strong> (<em>int</em>) – Reduction factor.</li><li><strong>spks</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Number of speakers. If set to &gt; 1, assume that the sids will be provided as the input and use sid embedding layer.</li><li><strong>langs</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Number of languages. If set to &gt; 1, assume that the lids will be provided as the input and use sid embedding layer.</li><li><strong>spk_embed_dim</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Speaker embedding dimension. If set to &gt; 0, assume that spembs will be provided as the input.</li><li><strong>spk_embed_integration_type</strong> (<em>str</em>) – How to integrate speaker embedding.</li><li><strong>use_gst</strong> (<em>str</em>) – Whether to use global style token.</li><li><strong>gst_tokens</strong> (<em>int</em>) – Number of GST embeddings.</li><li><strong>gst_heads</strong> (<em>int</em>) – Number of heads in GST multihead attention.</li><li><strong>gst_conv_layers</strong> (<em>int</em>) – Number of conv layers in GST.</li><li><strong>gst_conv_chans_list</strong> – (Sequence[int]): List of the number of channels of conv layers in GST.</li><li><strong>gst_conv_kernel_size</strong> (<em>int</em>) – Kernel size of conv layers in GST.</li><li><strong>gst_conv_stride</strong> (<em>int</em>) – Stride size of conv layers in GST.</li><li><strong>gst_gru_layers</strong> (<em>int</em>) – Number of GRU layers in GST.</li><li><strong>gst_gru_units</strong> (<em>int</em>) – Number of GRU units in GST.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li><li><strong>zoneout_rate</strong> (<em>float</em>) – Zoneout rate.</li><li><strong>use_masking</strong> (<em>bool</em>) – Whether to mask padded part in loss calculation.</li><li><strong>use_weighted_masking</strong> (<em>bool</em>) – Whether to apply weighted masking in loss calculation.</li><li><strong>bce_pos_weight</strong> (<em>float</em>) – Weight of positive sample of stop token (only for use_masking=True).</li><li><strong>loss_type</strong> (<em>str</em>) – Loss function type (“L1”, “L2”, or “L1+L2”).</li><li><strong>use_guided_attn_loss</strong> (<em>bool</em>) – Whether to use guided attention loss.</li><li><strong>guided_attn_loss_sigma</strong> (<em>float</em>) – Sigma in guided attention loss.</li><li><strong>guided_attn_loss_lambda</strong> (<em>float</em>) – Lambda in guided attention loss.</li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, ying: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, flag_IsValid=False)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><ul><li><p><strong>text</strong> (<em>LongTensor</em>) – Batch of padded character ids (B, T_text).</p></li><li><p><strong>text_lengths</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</p></li><li><p><strong>feats</strong> (<em>Tensor</em>) – Batch of padded target features (B, T_feats, odim).</p></li><li><p><strong>feats_lengths</strong> (<em>LongTensor</em>) –</p><p>Batch of the lengths of each target (B,). : label (Optional[Dict]): key is “lab” or “score”;</p><p>value (LongTensor): Batch of padded label ids (B, Tmax).</p></li><li><p><strong>label_lengths</strong> (<em>Optional</em> *[*<em>Dict</em> <em>]</em>) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).</p></li><li><p><strong>melody</strong> (<em>Optional</em> *[*<em>Dict</em> <em>]</em>) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).</p></li><li><p><strong>melody_lengths</strong> (<em>Optional</em> *[*<em>Dict</em> <em>]</em>) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).</p></li><li><p><strong>pitch</strong> (<em>FloatTensor</em>) – Batch of padded f0 (B, Tmax).</p></li><li><p><strong>pitch_lengths</strong> (<em>LongTensor</em>) – Batch of the lengths of padded f0 (B, ).</p></li><li><p><strong>duration</strong> (<em>Optional</em> *[*<em>Dict</em> <em>]</em>) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).</p></li><li><p><strong>duration_length</strong> (<em>Optional</em> *[*<em>Dict</em> <em>]</em>) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of the lengths of padded duration (B, ).</p></li><li><p><strong>slur</strong> (<em>LongTensor</em>) – Batch of padded slur (B, Tmax).</p></li><li><p><strong>slur_lengths</strong> (<em>LongTensor</em>) – Batch of the lengths of padded slur (B, ).</p></li><li><p><strong>spembs</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of speaker embeddings (B, spk_embed_dim).</p></li><li><p><strong>sids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of speaker IDs (B, 1).</p></li><li><p><strong>lids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of language IDs (B, 1).</p></li><li><p><strong>joint_training</strong> (<em>bool</em>) – Whether to perform joint training with vocoder.</p></li></ul></li><li><strong>Returns:</strong> Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>inference<span class="small-bracket">(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 30.0, use_att_constraint: bool = False, use_dynamic_filter: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False)</span></p></div><p>Generate the sequence of features given the sequences of characters.</p><ul><li><strong>Parameters:</strong><ul><li><strong>text</strong> (<em>LongTensor</em>) – Input sequence of characters (T_text,).</li><li><strong>feats</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Feature sequence to extract style (N, idim).</li><li><strong>label</strong> (<em>Optional</em> *[*<em>Dict</em> <em>]</em>) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).</li><li><strong>melody</strong> (<em>Optional</em> *[*<em>Dict</em> <em>]</em>) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).</li><li><strong>pitch</strong> (<em>FloatTensor</em>) – Batch of padded f0 (Tmax).</li><li><strong>duration</strong> (<em>Optional</em> *[*<em>Dict</em> <em>]</em>) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (Tmax).</li><li><strong>slur</strong> (<em>LongTensor</em>) – Batch of padded slur (B, Tmax).</li><li><strong>spembs</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Speaker embedding (spk_embed_dim,).</li><li><strong>sids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Speaker ID (1,).</li><li><strong>lids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Language ID (1,).</li><li><strong>threshold</strong> (<em>float</em>) – Threshold in inference.</li><li><strong>minlenratio</strong> (<em>float</em>) – Minimum length ratio in inference.</li><li><strong>maxlenratio</strong> (<em>float</em>) – Maximum length ratio in inference.</li><li><strong>use_att_constraint</strong> (<em>bool</em>) – Whether to apply attention constraint.</li><li><strong>use_dynamic_filter</strong> (<em>bool</em>) – Whether to apply dynamic filter.</li><li><strong>backward_window</strong> (<em>int</em>) – Backward window in attention constraint or dynamic filter.</li><li><strong>forward_window</strong> (<em>int</em>) – Forward window in attention constraint or dynamic filter.</li><li><strong>use_teacher_forcing</strong> (<em>bool</em>) – Whether to use teacher forcing.</li></ul></li><li><strong>Returns:</strong> Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). <ul><li>prob (Tensor): Output sequence of stop probabilities (T_feats,).</li><li>att_w (Tensor): Attention weights (T_feats, T).</li></ul></li><li><strong>Return type:</strong> Dict[str, Tensor]</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',10);function f(b,T){const s=r("RouteLink");return g(),l("div",null,[n(" _espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron "),d,_,e("p",null,[t("Bases: "),a(s,{to:"/guide/espnet2/svs/AbsSVS.html#espnet2.svs.abs_svs.AbsSVS"},{default:m(()=>[u]),_:1})]),p,n(" _`Singing-Tacotron: Global Duration Control Attention and Dynamic "),h])}const y=i(c,[["render",f],["__file","singing_tacotron.html.vue"]]),k=JSON.parse(`{"path":"/guide/espnet2/svs/singing_tacotron.html","title":"espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":4.36,"words":1307},"filePathRelative":"guide/espnet2/svs/singing_tacotron.md","excerpt":"<!-- _espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron -->\\n<h1>espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron<span class=\\"small-bracket\\">(idim: int, odim: int, midi_dim: int = 129, duration_dim: int = 500, embed_dim: int = 512, elayers: int = 1, eunits: int = 512, econv_layers: int = 3, econv_chans: int = 512, econv_filts: int = 5, atype: str = 'GDCA', adim: int = 512, aconv_chans: int = 32, aconv_filts: int = 15, cumulate_att_w: bool = True, dlayers: int = 2, dunits: int = 1024, prenet_layers: int = 2, prenet_units: int = 256, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, output_activation: str | None = None, use_batch_norm: bool = True, use_concate: bool = True, use_residual: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128)</span>, gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, dropout_rate: float = 0.5, zoneout_rate: float = 0.1, use_masking: bool = True, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1', use_guided_attn_loss: bool = True, guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)</p></div>"}`);export{y as comp,k as data};

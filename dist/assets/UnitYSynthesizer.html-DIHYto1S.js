import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as i,c as o,f as r,b as e,d as t,e as a,w as l,a as d,o as p}from"./app-KOUU_Wij.js";const u={},c=e("h1",{id:"espnet2-s2st-synthesizer-unity-synthesizer-unitysynthesizer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-s2st-synthesizer-unity-synthesizer-unitysynthesizer"},[e("span",null,"espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer")])],-1),m=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer"),e("span",{class:"small-bracket"},"(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, layer_drop_rate: float = 0.0, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat')")])],-1),h=e("code",null,"AbsSynthesizer",-1),_=d('<p>UnitY Synthesizer related modules for speech-to-speech translation.</p><p>This is a module of discrete unit prediction network in discrete-unit described in</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>`Direct speech-to-speech translation with discrete units`_</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>, which converts the sequence of hidden states into the sequence of discrete unit (from SSLs).</p><p>Transfomer decoder for discrete unit module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>vocab_size</strong> – output dim</li><li><strong>encoder_output_size</strong> – dimension of attention</li><li><strong>attention_heads</strong> – the number of heads of multi head attention</li><li><strong>linear_units</strong> – the number of units of position-wise feed forward</li><li><strong>num_blocks</strong> – the number of decoder blocks</li><li><strong>dropout_rate</strong> – dropout rate</li><li><strong>self_attention_dropout_rate</strong> – dropout rate for attention</li><li><strong>input_layer</strong> – input layer type</li><li><strong>use_output_layer</strong> – whether to use output layer</li><li><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</li><li><strong>normalize_before</strong> – whether to use layer_norm before the first block</li><li><strong>concat_after</strong> – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -&gt; x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -&gt; x + att(x)</li><li><strong>spks</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Number of speakers. If set to &gt; 1, assume that the sids will be provided as the input and use sid embedding layer.</li><li><strong>langs</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Number of languages. If set to &gt; 1, assume that the lids will be provided as the input and use sid embedding layer.</li><li><strong>spk_embed_dim</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Speaker embedding dimension. If set to &gt; 0, assume that spembs will be provided as the input.</li><li><strong>spk_embed_integration_type</strong> (<em>str</em>) – How to integrate speaker embedding.</li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(enc_outputs: Tensor, enc_outputs_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, return_last_hidden: bool = False, return_all_hiddens: bool = False)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>enc_outputs</strong> (<em>LongTensor</em>) – Batch of padded character ids (B, T, idim).</li><li><strong>enc_outputs_lengths</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</li><li><strong>feats</strong> (<em>Tensor</em>) – Batch of padded target features (B, T_feats, odim).</li><li><strong>feats_lengths</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</li><li><strong>spembs</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of speaker embeddings (B, spk_embed_dim).</li><li><strong>sids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of speaker IDs (B, 1).</li><li><strong>lids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of language IDs (B, 1).</li></ul></li><li><strong>Returns:</strong> hs hlens</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',10);function g(f,b){const s=i("RouteLink");return p(),o("div",null,[r(" _espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer "),c,m,e("p",null,[t("Bases: "),a(s,{to:"/guide/espnet2/s2st/AbsSynthesizer.html#espnet2.s2st.synthesizer.abs_synthesizer.AbsSynthesizer"},{default:l(()=>[h]),_:1})]),_])}const k=n(u,[["render",g],["__file","UnitYSynthesizer.html.vue"]]),T=JSON.parse(`{"path":"/guide/espnet2/s2st/UnitYSynthesizer.html","title":"espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.48,"words":445},"filePathRelative":"guide/espnet2/s2st/UnitYSynthesizer.md","excerpt":"<!-- _espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer -->\\n<h1>espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer<span class=\\"small-bracket\\">(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;, normalize_before: bool = True, concat_after: bool = False, layer_drop_rate: float = 0.0, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat')</span></p></div>"}`);export{k as comp,T as data};

import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as s,c as r,f as a,b as e,d as t,e as i,w as l,a as c,o as d}from"./app-KOUU_Wij.js";const p={},m=e("h1",{id:"espnet-nets-pytorch-backend-transformer-attention-legacyrelpositionmultiheadedattention",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet-nets-pytorch-backend-transformer-attention-legacyrelpositionmultiheadedattention"},[e("span",null,"espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention")])],-1),u=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention"),e("span",{class:"small-bracket"},"(n_head, n_feat, dropout_rate, zero_triu=False)")])],-1),h=e("code",null,"MultiHeadedAttention",-1),g=c('<p>Multi-Head Attention layer with relative position encoding (old version).</p><p>Details can be found in <a href="https://github.com/espnet/espnet/pull/2816" target="_blank" rel="noopener noreferrer">https://github.com/espnet/espnet/pull/2816</a>.</p><p>Paper: <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1901.02860</a></p><ul><li><strong>Parameters:</strong><ul><li><strong>n_head</strong> (<em>int</em>) – The number of heads.</li><li><strong>n_feat</strong> (<em>int</em>) – The number of features.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li><li><strong>zero_triu</strong> (<em>bool</em>) – Whether to zero the upper triangular part of attention matrix.</li></ul></li></ul><p>Construct an RelPositionMultiHeadedAttention object.</p><div class="custom-h4"><p>forward<span class="small-bracket">(query, key, value, pos_emb, mask)</span></p></div><p>Compute ‘Scaled Dot Product Attention’ with rel. positional encoding.</p><ul><li><strong>Parameters:</strong><ul><li><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor (#batch, time1, size).</li><li><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor (#batch, time2, size).</li><li><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor (#batch, time2, size).</li><li><strong>pos_emb</strong> (<em>torch.Tensor</em>) – Positional embedding tensor (#batch, time1, size).</li><li><strong>mask</strong> (<em>torch.Tensor</em>) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).</li></ul></li><li><strong>Returns:</strong> Output tensor (#batch, time1, d_model).</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>rel_shift<span class="small-bracket">(x)</span></p></div><p>Compute relative positional encoding.</p><ul><li><strong>Parameters:</strong><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor (batch, head, time1, time2).</li><li><strong>Returns:</strong> Output tensor.</li><li><strong>Return type:</strong> torch.Tensor</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',12);function _(b,f){const n=s("RouteLink");return d(),r("div",null,[a(" _espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention "),m,u,e("p",null,[t("Bases: "),i(n,{to:"/guide/espnet/nets/MultiHeadedAttention.html#espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention"},{default:l(()=>[h]),_:1})]),g])}const v=o(p,[["render",_],["__file","LegacyRelPositionMultiHeadedAttention.html.vue"]]),P=JSON.parse('{"path":"/guide/espnet/nets/LegacyRelPositionMultiHeadedAttention.html","title":"espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.71,"words":214},"filePathRelative":"guide/espnet/nets/LegacyRelPositionMultiHeadedAttention.md","excerpt":"<!-- _espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention -->\\n<h1>espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention<span class=\\"small-bracket\\">(n_head, n_feat, dropout_rate, zero_triu=False)</span></p></div>"}');export{v as comp,P as data};

import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,f as n,a as s,o}from"./app-KOUU_Wij.js";const i={},a=s('<h1 id="espnet-nets-pytorch-backend-transformer-plot-plot-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#espnet-nets-pytorch-backend-transformer-plot-plot-multi-head-attention"><span>espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention</span></a></h1><div class="custom-h3"><p>espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention<span class="small-bracket">(data, uttid_list, attn_dict, outdir, suffix=&#39;png&#39;, savefn=&lt;function savefig&gt;, ikey=&#39;input&#39;, iaxis=0, okey=&#39;output&#39;, oaxis=0, subsampling_factor=4)</span></p></div><p>Plot multi head attentions.</p><ul><li><strong>Parameters:</strong><ul><li><strong>data</strong> (<em>dict</em>) – utts info from json file</li><li><strong>uttid_list</strong> (<em>List</em>) – utterance IDs</li><li><strong>attn_dict</strong> (<em>dict</em> *[*<em>str</em> <em>,</em> <em>torch.Tensor</em> <em>]</em>) – multi head attention dict. values should be torch.Tensor (head, input_length, output_length)</li><li><strong>outdir</strong> (<em>str</em>) – dir to save fig</li><li><strong>suffix</strong> (<em>str</em>) – filename suffix including image type (e.g., png)</li><li><strong>savefn</strong> – function to save</li><li><strong>ikey</strong> (<em>str</em>) – key to access input</li><li><strong>iaxis</strong> (<em>int</em>) – dimension to access input</li><li><strong>okey</strong> (<em>str</em>) – key to access output</li><li><strong>oaxis</strong> (<em>int</em>) – dimension to access output</li><li><strong>subsampling_factor</strong> – subsampling factor in encoder</li></ul></li></ul>',4);function r(l,m){return o(),e("div",null,[n(" _espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention "),a])}const d=t(i,[["render",r],["__file","plot_multi_head_attention.html.vue"]]),u=JSON.parse(`{"path":"/guide/espnet/nets/plot_multi_head_attention.html","title":"espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.38,"words":115},"filePathRelative":"guide/espnet/nets/plot_multi_head_attention.md","excerpt":"<!-- _espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention -->\\n<h1>espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention</h1>\\n<div class=\\"custom-h3\\"><p>espnet.nets.pytorch_backend.transformer.plot.plot_multi_head_attention<span class=\\"small-bracket\\">(data, uttid_list, attn_dict, outdir, suffix='png', savefn=&lt;function savefig&gt;, ikey='input', iaxis=0, okey='output', oaxis=0, subsampling_factor=4)</span></p></div>"}`);export{d as comp,u as data};

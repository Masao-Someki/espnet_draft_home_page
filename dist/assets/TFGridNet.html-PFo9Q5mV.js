import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,c as i,f as o,b as e,d as t,e as a,w as l,a as d,o as p}from"./app-KOUU_Wij.js";const m={},c=e("h1",{id:"espnet2-enh-separator-tfgridnet-separator-tfgridnet",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-enh-separator-tfgridnet-separator-tfgridnet"},[e("span",null,"espnet2.enh.separator.tfgridnet_separator.TFGridNet")])],-1),u=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.enh.separator.tfgridnet_separator.TFGridNet"),e("span",{class:"small-bracket"},"(input_dim, n_srcs=2, n_fft=128, stride=64, window='hann', n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_approx_qk_dim=512, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05, use_builtin_complex=False, ref_channel=-1)")])],-1),h=e("code",null,"AbsSeparator",-1),_=d('<p>Offline TFGridNet</p><p>Reference: [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation”, in arXiv preprint arXiv:2211.12433, 2022. [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation”, in arXiv preprint arXiv:2209.03952, 2022.</p><p>NOTES: As outlined in the Reference, this model works best when trained with variance normalized mixture input and target, e.g., with mixture of shape [batch, samples, microphones], you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signals. It is encouraged to do so when not using scale-invariant loss functions such as SI-SDR.</p><ul><li><strong>Parameters:</strong><ul><li><strong>input_dim</strong> – placeholder, not used</li><li><strong>n_srcs</strong> – number of output sources/speakers.</li><li><strong>n_fft</strong> – stft window size.</li><li><strong>stride</strong> – stft stride.</li><li><strong>window</strong> – stft window type choose between ‘hamming’, ‘hanning’ or None.</li><li><strong>n_imics</strong> – number of microphones channels (only fixed-array geometry supported).</li><li><strong>n_layers</strong> – number of TFGridNet blocks.</li><li><strong>lstm_hidden_units</strong> – number of hidden units in LSTM.</li><li><strong>attn_n_head</strong> – number of heads in self-attention</li><li><strong>attn_approx_qk_dim</strong> – approximate dimention of frame-level key and value tensors</li><li><strong>emb_dim</strong> – embedding dimension</li><li><strong>emb_ks</strong> – kernel size for unfolding and deconv1D</li><li><strong>emb_hs</strong> – hop size for unfolding and deconv1D</li><li><strong>activation</strong> – activation function to use in the whole TFGridNet model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.</li><li><strong>eps</strong> – small epsilon for normalization layers.</li><li><strong>use_builtin_complex</strong> – whether to use builtin complex type or not.</li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(input: Tensor, ilens: Tensor, additional: Dict | None = None)</span></p></div><p>Forward.</p><ul><li><p><strong>Parameters:</strong></p><ul><li><strong>input</strong> (<em>torch.Tensor</em>) – batched multi-channel audio tensor with M audio channels and N samples [B, N, M]</li><li><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [B]</li><li><strong>additional</strong> (<em>Dict</em> <em>or</em> <em>None</em>) – other data, currently unused in this model.</li></ul></li><li><p><strong>Returns:</strong> [(B, T), …] list of len n_srcs : of mono audio tensors with T samples.</p><p>ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,</p><blockquote><p>we return it also in output.</p></blockquote></li><li><p><strong>Return type:</strong> enhanced (List[Union(torch.Tensor)])</p></li></ul><div class="custom-h4"><p><em>property</em> num_spk</p></div><div class="custom-h4"><p><em>static</em> pad2<span class="small-bracket">(input_tensor, target_len)</span></p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',11);function g(f,b){const n=r("RouteLink");return p(),i("div",null,[o(" _espnet2.enh.separator.tfgridnet_separator.TFGridNet "),c,u,e("p",null,[t("Bases: "),a(n,{to:"/guide/espnet2/enh/AbsSeparator.html#espnet2.enh.separator.abs_separator.AbsSeparator"},{default:l(()=>[h]),_:1})]),_])}const N=s(m,[["render",g],["__file","TFGridNet.html.vue"]]),w=JSON.parse(`{"path":"/guide/espnet2/enh/TFGridNet.html","title":"espnet2.enh.separator.tfgridnet_separator.TFGridNet","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.53,"words":458},"filePathRelative":"guide/espnet2/enh/TFGridNet.md","excerpt":"<!-- _espnet2.enh.separator.tfgridnet_separator.TFGridNet -->\\n<h1>espnet2.enh.separator.tfgridnet_separator.TFGridNet</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.separator.tfgridnet_separator.TFGridNet<span class=\\"small-bracket\\">(input_dim, n_srcs=2, n_fft=128, stride=64, window='hann', n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_approx_qk_dim=512, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05, use_builtin_complex=False, ref_channel=-1)</span></p></div>"}`);export{N as comp,w as data};

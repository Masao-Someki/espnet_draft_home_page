import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as a,c as l,f as i,b as e,d as t,e as o,w as s,a as c,o as d}from"./app-KOUU_Wij.js";const u={},p=e("h1",{id:"espnet2-asr-decoder-transformer-decoder-basetransformerdecoder",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-asr-decoder-transformer-decoder-basetransformerdecoder"},[e("span",null,"espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder")])],-1),m=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"),e("span",{class:"small-bracket"},"(vocab_size: int, encoder_output_size: int, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True)")])],-1),_=e("code",null,"AbsDecoder",-1),h=e("code",null,"BatchScorerInterface",-1),g=e("code",null,"MaskParallelScorerInterface",-1),f=c('<p>Base class of Transfomer decoder module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>vocab_size</strong> – output dim</li><li><strong>encoder_output_size</strong> – dimension of attention</li><li><strong>attention_heads</strong> – the number of heads of multi head attention</li><li><strong>linear_units</strong> – the number of units of position-wise feed forward</li><li><strong>num_blocks</strong> – the number of decoder blocks</li><li><strong>dropout_rate</strong> – dropout rate</li><li><strong>self_attention_dropout_rate</strong> – dropout rate for attention</li><li><strong>input_layer</strong> – input layer type</li><li><strong>use_output_layer</strong> – whether to use output layer</li><li><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</li><li><strong>normalize_before</strong> – whether to use layer_norm before the first block</li><li><strong>concat_after</strong> – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -&gt; x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -&gt; x + att(x)</li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>batch_score<span class="small-bracket">(ys: Tensor, states: List[Any], xs: Tensor, return_hs: bool = False)</span></p></div><p>Score new token batch.</p><ul><li><strong>Parameters:</strong><ul><li><strong>ys</strong> (<em>torch.Tensor</em>) – torch.int64 prefix tokens (n_batch, ylen).</li><li><strong>states</strong> (<em>List</em> *[*<em>Any</em> <em>]</em>) – Scorer states for prefix tokens.</li><li><strong>xs</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys (n_batch, xlen, n_feat).</li></ul></li><li><strong>Returns:</strong> Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.</li><li><strong>Return type:</strong> tuple[torch.Tensor, List[Any]]</li></ul><div class="custom-h4"><p>batch_score_partially_AR<span class="small-bracket">(ys: Tensor, states: List[Any], xs: Tensor, yseq_lengths: Tensor)</span></p></div><div class="custom-h4"><p>forward<span class="small-bracket">(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor, return_hs: bool = False, return_all_hs: bool = False)</span></p></div><p>Forward decoder.</p><ul><li><p><strong>Parameters:</strong></p><ul><li><strong>hs_pad</strong> – encoded memory, float32 (batch, maxlen_in, feat)</li><li><strong>hlens</strong> – (batch)</li><li><strong>ys_in_pad</strong> – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases</li><li><strong>ys_in_lens</strong> – (batch)</li><li><strong>return_hs</strong> – (bool) whether to return the last hidden output before output layer</li><li><strong>return_all_hs</strong> – (bool) whether to return all the hidden intermediates</li></ul></li><li><p><strong>Returns:</strong> tuple containing:</p><p>x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,</p><p>olens: (batch, )</p></li><li><p><strong>Return type:</strong> (tuple)</p></li></ul><div class="custom-h4"><p>forward_one_step<span class="small-bracket">(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, memory_mask: Tensor | None = None, *, cache: List[Tensor] | None = None, return_hs: bool = False)</span></p></div><p>Forward one step.</p><ul><li><strong>Parameters:</strong><ul><li><strong>tgt</strong> – input token ids, int64 (batch, maxlen_out)</li><li><strong>tgt_mask</strong> – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)</li><li><strong>memory</strong> – encoded memory, float32 (batch, maxlen_in, feat)</li><li><strong>memory_mask</strong> – encoded memory mask (batch, 1, maxlen_in)</li><li><strong>cache</strong> – cached output list of (batch, max_time_out-1, size)</li><li><strong>return_hs</strong> – dec hidden state corresponding to ys, used for searchable hidden ints</li></ul></li><li><strong>Returns:</strong> NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)</li><li><strong>Return type:</strong> y, cache</li></ul><div class="custom-h4"><p>forward_partially_AR<span class="small-bracket">(tgt: Tensor, tgt_mask: Tensor, tgt_lengths: Tensor, memory: Tensor, cache: List[Tensor] | None = None)</span></p></div><p>Forward one step.</p><ul><li><strong>Parameters:</strong><ul><li><strong>tgt</strong> – input token ids, int64 (n_mask * n_beam, maxlen_out)</li><li><strong>tgt_mask</strong> – input token mask, (n_mask * n_beam, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)</li><li><strong>tgt_lengths</strong> – (n_mask * n_beam, )</li><li><strong>memory</strong> – encoded memory, float32 (batch, maxlen_in, feat)</li><li><strong>cache</strong> – cached output list of (batch, max_time_out-1, size)</li></ul></li><li><strong>Returns:</strong> NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)</li><li><strong>Return type:</strong> y, cache</li></ul><div class="custom-h4"><p>score<span class="small-bracket">(ys, state, x, return_hs=False)</span></p></div><p>Score.</p><div class="custom-h4"><p>training <em>: bool</em></p></div>',19);function b(y,T){const r=a("RouteLink");return d(),l("div",null,[i(" _espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder "),p,m,e("p",null,[t("Bases: "),o(r,{to:"/guide/espnet2/asr/AbsDecoder.html#espnet2.asr.decoder.abs_decoder.AbsDecoder"},{default:s(()=>[_]),_:1}),t(", "),o(r,{to:"/guide/espnet/nets/BatchScorerInterface.html#espnet.nets.scorer_interface.BatchScorerInterface"},{default:s(()=>[h]),_:1}),t(", "),o(r,{to:"/guide/espnet/nets/MaskParallelScorerInterface.html#espnet.nets.scorer_interface.MaskParallelScorerInterface"},{default:s(()=>[g]),_:1})]),f])}const v=n(u,[["render",b],["__file","BaseTransformerDecoder.html.vue"]]),w=JSON.parse(`{"path":"/guide/espnet2/asr/BaseTransformerDecoder.html","title":"espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.03,"words":610},"filePathRelative":"guide/espnet2/asr/BaseTransformerDecoder.md","excerpt":"<!-- _espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder -->\\n<h1>espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder<span class=\\"small-bracket\\">(vocab_size: int, encoder_output_size: int, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;, normalize_before: bool = True)</span></p></div>"}`);export{v as comp,w as data};

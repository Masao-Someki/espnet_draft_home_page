import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,o as e,a}from"./app-KOUU_Wij.js";const s={},d=a(`<h2 id="citations" tabindex="-1"><a class="header-anchor" href="#citations"><span>Citations</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>@inproceedings{watanabe2018espnet,</span></span>
<span class="line"><span>  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},</span></span>
<span class="line"><span>  title={{ESPnet}: End-to-End Speech Processing Toolkit},
  year={2018},
  booktitle={Proceedings of Interspeech},
  pages={2207--2211},
  doi={10.21437/Interspeech.2018-1456},
  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}
}
@inproceedings{hayashi2020espnet,
  title={{Espnet-TTS}: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit},
  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu},
  booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7654--7658},
  year={2020},
  organization={IEEE}
}
@inproceedings{inaguma-etal-2020-espnet,
    title = &quot;{ESP}net-{ST}: All-in-One Speech Translation Toolkit&quot;,
    author = &quot;Inaguma, Hirofumi  and
      Kiyono, Shun  and
      Duh, Kevin  and
      Karita, Shigeki  and
      Yalta, Nelson  and
      Hayashi, Tomoki  and
      Watanabe, Shinji&quot;,
    booktitle = &quot;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations&quot;,
    month = jul,
    year = &quot;2020&quot;,
    address = &quot;Online&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://www.aclweb.org/anthology/2020.acl-demos.34&quot;,
    pages = &quot;302--311&quot;,
}
@article{hayashi2021espnet2,
  title={Espnet2-tts: Extending the edge of tts research},
  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji},
  journal={arXiv preprint arXiv:2110.07840},
  year={2021}
}
@inproceedings{li2020espnet,
  title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},
  author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},
  booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},
  pages={785--792},
  year={2021},
  organization={IEEE},
}
@inproceedings{arora2021espnet,
  title={{ESPnet-SLU}: Advancing Spoken Language Understanding through ESPnet},
  author={Arora, Siddhant and Dalmia, Siddharth and Denisov, Pavel and Chang, Xuankai and Ueda, Yushi and Peng, Yifan and Zhang, Yuekai and Kumar, Sujay and Ganesan, Karthik and Yan, Brian and others},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7167--7171},
  year={2022},
  organization={IEEE}
}
@inproceedings{shi2022muskits,
  author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin},
  title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis},
  year={2022},
  booktitle={Proceedings of Interspeech},
  pages={4277-4281},
  url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf}
}
@inproceedings{lu22c_interspeech,
  author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe},
  title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}},</span></span>
<span class="line"><span>  year=2022,</span></span>
<span class="line"><span>  booktitle={Proc. Interspeech 2022},</span></span>
<span class="line"><span>  pages={5458--5462},</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>@article{gao2022euro,</span></span>
<span class="line"><span>  title={{EURO}: {ESPnet} Unsupervised ASR Open-source Toolkit},</span></span>
<span class="line"><span>  author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev},</span></span>
<span class="line"><span>  journal={arXiv preprint arXiv:2211.17196},</span></span>
<span class="line"><span>  year={2022}</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>@article{peng2023reproducing,</span></span>
<span class="line"><span>  title={Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data},</span></span>
<span class="line"><span>  author={Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and others},</span></span>
<span class="line"><span>  journal={arXiv preprint arXiv:2309.13876},</span></span>
<span class="line"><span>  year={2023}</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2),t=[d];function l(r,o){return e(),i("div",null,t)}const p=n(s,[["render",l],["__file","index.html.vue"]]),v=JSON.parse('{"path":"/","title":"ESPnet","lang":"en-US","frontmatter":{"home":true,"icon":"/assets/image/espnet.png","title":"ESPnet","heroImage":"/assets/image/espnet_logo1.png","heroImageStyle":[{"width":"80%"}],"heroText":"ESPnet: end-to-end speech processing toolkit","tagline":"ESPnet is an end-to-end speech processing toolkit covering many speech-related tasks.","actions":[{"text":"Get Started","icon":"book","link":"./espnet2_tutorial.md","type":"primary"},{"text":"Demos","icon":"lightbulb","link":"./notebook/"}],"highlights":[{"header":"Easy to install","image":"/assets/image/box.svg","bgImage":"https://theme-hope-assets.vuejs.press/bg/3-light.svg","bgImageDark":"https://theme-hope-assets.vuejs.press/bg/3-dark.svg","highlights":[{"title":"<code>pip install espnet</code> for easy install."},{"title":"See <a href=\\"./installation.md\\">this instruction</a> for more details."}]},{"header":"Supports many tasks","description":"We provide a complete setup for various speech processing tasks.","image":"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3mOiQTPh_S9XW6m94OQYjucUzUu7L9uEcHP9YsADUGWTcmscynkrLc1Zs8o5rA3G9lSNnEpyHBMCnZzBepYdW8jVofKnLflvOsu-ywIZpQf1Kw5l6tzvhEA1q2cbnFDIzIDlOUOKPOarf/s800/cooking_recipe.png","bgImage":"https://theme-hope-assets.vuejs.press/bg/2-light.svg","bgImageDark":"https://theme-hope-assets.vuejs.press/bg/2-dark.svg","bgImageStyle":{"background-repeat":"repeat","background-size":"initial"},"features":[{"title":"ASR: Automatic Speech Recognition","link":"https://github.com/espnet/espnet?tab=readme-ov-file#asr-automatic-speech-recognition"},{"title":"TTS: Text-to-speech","link":"https://github.com/espnet/espnet?tab=readme-ov-file#tts-text-to-speech"},{"title":"SE: Speech enhancement (and separation)","link":"https://github.com/espnet/espnet?tab=readme-ov-file#se-speech-enhancement-and-separation"},{"title":"SUM: Speech Summarization","link":"https://github.com/espnet/espnet?tab=readme-ov-file#sum-speech-summarization"},{"title":"SVS: Singing Voice Synthesis","link":"https://github.com/espnet/espnet?tab=readme-ov-file#svs-singing-voice-synthesis"},{"title":"SSL: Self-supervised Learning","link":"https://github.com/espnet/espnet?tab=readme-ov-file#ssl-self-supervised-learning"},{"title":"UASR: Unsupervised ASR ","details":"EURO: ESPnet Unsupervised Recognition - Open-source","link":"https://github.com/espnet/espnet?tab=readme-ov-file#uasr-unsupervised-asr-euro-espnet-unsupervised-recognition---open-source"},{"title":"S2T: Speech-to-text with Whisper-style multilingual   multitask models","link":"https://github.com/espnet/espnet?tab=readme-ov-file#s2t-speech-to-text-with-whisper-style-multilingual-multitask-models"}]},{"header":"More Documents","description":"You can find tutorials on ESPnet packages.","bgImage":"https://theme-hope-assets.vuejs.press/bg/10-light.svg","bgImageDark":"https://theme-hope-assets.vuejs.press/bg/10-dark.svg","bgImageStyle":{"background-repeat":"repeat","background-size":"initial"},"highlights":[{"title":"Tutorial","icon":"/assets/icon/school_24dp_5F6368_FILL0_wght400_GRAD0_opsz24.svg","link":"./tutorial.md"},{"title":"Tutorial (ESPnet1)","icon":"/assets/icon/school_24dp_5F6368_FILL0_wght400_GRAD0_opsz24.svg","link":"./espnet1_tutorial.md"},{"title":"Training Config","icon":"sliders","link":"./espnet2_training_option.md"},{"title":"Format audio to wav.scp","icon":"arrow-rotate-right","link":"./espnet2_format_wav_scp.md"},{"title":"Task class and data","icon":"database","link":"./espnet2_task.md"},{"title":"Docker","icon":"/assets/icon/docker-mark-blue.svg","link":"./docker.md"},{"title":"Job scheduling system","icon":"server","link":"./parallelization.md"},{"title":"Distributed training","icon":"server","link":"./espnet2_distributed.md"},{"title":"Document Generation","icon":"book","link":"./document.md"}]}],"footer":"Apache License 2.0, Copyright Â© 2024-present ESPnet community"},"headers":[{"level":2,"title":"Citations","slug":"citations","link":"#citations","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":3.31,"words":992},"filePathRelative":"index.md","excerpt":"<h2>Citations</h2>\\n<div class=\\"language- line-numbers-mode\\" data-highlighter=\\"shiki\\" data-ext=\\"\\" data-title=\\"\\" style=\\"--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34\\"><pre class=\\"shiki shiki-themes github-light one-dark-pro vp-code\\"><code><span class=\\"line\\"><span>@inproceedings{watanabe2018espnet,</span></span>\\n<span class=\\"line\\"><span>  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},</span></span>\\n<span class=\\"line\\"><span>  title={{ESPnet}: End-to-End Speech Processing Toolkit},\\n  year={2018},\\n  booktitle={Proceedings of Interspeech},\\n  pages={2207--2211},\\n  doi={10.21437/Interspeech.2018-1456},\\n  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}\\n}\\n@inproceedings{hayashi2020espnet,\\n  title={{Espnet-TTS}: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit},\\n  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu},\\n  booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n  pages={7654--7658},\\n  year={2020},\\n  organization={IEEE}\\n}\\n@inproceedings{inaguma-etal-2020-espnet,\\n    title = \\"{ESP}net-{ST}: All-in-One Speech Translation Toolkit\\",\\n    author = \\"Inaguma, Hirofumi  and\\n      Kiyono, Shun  and\\n      Duh, Kevin  and\\n      Karita, Shigeki  and\\n      Yalta, Nelson  and\\n      Hayashi, Tomoki  and\\n      Watanabe, Shinji\\",\\n    booktitle = \\"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\\",\\n    month = jul,\\n    year = \\"2020\\",\\n    address = \\"Online\\",\\n    publisher = \\"Association for Computational Linguistics\\",\\n    url = \\"https://www.aclweb.org/anthology/2020.acl-demos.34\\",\\n    pages = \\"302--311\\",\\n}\\n@article{hayashi2021espnet2,\\n  title={Espnet2-tts: Extending the edge of tts research},\\n  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji},\\n  journal={arXiv preprint arXiv:2110.07840},\\n  year={2021}\\n}\\n@inproceedings{li2020espnet,\\n  title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},\\n  author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},\\n  booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},\\n  pages={785--792},\\n  year={2021},\\n  organization={IEEE},\\n}\\n@inproceedings{arora2021espnet,\\n  title={{ESPnet-SLU}: Advancing Spoken Language Understanding through ESPnet},\\n  author={Arora, Siddhant and Dalmia, Siddharth and Denisov, Pavel and Chang, Xuankai and Ueda, Yushi and Peng, Yifan and Zhang, Yuekai and Kumar, Sujay and Ganesan, Karthik and Yan, Brian and others},\\n  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\\n  pages={7167--7171},\\n  year={2022},\\n  organization={IEEE}\\n}\\n@inproceedings{shi2022muskits,\\n  author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin},\\n  title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis},\\n  year={2022},\\n  booktitle={Proceedings of Interspeech},\\n  pages={4277-4281},\\n  url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf}\\n}\\n@inproceedings{lu22c_interspeech,\\n  author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe},\\n  title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}},</span></span>\\n<span class=\\"line\\"><span>  year=2022,</span></span>\\n<span class=\\"line\\"><span>  booktitle={Proc. Interspeech 2022},</span></span>\\n<span class=\\"line\\"><span>  pages={5458--5462},</span></span>\\n<span class=\\"line\\"><span>}</span></span>\\n<span class=\\"line\\"><span>@article{gao2022euro,</span></span>\\n<span class=\\"line\\"><span>  title={{EURO}: {ESPnet} Unsupervised ASR Open-source Toolkit},</span></span>\\n<span class=\\"line\\"><span>  author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev},</span></span>\\n<span class=\\"line\\"><span>  journal={arXiv preprint arXiv:2211.17196},</span></span>\\n<span class=\\"line\\"><span>  year={2022}</span></span>\\n<span class=\\"line\\"><span>}</span></span>\\n<span class=\\"line\\"><span>@article{peng2023reproducing,</span></span>\\n<span class=\\"line\\"><span>  title={Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data},</span></span>\\n<span class=\\"line\\"><span>  author={Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and others},</span></span>\\n<span class=\\"line\\"><span>  journal={arXiv preprint arXiv:2309.13876},</span></span>\\n<span class=\\"line\\"><span>  year={2023}</span></span>\\n<span class=\\"line\\"><span>}</span></span></code></pre>\\n<div class=\\"line-numbers\\" aria-hidden=\\"true\\" style=\\"counter-reset:line-number 0\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>"}');export{p as comp,v as data};

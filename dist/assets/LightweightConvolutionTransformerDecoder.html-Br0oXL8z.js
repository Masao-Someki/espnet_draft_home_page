import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as n,c as s,f as a,b as e,d as o,e as i,w as d,o as c}from"./app-KOUU_Wij.js";const l={},_=e("h1",{id:"espnet2-asr-decoder-transformer-decoder-lightweightconvolutiontransformerdecoder",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-asr-decoder-transformer-decoder-lightweightconvolutiontransformerdecoder"},[e("span",null,"espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder")])],-1),u=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),o(" espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder"),e("span",{class:"small-bracket"},"(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11)"),o(", conv_usebias: int = False)")])],-1),m=e("code",null,"BaseTransformerDecoder",-1),p=e("p",null,"Initializes internal Module state, shared by both nn.Module and ScriptModule.",-1),h=e("div",{class:"custom-h4"},[e("p",null,[o("training "),e("em",null,": bool")])],-1);function f(g,v){const t=n("RouteLink");return c(),s("div",null,[a(" _espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder "),_,u,e("p",null,[o("Bases: "),i(t,{to:"/guide/espnet2/asr/BaseTransformerDecoder.html#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"},{default:d(()=>[m]),_:1})]),p,h])}const w=r(l,[["render",f],["__file","LightweightConvolutionTransformerDecoder.html.vue"]]),C=JSON.parse(`{"path":"/guide/espnet2/asr/LightweightConvolutionTransformerDecoder.html","title":"espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.33,"words":100},"filePathRelative":"guide/espnet2/asr/LightweightConvolutionTransformerDecoder.md","excerpt":"<!-- _espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder -->\\n<h1>espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder<span class=\\"small-bracket\\">(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11)</span>, conv_usebias: int = False)</p></div>"}`);export{w as comp,C as data};

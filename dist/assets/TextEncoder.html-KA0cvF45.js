import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,f as o,a as n,o as r}from"./app-KOUU_Wij.js";const s={},i=n('<h1 id="espnet2-gan-tts-vits-text-encoder-textencoder" tabindex="-1"><a class="header-anchor" href="#espnet2-gan-tts-vits-text-encoder-textencoder"><span>espnet2.gan_tts.vits.text_encoder.TextEncoder</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.gan_tts.vits.text_encoder.TextEncoder<span class="small-bracket">(vocabs: int, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, positionwise_layer_type: str = &#39;conv1d&#39;, positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = &#39;rel_pos&#39;, self_attention_layer_type: str = &#39;rel_selfattn&#39;, activation_type: str = &#39;swish&#39;, normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0)</span></p></div><p>Bases: <code>Module</code></p><p>Text encoder module in VITS.</p><p>This is a module of text encoder described in <a href="https://arxiv.org/abs/2006.04558" target="_blank" rel="noopener noreferrer">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a>.</p><p>Instead of the relative positional Transformer, we use conformer architecture as the encoder module, which contains additional convolution layers.</p><p>Initialize TextEncoder module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>vocabs</strong> (<em>int</em>) – Vocabulary size.</li><li><strong>attention_dim</strong> (<em>int</em>) – Attention dimension.</li><li><strong>attention_heads</strong> (<em>int</em>) – Number of attention heads.</li><li><strong>linear_units</strong> (<em>int</em>) – Number of linear units of positionwise layers.</li><li><strong>blocks</strong> (<em>int</em>) – Number of encoder blocks.</li><li><strong>positionwise_layer_type</strong> (<em>str</em>) – Positionwise layer type.</li><li><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Positionwise layer’s kernel size.</li><li><strong>positional_encoding_layer_type</strong> (<em>str</em>) – Positional encoding layer type.</li><li><strong>self_attention_layer_type</strong> (<em>str</em>) – Self-attention layer type.</li><li><strong>activation_type</strong> (<em>str</em>) – Activation function type.</li><li><strong>normalize_before</strong> (<em>bool</em>) – Whether to apply LayerNorm before attention.</li><li><strong>use_macaron_style</strong> (<em>bool</em>) – Whether to use macaron style components.</li><li><strong>use_conformer_conv</strong> (<em>bool</em>) – Whether to use conformer conv layers.</li><li><strong>conformer_kernel_size</strong> (<em>int</em>) – Conformer’s conv kernel size.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li><li><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate for positional encoding.</li><li><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate for attention.</li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(x: Tensor, x_lengths: Tensor)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> (<em>Tensor</em>) – Input index tensor (B, T_text).</li><li><strong>x_lengths</strong> (<em>Tensor</em>) – Length tensor (B,).</li></ul></li><li><strong>Returns:</strong> Encoded hidden representation (B, attention_dim, T_text). Tensor: Projected mean tensor (B, attention_dim, T_text). Tensor: Projected scale tensor (B, attention_dim, T_text). Tensor: Mask tensor for input tensor (B, 1, T_text).</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',12);function a(l,_){return r(),t("div",null,[o(" _espnet2.gan_tts.vits.text_encoder.TextEncoder "),i])}const p=e(s,[["render",a],["__file","TextEncoder.html.vue"]]),m=JSON.parse(`{"path":"/guide/espnet2/gan_tts/TextEncoder.html","title":"espnet2.gan_tts.vits.text_encoder.TextEncoder","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.04,"words":312},"filePathRelative":"guide/espnet2/gan_tts/TextEncoder.md","excerpt":"<!-- _espnet2.gan_tts.vits.text_encoder.TextEncoder -->\\n<h1>espnet2.gan_tts.vits.text_encoder.TextEncoder</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.gan_tts.vits.text_encoder.TextEncoder<span class=\\"small-bracket\\">(vocabs: int, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0)</span></p></div>"}`);export{p as comp,m as data};

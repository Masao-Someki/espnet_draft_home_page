import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as s,c as a,f as r,b as e,d as t,e as o,w as l,a as d,o as p}from"./app-KOUU_Wij.js";const c={},h=e("h1",{id:"espnet2-enh-separator-tfgridnetv2-separator-tfgridnetv2",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-enh-separator-tfgridnetv2-separator-tfgridnetv2"},[e("span",null,"espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2")])],-1),u=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2"),e("span",{class:"small-bracket"},"(input_dim, n_srcs=2, n_fft=128, stride=64, window='hann', n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_approx_qk_dim=512, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05, use_builtin_complex=False)")])],-1),m=e("code",null,"AbsSeparator",-1),g=d('<p>Offline TFGridNetV2. Compared with TFGridNet, TFGridNetV2 speeds up the code</p><p>by vectorizing multiple heads in self-attention, and better dealing with Deconv1D in each intra- and inter-block when emb_ks == emb_hs.</p><p>Reference: [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation”, in TASLP, 2023. [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation”, in ICASSP, 2023.</p><p>NOTES: As outlined in the Reference, this model works best when trained with variance normalized mixture input and target, e.g., with mixture of shape [batch, samples, microphones], you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signals. It is encouraged to do so when not using scale-invariant loss functions such as SI-SDR. Specifically, use:</p><blockquote><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>std_</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></blockquote><blockquote><p>= std(mix) mix = mix /</p></blockquote><blockquote><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>std_</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></blockquote><blockquote><p>tgt = tgt /</p></blockquote><blockquote><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>std_</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></blockquote><ul><li><strong>Parameters:</strong><ul><li><strong>input_dim</strong> – placeholder, not used</li><li><strong>n_srcs</strong> – number of output sources/speakers.</li><li><strong>n_fft</strong> – stft window size.</li><li><strong>stride</strong> – stft stride.</li><li><strong>window</strong> – stft window type choose between ‘hamming’, ‘hanning’ or None.</li><li><strong>n_imics</strong> – number of microphones channels (only fixed-array geometry supported).</li><li><strong>n_layers</strong> – number of TFGridNetV2 blocks.</li><li><strong>lstm_hidden_units</strong> – number of hidden units in LSTM.</li><li><strong>attn_n_head</strong> – number of heads in self-attention</li><li><strong>attn_approx_qk_dim</strong> – approximate dimention of frame-level key and value tensors</li><li><strong>emb_dim</strong> – embedding dimension</li><li><strong>emb_ks</strong> – kernel size for unfolding and deconv1D</li><li><strong>emb_hs</strong> – hop size for unfolding and deconv1D</li><li><strong>activation</strong> – activation function to use in the whole TFGridNetV2 model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.</li><li><strong>eps</strong> – small epsilon for normalization layers.</li><li><strong>use_builtin_complex</strong> – whether to use builtin complex type or not.</li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(input: Tensor, ilens: Tensor, additional: Dict | None = None)</span></p></div><p>Forward.</p><ul><li><p><strong>Parameters:</strong></p><ul><li><strong>input</strong> (<em>torch.Tensor</em>) – batched multi-channel audio tensor with M audio channels and N samples [B, N, M]</li><li><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [B]</li><li><strong>additional</strong> (<em>Dict</em> <em>or</em> <em>None</em>) – other data, currently unused in this model.</li></ul></li><li><p><strong>Returns:</strong> [(B, T), …] list of len n_srcs : of mono audio tensors with T samples.</p><p>ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,</p><blockquote><p>we return it also in output.</p></blockquote></li><li><p><strong>Return type:</strong> enhanced (List[Union(torch.Tensor)])</p></li></ul><div class="custom-h4"><p><em>property</em> num_spk</p></div><div class="custom-h4"><p><em>static</em> pad2<span class="small-bracket">(input_tensor, target_len)</span></p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',17);function _(b,f){const n=s("RouteLink");return p(),a("div",null,[r(" _espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2 "),h,u,e("p",null,[t("Bases: "),o(n,{to:"/guide/espnet2/enh/AbsSeparator.html#espnet2.enh.separator.abs_separator.AbsSeparator"},{default:l(()=>[m]),_:1})]),g])}const T=i(c,[["render",_],["__file","TFGridNetV2.html.vue"]]),N=JSON.parse(`{"path":"/guide/espnet2/enh/TFGridNetV2.html","title":"espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.64,"words":492},"filePathRelative":"guide/espnet2/enh/TFGridNetV2.md","excerpt":"<!-- _espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2 -->\\n<h1>espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2<span class=\\"small-bracket\\">(input_dim, n_srcs=2, n_fft=128, stride=64, window='hann', n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_approx_qk_dim=512, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05, use_builtin_complex=False)</span></p></div>"}`);export{T as comp,N as data};

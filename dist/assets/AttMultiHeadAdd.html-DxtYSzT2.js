import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,f as n,a as s,o as a}from"./app-KOUU_Wij.js";const i={},d=s('<h1 id="espnet-nets-pytorch-backend-rnn-attentions-attmultiheadadd" tabindex="-1"><a class="header-anchor" href="#espnet-nets-pytorch-backend-rnn-attentions-attmultiheadadd"><span>espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd</span></a></h1><div class="custom-h3"><p><em>class</em> espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd<span class="small-bracket">(eprojs, dunits, aheads, att_dim_k, att_dim_v, han_mode=False)</span></p></div><p>Bases: <code>Module</code></p><p>Multi head additive attention</p><p>Reference: Attention is all you need : (<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1706.03762</a>)</p><p>This attention is multi head attention using additive attention for each head.</p><ul><li><strong>Parameters:</strong><ul><li><strong>eprojs</strong> (<em>int</em>) – # projection-units of encoder</li><li><strong>dunits</strong> (<em>int</em>) – # units of decoder</li><li><strong>aheads</strong> (<em>int</em>) – # heads of multi head attention</li><li><strong>att_dim_k</strong> (<em>int</em>) – dimension k in multi head attention</li><li><strong>att_dim_v</strong> (<em>int</em>) – dimension v in multi head attention</li><li><strong>han_mode</strong> (<em>bool</em>) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v</li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(enc_hs_pad, enc_hs_len, dec_z, att_prev, **kwargs)</span></p></div><p>AttMultiHeadAdd forward</p><ul><li><strong>Parameters:</strong><ul><li><strong>enc_hs_pad</strong> (<em>torch.Tensor</em>) – padded encoder hidden state (B x T_max x D_enc)</li><li><strong>enc_hs_len</strong> (<em>list</em>) – padded encoder hidden state length (B)</li><li><strong>dec_z</strong> (<em>torch.Tensor</em>) – decoder hidden state (B x D_dec)</li><li><strong>att_prev</strong> (<em>torch.Tensor</em>) – dummy (does not use)</li></ul></li><li><strong>Returns:</strong> attention weighted encoder state (B, D_enc)</li><li><strong>Return type:</strong> torch.Tensor</li><li><strong>Returns:</strong> list of previous attention weight (B x T_max) * aheads</li><li><strong>Return type:</strong> list</li></ul><div class="custom-h4"><p>reset()</p></div><p>reset states</p><div class="custom-h4"><p>training <em>: bool</em></p></div>',14);function o(r,l){return a(),e("div",null,[n(" _espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd "),d])}const m=t(i,[["render",o],["__file","AttMultiHeadAdd.html.vue"]]),h=JSON.parse('{"path":"/guide/espnet/nets/AttMultiHeadAdd.html","title":"espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.71,"words":212},"filePathRelative":"guide/espnet/nets/AttMultiHeadAdd.md","excerpt":"<!-- _espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd -->\\n<h1>espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd<span class=\\"small-bracket\\">(eprojs, dunits, aheads, att_dim_k, att_dim_v, han_mode=False)</span></p></div>"}');export{m as comp,h as data};

import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as l,c as a,f as c,b as e,d as t,e as n,w as r,a as s,o as _}from"./app-KOUU_Wij.js";const d={},m=e("h1",{id:"espnet2-asr-encoder-longformer-encoder-longformerencoder",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-asr-encoder-longformer-encoder-longformerencoder"},[e("span",null,"espnet2.asr.encoder.longformer_encoder.LongformerEncoder")])],-1),u=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.asr.encoder.longformer_encoder.LongformerEncoder"),e("span",{class:"small-bracket"},"(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'abs_pos', selfattention_layer_type: str = 'lf_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, attention_windows: list = [100, 100, 100, 100, 100, 100], attention_dilation: list = [1, 1, 1, 1, 1, 1], attention_mode: str = 'sliding_chunks')")])],-1),p=e("code",null,"ConformerEncoder",-1),g=s('<p>Longformer SA Conformer encoder module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>input_size</strong> (<em>int</em>) – Input dimension.</li><li><strong>output_size</strong> (<em>int</em>) – Dimension of attention.</li><li><strong>attention_heads</strong> (<em>int</em>) – The number of heads of multi head attention.</li><li><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</li><li><strong>num_blocks</strong> (<em>int</em>) – The number of decoder blocks.</li><li><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</li><li><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</li><li><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</li><li><strong>input_layer</strong> (<em>Union</em> *[*<em>str</em> <em>,</em> <em>torch.nn.Module</em> <em>]</em>) – Input layer type.</li><li><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</li><li><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output. If True, additional linear will be applied. i.e. x -&gt; x + linear(concat(x, att(x))) If False, no additional linear will be applied. i.e. x -&gt; x + att(x)</li><li><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</li><li><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</li><li><strong>rel_pos_type</strong> (<em>str</em>) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in <a href="https://github.com/espnet/espnet/pull/2816" target="_blank" rel="noopener noreferrer">https://github.com/espnet/espnet/pull/2816</a>.</li><li><strong>encoder_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</li><li><strong>encoder_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</li><li><strong>activation_type</strong> (<em>str</em>) – Encoder activation function type.</li><li><strong>macaron_style</strong> (<em>bool</em>) – Whether to use macaron style for positionwise layer.</li><li><strong>use_cnn_module</strong> (<em>bool</em>) – Whether to use convolution module.</li><li><strong>zero_triu</strong> (<em>bool</em>) – Whether to zero the upper triangular part of attention matrix.</li><li><strong>cnn_module_kernel</strong> (<em>int</em>) – Kernerl size of convolution module.</li><li><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</li><li><strong>attention_windows</strong> (<em>list</em>) – Layer-wise attention window sizes for longformer self-attn</li><li><strong>attention_dilation</strong> (<em>list</em>) – Layer-wise attention dilation sizes for longformer self-attn</li><li><strong>attention_mode</strong> (<em>str</em>) – Implementation for longformer self-attn. Default=”sliding_chunks” Choose ‘n2’, ‘tvm’ or ‘sliding_chunks’. More details in <a href="https://github.com/allenai/longformer" target="_blank" rel="noopener noreferrer">https://github.com/allenai/longformer</a></li></ul></li></ul><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p><div class="custom-h4"><p>forward<span class="small-bracket">(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, ctc: <a href="CTC.md#espnet2.asr.ctc.CTC">CTC</a></span> | None = None, return_all_hs: bool = False)</p></div><p>Calculate forward propagation.</p>',5),f=e("strong",null,"Parameters:",-1),h=s("<li><strong>xs_pad</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</li><li><strong>ilens</strong> (<em>torch.Tensor</em>) – Input length (#batch).</li><li><strong>prev_states</strong> (<em>torch.Tensor</em>) – Not to be used now.</li>",3),b=e("strong",null,"ctc",-1),y=e("em",null,"CTC",-1),w=e("li",null,[e("strong",null,"return_all_hs"),t(" ("),e("em",null,"bool"),t(") – whether to return all hidden states")],-1),v=e("li",null,[e("strong",null,"Returns:"),t(" Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.")],-1),T=e("li",null,[e("strong",null,"Return type:"),t(" torch.Tensor")],-1),k=e("div",{class:"custom-h4"},[e("p",null,"output_size()")],-1),z=e("div",{class:"custom-h4"},[e("p",null,[t("training "),e("em",null,": bool")])],-1);function x(C,L){const o=l("RouteLink");return _(),a("div",null,[c(" _espnet2.asr.encoder.longformer_encoder.LongformerEncoder "),m,u,e("p",null,[t("Bases: "),n(o,{to:"/guide/espnet2/asr/ConformerEncoder.html#espnet2.asr.encoder.conformer_encoder.ConformerEncoder"},{default:r(()=>[p]),_:1})]),g,e("ul",null,[e("li",null,[f,e("ul",null,[h,e("li",null,[b,t(" ("),n(o,{to:"/guide/espnet/nets/CTC.html#espnet.nets.chainer_backend.transformer.ctc.CTC"},{default:r(()=>[y]),_:1}),t(") – ctc module for intermediate CTC loss")]),w])]),v,T]),k,z])}const F=i(d,[["render",x],["__file","LongformerEncoder.html.vue"]]),I=JSON.parse(`{"path":"/guide/espnet2/asr/LongformerEncoder.html","title":"espnet2.asr.encoder.longformer_encoder.LongformerEncoder","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.71,"words":513},"filePathRelative":"guide/espnet2/asr/LongformerEncoder.md","excerpt":"<!-- _espnet2.asr.encoder.longformer_encoder.LongformerEncoder -->\\n<h1>espnet2.asr.encoder.longformer_encoder.LongformerEncoder</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.asr.encoder.longformer_encoder.LongformerEncoder<span class=\\"small-bracket\\">(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'abs_pos', selfattention_layer_type: str = 'lf_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, attention_windows: list = [100, 100, 100, 100, 100, 100], attention_dilation: list = [1, 1, 1, 1, 1, 1], attention_mode: str = 'sliding_chunks')</span></p></div>"}`);export{F as comp,I as data};

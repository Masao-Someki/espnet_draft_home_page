import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,f as o,a as n,o as s}from"./app-KOUU_Wij.js";const a={},r=n('<h1 id="espnet-nets-pytorch-backend-transformer-plot-plotattentionreport" tabindex="-1"><a class="header-anchor" href="#espnet-nets-pytorch-backend-transformer-plot-plotattentionreport"><span>espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport</span></a></h1><div class="custom-h3"><p><em>class</em> espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport<span class="small-bracket">(att_vis_fn, data, outdir, converter, transform, device, reverse=False, ikey=&#39;input&#39;, iaxis=0, okey=&#39;output&#39;, oaxis=0, subsampling_factor=1)</span></p></div><p>Bases: <code>PlotAttentionReport</code></p><div class="custom-h4"><p>get_attention_weights()</p></div><p>Return attention weights.</p><ul><li><strong>Returns:</strong> attention weights. float. Its shape would be : differ from backend. * pytorch-&gt; 1) multi-head case =&gt; (B, H, Lmax, Tmax), 2) <blockquote><p>other case =&gt; (B, Lmax, Tmax).</p></blockquote><ul><li>chainer-&gt; (B, Lmax, Tmax)</li></ul></li><li><strong>Return type:</strong> numpy.ndarray</li></ul><div class="custom-h4"><p>log_attentions<span class="small-bracket">(logger, step)</span></p></div><p>Add image files of att_ws matrix to the tensorboard.</p><div class="custom-h4"><p>plotfn<span class="small-bracket">(*args, **kwargs)</span></p></div>',9);function p(i,l){return s(),e("div",null,[o(" _espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport "),r])}const d=t(a,[["render",p],["__file","PlotAttentionReport.html.vue"]]),h=JSON.parse(`{"path":"/guide/espnet/nets/PlotAttentionReport.html","title":"espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.41,"words":122},"filePathRelative":"guide/espnet/nets/PlotAttentionReport.md","excerpt":"<!-- _espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport -->\\n<h1>espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport<span class=\\"small-bracket\\">(att_vis_fn, data, outdir, converter, transform, device, reverse=False, ikey='input', iaxis=0, okey='output', oaxis=0, subsampling_factor=1)</span></p></div>"}`);export{d as comp,h as data};

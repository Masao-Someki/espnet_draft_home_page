import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as s,c as n,f as a,b as e,d as t,e as i,w as l,a as d,o as c}from"./app-KOUU_Wij.js";const m={},p=e("h1",{id:"espnet-nets-pytorch-backend-e2e-tts-fastspeech-feedforwardtransformer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet-nets-pytorch-backend-e2e-tts-fastspeech-feedforwardtransformer"},[e("span",null,"espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer")])],-1),u=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer"),e("span",{class:"small-bracket"},"(idim, odim, args=None)")])],-1),f=e("code",null,"TTSInterface",-1),g=e("code",null,"Module",-1),h=d('<p>Feed Forward Transformer for TTS a.k.a. FastSpeech.</p><p>This is a module of FastSpeech, feed-forward Transformer with duration predictor described in <a href="https://arxiv.org/pdf/1905.09263.pdf" target="_blank" rel="noopener noreferrer">FastSpeech: Fast, Robust and Controllable Text to Speech</a>, which does not require any auto-regressive processing during inference, resulting in fast decoding compared with auto-regressive Transformer.</p><p>Initialize feed-forward Transformer module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>idim</strong> (<em>int</em>) – Dimension of the inputs.</li><li><strong>odim</strong> (<em>int</em>) – Dimension of the outputs.</li><li><strong>args</strong> (<em>Namespace</em> <em>,</em> <em>optional</em>) – <ul><li>elayers (int): Number of encoder layers.</li><li>eunits (int): Number of encoder hidden units.</li><li>adim (int): Number of attention transformation dimensions.</li><li>aheads (int): Number of heads for multi head attention.</li><li>dlayers (int): Number of decoder layers.</li><li>dunits (int): Number of decoder hidden units.</li><li>use_scaled_pos_enc (bool): : Whether to use trainable scaled positional encoding.</li><li>encoder_normalize_before (bool): : Whether to perform layer normalization before encoder block.</li><li>decoder_normalize_before (bool): : Whether to perform layer normalization before decoder block.</li><li>encoder_concat_after (bool): Whether to concatenate attention : layer’s input and output in encoder.</li><li>decoder_concat_after (bool): Whether to concatenate attention : layer’s input and output in decoder.</li><li>duration_predictor_layers (int): Number of duration predictor layers.</li><li>duration_predictor_chans (int): Number of duration predictor channels.</li><li>duration_predictor_kernel_size (int): : Kernel size of duration predictor.</li><li>spk_embed_dim (int): Number of speaker embedding dimensions.</li><li>spk_embed_integration_type: How to integrate speaker embedding.</li><li>teacher_model (str): Teacher auto-regressive transformer model path.</li><li>reduction_factor (int): Reduction factor.</li><li>transformer_init (float): How to initialize transformer parameters.</li><li>transformer_lr (float): Initial value of learning rate.</li><li>transformer_warmup_steps (int): Optimizer warmup steps.</li><li>transformer_enc_dropout_rate (float): : Dropout rate in encoder except attention &amp; positional encoding.</li><li>transformer_enc_positional_dropout_rate (float): : Dropout rate after encoder positional encoding.</li><li>transformer_enc_attn_dropout_rate (float): : Dropout rate in encoder self-attention module.</li><li>transformer_dec_dropout_rate (float): : Dropout rate in decoder except attention &amp; positional encoding.</li><li>transformer_dec_positional_dropout_rate (float): : Dropout rate after decoder positional encoding.</li><li>transformer_dec_attn_dropout_rate (float): : Dropout rate in deocoder self-attention module.</li><li>transformer_enc_dec_attn_dropout_rate (float): : Dropout rate in encoder-deocoder attention module.</li><li>use_masking (bool): : Whether to apply masking for padded part in loss calculation.</li><li>use_weighted_masking (bool): : Whether to apply weighted masking in loss calculation.</li><li>transfer_encoder_from_teacher: : Whether to transfer encoder using teacher encoder parameters.</li><li>transferred_encoder_module: : Encoder module to be initialized using teacher parameters.</li></ul></li></ul></li></ul><div class="custom-h4"><p><em>static</em> add_arguments<span class="small-bracket">(parser)</span></p></div><p>Add model-specific arguments to the parser.</p><div class="custom-h4"><p><em>property</em> attention_plot_class</p></div><p>Return plot class for attention weight plot.</p><div class="custom-h4"><p><em>property</em> base_plot_keys</p></div><p>Return base key names to plot during training.</p><p>keys should match what chainer.reporter reports. If you add the key loss, the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values.</p><ul><li><strong>Returns:</strong> List of strings which are base keys to plot during training.</li><li><strong>Return type:</strong> list</li></ul><div class="custom-h4"><p>calculate_all_attentions<span class="small-bracket">(xs, ilens, ys, olens, spembs=None, extras=None, *args, **kwargs)</span></p></div><p>Calculate all of the attention weights.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</li><li><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</li><li><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</li><li><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</li><li><strong>spembs</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of speaker embedding vectors (B, spk_embed_dim).</li><li><strong>extras</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of precalculated durations (B, Tmax, 1).</li></ul></li><li><strong>Returns:</strong> Dict of attention weights and outputs.</li><li><strong>Return type:</strong> dict</li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(xs, ilens, ys, olens, spembs=None, extras=None, *args, **kwargs)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</li><li><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</li><li><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</li><li><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</li><li><strong>spembs</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of speaker embedding vectors (B, spk_embed_dim).</li><li><strong>extras</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Batch of precalculated durations (B, Tmax, 1).</li></ul></li><li><strong>Returns:</strong> Loss value.</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>inference<span class="small-bracket">(x, inference_args, spemb=None, *args, **kwargs)</span></p></div><p>Generate the sequence of features given the sequences of characters.</p><ul><li><strong>Parameters:</strong><ul><li><strong>x</strong> (<em>Tensor</em>) – Input sequence of characters (T,).</li><li><strong>inference_args</strong> (<em>Namespace</em>) – Dummy for compatibility.</li><li><strong>spemb</strong> (<em>Tensor</em> <em>,</em> <em>optional</em>) – Speaker embedding vector (spk_embed_dim).</li></ul></li><li><strong>Returns:</strong> Output sequence of features (L, odim). None: Dummy for compatibility. None: Dummy for compatibility.</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',22);function _(b,T){const r=s("RouteLink");return c(),n("div",null,[a(" _espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer "),p,u,e("p",null,[t("Bases: "),i(r,{to:"/guide/espnet/nets/TTSInterface.html#espnet.nets.tts_interface.TTSInterface"},{default:l(()=>[f]),_:1}),t(", "),g]),h])}const w=o(m,[["render",_],["__file","FeedForwardTransformer.html.vue"]]),v=JSON.parse('{"path":"/guide/espnet/nets/FeedForwardTransformer.html","title":"espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.52,"words":756},"filePathRelative":"guide/espnet/nets/FeedForwardTransformer.md","excerpt":"<!-- _espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer -->\\n<h1>espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer<span class=\\"small-bracket\\">(idim, odim, args=None)</span></p></div>"}');export{w as comp,v as data};

import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r,c as s,f as i,b as e,d as t,e as a,w as l,a as m,o as _}from"./app-KOUU_Wij.js";const d={},g=e("h1",{id:"espnet2-tts-transformer-transformer-transformer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#espnet2-tts-transformer-transformer-transformer"},[e("span",null,"espnet2.tts.transformer.transformer.Transformer")])],-1),p=e("div",{class:"custom-h3"},[e("p",null,[e("em",null,"class"),t(" espnet2.tts.transformer.transformer.Transformer"),e("span",{class:"small-bracket"},"(idim: int, odim: int, embed_dim: int = 512, eprenet_conv_layers: int = 3, eprenet_conv_chans: int = 256, eprenet_conv_filts: int = 5, dprenet_layers: int = 2, dprenet_units: int = 256, elayers: int = 6, eunits: int = 1024, adim: int = 512, aheads: int = 4, dlayers: int = 6, dunits: int = 1024, postnet_layers: int = 5, postnet_chans: int = 256, postnet_filts: int = 5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128)"),t(", gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, transformer_enc_dec_attn_dropout_rate: float = 0.1, eprenet_dropout_rate: float = 0.5, dprenet_dropout_rate: float = 0.5, postnet_dropout_rate: float = 0.5, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1', use_guided_attn_loss: bool = True, num_heads_applied_guided_attn: int = 2, num_layers_applied_guided_attn: int = 2, modules_applied_guided_attn: Sequence[str] = 'encoder-decoder', guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)")])],-1),c=e("code",null,"AbsTTS",-1),u=m('<p>Transformer-TTS module.</p><p>This is a module of text-to-speech Transformer described in <a href="https://arxiv.org/pdf/1809.08895.pdf" target="_blank" rel="noopener noreferrer">Neural Speech Synthesis with Transformer Network</a>, which convert the sequence of tokens into the sequence of Mel-filterbanks.</p><p>Initialize Transformer module.</p><ul><li><strong>Parameters:</strong><ul><li><strong>idim</strong> (<em>int</em>) – Dimension of the inputs.</li><li><strong>odim</strong> (<em>int</em>) – Dimension of the outputs.</li><li><strong>embed_dim</strong> (<em>int</em>) – Dimension of character embedding.</li><li><strong>eprenet_conv_layers</strong> (<em>int</em>) – Number of encoder prenet convolution layers.</li><li><strong>eprenet_conv_chans</strong> (<em>int</em>) – Number of encoder prenet convolution channels.</li><li><strong>eprenet_conv_filts</strong> (<em>int</em>) – Filter size of encoder prenet convolution.</li><li><strong>dprenet_layers</strong> (<em>int</em>) – Number of decoder prenet layers.</li><li><strong>dprenet_units</strong> (<em>int</em>) – Number of decoder prenet hidden units.</li><li><strong>elayers</strong> (<em>int</em>) – Number of encoder layers.</li><li><strong>eunits</strong> (<em>int</em>) – Number of encoder hidden units.</li><li><strong>adim</strong> (<em>int</em>) – Number of attention transformation dimensions.</li><li><strong>aheads</strong> (<em>int</em>) – Number of heads for multi head attention.</li><li><strong>dlayers</strong> (<em>int</em>) – Number of decoder layers.</li><li><strong>dunits</strong> (<em>int</em>) – Number of decoder hidden units.</li><li><strong>postnet_layers</strong> (<em>int</em>) – Number of postnet layers.</li><li><strong>postnet_chans</strong> (<em>int</em>) – Number of postnet channels.</li><li><strong>postnet_filts</strong> (<em>int</em>) – Filter size of postnet.</li><li><strong>use_scaled_pos_enc</strong> (<em>bool</em>) – Whether to use trainable scaled pos encoding.</li><li><strong>use_batch_norm</strong> (<em>bool</em>) – Whether to use batch normalization in encoder prenet.</li><li><strong>encoder_normalize_before</strong> (<em>bool</em>) – Whether to apply layernorm layer before encoder block.</li><li><strong>decoder_normalize_before</strong> (<em>bool</em>) – Whether to apply layernorm layer before decoder block.</li><li><strong>encoder_concat_after</strong> (<em>bool</em>) – Whether to concatenate attention layer’s input and output in encoder.</li><li><strong>decoder_concat_after</strong> (<em>bool</em>) – Whether to concatenate attention layer’s input and output in decoder.</li><li><strong>positionwise_layer_type</strong> (<em>str</em>) – Position-wise operation type.</li><li><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size in position wise conv 1d.</li><li><strong>reduction_factor</strong> (<em>int</em>) – Reduction factor.</li><li><strong>spks</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Number of speakers. If set to &gt; 1, assume that the sids will be provided as the input and use sid embedding layer.</li><li><strong>langs</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Number of languages. If set to &gt; 1, assume that the lids will be provided as the input and use sid embedding layer.</li><li><strong>spk_embed_dim</strong> (<em>Optional</em> *[*<em>int</em> <em>]</em>) – Speaker embedding dimension. If set to &gt; 0, assume that spembs will be provided as the input.</li><li><strong>spk_embed_integration_type</strong> (<em>str</em>) – How to integrate speaker embedding.</li><li><strong>use_gst</strong> (<em>str</em>) – Whether to use global style token.</li><li><strong>gst_tokens</strong> (<em>int</em>) – Number of GST embeddings.</li><li><strong>gst_heads</strong> (<em>int</em>) – Number of heads in GST multihead attention.</li><li><strong>gst_conv_layers</strong> (<em>int</em>) – Number of conv layers in GST.</li><li><strong>gst_conv_chans_list</strong> – (Sequence[int]): List of the number of channels of conv layers in GST.</li><li><strong>gst_conv_kernel_size</strong> (<em>int</em>) – Kernel size of conv layers in GST.</li><li><strong>gst_conv_stride</strong> (<em>int</em>) – Stride size of conv layers in GST.</li><li><strong>gst_gru_layers</strong> (<em>int</em>) – Number of GRU layers in GST.</li><li><strong>gst_gru_units</strong> (<em>int</em>) – Number of GRU units in GST.</li><li><strong>transformer_lr</strong> (<em>float</em>) – Initial value of learning rate.</li><li><strong>transformer_warmup_steps</strong> (<em>int</em>) – Optimizer warmup steps.</li><li><strong>transformer_enc_dropout_rate</strong> (<em>float</em>) – Dropout rate in encoder except attention and positional encoding.</li><li><strong>transformer_enc_positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after encoder positional encoding.</li><li><strong>transformer_enc_attn_dropout_rate</strong> (<em>float</em>) – Dropout rate in encoder self-attention module.</li><li><strong>transformer_dec_dropout_rate</strong> (<em>float</em>) – Dropout rate in decoder except attention &amp; positional encoding.</li><li><strong>transformer_dec_positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after decoder positional encoding.</li><li><strong>transformer_dec_attn_dropout_rate</strong> (<em>float</em>) – Dropout rate in decoder self-attention module.</li><li><strong>transformer_enc_dec_attn_dropout_rate</strong> (<em>float</em>) – Dropout rate in source attention module.</li><li><strong>init_type</strong> (<em>str</em>) – How to initialize transformer parameters.</li><li><strong>init_enc_alpha</strong> (<em>float</em>) – Initial value of alpha in scaled pos encoding of the encoder.</li><li><strong>init_dec_alpha</strong> (<em>float</em>) – Initial value of alpha in scaled pos encoding of the decoder.</li><li><strong>eprenet_dropout_rate</strong> (<em>float</em>) – Dropout rate in encoder prenet.</li><li><strong>dprenet_dropout_rate</strong> (<em>float</em>) – Dropout rate in decoder prenet.</li><li><strong>postnet_dropout_rate</strong> (<em>float</em>) – Dropout rate in postnet.</li><li><strong>use_masking</strong> (<em>bool</em>) – Whether to apply masking for padded part in loss calculation.</li><li><strong>use_weighted_masking</strong> (<em>bool</em>) – Whether to apply weighted masking in loss calculation.</li><li><strong>bce_pos_weight</strong> (<em>float</em>) – Positive sample weight in bce calculation (only for use_masking=true).</li><li><strong>loss_type</strong> (<em>str</em>) – How to calculate loss.</li><li><strong>use_guided_attn_loss</strong> (<em>bool</em>) – Whether to use guided attention loss.</li><li><strong>num_heads_applied_guided_attn</strong> (<em>int</em>) – Number of heads in each layer to apply guided attention loss.</li><li><strong>num_layers_applied_guided_attn</strong> (<em>int</em>) – Number of layers to apply guided attention loss.</li><li><strong>modules_applied_guided_attn</strong> (<em>Sequence</em> *[*<em>str</em> <em>]</em>) – List of module names to apply guided attention loss.</li><li><strong>guided_attn_loss_sigma</strong> (<em>float</em>) –</li><li><strong>guided_attn_loss_lambda</strong> (<em>float</em>) – Lambda in guided attention loss.</li></ul></li></ul><div class="custom-h4"><p>forward<span class="small-bracket">(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False)</span></p></div><p>Calculate forward propagation.</p><ul><li><strong>Parameters:</strong><ul><li><strong>text</strong> (<em>LongTensor</em>) – Batch of padded character ids (B, Tmax).</li><li><strong>text_lengths</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</li><li><strong>feats</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</li><li><strong>feats_lengths</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</li><li><strong>spembs</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of speaker embeddings (B, spk_embed_dim).</li><li><strong>sids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of speaker IDs (B, 1).</li><li><strong>lids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Batch of language IDs (B, 1).</li><li><strong>joint_training</strong> (<em>bool</em>) – Whether to perform joint training with vocoder.</li></ul></li><li><strong>Returns:</strong> Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.</li><li><strong>Return type:</strong> Tensor</li></ul><div class="custom-h4"><p>inference<span class="small-bracket">(text: Tensor, feats: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_teacher_forcing: bool = False)</span></p></div><p>Generate the sequence of features given the sequences of characters.</p><ul><li><strong>Parameters:</strong><ul><li><strong>text</strong> (<em>LongTensor</em>) – Input sequence of characters (T_text,).</li><li><strong>feats</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Feature sequence to extract style embedding (T_feats’, idim).</li><li><strong>spembs</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Speaker embedding (spk_embed_dim,).</li><li><strong>sids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Speaker ID (1,).</li><li><strong>lids</strong> (<em>Optional</em> *[*<em>Tensor</em> <em>]</em>) – Language ID (1,).</li><li><strong>threshold</strong> (<em>float</em>) – Threshold in inference.</li><li><strong>minlenratio</strong> (<em>float</em>) – Minimum length ratio in inference.</li><li><strong>maxlenratio</strong> (<em>float</em>) – Maximum length ratio in inference.</li><li><strong>use_teacher_forcing</strong> (<em>bool</em>) – Whether to use teacher forcing.</li></ul></li><li><strong>Returns:</strong> Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). <ul><li>prob (Tensor): Output sequence of stop probabilities (T_feats,).</li><li>att_w (Tensor): Source attn weight (#layers, #heads, T_feats, T_text).</li></ul></li><li><strong>Return type:</strong> Dict[str, Tensor]</li></ul><div class="custom-h4"><p>training <em>: bool</em></p></div>',11);function f(h,b){const o=r("RouteLink");return _(),s("div",null,[i(" _espnet2.tts.transformer.transformer.Transformer "),g,p,e("p",null,[t("Bases: "),a(o,{to:"/guide/espnet2/tts/AbsTTS.html#espnet2.tts.abs_tts.AbsTTS"},{default:l(()=>[c]),_:1})]),u])}const v=n(d,[["render",f],["__file","Transformer.html.vue"]]),N=JSON.parse(`{"path":"/guide/espnet2/tts/Transformer.html","title":"espnet2.tts.transformer.transformer.Transformer","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":3.83,"words":1148},"filePathRelative":"guide/espnet2/tts/Transformer.md","excerpt":"<!-- _espnet2.tts.transformer.transformer.Transformer -->\\n<h1>espnet2.tts.transformer.transformer.Transformer</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.tts.transformer.transformer.Transformer<span class=\\"small-bracket\\">(idim: int, odim: int, embed_dim: int = 512, eprenet_conv_layers: int = 3, eprenet_conv_chans: int = 256, eprenet_conv_filts: int = 5, dprenet_layers: int = 2, dprenet_units: int = 256, elayers: int = 6, eunits: int = 1024, adim: int = 512, aheads: int = 4, dlayers: int = 6, dunits: int = 1024, postnet_layers: int = 5, postnet_chans: int = 256, postnet_filts: int = 5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128)</span>, gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, transformer_enc_dec_attn_dropout_rate: float = 0.1, eprenet_dropout_rate: float = 0.5, dprenet_dropout_rate: float = 0.5, postnet_dropout_rate: float = 0.5, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1', use_guided_attn_loss: bool = True, num_heads_applied_guided_attn: int = 2, num_layers_applied_guided_attn: int = 2, modules_applied_guided_attn: Sequence[str] = 'encoder-decoder', guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)</p></div>"}`);export{v as comp,N as data};

import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,o as e,a}from"./app-KOUU_Wij.js";const t={},n=a(`<h1 id="cmu-11492-11692-spring-2023-speaker-recognition" tabindex="-1"><a class="header-anchor" href="#cmu-11492-11692-spring-2023-speaker-recognition"><span>CMU 11492/11692 Spring 2023: Speaker Recognition</span></a></h1><p>In this demonstration, we will show you the procedure to conduct speaker recognition with the ASR functions of ESPnet.</p><p>Main references:</p><ul><li><a href="https://github.com/espnet/espnet" target="_blank" rel="noopener noreferrer">ESPnet repository</a></li><li><a href="https://espnet.github.io/espnet/" target="_blank" rel="noopener noreferrer">ESPnet documentation</a></li></ul><p>Author:</p><ul><li>Jiatong Shi (jiatongs@andrew.cmu.edu)</li></ul><h2 id="objectives" tabindex="-1"><a class="header-anchor" href="#objectives"><span>Objectives</span></a></h2><p>After this demonstration, you are expected to understand the main procedure of using ESPnet ASR functions for speaker recognition.</p><h2 id="❗important-notes❗" tabindex="-1"><a class="header-anchor" href="#❗important-notes❗"><span>❗Important Notes❗</span></a></h2><ul><li>We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.</li><li>There are multiple in-class checkpoints ✅ throughout this tutorial. <strong>Your participation points are based on these tasks.</strong> Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.</li><li>Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using <code>File -&gt; Print</code> in the menu bar.</li></ul><h2 id="espnet-installation" tabindex="-1"><a class="header-anchor" href="#espnet-installation"><span>ESPnet installation</span></a></h2><p>We follow the ESPnet installation as the previous tutorials (takes around 15 minutes).</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">!</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">git clone </span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">--</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">depth </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;"> -</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">b </span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">2023spring_speaker_recognition</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> https:</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">//</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">github.com</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">espnet</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">espnet</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">%</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">cd </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">content</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">espnet</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tools</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!.</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">setup_anaconda.sh anaconda espnet </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">3.9</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># # It may take 12 minutes</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">%</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">cd </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">content</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">espnet</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tools</span></span>
<span class="line"><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">!</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">make </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">TH_VERSION</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1.12</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">.1 </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">CUDA_VERSION</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">11.6</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!. .</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">activate_python.sh </span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">&amp;&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> installers</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">install_speechbrain.sh</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!. .</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">activate_python.sh </span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">&amp;&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> installers</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">install_rawnet.sh</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!. .</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">activate_python.sh </span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">&amp;&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> pip install ipykernel</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="speaker-recognition" tabindex="-1"><a class="header-anchor" href="#speaker-recognition"><span>Speaker Recognition</span></a></h2><p>Speaker recognition is a typical task that conduct utterance-level classification. Specifically, we will map an utterance into a pre-defined category. Recall that the ASR is doing a sequence-to-sequence task, so we can easily utilize ASR by using a 1-length sequence (i.e., class). Following this concept, we can start to implement the speaker recognition system! Noted that following the definition of the lecture, today, we will focus on <strong>speaker identification</strong> (close-set classification) instead of <strong>speaker verification</strong>.</p><h2 id="dataset" tabindex="-1"><a class="header-anchor" href="#dataset"><span>Dataset</span></a></h2><p><code>mini_librispeech</code> is a tiny subset of <code>librispeech</code> dataset for development usage. Because of the free-license and cleaness of the data, <code>librispeech</code> has been one of the most widely used corpora in the speech community. For more details, please refer to its <a href="http://www.danielpovey.com/files/2015_icassp_librispeech.pdf" target="_blank" rel="noopener noreferrer">original paper</a>. In this demonstration, we will use the train set of <code>mini_librispeech</code> to train and test a simple speaker recognition model.</p><p>First of all, let&#39;s get into the directory to check the structure.</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">%</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">cd </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">content</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">espnet</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">egs2</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">mini_librispeech</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">sid1</span></span>
<span class="line"><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">!</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">ls </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">l</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="data-preparation" tabindex="-1"><a class="header-anchor" href="#data-preparation"><span>Data Preparation</span></a></h2><p>Similar to the previous tutorials, we will use the Kaldi-style format for the data preparation. The differences in this recipe is that we need to predict speaker ID instead of predicting transcription. Therefore, a straightforward process is to simply change the <code>text</code> into <code>utt2spk</code>.</p><p>So final files after preparation should be:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>wav.scp text utt2spk spk2utt</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>But on the other hand, we change the format of text into</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>utt_id1 spk_id0</span></span>
<span class="line"><span>utt_id2 spk_id0</span></span>
<span class="line"><span>utt_id3 spk_id1</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>where <code>spk_id0</code> and <code>spk_id1</code> refers to the speaker IDs</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!.</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">run.sh </span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">--</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">stage </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;"> --</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">stop_stage </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h2 id="data-preprocessing" tabindex="-1"><a class="header-anchor" href="#data-preprocessing"><span>Data Preprocessing</span></a></h2><p>For data preprocessing, we follow the similar way in previous tutorials/assignments.</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!.</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">run.sh </span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">--</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">stage </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;"> --</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">stop_stage </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">5</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="question1-✅-checkpoint-1-1-point" tabindex="-1"><a class="header-anchor" href="#question1-✅-checkpoint-1-1-point"><span>Question1 (✅ Checkpoint 1 (1 point))</span></a></h3><p>In previous tutorials, we usually use character as our modeling units. But for here, we use a speaker id, which is a sequence of character, representing one speaker. So, in our preprocessing, which tokenizer (e.g., char, bpe, phn, word) is actually used to achieve speaker prediction? Please also indicate your reason(s).</p><p>To help you understand more, please check the documentation at https://espnet.github.io/espnet/search.html?q=tokenizer&amp;check_keywords=yes&amp;area=default</p><p>(For question-based checkpoint: please directly answer it in the text box)</p><p>[ANSWER HERE]</p><h2 id="use-pre-trained-speaker-representation" tabindex="-1"><a class="header-anchor" href="#use-pre-trained-speaker-representation"><span>Use Pre-trained speaker representation</span></a></h2><p>One feature in ESPnet is to adopt pre-trained speaker representation from other toolkits (including TDNN-based speaker embedding extraction from <a href="https://github.com/speechbrain/speechbrain" target="_blank" rel="noopener noreferrer">speechbrain</a> and RawNet-based speaker embedding from <a href="https://github.com/Jungjee/RawNet" target="_blank" rel="noopener noreferrer">RawNet</a>. We can efficiently extract the speaker embedding with our supported scripts.</p><p>The speaker embedding can be used for text-to-speech purpose to handle multi-speaker synthesis. In this demonstration, we directly use the extraction model for speaker recognition.</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">!</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">cat .</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">local</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">extract_xvector.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!.</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">local</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">extract_xvector.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>After calculating the xvectors, we also can analysis the embedding by <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank" rel="noopener noreferrer">t-SNE algorithm</a>. The t-sne image is located at the extracted xvector folder</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> IPython.display </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> Image, display</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">display</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">Image</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&#39;dump/extracted/train/tsne.png&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="extract-speaker-embedding-from-speechbrain" tabindex="-1"><a class="header-anchor" href="#extract-speaker-embedding-from-speechbrain"><span>Extract speaker embedding from SpeechBrain</span></a></h3><p>Similarly, we can also extract speaker embedding from speechbrain.</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">!</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">cat .</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">local</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">extract_xvector_speechbrain.sh</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!.</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">local</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">extract_xvector_speechbrain.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>Similar to the speechbrain-based embedding, we can visualize the embeddings from RawNet with t-SNE plot.</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> IPython.display </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> Image, display</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">display</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">Image</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&#39;dump/extracted_speechbrain/train/tsne.png&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="training-for-speaker-recognition" tabindex="-1"><a class="header-anchor" href="#training-for-speaker-recognition"><span>Training for speaker recognition</span></a></h2><p>First, let&#39;s use xvector trained from TDNN (speech-brain model) to conduct speaker recognition.</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">!</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">cat .</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">run_xvector_speechbrain.sh</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!.</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">run_xvector_speechbrain.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="question2-✅-checkpoint-2-0-5-point" tabindex="-1"><a class="header-anchor" href="#question2-✅-checkpoint-2-0-5-point"><span>Question2 (✅ Checkpoint 2 (0.5 point))</span></a></h3><p>We still use the ASR scoring scheme for our evaluation because it is already sufficient. Please briefly discuss which metric can be used for evaluation of the accuracy/error rate of speaker recognition results.</p><p>(For question-based checkpoint: please directly answer it in the text box)</p><p>[ANSWER HERE]</p><p>Then, let&#39;s use RawNet-based xvector to conduct speaker recognition</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">!</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">cat .</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">run_xvector.sh</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!.</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">run_xvector.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="question3-✅-checkpoint-3-0-5-point" tabindex="-1"><a class="header-anchor" href="#question3-✅-checkpoint-3-0-5-point"><span>Question3 (✅ Checkpoint 3 (0.5 point))</span></a></h3><p>Clearly, we find some differences in the number between TDNN-based speaker embedding and RawNet-based speaker embedding. Could you briefly exaplin some possible reasons that why we could get such different results?</p><p>References:</p><ul><li><a href="https://arxiv.org/abs/2203.08488" target="_blank" rel="noopener noreferrer">RawNet</a></li><li><a href="https://www.danielpovey.com/files/2018_icassp_xvectors.pdf" target="_blank" rel="noopener noreferrer">Xvector (TDNN-based)</a></li></ul><p>(For question-based checkpoint: please directly answer it in the text box)</p><p>[ANSWER HERE]</p><p>We can also use ESPnet ASR model directly for speaker recognition purpose by predicting the target as speaker ID.</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">!.</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">run.sh </span><span style="--shiki-light:#B31D28;--shiki-dark:#FFFFFF;--shiki-light-font-style:italic;--shiki-dark-font-style:inherit;">--</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">stage </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="question4-✅-checkpoint-4-0-5-point" tabindex="-1"><a class="header-anchor" href="#question4-✅-checkpoint-4-0-5-point"><span>Question4 (✅ Checkpoint 4 (0.5 point))</span></a></h3><p>We could get reasonable performances with the ASR model. However, we could easily find that the training is much more time-consuming than those with speaker embeddings. Could you please explain why we have such differences?</p><p>(For question-based checkpoint: please directly answer it in the text box)</p><p>[ANSWER HERE]</p>`,68),h=[n];function l(r,p){return e(),s("div",null,h)}const d=i(t,[["render",l],["__file","assignment3_spk.html.vue"]]),c=JSON.parse('{"path":"/notebook/ESPnet2/Course/CMU_SpeechProcessing_Spring2023/assignment3_spk.html","title":"CMU 11492/11692 Spring 2023: Speaker Recognition","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"Objectives","slug":"objectives","link":"#objectives","children":[]},{"level":2,"title":"❗Important Notes❗","slug":"❗important-notes❗","link":"#❗important-notes❗","children":[]},{"level":2,"title":"ESPnet installation","slug":"espnet-installation","link":"#espnet-installation","children":[]},{"level":2,"title":"Speaker Recognition","slug":"speaker-recognition","link":"#speaker-recognition","children":[]},{"level":2,"title":"Dataset","slug":"dataset","link":"#dataset","children":[]},{"level":2,"title":"Data Preparation","slug":"data-preparation","link":"#data-preparation","children":[]},{"level":2,"title":"Data Preprocessing","slug":"data-preprocessing","link":"#data-preprocessing","children":[{"level":3,"title":"Question1  (✅ Checkpoint 1 (1 point))","slug":"question1-✅-checkpoint-1-1-point","link":"#question1-✅-checkpoint-1-1-point","children":[]}]},{"level":2,"title":"Use Pre-trained speaker representation","slug":"use-pre-trained-speaker-representation","link":"#use-pre-trained-speaker-representation","children":[{"level":3,"title":"Extract speaker embedding from SpeechBrain","slug":"extract-speaker-embedding-from-speechbrain","link":"#extract-speaker-embedding-from-speechbrain","children":[]}]},{"level":2,"title":"Training for speaker recognition","slug":"training-for-speaker-recognition","link":"#training-for-speaker-recognition","children":[{"level":3,"title":"Question2  (✅ Checkpoint 2 (0.5 point))","slug":"question2-✅-checkpoint-2-0-5-point","link":"#question2-✅-checkpoint-2-0-5-point","children":[]},{"level":3,"title":"Question3  (✅ Checkpoint 3 (0.5 point))","slug":"question3-✅-checkpoint-3-0-5-point","link":"#question3-✅-checkpoint-3-0-5-point","children":[]},{"level":3,"title":"Question4  (✅ Checkpoint 4 (0.5 point))","slug":"question4-✅-checkpoint-4-0-5-point","link":"#question4-✅-checkpoint-4-0-5-point","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":3.29,"words":986},"filePathRelative":"notebook/ESPnet2/Course/CMU_SpeechProcessing_Spring2023/assignment3_spk.md","excerpt":"\\n<p>In this demonstration, we will show you the procedure to conduct speaker recognition with the ASR functions of ESPnet.</p>\\n<p>Main references:</p>\\n<ul>\\n<li><a href=\\"https://github.com/espnet/espnet\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">ESPnet repository</a></li>\\n<li><a href=\\"https://espnet.github.io/espnet/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">ESPnet documentation</a></li>\\n</ul>"}');export{d as comp,c as data};

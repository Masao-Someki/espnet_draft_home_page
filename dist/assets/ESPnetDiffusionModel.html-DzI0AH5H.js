import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as o,c as i,f as a,b as e,d as r,e as l,w as c,a as s,o as d}from"./app-KOUU_Wij.js";const p={},h=s('<h1 id="espnet2-enh-diffusion-enh-espnetdiffusionmodel" tabindex="-1"><a class="header-anchor" href="#espnet2-enh-diffusion-enh-espnetdiffusionmodel"><span>espnet2.enh.diffusion_enh.ESPnetDiffusionModel</span></a></h1><div class="custom-h3"><p><em>class</em> espnet2.enh.diffusion_enh.ESPnetDiffusionModel<span class="small-bracket">(encoder: <a href="AbsEncoder.md#espnet2.enh.encoder.abs_encoder.AbsEncoder">AbsEncoder</a></span>, diffusion: <a href="AbsDiffusion.md#espnet2.enh.diffusion.abs_diffusion.AbsDiffusion">AbsDiffusion</a>, decoder: <a href="AbsDecoder.md#espnet2.enh.decoder.abs_decoder.AbsDecoder">AbsDecoder</a>, num_spk: int = 1, normalize: bool = False, **kwargs)</p></div>',2),f=e("code",null,"ESPnetEnhancementModel",-1),m=s('<p>Target Speaker Extraction Frontend model</p><p>Main entry of speech enhancement/separation model training.</p><ul><li><strong>Parameters:</strong><ul><li><p><strong>encoder</strong> – waveform encoder that converts waveforms to feature representations</p></li><li><p><strong>separator</strong> – separator that enhance or separate the feature representations</p></li><li><p><strong>decoder</strong> – waveform decoder that converts the feature back to waveforms</p></li><li><p><strong>mask_module</strong> – mask module that converts the feature to masks NOTE: Only used for compatibility with joint speaker diarization. See test/espnet2/enh/test_espnet_enh_s2t_model.py for details.</p></li><li><p><strong>loss_wrappers</strong> – list of loss wrappers Each loss wrapper contains a criterion for loss calculation and the corresonding loss weight. The losses will be calculated in the order of the list and summed up.</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>stft_consistency</strong> – (deprecated, kept for compatibility) whether to compute the TF-domain loss while enforcing STFT consistency NOTE: STFT consistency is now always used for frequency-domain spectrum losses.</p></li><li><p><strong>loss_type</strong> – (deprecated, kept for compatibility) loss type</p></li><li><p><strong>mask_type</strong> – (deprecated, kept for compatibility) mask type in TF-domain model</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>flexible_numspk</strong> – whether to allow the model to predict a variable number of speakers in its output. NOTE: This should be used when training a speech separation model for unknown number of speakers.</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>extract_feats_in_collect_stats</strong> – used in espnet2/tasks/abs_task.py for determining whether or not to skip model building in collect_stats stage (stage 5 in egs2/</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>*</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>/enh1/enh.sh).</p></li><li><p><strong>normalize_variance</strong> – whether to normalize the signal variance before model forward, and revert it back after.</p></li><li><p><strong>normalize_variance_per_ch</strong> – whether to normalize the signal variance for each channel instead of the whole signal. NOTE: normalize_variance and normalize_variance_per_ch cannot be True at the same time.</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>categories</strong> – list of all possible categories of minibatches (order matters!) (e.g. [“1ch_8k_reverb”, “1ch_8k_both”] for multi-condition training) NOTE: this will be used to convert category index to the corresponding name for logging in forward_loss. Different categories will have different loss name suffixes.</p></li><li><p><strong>category_weights</strong> – list of weights for each category. Used to set loss weights for batches of different categories.</p></li><li><p><strong>------------------------------------------------------------------</strong> –</p></li><li><p><strong>always_forward_in_48k</strong> – whether to always upsample the input speech to 48kHz for forward, and then downsample to the original sample rate for loss calculation. NOTE: this can be useful to train a model capable of handling various sampling rates while unifying bandwidth extension + speech enhancement.</p></li></ul></li></ul><div class="custom-h4"><p>collect_feats<span class="small-bracket">(speech_mix: Tensor, speech_mix_lengths: Tensor, **kwargs)</span></p></div><div class="custom-h4"><p>enhance<span class="small-bracket">(feature_mix)</span></p></div><div class="custom-h4"><p>forward<span class="small-bracket">(speech_mix: Tensor, speech_mix_lengths: Tensor | None = None, **kwargs)</span></p></div><p>Frontend + Encoder + Decoder + Calc loss</p><ul><li><strong>Parameters:</strong><ul><li><strong>speech_mix</strong> – (Batch, samples) or (Batch, samples, channels)</li><li><strong>speech_ref1</strong> – (Batch, samples) or (Batch, samples, channels)</li><li><strong>speech_ref2</strong> – (Batch, samples) or (Batch, samples, channels)</li><li><strong>...</strong> –</li><li><strong>speech_mix_lengths</strong> – (Batch,), default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py</li><li><strong>enroll_ref1</strong> – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 1</li><li><strong>enroll_ref2</strong> – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 2</li><li><strong>...</strong> –</li><li><strong>kwargs</strong> – “utt_id” is among the input.</li></ul></li></ul><div class="custom-h4"><p>forward_loss<span class="small-bracket">(speech_ref, speech_mix, speech_lengths)</span></p></div><div class="custom-h4"><p>training <em>: bool</em></p></div>',10);function g(u,_){const n=o("RouteLink");return d(),i("div",null,[a(" _espnet2.enh.diffusion_enh.ESPnetDiffusionModel "),h,e("p",null,[r("Bases: "),l(n,{to:"/guide/espnet2/enh/ESPnetEnhancementModel.html#espnet2.enh.espnet_model.ESPnetEnhancementModel"},{default:c(()=>[f]),_:1})]),m])}const w=t(p,[["render",g],["__file","ESPnetDiffusionModel.html.vue"]]),v=JSON.parse('{"path":"/guide/espnet2/enh/ESPnetDiffusionModel.html","title":"espnet2.enh.diffusion_enh.ESPnetDiffusionModel","lang":"en-US","frontmatter":{},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":1.82,"words":546},"filePathRelative":"guide/espnet2/enh/ESPnetDiffusionModel.md","excerpt":"<!-- _espnet2.enh.diffusion_enh.ESPnetDiffusionModel -->\\n<h1>espnet2.enh.diffusion_enh.ESPnetDiffusionModel</h1>\\n<div class=\\"custom-h3\\"><p><em>class</em> espnet2.enh.diffusion_enh.ESPnetDiffusionModel<span class=\\"small-bracket\\">(encoder: <a href=\\"AbsEncoder.md#espnet2.enh.encoder.abs_encoder.AbsEncoder\\">AbsEncoder</a></span>, diffusion: <a href=\\"AbsDiffusion.md#espnet2.enh.diffusion.abs_diffusion.AbsDiffusion\\">AbsDiffusion</a>, decoder: <a href=\\"AbsDecoder.md#espnet2.enh.decoder.abs_decoder.AbsDecoder\\">AbsDecoder</a>, num_spk: int = 1, normalize: bool = False, **kwargs)</p></div>"}');export{w as comp,v as data};
